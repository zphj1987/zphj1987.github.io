<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>rbd块设备的id修改</title>
    <link href="/2025/04/10/rbd%E5%9D%97%E8%AE%BE%E5%A4%87%E7%9A%84id%E4%BF%AE%E6%94%B9/"/>
    <url>/2025/04/10/rbd%E5%9D%97%E8%AE%BE%E5%A4%87%E7%9A%84id%E4%BF%AE%E6%94%B9/</url>
    
    <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>看到有这个需求，具体碰到什么场景了不太清楚，之前做过rbd的重构的研究，既然能重构，那么修改应该是比重构还要简单一点的，我们具体看下怎么操作</p><h2 id="数据结构分析"><a href="#数据结构分析" class="headerlink" title="数据结构分析"></a>数据结构分析</h2><h3 id="rbd的元数据信息"><a href="#rbd的元数据信息" class="headerlink" title="rbd的元数据信息"></a>rbd的元数据信息</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab104 ~]<span class="hljs-comment"># rbd create testrbd --size 1T</span><br>[root@lab104 ~]<span class="hljs-comment"># rbd info testrbd</span><br>rbd image <span class="hljs-string">&#x27;testrbd&#x27;</span>:<br>    size 1 TiB <span class="hljs-keyword">in</span> 262144 objects<br>    order 22 (4 MiB objects)<br>    snapshot_count: 0<br>    <span class="hljs-built_in">id</span>: 5f0c22d39dda<br>    block_name_prefix: rbd_data.5f0c22d39dda<br>    format: 2<br>    features: layering, exclusive-lock, object-map, fast-diff, deep-flatten<br>    op_features:<br>    flags:<br>    create_timestamp: Thu Apr 10 11:01:08 2025<br>    access_timestamp: Thu Apr 10 11:01:08 2025<br>    modify_timestamp: Thu Apr 10 11:01:08 2025<br>[root@lab104 ~]<span class="hljs-comment"># rbd rm testrbd</span><br>Removing image: 100% complete...done.<br>[root@lab104 ~]<span class="hljs-comment"># rbd create testrbd --size 1T</span><br>[root@lab104 ~]<span class="hljs-comment"># rbd info testrbd</span><br>rbd image <span class="hljs-string">&#x27;testrbd&#x27;</span>:<br>    size 1 TiB <span class="hljs-keyword">in</span> 262144 objects<br>    order 22 (4 MiB objects)<br>    snapshot_count: 0<br>    <span class="hljs-built_in">id</span>: 5f57bde3f24c<br>    block_name_prefix: rbd_data.5f57bde3f24c<br>    format: 2<br>    features: layering, exclusive-lock, object-map, fast-diff, deep-flatten<br>    op_features:<br>    flags:<br>    create_timestamp: Thu Apr 10 11:07:17 2025<br>    access_timestamp: Thu Apr 10 11:07:17 2025<br>    modify_timestamp: Thu Apr 10 11:07:17 2025<br></code></pre></td></tr></table></figure><p>这个id信息就是那个block_name_prefix里面的，可以看到，同样的名称同样的大小，重建一次，这个信息就发生了改变了，说明这个信息是随机性的，并没有固定算法，这些信息是在几个地方有记录的我们把这几个地方找出来</p><h3 id="rbd相关的对象"><a href="#rbd相关的对象" class="headerlink" title="rbd相关的对象"></a>rbd相关的对象</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">rbd_id.testrbd<br>rbd_directory<br>rbd_header.5f57bde3f24c<br>rbd_object_map.5f57bde3f24c<br></code></pre></td></tr></table></figure><p>这几个是元数据的对象,其它就是数据的信息，我们假如要改id名称，需要先处理元数据</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">rbd_data.5f57bde3f24c.xxxxxxxx<br></code></pre></td></tr></table></figure><p>数据存储是这样的</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab104 ~]<span class="hljs-comment"># rados -p rbd listomapvals rbd_directory</span><br>id_5f57bde3f24c<br>value (11 bytes) :<br>00000000  07 00 00 00 74 65 73 74  72 62 64                 |....testrbd|<br>0000000b<br><br>name_testrbd<br>value (16 bytes) :<br>00000000  0c 00 00 00 35 66 35 37  62 64 65 33 66 32 34 63  |....5f57bde3f24c|<br>00000010<br></code></pre></td></tr></table></figure><p>rbd_directory通过omap老存储id和名称的对应关系</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab104 ~]<span class="hljs-comment"># rados -p rbd listomapvals rbd_header.5f57bde3f24c</span><br>access_timestamp<br>value (8 bytes) :<br>00000000  e5 35 f7 67 95 6f 07 08                           |.5.g.o..|<br>00000008<br><br>create_timestamp<br>value (8 bytes) :<br>00000000  e5 35 f7 67 95 6f 07 08                           |.5.g.o..|<br>00000008<br><br>features<br>value (8 bytes) :<br>00000000  3d 00 00 00 00 00 00 00                           |=.......|<br>00000008<br><br>modify_timestamp<br>value (8 bytes) :<br>00000000  e5 35 f7 67 95 6f 07 08                           |.5.g.o..|<br>00000008<br><br>object_prefix<br>value (25 bytes) :<br>00000000  15 00 00 00 72 62 64 5f  64 61 74 61 2e 35 66 35  |....rbd_data.5f5|<br>00000010  37 62 64 65 33 66 32 34  63                       |7bde3f24c|<br>00000019<br><br>order<br>value (1 bytes) :<br>00000000  16                                                |.|<br>00000001<br><br>size<br>value (8 bytes) :<br>00000000  00 00 00 00 00 01 00 00                           |........|<br>00000008<br><br>snap_seq<br>value (8 bytes) :<br>00000000  00 00 00 00 00 00 00 00                           |........|<br>00000008<br></code></pre></td></tr></table></figure><p>rbd_header.5f57bde3f24c 这个存储的是这个对象的rbd info看到的一些信息</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab104 ~]<span class="hljs-comment"># rados -p  rbd get  rbd_id.testrbd rbd_id.testrbd</span><br>[root@lab104 ~]<span class="hljs-comment"># cat rbd_id.testrbd</span><br>5f57bde3f24c[root@lab104 ~]<br></code></pre></td></tr></table></figure><p>rbd_id.testrbd是用实体数据存储的rbd的id信息</p><p>rbd_object_map.5f57bde3f24c这个可以忽略先，这个是支持重建的</p><h2 id="操作实践"><a href="#操作实践" class="headerlink" title="操作实践"></a>操作实践</h2><h3 id="修改-rbd-directory-的omap信息"><a href="#修改-rbd-directory-的omap信息" class="headerlink" title="修改 rbd_directory 的omap信息"></a>修改 rbd_directory 的omap信息</h3><p>我们先修改 rbd_directory 的信息</p><p>存在两组关系</p><ul><li>id_5f57bde3f24c   对应  |….testrbd|</li><li>name_testrbd     对应|….5f57bde3f24c|</li></ul><p>这里我们任意指定一个名称，这个应该是16进制的，所以注意字母不要超过f</p><p>我们改掉中间的一部分</p><blockquote><p>id_5f57bde3f24c  改成id_5f57abc3f24c</p></blockquote><p>那么新的对应关系是</p><ul><li>id_5f57abc3f24c 对应  |….testrbd|</li><li>name_testrbd  对应 |….5f57bde3f24c|</li></ul><p>那么就是一个改val 一个改key了 ，rados没有rename key val的接口。那么只能set操作</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">echo</span> -en \\x07\\x00\\x00\\x00\\x74\\x65\\x73\\x74\\x72\\x62\\x64 |rados -p rbd setomapval rbd_directory id_5f57abc3f24c<br></code></pre></td></tr></table></figure><p>这个val直接取上面查询到的val即可了，这个是没变的</p><p>我们检查效果</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab104 ~]<span class="hljs-comment"># rados -p rbd listomapvals rbd_directory</span><br>id_5f57abc3f24c<br>value (11 bytes) :<br>00000000  07 00 00 00 74 65 73 74  72 62 64                 |....testrbd|<br>0000000b<br><br>id_5f57bde3f24c<br>value (11 bytes) :<br>00000000  07 00 00 00 74 65 73 74  72 62 64                 |....testrbd|<br>0000000b<br></code></pre></td></tr></table></figure><p>可以看到完整的复刻了，那么我们可以删除老的key了</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">rados -p rbd rmomapkey rbd_directory id_5f57bde3f24<br></code></pre></td></tr></table></figure><p>再修改name里面的val的信息</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab104 ~]<span class="hljs-comment"># rados -p rbd listomapvals rbd_directory</span><br><br><br>name_testrbd<br>value (16 bytes) :<br>00000000  0c 00 00 00 35 66 35 37  62 64 65 33 66 32 34 63  |....5f57bde3f24c|<br>00000010<br></code></pre></td></tr></table></figure><p>这个里面左边的就是表示右边的内容的，这个是</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">35 66 35 37：分别是 ASCII 字符 `<span class="hljs-string">&#x27;5&#x27;</span> <span class="hljs-string">&#x27;f&#x27;</span> <span class="hljs-string">&#x27;5&#x27;</span> <span class="hljs-string">&#x27;7&#x27;</span><br></code></pre></td></tr></table></figure><p><img src="/images/blog/Pasted%20image%2020250410114044.png"></p><p>那么我们反向计算<br>5f57bde3f24c这个我们改成的是5f57abc3f24c<br>那么对应左边的值就是</p><p><img src="/images/blog/Pasted%20image%2020250410114331.png"></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">0c 00 00 00 35 66 35 37 61 62 63 33 66 32 34 63<br></code></pre></td></tr></table></figure><p>先删除之前的</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">rados -p rbd rmomapkey rbd_directory name_testrbd<br></code></pre></td></tr></table></figure><p>然后设置</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">echo</span> -en \\x0c\\x00\\x00\\x00\\x35\\x66\\x35\\x37\\x61\\x62\\x63\\x33\\x66\\x32\\x34\\x63|rados -p rbd setomapval rbd_directory name_testrbd<br></code></pre></td></tr></table></figure><p>检查</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab104 ~]<span class="hljs-comment"># rados -p rbd listomapvals rbd_directory</span><br>id_5f57abc3f24c<br>value (11 bytes) :<br>00000000  07 00 00 00 74 65 73 74  72 62 64                 |....testrbd|<br>0000000b<br><br>name_testrbd<br>value (16 bytes) :<br>00000000  0c 00 00 00 35 66 35 37  61 62 63 33 66 32 34 63  |....5f57abc3f24c|<br>00000010<br></code></pre></td></tr></table></figure><p>可以看到已经修改好了 </p><h3 id="修改rbd-id-testrbd二进制"><a href="#修改rbd-id-testrbd二进制" class="headerlink" title="修改rbd_id.testrbd二进制"></a>修改rbd_id.testrbd二进制</h3><p>再改动rbd_id.testrbd信息</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab104 ~]<span class="hljs-comment"># hexdump -C rbd_id.testrbd</span><br>00000000  0c 00 00 00 35 66 35 37  62 64 65 33 66 32 34 63  |....5f57bde3f24c|<br>00000010<br></code></pre></td></tr></table></figure><p>可以看到这是一个二进制的文件，那么我们手动编辑了</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">vim -b rbd_id.testrbd<br>文本转16进制编辑<br>:%!xxd<br></code></pre></td></tr></table></figure><p>内容</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">0000000: 0c00 0000 3566 3537 6264 6533 6632 3463  ....5f57bde3f24c<br></code></pre></td></tr></table></figure><p>我们改成</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">0000000: 0c00 0000 3566 3537 6162 6333 6632 3463  ....5f57abc3f24c<br></code></pre></td></tr></table></figure><p>上面已经计算过了，直接套用即可</p><p>16进制编辑还原成文本</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">:%!xxd -r<br></code></pre></td></tr></table></figure><p>检查</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab104 ~]<span class="hljs-comment"># hexdump -C rbd_id.testrbd</span><br>00000000  0c 00 00 00 35 66 35 37  61 62 63 33 66 32 34 63  |....5f57abc3f24c|<br>00000010<br></code></pre></td></tr></table></figure><p>没有问题，可以把对象覆盖回去了</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">rados -p rbd put rbd_id.testrbd rbd_id.testrbd<br></code></pre></td></tr></table></figure><h3 id="修改rbd-header的omap信息"><a href="#修改rbd-header的omap信息" class="headerlink" title="修改rbd_header的omap信息"></a>修改rbd_header的omap信息</h3><p>还剩最后一个了rbd_header.5f57bde3f24c</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">rados -p rbd <span class="hljs-built_in">cp</span> rbd_header.5f57bde3f24c rbd_header.5f57abc3f24c<br></code></pre></td></tr></table></figure><p>复制下对象后，检查下相关的信息是否复制过来了</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab104 ~]<span class="hljs-comment"># rados -p rbd listomapvals rbd_header.5f57abc3f24c</span><br>access_timestamp<br>value (8 bytes) :<br>00000000  e5 35 f7 67 95 6f 07 08                           |.5.g.o..|<br>00000008<br><br>create_timestamp<br>value (8 bytes) :<br>00000000  e5 35 f7 67 95 6f 07 08                           |.5.g.o..|<br>00000008<br><br>features<br>value (8 bytes) :<br>00000000  3d 00 00 00 00 00 00 00                           |=.......|<br>00000008<br><br>modify_timestamp<br>value (8 bytes) :<br>00000000  e5 35 f7 67 95 6f 07 08                           |.5.g.o..|<br>00000008<br><br>object_prefix<br>value (25 bytes) :<br>00000000  15 00 00 00 72 62 64 5f  64 61 74 61 2e 35 66 35  |....rbd_data.5f5|<br>00000010  37 62 64 65 33 66 32 34  63                       |7bde3f24c|<br>00000019<br><br>order<br>value (1 bytes) :<br>00000000  16                                                |.|<br>00000001<br><br>size<br>value (8 bytes) :<br>00000000  00 00 00 00 00 01 00 00                           |........|<br>00000008<br><br>snap_seq<br>value (8 bytes) :<br>00000000  00 00 00 00 00 00 00 00                           |........|<br>00000008<br></code></pre></td></tr></table></figure><p>这个里面只有一条信息需要修改object_prefix</p><p><img src="/images/blog/Pasted%20image%2020250410115828.png"></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">object_prefix<br>value (25 bytes) :<br>00000000  15 00 00 00 72 62 64 5f  64 61 74 61 2e 35 66 35  |....rbd_data.5f5|<br>00000010  37 62 64 65 33 66 32 34  63                       |7bde3f24c|<br>00000019<br></code></pre></td></tr></table></figure><p>可以看到这个地方的对应关系是这样转换的，我们还是反向转换一下即可</p><p><img src="/images/blog/Pasted%20image%2020250410115930.png"></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">rbd_data.5f57abc3f24c 对应为72 62 64 5f 64 61 74 61 2e 35 66 35 37 61 62 63 33 66 32 34 63<br></code></pre></td></tr></table></figure><p>我们设置下</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">rados -p rbd rmomapkey rbd_header.5f57abc3f24c  object_prefix<br><span class="hljs-built_in">echo</span> -en \\x15\\x00\\x00\\x00\\x72\\x62\\x64\\x5f\\x64\\x61\\x74\\x61\\x2e\\x35\\x66\\x35\\x37\\x61\\x62\\x63\\x33\\x66\\x32\\x34\\x63|rados -p rbd setomapval rbd_header.5f57abc3f24c object_prefix<br></code></pre></td></tr></table></figure><p>注意前面的<code>\\x15\\x00\\x00\\x00</code>前缀不要漏了</p><p>查看信息</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab104 ~]<span class="hljs-comment"># rados -p rbd listomapvals rbd_header.5f57abc3f24c</span><br>access_timestamp<br>value (8 bytes) :<br>00000000  e5 35 f7 67 95 6f 07 08                           |.5.g.o..|<br>00000008<br><br>create_timestamp<br>value (8 bytes) :<br>00000000  e5 35 f7 67 95 6f 07 08                           |.5.g.o..|<br>00000008<br><br>features<br>value (8 bytes) :<br>00000000  3d 00 00 00 00 00 00 00                           |=.......|<br>00000008<br><br>modify_timestamp<br>value (8 bytes) :<br>00000000  e5 35 f7 67 95 6f 07 08                           |.5.g.o..|<br>00000008<br><br>object_prefix<br>value (25 bytes) :<br>00000000  15 00 00 00 72 62 64 5f  64 61 74 61 2e 35 66 35  |....rbd_data.5f5|<br>00000010  37 61 62 63 33 66 32 34  63                       |7abc3f24c|<br>00000019<br></code></pre></td></tr></table></figure><p>可以看到已经改好了</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab104 ~]<span class="hljs-comment"># rbd info testrbd</span><br>rbd image <span class="hljs-string">&#x27;testrbd&#x27;</span>:<br>    size 1 TiB <span class="hljs-keyword">in</span> 262144 objects<br>    order 22 (4 MiB objects)<br>    snapshot_count: 0<br>    <span class="hljs-built_in">id</span>: 5f57abc3f24c<br>    block_name_prefix: rbd_data.5f57abc3f24c<br>    format: 2<br>    features: layering, exclusive-lock, object-map, fast-diff, deep-flatten<br>    op_features:<br>    flags:<br>    create_timestamp: Thu Apr 10 11:07:17 2025<br>    access_timestamp: Thu Apr 10 11:07:17 2025<br>    modify_timestamp: Thu Apr 10 11:07:17 2025<br></code></pre></td></tr></table></figure><p>可以看到已经可以查询到信息了，并且prefix已经改好了，我们还需要处理数据</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">rados -p rbd <span class="hljs-built_in">ls</span>|grep rbd_data.5f57bde3f24c &gt; obj.list<br></code></pre></td></tr></table></figure><p>我们需要把这些对象全部复制一份</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-keyword">for</span> obj <span class="hljs-keyword">in</span> `<span class="hljs-built_in">cat</span> obj.list`;<span class="hljs-keyword">do</span> target=`<span class="hljs-built_in">echo</span> <span class="hljs-variable">$obj</span>|sed <span class="hljs-string">&#x27;s/\(rbd_data\.\)[^.]*\(\..*\)/\15f57abc3f24c\2/&#x27;</span>`;<span class="hljs-built_in">echo</span> <span class="hljs-variable">$obj</span>;<span class="hljs-built_in">echo</span> <span class="hljs-variable">$target</span>;rados  -p rbd <span class="hljs-built_in">cp</span> <span class="hljs-variable">$obj</span> <span class="hljs-variable">$target</span>   ;<span class="hljs-keyword">done</span><br></code></pre></td></tr></table></figure><p>这个就是把数据改掉prefix后全部复制了一遍</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab104 ~]<span class="hljs-comment"># for obj in `cat obj.list`;do rados  -p rbd rm  $obj ;done</span><br></code></pre></td></tr></table></figure><p>确认没问题就可以删掉了</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab104 ~]<span class="hljs-comment"># rados -p rbd ls|grep rbd_objec</span><br>rbd_object_map.5f57bde3f24c<br></code></pre></td></tr></table></figure><h3 id="重建object-map"><a href="#重建object-map" class="headerlink" title="重建object-map"></a>重建object-map</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab104 ~]<span class="hljs-comment"># rbd  object-map rebuild testrbd</span><br>2025-04-10T12:11:16.762+0800 7f601b7fe700 -1 librbd::object_map::RefreshRequest: failed to load object map: rbd_object_map.5f57abc3f24c<br>2025-04-10T12:11:16.773+0800 7f601b7fe700 -1 librbd::object_map::InvalidateRequest: 0x7f600c00c560 invalidating object map in-memory<br>2025-04-10T12:11:16.773+0800 7f601b7fe700 -1 librbd::object_map::InvalidateRequest: 0x7f600c00c560 invalidating object map on-disk<br>2025-04-10T12:11:16.774+0800 7f601b7fe700 -1 librbd::object_map::InvalidateRequest: 0x7f600c00c560 should_complete: r=0<br>Object Map Rebuild: 100% complete...done.<br>[root@lab104 ~]<span class="hljs-comment"># rados -p rbd ls|grep rbd_objec</span><br>rbd_object_map.5f57abc3f24c<br>rbd_object_map.5f57bde3f24c<br><br>[root@lab104 ~]<span class="hljs-comment"># rados -p rbd rm rbd_object_map.5f57bde3f24c</span><br></code></pre></td></tr></table></figure><p>老的可以清理了</p><h3 id="数据访问确认"><a href="#数据访问确认" class="headerlink" title="数据访问确认"></a>数据访问确认</h3><p>到这里工作就都完成了，我们需要去确认下我们的数据是不是完整的</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab103 ~]<span class="hljs-comment"># rbd ls</span><br>testrbd<br>zp<br>[root@lab103 ~]<span class="hljs-comment"># rbd map testrbd</span><br>/dev/rbd0<br>[root@lab103 ~]<span class="hljs-comment"># mount /dev/rbd0 /mnt</span><br>[root@lab103 ~]<span class="hljs-comment"># rbd info testrbd</span><br>rbd image <span class="hljs-string">&#x27;testrbd&#x27;</span>:<br>    size 1 TiB <span class="hljs-keyword">in</span> 262144 objects<br>    order 22 (4 MiB objects)<br>    snapshot_count: 0<br>    <span class="hljs-built_in">id</span>: 5f57abc3f24c<br>    block_name_prefix: rbd_data.5f57abc3f24c<br>    format: 2<br>    features: layering, exclusive-lock, object-map, fast-diff, deep-flatten<br>    op_features:<br>    flags:<br>    create_timestamp: Thu Apr 10 11:07:17 2025<br>    access_timestamp: Thu Apr 10 11:07:17 2025<br>    modify_timestamp: Thu Apr 10 11:07:17 2025<br>[root@lab103 ~]<span class="hljs-comment"># df -h /mnt</span><br>Filesystem      Size  Used Avail Use% Mounted on<br>/dev/rbd0       1.0T  7.2G 1017G   1% /mnt<br></code></pre></td></tr></table></figure><p>可以看到可以挂载，文件系统还在</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>通过上面的一系列的操作，我们可以看到这个id是可以更改的，这个地方对元数据的控制能够让我们在面对极端故障的时候能够更好的挽回数据，比如元数据对象丢失，就需要我们去重构下这个数据了，以上操作是操作的一个比较原始使用场景的rbd，没有做快照，没有克隆，如果有用到，需要对更多信息进行修改<br>大致上说就是该两部分数据:</p><ul><li>通过echo修改omap的数据</li><li>通过vim编辑二进制对象的数据</li></ul>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>tidb数据库的恢复操作</title>
    <link href="/2025/04/09/tidb%E6%95%B0%E6%8D%AE%E5%BA%93%E7%9A%84%E6%81%A2%E5%A4%8D%E6%93%8D%E4%BD%9C/"/>
    <url>/2025/04/09/tidb%E6%95%B0%E6%8D%AE%E5%BA%93%E7%9A%84%E6%81%A2%E5%A4%8D%E6%93%8D%E4%BD%9C/</url>
    
    <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>tidb数据库是多副本的一个集群数据库，类似ceph，三节点出现两节点的时候也是无法选举，以及内部数据的leader不同，会出现无法访问的情况，本篇就是基于这个来进行恢复的实践</p><ul><li>三节点坏两个，做恢复</li><li>备份了一个节点数据，三个节点都坏了</li></ul><p>这两个场景基本一致的</p><h2 id="相关操作"><a href="#相关操作" class="headerlink" title="相关操作"></a>相关操作</h2><p>清理集群</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">tiup cluster destroy tidb-test<br></code></pre></td></tr></table></figure><p>关闭顺序 tidb  tikv  pd  </p><p>初始化集群</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">tiup cluster deploy tidb-test v5.4.0 ./topology.yaml --user root<br></code></pre></td></tr></table></figure><p>启动集群</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">tiup cluster start tidb-test<br></code></pre></td></tr></table></figure><p>设置密码</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">mysql -u root -h 192.168.0.101 -P 4000 -p<br></code></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">SET PASSWORD FOR <span class="hljs-string">&#x27;root&#x27;</span>@<span class="hljs-string">&#x27;%&#x27;</span> = <span class="hljs-string">&#x27;123456&#x27;</span>;<br></code></pre></td></tr></table></figure><h2 id="备份集群数据"><a href="#备份集群数据" class="headerlink" title="备份集群数据"></a>备份集群数据</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">/root/.tiup/storage/cluster/clusters/<br>/data/tidb-deploy/<br>/data/tidb-data/<br></code></pre></td></tr></table></figure><p>第一个是集群的拓扑信息<br>第二个数数据库的部署和启动相关的<br>第三个是数据目录<br>这三个都建议备份下</p><h2 id="模拟pd损坏"><a href="#模拟pd损坏" class="headerlink" title="模拟pd损坏"></a>模拟pd损坏</h2><p>模拟pd三节点坏两个</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">tiup cluster display tidb-test<br></code></pre></td></tr></table></figure><p>查看集群id</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 data]<span class="hljs-comment"># cat tidb-deploy/pd-2379/log/pd.log |grep &quot;init cluster id&quot;</span><br>[2025/04/09 10:16:00.265 +08:00] [INFO] [server.go:358] [<span class="hljs-string">&quot;init cluster id&quot;</span>] [cluster-id=7491131464661495325]<br></code></pre></td></tr></table></figure><p>查看pd的leader的id</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab102 data]<span class="hljs-comment"># cat tidb-deploy/pd-2379/log/pd.log |grep &quot;idAllocator&quot;</span><br>[2025/04/09 10:21:21.326 +08:00] [INFO] [id.go:122] [<span class="hljs-string">&quot;idAllocator allocates a new id&quot;</span>] [alloc-id=2000]<br></code></pre></td></tr></table></figure><p>这个id每切换一次，leader的id增加1000 注意后面设置的id比最新最大的id还大就行</p><p>全部停止</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">tiup cluster stop  tidb-test -N 192.168.0.102:2379,192.168.0.102:2379,192.168.0.101:2379<br></code></pre></td></tr></table></figure><p>当前查询到的最大id为2000</p><p>假设坏了两个，这里把数据挪走</p><p>先缩减异常的节点</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">tiup cluster scale-in tidb-test -N 192.168.0.102:2379,192.168.0.103:2379 --force<br></code></pre></td></tr></table></figure><h4 id="方法一：纯新建pd"><a href="#方法一：纯新建pd" class="headerlink" title="方法一：纯新建pd"></a>方法一：纯新建pd</h4><p>再删除当前正常的pd异常的目录</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">rm</span> -rf /data/tidb-data/pd-2379<br></code></pre></td></tr></table></figure><p>修改启动脚本，改3节点为单节点</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">/data/tidb-deploy/pd-2379/scripts/run_pd.sh<br></code></pre></td></tr></table></figure><p>再启动</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">tiup cluster start tidb-test -N 192.168.0.101:2379<br></code></pre></td></tr></table></figure><p>再修改id信息</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">tiup pd-recover -endpoints http://192.168.0.101:2379 -cluster-id 7491131464661495325 -alloc-id 6000<br></code></pre></td></tr></table></figure><h4 id="方法二-基于老数据"><a href="#方法二-基于老数据" class="headerlink" title="方法二: 基于老数据"></a>方法二: 基于老数据</h4><p>这里有两种方法,上面的方法是需要日志查询id信息的，下面这种不用<br>使用老的数据目录<br>启动脚本<code>/data/tidb-deploy/pd-2379/scripts/run_pd.sh</code>增加，</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">--force-new-cluster<br></code></pre></td></tr></table></figure><p>然后启动pd<br>然后恢复</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 data]<span class="hljs-comment"># tiup pd-recover   -from-old-member  -endpoints http://192.168.0.101:2379</span><br></code></pre></td></tr></table></figure><p>提示restart</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">tiup cluster restart tidb-test -N 192.168.0.101:2379<br></code></pre></td></tr></table></figure><h2 id="tikv的恢复"><a href="#tikv的恢复" class="headerlink" title="tikv的恢复"></a>tikv的恢复</h2><p>先全部停止</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">tiup cluster stop  tidb-test -N 192.168.0.101:20160,192.168.0.102:20160,192.168.0.103:20160<br></code></pre></td></tr></table></figure><p>强制缩容</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">tiup cluster scale-in tidb-test -N  192.168.0.102:20160,192.168.0.103:20160 --force<br></code></pre></td></tr></table></figure><p>启动单个</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">tiup cluster start tidb-test -N 192.168.0.101:20160<br></code></pre></td></tr></table></figure><p>暂停pd调度</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs bash">tiup ctl:v5.4.0 pd -u <span class="hljs-string">&quot;http://192.168.0.101:2379&quot;</span> -i<br><br>» config <span class="hljs-built_in">set</span> region-schedule-limit 0<br>Success!<br>» config <span class="hljs-built_in">set</span> replica-schedule-limit 0<br>Success!<br>» config <span class="hljs-built_in">set</span> merge-schedule-limit 0<br>Success!<br>» config <span class="hljs-built_in">set</span> hot-region-schedule-limit 0<br>Success!<br></code></pre></td></tr></table></figure><p>查询store id</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">tiup ctl:v5.4.0 pd -u <span class="hljs-string">&quot;http://192.168.0.101:2379&quot;</span>  store<br><br>id7 101 <br>id1 103<br>id2 102<br></code></pre></td></tr></table></figure><p>获取region的id</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">tiup ctl:v5.4.0 pd -u <span class="hljs-string">&quot;http://192.168.0.101:2379&quot;</span>  region|jq|grep start_key -B 1|grep -v start_key|grep <span class="hljs-built_in">id</span>|<span class="hljs-built_in">cut</span> -d <span class="hljs-string">&quot;:&quot;</span> -f 2|<span class="hljs-built_in">cut</span> -d , -f 1 &gt; region.id<br></code></pre></td></tr></table></figure><p>暂停下</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">tiup cluster stop  tidb-test -N 192.168.0.101:20160<br></code></pre></td></tr></table></figure><p>tikv修复工具<br>cp &#x2F;root&#x2F;.tiup&#x2F;components&#x2F;ctl&#x2F;v5.4.0&#x2F;tikv-ctl &#x2F;sbin&#x2F;</p><p>处理regions</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-keyword">for</span> <span class="hljs-built_in">id</span> <span class="hljs-keyword">in</span> `<span class="hljs-built_in">cat</span> region.id`;<span class="hljs-keyword">do</span> <span class="hljs-built_in">echo</span> <span class="hljs-variable">$id</span>;tikv-ctl --data-dir /data/tidb-data/tikv-20160/  unsafe-recover remove-fail-stores -s 1,2 -r <span class="hljs-variable">$id</span>;<span class="hljs-keyword">done</span>;<br></code></pre></td></tr></table></figure><p>全部处理好了以后启动</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">tiup cluster start tidb-test -N 192.168.0.101:20160<br></code></pre></td></tr></table></figure><p>查看副本数</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 tidb-data]<span class="hljs-comment"># pd-ctl -u http://192.168.0.101:2379 config show|grep max-repl</span><br>    <span class="hljs-string">&quot;max-replicas&quot;</span>: 3,<br></code></pre></td></tr></table></figure><p>设置完成以后，就可以访问了也不报region的错误了 tikv的修复完成了</p><h2 id="tidb修复"><a href="#tidb修复" class="headerlink" title="tidb修复"></a>tidb修复</h2><p>全停了</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">tiup cluster stop  tidb-test -N 192.168.0.101:4000,192.168.0.102:4000,192.168.0.103:4000<br></code></pre></td></tr></table></figure><p>强制缩容</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">tiup cluster scale-in tidb-test -N  192.168.0.102:4000,192.168.0.103:4000 --force<br></code></pre></td></tr></table></figure><p>启动</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">tiup cluster start tidb-test -N  192.168.0.101:4000<br></code></pre></td></tr></table></figure><p>这个单独启动就可以了 不存在修复问题</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>tidb整体上跟ceph的架构有点像，这种按顺序进行处理恢复即可，数据在就可以恢复</p>]]></content>
    
    
    <categories>
      
      <category>数据库</category>
      
    </categories>
    
    
    <tags>
      
      <tag>tidb</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>rgw的d3n功能配置</title>
    <link href="/2025/04/02/rgw%E7%9A%84d3n%E5%8A%9F%E8%83%BD%E9%85%8D%E7%BD%AE/"/>
    <url>/2025/04/02/rgw%E7%9A%84d3n%E5%8A%9F%E8%83%BD%E9%85%8D%E7%BD%AE/</url>
    
    <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>最近在看缓存相关，文件系统可以通过fscache加速，加速的效果就是读取的时候能够缓存，原理是在网关的地方加入一个高速缓存盘，这样在后续读取的时候，能够直接从缓存盘读取，这样能够减少与集群的交互，从而提供更大的性能，并且这个是缓存读取，所以数据安全性没有问题</p><h2 id="rgw的d3n"><a href="#rgw的d3n" class="headerlink" title="rgw的d3n"></a>rgw的d3n</h2><p>这个功能就是给rgw的网关加入了一个缓存盘，指定一个目录，然后能够缓存数据到目录</p><h3 id="版本要求"><a href="#版本要求" class="headerlink" title="版本要求"></a>版本要求</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab201 ~]<span class="hljs-comment"># ceph -v</span><br>ceph version 17.2.8 (f817ceb7f187defb1d021d6328fa833eb8e943b3) quincy (stable)<br></code></pre></td></tr></table></figure><p>至少要这个版本才可以</p><p>操作系统要centos9</p><h2 id="配置方法"><a href="#配置方法" class="headerlink" title="配置方法"></a>配置方法</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs bash">debug_rgw=20<br>[client.rgw.lab201]<br>  host = lab201<br>  rgw_frontends=<span class="hljs-string">&quot;beast port=7481&quot;</span><br>  rgw_content_length_compat = <span class="hljs-literal">true</span><br>  rgw_d3n_l1_local_datacache_enabled = <span class="hljs-literal">true</span><br>  rgw_d3n_l1_datacache_persistent_path = <span class="hljs-string">&quot;/mnt/nvme0/rgw_datacache&quot;</span><br>  rgw_d3n_l1_datacache_size = 10737418240<br></code></pre></td></tr></table></figure><p>改好了重启进程即可</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">systemctl restart ceph-radosgw@rgw.lab201<br></code></pre></td></tr></table></figure><p>有问题的时候看日志</p><h3 id="注意问题"><a href="#注意问题" class="headerlink" title="注意问题"></a>注意问题</h3><ul><li>文件需要大于4M才会缓存</li><li>注意缓存目录的权限需要给ceph权限</li></ul><p>4M的原因是:<br>D3N 当前仅缓存尾部对象，因为它们是不可变的（默认情况下它是大于 4MB 的对象的一部分）。（NGINX RGW 数据缓存和 CDN 支持缓存所有对象大小）<br>也就是文件的4M之后的部分是不可变的，默认去缓存这部分的对象</p><p>没有权限的报错信息</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs bash">2025-04-02T17:38:30.910+0800 7fee1770e640  1 D3nDataCache: create_aio_write_request fail, r=-1<br>2025-04-02T17:38:30.916+0800 7fee1770e640 10 D3nDataCache::put(): oid=16e7b827-16ac-471c-a407-ef1872bd0058.34134.1__shadow_testfile8M.2~PGlAYwIJKyDrpFDYY4WWQJ1c_E0eVWV.1_1, len=4194304<br>2025-04-02T17:38:30.916+0800 7fee1770e640 20 D3nDataCache: Before eviction _free_data_cache_size:10737418240, _outstanding_write_size:0, freed_size:0<br>2025-04-02T17:38:30.916+0800 7fee1770e640  0 ERROR: D3nCacheAioWriteRequest::create_io: open file failed, errno=13, location=<span class="hljs-string">&#x27;/mnt/nvme0/rgw_datacache/16e7b827-16ac-471c-a407-ef1872bd0058.34134.1__shadow_testfile8M.2~PGlAYwIJKyDrpFDYY4WWQJ1c_E0eVWV.1_1&#x27;</span><br>2025-04-02T17:38:30.916+0800 7fee1770e640  0 ERROR: D3nDataCache: d3n_libaio_create_write_request() prepare libaio write op r=-1<br>2025-04-02T17:38:30.916+0800 7fee1770e640  1 D3nDataCache: create_aio_write_request fail, r=-1<br></code></pre></td></tr></table></figure><h2 id="测试读取"><a href="#测试读取" class="headerlink" title="测试读取"></a>测试读取</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab201 ceph]<span class="hljs-comment"># aws   --endpoint=http://192.168.0.201:7481 s3 cp s3://testbucket/testfile8M /tmp/cccccc</span><br>download: s3://testbucket/testfile8M to ../../tmp/cccccc<br></code></pre></td></tr></table></figure><h3 id="查看日志"><a href="#查看日志" class="headerlink" title="查看日志"></a>查看日志</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs bash">2025-04-02T17:43:07.435+0800 7f860fc987c0  1 D3N datacache enabled: 1<br>2025-04-02T17:43:07.596+0800 7f860fc987c0  5 D3nDataCache: init: evicting the persistent storage directory on start<br>2025-04-02T17:50:55.169+0800 7f85416e2640 20 req 134200468731872662 0.004000050s s3:get_obj D3nDataCache::get_obj_iterate_cb(): oid=16e7b827-16ac-471c-a407-ef1872bd0058.34134.1__multipart_testfile8M.2~PGlAYwIJKyDrpFDYY4WWQJ1c_E0eVWV.1, is_head_obj=0, obj-ofs=0, read_ofs=0, len=4194304<br>2025-04-02T17:50:55.169+0800 7f85416e2640 20 req 134200468731872662 0.004000050s s3:get_obj D3nDataCache: get_obj_iterate_cb(): WRITE TO CACHE: oid=16e7b827-16ac-471c-a407-ef1872bd0058.34134.1__multipart_testfile8M.2~PGlAYwIJKyDrpFDYY4WWQJ1c_E0eVWV.1, obj-ofs=0, read_ofs=0 len=4194304<br>2025-04-02T17:50:55.169+0800 7f85416e2640 20 req 134200468731872662 0.004000050s s3:get_obj D3nDataCache::get_obj_iterate_cb(): oid=16e7b827-16ac-471c-a407-ef1872bd0058.34134.1__shadow_testfile8M.2~PGlAYwIJKyDrpFDYY4WWQJ1c_E0eVWV.1_1, is_head_obj=0, obj-ofs=4194304, read_ofs=0, len=4194304<br>2025-04-02T17:50:55.169+0800 7f85416e2640 20 req 134200468731872662 0.004000050s s3:get_obj D3nDataCache: get_obj_iterate_cb(): WRITE TO CACHE: oid=16e7b827-16ac-471c-a407-ef1872bd0058.34134.1__shadow_testfile8M.2~PGlAYwIJKyDrpFDYY4WWQJ1c_E0eVWV.1_1, obj-ofs=4194304, read_ofs=0 len=4194304<br>2025-04-02T17:50:55.183+0800 7f853eedd640 10 D3nDataCache::put(): oid=16e7b827-16ac-471c-a407-ef1872bd0058.34134.1__multipart_testfile8M.2~PGlAYwIJKyDrpFDYY4WWQJ1c_E0eVWV.1, len=4194304<br>2025-04-02T17:50:55.183+0800 7f853eedd640 20 D3nDataCache: Before eviction _free_data_cache_size:10737418240, _outstanding_write_size:0, freed_size:0<br>2025-04-02T17:50:55.196+0800 7f85446e8640 10 D3nDataCache::put(): oid=16e7b827-16ac-471c-a407-ef1872bd0058.34134.1__shadow_testfile8M.2~PGlAYwIJKyDrpFDYY4WWQJ1c_E0eVWV.1_1, len=4194304<br>2025-04-02T17:50:55.196+0800 7f85446e8640 20 D3nDataCache: Before eviction _free_data_cache_size:10737418240, _outstanding_write_size:4194304, freed_size:0<br>2025-04-02T17:50:55.212+0800 7f8461522640  5 D3nDataCache: d3n_libaio_write_completion_cb(): oid=16e7b827-16ac-471c-a407-ef1872bd0058.34134.1__multipart_testfile8M.2~PGlAYwIJKyDrpFDYY4WWQJ1c_E0eVWV.1<br>2025-04-02T17:50:55.367+0800 7f8461522640  5 D3nDataCache: d3n_libaio_write_completion_cb(): oid=16e7b827-16ac-471c-a407-ef1872bd0058.34134.1__shadow_testfile8M.2~PGlAYwIJKyDrpFDYY4WWQJ1c_E0eVWV.1_1<br>[root@lab201 ~]<span class="hljs-comment"># ll /mnt/nvme0/rgw_datacache/</span><br>total 8192<br>-rw-r--r-- 1 ceph ceph 4194304 Apr  2 17:50 16e7b827-16ac-471c-a407-ef1872bd0058.34134.1__multipart_testfile8M.2~PGlAYwIJKyDrpFDYY4WWQJ1c_E0eVWV.1<br>-rw-r--r-- 1 ceph ceph 4194304 Apr  2 17:50 16e7b827-16ac-471c-a407-ef1872bd0058.34134.1__shadow_testfile8M.2~PGlAYwIJKyDrpFDYY4WWQJ1c_E0eVWV.1_1<br></code></pre></td></tr></table></figure><p>上面的可以看到能够缓存文件到目录</p><h3 id="第二次读取"><a href="#第二次读取" class="headerlink" title="第二次读取"></a>第二次读取</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab201 ~]<span class="hljs-comment"># cat  /var/log/ceph/ceph-client.rgw.lab201.log |grep -i d3n</span><br>2025-04-02T17:52:29.311+0800 7f850de7b640 20 req 10062238110711655751 0.004000050s s3:get_obj D3nDataCache::get_obj_iterate_cb(): oid=16e7b827-16ac-471c-a407-ef1872bd0058.34134.1__multipart_testfile8M.2~PGlAYwIJKyDrpFDYY4WWQJ1c_E0eVWV.1, is_head_obj=0, obj-ofs=0, read_ofs=0, len=4194304<br>2025-04-02T17:52:29.311+0800 7f850de7b640 20 req 10062238110711655751 0.004000050s s3:get_obj D3nDataCache: get_obj_iterate_cb(): READ FROM CACHE: oid=16e7b827-16ac-471c-a407-ef1872bd0058.34134.1__multipart_testfile8M.2~PGlAYwIJKyDrpFDYY4WWQJ1c_E0eVWV.1, obj-ofs=0, read_ofs=0, len=4194304<br>2025-04-02T17:52:29.311+0800 7f850de7b640 20 req 10062238110711655751 0.004000050s s3:get_obj D3nDataCache: file_aio_read_abstract(): oid=16e7b827-16ac-471c-a407-ef1872bd0058.34134.1__multipart_testfile8M.2~PGlAYwIJKyDrpFDYY4WWQJ1c_E0eVWV.1<br>2025-04-02T17:52:29.311+0800 7f850de7b640 20 req 10062238110711655751 0.004000050s s3:get_obj D3nDataCache: async_read(): location=/mnt/nvme0/rgw_datacache//16e7b827-16ac-471c-a407-ef1872bd0058.34134.1__multipart_testfile8M.2~PGlAYwIJKyDrpFDYY4WWQJ1c_E0eVWV.1<br>2025-04-02T17:52:29.311+0800 7f850de7b640 20 req 10062238110711655751 0.004000050s s3:get_obj D3nDataCache: init_async_read(): location=/mnt/nvme0/rgw_datacache//16e7b827-16ac-471c-a407-ef1872bd0058.34134.1__multipart_testfile8M.2~PGlAYwIJKyDrpFDYY4WWQJ1c_E0eVWV.1<br>2025-04-02T17:52:29.312+0800 7f850de7b640 20 req 10062238110711655751 0.005000062s s3:get_obj D3nDataCache: async_read(): ::aio_read(), ret=0<br>2025-04-02T17:52:29.312+0800 7f850de7b640 20 req 10062238110711655751 0.005000062s s3:get_obj D3nDataCache::get_obj_iterate_cb(): oid=16e7b827-16ac-471c-a407-ef1872bd0058.34134.1__shadow_testfile8M.2~PGlAYwIJKyDrpFDYY4WWQJ1c_E0eVWV.1_1, is_head_obj=0, obj-ofs=4194304, read_ofs=0, len=4194304<br>2025-04-02T17:52:29.312+0800 7f850de7b640 20 req 10062238110711655751 0.005000062s s3:get_obj D3nDataCache: get_obj_iterate_cb(): READ FROM CACHE: oid=16e7b827-16ac-471c-a407-ef1872bd0058.34134.1__shadow_testfile8M.2~PGlAYwIJKyDrpFDYY4WWQJ1c_E0eVWV.1_1, obj-ofs=4194304, read_ofs=0, len=4194304<br>2025-04-02T17:52:29.312+0800 7f850de7b640 20 req 10062238110711655751 0.005000062s s3:get_obj D3nDataCache: file_aio_read_abstract(): oid=16e7b827-16ac-471c-a407-ef1872bd0058.34134.1__shadow_testfile8M.2~PGlAYwIJKyDrpFDYY4WWQJ1c_E0eVWV.1_1<br>2025-04-02T17:52:29.312+0800 7f850de7b640 20 req 10062238110711655751 0.005000062s s3:get_obj D3nDataCache: async_read(): location=/mnt/nvme0/rgw_datacache//16e7b827-16ac-471c-a407-ef1872bd0058.34134.1__shadow_testfile8M.2~PGlAYwIJKyDrpFDYY4WWQJ1c_E0eVWV.1_1<br></code></pre></td></tr></table></figure><p>可以看到从本地磁盘读取的</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>d3n功能非常适合作为读取的场景的使用，直接从本地盘的缓存读取，能够加速并且能够降低内部的网络消耗</p><h2 id="附加"><a href="#附加" class="headerlink" title="附加"></a>附加</h2><p>后面还有个D4N，这个功能多了一个write-back，从数据安全性角度来看，读取的不会破坏存储的当前的稳定性，带写入的，这个就需要多考虑下了，后面稳定了再看看,写缓存涉及的东西就多了，缓存什么时候下刷，什么时候清理，满了怎么处理客户端io，维持一个什么缓存数目，等等，需要调测的东西比较多</p>]]></content>
    
    
    <categories>
      
      <category>存储相关</category>
      
    </categories>
    
    
    <tags>
      
      <tag>ceph</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>kvm下的ceph主机启动io请求统计</title>
    <link href="/2025/02/19/kvm%E4%B8%8B%E7%9A%84ceph%E4%B8%BB%E6%9C%BA%E5%90%AF%E5%8A%A8io%E8%AF%B7%E6%B1%82%E7%BB%9F%E8%AE%A1/"/>
    <url>/2025/02/19/kvm%E4%B8%8B%E7%9A%84ceph%E4%B8%BB%E6%9C%BA%E5%90%AF%E5%8A%A8io%E8%AF%B7%E6%B1%82%E7%BB%9F%E8%AE%A1/</url>
    
    <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>假如一个主机存储在ceph里面，我们想统计下一次启动过程中的io读取的情况，那么可以通过下面的方法来统计<br>启动时间也可以通过在宿主机里面去查看，通过日志这边要方便一点，无需登录到虚拟机内部</p><h2 id="日志开启"><a href="#日志开启" class="headerlink" title="日志开启"></a>日志开启</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs bash"><br>[global]<br>fsid = 4064c56e-c9ad-4b19-bf74-c4e291be5920<br>mon_initial_members = lab104<br>mon_host = 192.168.19.104<br>auth_cluster_required = none<br>auth_service_required = none<br>auth_client_required = none<br><br>[client]<br>admin socket = /var/run/ceph/ceph-client/<span class="hljs-variable">$cluster</span>-<span class="hljs-variable">$type</span>.<span class="hljs-variable">$id</span>.<span class="hljs-variable">$pid</span>.asok<br>debug rbd = 20<br>debug client = 20<br>debug objectcacher = 20<br><span class="hljs-built_in">log</span> file = /var/run/ceph/ceph-client/<span class="hljs-variable">$cluster</span>-<span class="hljs-variable">$name</span>.<span class="hljs-built_in">log</span><br></code></pre></td></tr></table></figure><p>通过这个可以看到 这个是开启日志的方法</p><p>在kvm的主机上面添加上面的日志，这个写到ceph的配置文件即可</p><h2 id="日志处理"><a href="#日志处理" class="headerlink" title="日志处理"></a>日志处理</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab102 kvm]<span class="hljs-comment"># cat  ceph-client.admin.log |grep ObjectDispatch|grep read|grep ObjectDispatch|grep -v SimpleSchedule|tail -n 10</span><br>2025-02-19T14:54:10.524+0800 7f3a48873700 20 librbd::io::ObjectDispatch: 0x564ea4735e50 <span class="hljs-built_in">read</span>: rbd_data.3a5db2d07ea.0000000000000001 3833856~360448<br>2025-02-19T14:54:10.524+0800 7f3a48873700 20 librbd::io::ObjectDispatch: 0x564ea4735e50 <span class="hljs-built_in">read</span>: rbd_data.3a5db2d07ea.0000000000000002 0~155648<br>2025-02-19T14:54:10.524+0800 7f3a48873700 20 librbd::io::ObjectDispatch: 0x564ea4735e50 <span class="hljs-built_in">read</span>: rbd_data.3a5db2d07ea.0000000000000002 155648~516096<br>2025-02-19T14:54:10.524+0800 7f3a48873700 20 librbd::io::ObjectDispatch: 0x564ea4735e50 <span class="hljs-built_in">read</span>: rbd_data.3a5db2d07ea.0000000000000002 671744~516096<br>2025-02-19T14:54:10.524+0800 7f3a48873700 20 librbd::io::ObjectDispatch: 0x564ea4735e50 <span class="hljs-built_in">read</span>: rbd_data.3a5db2d07ea.0000000000000002 1187840~65536<br>2025-02-19T14:54:10.527+0800 7f3a48873700 20 librbd::io::ObjectDispatch: 0x564ea4735e50 <span class="hljs-built_in">read</span>: rbd_data.3a5db2d07ea.0000000000000002 1253376~516096<br>2025-02-19T14:54:10.528+0800 7f3a48873700 20 librbd::io::ObjectDispatch: 0x564ea4735e50 <span class="hljs-built_in">read</span>: rbd_data.3a5db2d07ea.0000000000000002 1769472~516096<br>2025-02-19T14:54:10.528+0800 7f3a48873700 20 librbd::io::ObjectDispatch: 0x564ea4735e50 <span class="hljs-built_in">read</span>: rbd_data.3a5db2d07ea.0000000000000002 2285568~516096<br></code></pre></td></tr></table></figure><p>可以看到请求的时间，请求的对象名称，请求的起点，请求的长度</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">rados的请求收到了7617次<br>[root@lab102 kvm]<span class="hljs-comment"># cat readnew.txt |wc -l</span><br>7617<br></code></pre></td></tr></table></figure><p>一次启动在ceph这边的rados请求发起了这么多次</p><p>下面是一个系统启动的时候读取的总的数据量</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab102 kvm]<span class="hljs-comment"># cat  readnew.txt |awk &#x27;&#123;print $2&#125;&#x27;|awk -F~ &#x27;&#123;sum += $2&#125; END&#123;print sum&#125;&#x27;</span><br>167396352<br></code></pre></td></tr></table></figure><p>总的数据读取大小大概在160MB&#x2F;s左右，主要也就是内核，内核大概也就100M左右，还有其它的一些数据</p><p>对象被读取的情况</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab102 kvm]<span class="hljs-comment"># cat new1.txt |sort  -n -k 2|tail -n 15</span><br>rbd_data.3a5db2d07ea.00000000000026d5 3117056<br>rbd_data.3a5db2d07ea.0000000000000019 3407872<br>rbd_data.3a5db2d07ea.0000000000000dc5 3436544<br>rbd_data.3a5db2d07ea.0000000000000080 3542528<br>rbd_data.3a5db2d07ea.0000000000000003 4194304<br>rbd_data.3a5db2d07ea.0000000000000004 4194304<br>rbd_data.3a5db2d07ea.0000000000000005 4194304<br>rbd_data.3a5db2d07ea.000000000000001a 4194304<br>rbd_data.3a5db2d07ea.000000000000001b 4194304<br>rbd_data.3a5db2d07ea.000000000000001c 4194304<br>rbd_data.3a5db2d07ea.000000000000001d 4194304<br>rbd_data.3a5db2d07ea.0000000000000000 4820992<br>rbd_data.3a5db2d07ea.0000000000000100 4821504<br>rbd_data.3a5db2d07ea.0000000000000001 6348800<br>rbd_data.3a5db2d07ea.0000000000000002 7798784<br></code></pre></td></tr></table></figure><p>可以看到有一部分对象还是比较连续的读取的，这个应该是一些操作系统大文件的，比如内核的，这个经过一层文件系统后的有一部分是连续id</p><h2 id="如何利用这些日志信息"><a href="#如何利用这些日志信息" class="headerlink" title="如何利用这些日志信息"></a>如何利用这些日志信息</h2><p>我们可以看下，在不同的并发下面，这个启动的区别，理论上是越多并发，单个的时间就会越大，开始可能比一个会好一点，会缓存一点其它主机读取的相同的数据，但是这个到了一定的程度就会成为瓶颈了</p><p>我们需要根据这个启动分析来得到一个比较合适的并发启动数目</p><h2 id="一些命令记录"><a href="#一些命令记录" class="headerlink" title="一些命令记录"></a>一些命令记录</h2><p>并发启动虚拟机</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">virsh list --all --name | xargs -P 11 -I &#123;&#125; virsh start  &#123;&#125;<br></code></pre></td></tr></table></figure><p>获取虚拟机的ip</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab102 kvm]<span class="hljs-comment"># cat check.sh</span><br><span class="hljs-keyword">for</span> vm <span class="hljs-keyword">in</span> $(virsh list --all --name); <span class="hljs-keyword">do</span>     virsh domifaddr  <span class="hljs-variable">$vm</span>; <span class="hljs-keyword">done</span>|grep ipv|awk <span class="hljs-string">&#x27;&#123;print $4&#125;&#x27;</span>|<span class="hljs-built_in">cut</span> -d / -f 1 &gt; ip.list<br></code></pre></td></tr></table></figure><p>查看启动时间</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-keyword">for</span> host <span class="hljs-keyword">in</span> `<span class="hljs-built_in">cat</span> ip.list`;<span class="hljs-keyword">do</span> ssh  root@<span class="hljs-variable">$host</span>  systemd-analyze ;<span class="hljs-keyword">done</span><br></code></pre></td></tr></table></figure><p>启动一个虚拟机的时间</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab102 kvm]<span class="hljs-comment"># ssh 192.168.122.48  systemd-analyze</span><br>Startup finished <span class="hljs-keyword">in</span> 806ms (kernel) + 1.920s (initrd) + 11.502s (userspace) = 14.229s<br></code></pre></td></tr></table></figure><p>并发启动10个虚拟机</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab102 kvm]<span class="hljs-comment"># for host in `cat ip.list`;do ssh  root@$host  systemd-analyze ;done</span><br>Startup finished <span class="hljs-keyword">in</span> 880ms (kernel) + 3.479s (initrd) + 19.364s (userspace) = 23.724s<br>Startup finished <span class="hljs-keyword">in</span> 883ms (kernel) + 3.404s (initrd) + 17.888s (userspace) = 22.176s<br>Startup finished <span class="hljs-keyword">in</span> 984ms (kernel) + 3.143s (initrd) + 19.173s (userspace) = 23.302s<br>Startup finished <span class="hljs-keyword">in</span> 862ms (kernel) + 3.512s (initrd) + 19.606s (userspace) = 23.981s<br>Startup finished <span class="hljs-keyword">in</span> 868ms (kernel) + 2.628s (initrd) + 17.788s (userspace) = 21.285s<br>Startup finished <span class="hljs-keyword">in</span> 900ms (kernel) + 2.661s (initrd) + 18.296s (userspace) = 21.859s<br>Startup finished <span class="hljs-keyword">in</span> 880ms (kernel) + 3.479s (initrd) + 19.364s (userspace) = 23.724s<br>Startup finished <span class="hljs-keyword">in</span> 870ms (kernel) + 3.442s (initrd) + 19.233s (userspace) = 23.546s<br>Startup finished <span class="hljs-keyword">in</span> 911ms (kernel) + 3.395s (initrd) + 17.976s (userspace) = 22.283s<br>Startup finished <span class="hljs-keyword">in</span> 901ms (kernel) + 3.309s (initrd) + 19.488s (userspace) = 23.699s<br>Startup finished <span class="hljs-keyword">in</span> 901ms (kernel) + 2.597s (initrd) + 18.168s (userspace) = 21.667s<br></code></pre></td></tr></table></figure><p>可以看到时间增加了7秒左右，大概是1.5倍，这个时间是11个虚拟机并发启动的情况，如果更多的情况可以继续查看</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs bash">3.97k op/s<br>3.16k op/s<br>3.75k op/s<br>3.68k op/s<br>3.10k op/s<br>3.52k op/s<br>1.52k op/s<br>1.53k op/s<br></code></pre></td></tr></table></figure><p>从ceph的日志看，可以看到启动11个时候的高峰期op大概在3k多</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>电源管理引起的nvme磁盘不识别</title>
    <link href="/2025/02/18/%E7%94%B5%E6%BA%90%E7%AE%A1%E7%90%86%E5%BC%95%E8%B5%B7%E7%9A%84nvme%E7%A3%81%E7%9B%98%E4%B8%8D%E8%AF%86%E5%88%AB/"/>
    <url>/2025/02/18/%E7%94%B5%E6%BA%90%E7%AE%A1%E7%90%86%E5%BC%95%E8%B5%B7%E7%9A%84nvme%E7%A3%81%E7%9B%98%E4%B8%8D%E8%AF%86%E5%88%AB/</url>
    
    <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>有一台arm服务器上面出现了nvme磁盘无法识别的情况，比较奇怪</p><ul><li>硬件1+ 内核1+ debian系统可以识别</li><li>硬件1+ 内核1+ openeuler系统可以识别</li><li>硬件2+ 内核2+ debian系统可以识别</li><li>硬件2+ 内核2+ openeuler系统不能识别</li></ul><p>从上面的组合来看，内核没有问题，操作系统只是软件层面的，识别的模块都在内核里面，系统也没有问题的，在组合一的时候也能正常识别</p><p>硬件2与硬件1的区别就是带的盘更多了，其它没什么变化</p><p>这里有个现象是</p><p>如果在启动好了以后，再重新加载一次pcie的nvme，又能够识别</p><h2 id="排查过程"><a href="#排查过程" class="headerlink" title="排查过程"></a>排查过程</h2><p>正常识别的有下面的驱动显示</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">root@node118:~<span class="hljs-comment"># lspci -k</span><br>0001:11:00.0 Non-Volatile memory controller: Phison Electronics Corporation Device 5018 (rev 01)<br>    Subsystem: Phison Electronics Corporation Device 5018<br>    Kernel driver <span class="hljs-keyword">in</span> use: nvme<br></code></pre></td></tr></table></figure><p>正常识别的日志</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">Feb 14 08:42:21 linaro-alip kernel: [    9.649140] nvme nvme0: pci <span class="hljs-keyword">function</span> 0001:11:00.0<br>Feb 14 08:42:21 linaro-alip kernel: [    9.659798] nvme 0001:11:00.0: enabling device (0000 -&gt; 0002)<br>Feb 14 08:42:21 linaro-alip kernel: [    9.663384] nvme nvme0: Shutdown <span class="hljs-built_in">timeout</span> <span class="hljs-built_in">set</span> to 10 seconds<br>Feb 14 08:42:21 linaro-alip kernel: [    9.671126] nvme nvme0: 8/0/0 default/read/poll queues<br>Feb 14 08:42:21 linaro-alip kernel: [    9.673336]  nvme0n1: <br></code></pre></td></tr></table></figure><p>异常的情况日志</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">Feb 17 15:31:57 openeuler kernel: [    9.657886] nvme nvme0: pci <span class="hljs-keyword">function</span> 0001:11:00.0<br>Feb 17 15:31:57 openeuler kernel: [    9.664677] nvme 0001:11:00.0: enabling device (0000 -&gt; 0002)<br>Feb 17 15:31:57 openeuler kernel: [    9.664733] nvme nvme0: Removing after probe failure status: -19<br></code></pre></td></tr></table></figure><h2 id="重新加载"><a href="#重新加载" class="headerlink" title="重新加载"></a>重新加载</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@openeuler ~]<span class="hljs-comment"># echo 1 &gt;  /sys/bus/pci/devices/0001\:11\:00.0/remove</span><br>[root@openeuler ~]<span class="hljs-comment"># echo 1 &gt;  /sys/bus/pci/rescan</span><br>[  779.433580] pci 0001:11:00.0: 15.752 Gb/s available PCIe bandwidth, limited by 8.0 GT/s PCIe x2 <span class="hljs-built_in">link</span> at 0001:10:00.0 (capable of 63.012 Gb/s with 16.0 GT/s PCIe x4 <span class="hljs-built_in">link</span>)<br>[  779.438205] pci 0001:11:00.0: BAR 0: assigned [mem 0xf1200000-0xf1203fff 64bit]<br>[  779.438562] nvme nvme0: pci <span class="hljs-keyword">function</span> 0001:11:00.0<br>[  779.442613] nvme nvme0: Shutdown <span class="hljs-built_in">timeout</span> <span class="hljs-built_in">set</span> to 10 seconds<br>[  779.450280] nvme nvme0: 8/0/0 default/read/poll queues<br>[  779.452607]  nvme0n1: p1<br></code></pre></td></tr></table></figure><p>这里可以看到设备能够识别，但是跟nvme通信的时候存在问题</p><p>搜索相关的资料发现了一个相关的问题</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">cat</span> /sys/module/pcie_aspm/parameters/policy<br>这个看到是powersave<br></code></pre></td></tr></table></figure><p>这个是pcie的aspm管理，这个是电源管理相关的，上次碰到一个三星的nvmessd 使用比较长一段时间后出现离线的问题，就是这个类似的问题<br>这次不同的情况是，在启动加载的时候就出现了无法加载的情况<br>猜测可能默认节能模式，然后机器的电流不足</p><h2 id="处理方式"><a href="#处理方式" class="headerlink" title="处理方式"></a>处理方式</h2><p>在内核启动参数里面增加</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">pcie_aspm=off<br></code></pre></td></tr></table></figure><p>检查设置的情况</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@openeuler <span class="hljs-built_in">log</span>]<span class="hljs-comment"># lspci -vv</span><br>0001:11:00.0 Non-Volatile memory controller: Phison Electronics Corporation E18 PCIe4 NVMe Controller (rev 01) (prog-if 02 [NVM Express])<br>    Subsystem: Phison Electronics Corporation E18 PCIe4 NVMe Controller<br>    Control: I/O- Mem+ BusMaster+ SpecCycle- MemWINV- VGASnoop- ParErr- Stepping- SERR- FastB2B- DisINTx+<br>    Status: Cap+ 66MHz- UDF- FastB2B- ParErr- DEVSEL=fast &gt;TAbort- &lt;TAbort- &lt;MAbort- &gt;SERR- &lt;PERR- INTx-<br>    Latency: 0<br>    Interrupt: pin A routed to IRQ 149<br>    Region 0: Memory at f1200000 (64-bit, non-prefetchable) [size=16K]<br>    Capabilities: [80] Express (v2) Endpoint, MSI 00<br>        DevCap: MaxPayload 512 bytes, PhantFunc 0, Latency L0s unlimited, L1 unlimited<br>            ExtTag+ AttnBtn- AttnInd- PwrInd- RBE+ FLReset+ SlotPowerLimit 0W<br>        DevCtl: CorrErr- NonFatalErr- FatalErr- UnsupReq-<br>            RlxdOrd+ ExtTag+ PhantFunc- AuxPwr- NoSnoop+ FLReset-<br>            MaxPayload 128 bytes, MaxReadReq 512 bytes<br>        DevSta: CorrErr- NonFatalErr- FatalErr- UnsupReq- AuxPwr- TransPend-<br>        LnkCap: Port <span class="hljs-comment">#0, Speed 16GT/s, Width x4, ASPM L1, Exit Latency L1 &lt;64us</span><br>            ClockPM- Surprise- LLActRep- BwNot- ASPMOptComp+<br>        LnkCtl: ASPM Disabled; RCB 64 bytes, Disabled- CommClk-<br>            ExtSynch- ClockPM- AutWidDis- BWInt- AutBWInt-<br></code></pre></td></tr></table></figure><p>设置成功后，通过lscpi可以检查</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">LnkCtl: ASPM Disabled<br></code></pre></td></tr></table></figure><p>这个就是关闭了</p><p>重启后验证识别了</p><h3 id="增加措施"><a href="#增加措施" class="headerlink" title="增加措施"></a>增加措施</h3><ul><li>1、内核编译选项：删除“CONFIG_PCIEASPM_POWERSAVE&#x3D;y”并设置“CONFIG_PCIEASPM_PERFORMANCE&#x3D;y”</li><li>2、内核启动参数里面设置pcie_aspm&#x3D;off</li><li>3、内核启动参数nvme_core.default_ps_max_latency_us&#x3D;0</li></ul><h3 id="新措施"><a href="#新措施" class="headerlink" title="新措施"></a>新措施</h3><p>通过rootdelay&#x3D;35 加入到grub的参数里面，原因是kernel的启动以后，设备的初始化的时间跟系统的systemd的启动时间存在重叠，通过延迟系统的启动，让设备的初始化能在独立的时间完成，之后再启动，验证效果没有问题</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>在低功耗的arm硬件下，nvme盘的电源相关的管理可能触发一些问题，内核的驱动适配问题，低版本内核并没有提供很好的驱动管理，而arm的内核一般更新没那么快，硬件上面的固件又比较新</p><p>出现问题的情况下，可以考虑关闭掉这个地方的aspm的功能</p>]]></content>
    
    
    <categories>
      
      <category>系统管理</category>
      
    </categories>
    
    
    <tags>
      
      <tag>nvme</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>vdbench测试过程可视化</title>
    <link href="/2025/02/12/vdbench%E6%B5%8B%E8%AF%95%E8%BF%87%E7%A8%8B%E5%8F%AF%E8%A7%86%E5%8C%96/"/>
    <url>/2025/02/12/vdbench%E6%B5%8B%E8%AF%95%E8%BF%87%E7%A8%8B%E5%8F%AF%E8%A7%86%E5%8C%96/</url>
    
    <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>之前做过一个版本的vdbench的可视化，当时也是用python写的，但是需要引用bootstrap，整个页面也要做一些控制，配置起来就比较麻烦了<br>现在采用新的方法，安装一些软件之后，一个脚本就可以把测试数据进行可视化，并且测试过程都可以看到数据的波动情况</p><h2 id="软件"><a href="#软件" class="headerlink" title="软件"></a>软件</h2><p>采用了dash和plotly</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">pip3 install dash  -i https://pypi.tuna.tsinghua.edu.cn/simple<br>pip3 install pandas  -i https://pypi.tuna.tsinghua.edu.cn/simple<br><br></code></pre></td></tr></table></figure><h2 id="脚本如下"><a href="#脚本如下" class="headerlink" title="脚本如下"></a>脚本如下</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br></pre></td><td class="code"><pre><code class="hljs python">[root@lab102 plotly]<span class="hljs-comment"># cat vdisplay.py</span><br><span class="hljs-comment">#! /usr/bin/python3</span><br><span class="hljs-comment"># -*- coding: UTF-8 -*-</span><br><span class="hljs-keyword">import</span> dash<br><span class="hljs-keyword">from</span> dash <span class="hljs-keyword">import</span> dcc, html<br><span class="hljs-keyword">from</span> dash.dependencies <span class="hljs-keyword">import</span> Input, Output<br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">import</span> plotly.express <span class="hljs-keyword">as</span> px<br><span class="hljs-keyword">import</span> subprocess<br><span class="hljs-keyword">import</span> threading<br><span class="hljs-keyword">import</span> time<br><span class="hljs-comment">## 配置区域</span><br>vdbench_output_dir = <span class="hljs-string">&quot;/root/vdbench/output&quot;</span><br>vdbench_bin = <span class="hljs-string">&quot;/root/vdbench/vdbench&quot;</span><br>plotdir = <span class="hljs-string">&quot;/root/plotly/output/&quot;</span><br><span class="hljs-comment">##</span><br><br>plotdata = <span class="hljs-string">&quot;%s/data.csv&quot;</span> %(plotdir)<br><span class="hljs-comment"># Vdbench 解析命令</span><br>parse_command = <span class="hljs-string">&quot;%s parse -i %s/flatfile.html -c Interval rate resp read_rate read_resp write_rate write_resp mb_read mb_write mb/sec xfersize mkdir_rate mkdir_resp -o %s 2&gt;/dev/null&quot;</span> % (vdbench_bin, vdbench_output_dir, plotdata)<br>copy_config = <span class="hljs-string">&quot;cat %s/parmfile.html &gt; %s/config.txt 2&gt;/dev/null &quot;</span> %(vdbench_output_dir,plotdir)<br>copy_summary = <span class="hljs-string">&quot;cat %s/summary.html &gt; %s/summary.txt 2&gt;/dev/null&quot;</span> %(vdbench_output_dir,plotdir)<br><br><br><span class="hljs-keyword">try</span>:<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;执行拷贝配置文件命令...&quot;</span>)<br>    subprocess.run(copy_config, shell=<span class="hljs-literal">True</span>, check=<span class="hljs-literal">True</span>)<br><span class="hljs-keyword">except</span> subprocess.CalledProcessError <span class="hljs-keyword">as</span> e:<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;执行拷贝配置文件失败: <span class="hljs-subst">&#123;e&#125;</span>&quot;</span>)<br><br><br><span class="hljs-comment"># 配置文件路径</span><br>config_file_path = <span class="hljs-string">&quot;%s/config.txt&quot;</span> %(plotdir)<br><span class="hljs-comment"># 读取配置文件内容</span><br><span class="hljs-keyword">try</span>:<br>    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(config_file_path, <span class="hljs-string">&quot;r&quot;</span>) <span class="hljs-keyword">as</span> file:<br>        lines = file.readlines()  <span class="hljs-comment"># 读取所有行</span><br>        config_content = <span class="hljs-string">&#x27;&#x27;</span>.join(lines[<span class="hljs-number">4</span>:])  <span class="hljs-comment"># 跳过前四行，将剩余内容保存到变量中</span><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;配置文件内容已成功读取并保存到变量 config_content 中。&quot;</span>)<br><span class="hljs-keyword">except</span> FileNotFoundError:<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;错误：文件 <span class="hljs-subst">&#123;config_file_path&#125;</span> 未找到。&quot;</span>)<br><span class="hljs-keyword">except</span> Exception <span class="hljs-keyword">as</span> e:<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;读取文件时发生错误：<span class="hljs-subst">&#123;e&#125;</span>&quot;</span>)<br><br><br><span class="hljs-comment"># **后台线程：周期性执行 `vdbench parse` 命令**</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">run_vdbench_periodically</span>(<span class="hljs-params">interval=<span class="hljs-number">10</span></span>):<br>    <span class="hljs-keyword">while</span> <span class="hljs-literal">True</span>:<br>        <span class="hljs-keyword">try</span>:<br>            <span class="hljs-comment">#print(&quot;执行 Vdbench 解析命令...&quot;)</span><br>            subprocess.run(parse_command, shell=<span class="hljs-literal">True</span>, check=<span class="hljs-literal">True</span>)<br>        <span class="hljs-keyword">except</span> subprocess.CalledProcessError <span class="hljs-keyword">as</span> e:<br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Vdbench 解析失败: <span class="hljs-subst">&#123;e&#125;</span>&quot;</span>)<br>        time.sleep(interval)<br><br><span class="hljs-comment"># 启动后台线程</span><br>threading.Thread(target=run_vdbench_periodically, daemon=<span class="hljs-literal">True</span>).start()<br><br><span class="hljs-comment"># 配置文件路径</span><br>summary_file_path = <span class="hljs-string">&quot;%s/summary.txt&quot;</span> %(plotdir)<br><span class="hljs-comment">#summary_content=&quot;&quot;</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">run_summary_periodically</span>(<span class="hljs-params">interval=<span class="hljs-number">10</span></span>):<br>    <span class="hljs-keyword">while</span> <span class="hljs-literal">True</span>:<br>        <span class="hljs-keyword">try</span>:<br>            <span class="hljs-comment">#print(&quot;执行 summary 拷贝命令...&quot;)</span><br>            subprocess.run(copy_summary, shell=<span class="hljs-literal">True</span>, check=<span class="hljs-literal">True</span>)<br>        <span class="hljs-keyword">except</span> subprocess.CalledProcessError <span class="hljs-keyword">as</span> e:<br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;summary 拷贝失败: <span class="hljs-subst">&#123;e&#125;</span>&quot;</span>)<br>        time.sleep(interval)<br><br><span class="hljs-comment"># 启动后台线程</span><br>threading.Thread(target=run_summary_periodically, daemon=<span class="hljs-literal">True</span>).start()<br><br><br><span class="hljs-comment"># Dash应用</span><br>app = dash.Dash(__name__)<br><br>app.layout = html.Div([<br>    html.H3(<span class="hljs-string">&quot;vdbench测试性能监控看板&quot;</span>, style=&#123;<span class="hljs-string">&#x27;textAlign&#x27;</span>: <span class="hljs-string">&#x27;center&#x27;</span>&#125;),<br>    <span class="hljs-comment"># 控制刷新按钮</span><br>    html.Button(<span class="hljs-string">&quot;暂停刷新&quot;</span>, <span class="hljs-built_in">id</span>=<span class="hljs-string">&#x27;pause-button&#x27;</span>, n_clicks=<span class="hljs-number">0</span>, style=&#123;<span class="hljs-string">&#x27;marginTop&#x27;</span>: <span class="hljs-number">20</span>&#125;),<br><br>    <span class="hljs-comment"># 图表显示</span><br>    dcc.Graph(<span class="hljs-built_in">id</span>=<span class="hljs-string">&#x27;graph-rate&#x27;</span>),<br>    dcc.Graph(<span class="hljs-built_in">id</span>=<span class="hljs-string">&#x27;graph-resp&#x27;</span>),<br>    dcc.Graph(<span class="hljs-built_in">id</span>=<span class="hljs-string">&#x27;graph-read-rate&#x27;</span>),<br>    dcc.Graph(<span class="hljs-built_in">id</span>=<span class="hljs-string">&#x27;graph-read-resp&#x27;</span>),<br>    dcc.Graph(<span class="hljs-built_in">id</span>=<span class="hljs-string">&#x27;graph-write-rate&#x27;</span>),<br>    dcc.Graph(<span class="hljs-built_in">id</span>=<span class="hljs-string">&#x27;graph-write-resp&#x27;</span>),<br>    dcc.Graph(<span class="hljs-built_in">id</span>=<span class="hljs-string">&#x27;graph-mb-read&#x27;</span>),<br>    dcc.Graph(<span class="hljs-built_in">id</span>=<span class="hljs-string">&#x27;graph-mb-write&#x27;</span>),<br>    dcc.Graph(<span class="hljs-built_in">id</span>=<span class="hljs-string">&#x27;graph-mb&#x27;</span>),<br><br>    <span class="hljs-comment"># 自动刷新（每10秒刷新一次）</span><br>    dcc.Interval(<br>        <span class="hljs-built_in">id</span>=<span class="hljs-string">&#x27;interval-update&#x27;</span>,<br>        interval=<span class="hljs-number">10000</span>,  <span class="hljs-comment"># 默认每10秒刷新一次</span><br>        n_intervals=<span class="hljs-number">0</span>,<br>        disabled=<span class="hljs-literal">False</span>  <span class="hljs-comment"># 默认刷新启用</span><br>    ),<br><br>        <span class="hljs-comment"># 显示配置文件内容</span><br>    html.H4(<span class="hljs-string">&quot;vdbench测试配置文件内容：&quot;</span>, style=&#123;<span class="hljs-string">&#x27;marginTop&#x27;</span>: <span class="hljs-number">20</span>&#125;),<br>    html.Pre(config_content, style=&#123;<br>        <span class="hljs-string">&#x27;backgroundColor&#x27;</span>: <span class="hljs-string">&#x27;#f5f5f5&#x27;</span>,<br>        <span class="hljs-string">&#x27;padding&#x27;</span>: <span class="hljs-string">&#x27;10px&#x27;</span>,<br>        <span class="hljs-string">&#x27;border&#x27;</span>: <span class="hljs-string">&#x27;1px solid #ddd&#x27;</span>,<br>        <span class="hljs-string">&#x27;whiteSpace&#x27;</span>: <span class="hljs-string">&#x27;pre-wrap&#x27;</span>,  <span class="hljs-comment"># 保留换行和空格</span><br>        <span class="hljs-string">&#x27;overflowX&#x27;</span>: <span class="hljs-string">&#x27;auto&#x27;</span>  <span class="hljs-comment"># 如果内容过长，添加水平滚动条</span><br>    &#125;),<br>    html.H4(<span class="hljs-string">&quot;vdbench测试结果：&quot;</span>, style=&#123;<span class="hljs-string">&#x27;marginTop&#x27;</span>: <span class="hljs-number">20</span>&#125;),<br>    html.Pre(<span class="hljs-built_in">id</span>=<span class="hljs-string">&#x27;summary_content&#x27;</span>, style=&#123;<br>        <span class="hljs-string">&#x27;backgroundColor&#x27;</span>: <span class="hljs-string">&#x27;#f5f5f5&#x27;</span>,<br>        <span class="hljs-string">&#x27;padding&#x27;</span>: <span class="hljs-string">&#x27;10px&#x27;</span>,<br>        <span class="hljs-string">&#x27;border&#x27;</span>: <span class="hljs-string">&#x27;1px solid #ddd&#x27;</span>,<br>        <span class="hljs-string">&#x27;whiteSpace&#x27;</span>: <span class="hljs-string">&#x27;pre-wrap&#x27;</span>,  <span class="hljs-comment"># 保留换行和空格</span><br>        <span class="hljs-string">&#x27;overflowX&#x27;</span>: <span class="hljs-string">&#x27;auto&#x27;</span>  <span class="hljs-comment"># 如果内容过长，添加水平滚动条</span><br>    &#125;),<br>    <span class="hljs-comment"># 数据来源说明</span><br>    <span class="hljs-comment">#html.Div(&quot;数据来源：lab101&quot;, style=&#123;&#x27;marginTop&#x27;: 20&#125;)</span><br>])<br><br><br><span class="hljs-comment"># 定义回调函数，定期更新 summary_content</span><br><span class="hljs-meta">@app.callback(<span class="hljs-params"></span></span><br><span class="hljs-params"><span class="hljs-meta">    Output(<span class="hljs-params"><span class="hljs-string">&#x27;summary_content&#x27;</span>, <span class="hljs-string">&#x27;children&#x27;</span></span>),</span></span><br><span class="hljs-params"><span class="hljs-meta">    Input(<span class="hljs-params"><span class="hljs-string">&#x27;interval-update&#x27;</span>, <span class="hljs-string">&#x27;n_intervals&#x27;</span></span>)</span></span><br><span class="hljs-params"><span class="hljs-meta"></span>)</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">update_summary</span>(<span class="hljs-params">n_intervals</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    每次 interval 触发时，更新 summary_content 的内容</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-keyword">try</span>:<br>        <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(summary_file_path, <span class="hljs-string">&quot;r&quot;</span>) <span class="hljs-keyword">as</span> file:<br>            lines = file.readlines()  <span class="hljs-comment"># 读取所有行</span><br>            <span class="hljs-comment"># 检查最后一行是否包含 &quot;Vdbench execution completed successfully&quot;</span><br>            <span class="hljs-keyword">if</span> lines <span class="hljs-keyword">and</span> <span class="hljs-string">&quot;Vdbench execution completed successfully&quot;</span> <span class="hljs-keyword">in</span> lines[-<span class="hljs-number">1</span>]:<br>                <span class="hljs-comment"># 读取最后四行并保存到变量中</span><br>                summary_content = <span class="hljs-string">&#x27;&#x27;</span>.join(lines[-<span class="hljs-number">4</span>:])<br>                summary_data=summary_content<br>            <span class="hljs-keyword">else</span>:<br>                summary_data = <span class="hljs-string">&quot;summary未生成&quot;</span><br><br>    <span class="hljs-keyword">except</span> FileNotFoundError:<br>        summary_data = <span class="hljs-string">&quot;summary未生成&quot;</span><br><br>    <span class="hljs-keyword">return</span> summary_data<br><br><span class="hljs-comment"># 自动刷新和手动刷新控制的回调函数</span><br><span class="hljs-meta">@app.callback(<span class="hljs-params"></span></span><br><span class="hljs-params"><span class="hljs-meta">    [</span></span><br><span class="hljs-params"><span class="hljs-meta">        Output(<span class="hljs-params"><span class="hljs-string">&#x27;graph-rate&#x27;</span>, <span class="hljs-string">&#x27;figure&#x27;</span></span>),</span></span><br><span class="hljs-params"><span class="hljs-meta">        Output(<span class="hljs-params"><span class="hljs-string">&#x27;graph-resp&#x27;</span>, <span class="hljs-string">&#x27;figure&#x27;</span></span>),</span></span><br><span class="hljs-params"><span class="hljs-meta">        Output(<span class="hljs-params"><span class="hljs-string">&#x27;graph-read-rate&#x27;</span>, <span class="hljs-string">&#x27;figure&#x27;</span></span>),</span></span><br><span class="hljs-params"><span class="hljs-meta">        Output(<span class="hljs-params"><span class="hljs-string">&#x27;graph-read-resp&#x27;</span>, <span class="hljs-string">&#x27;figure&#x27;</span></span>),</span></span><br><span class="hljs-params"><span class="hljs-meta">        Output(<span class="hljs-params"><span class="hljs-string">&#x27;graph-write-rate&#x27;</span>, <span class="hljs-string">&#x27;figure&#x27;</span></span>),</span></span><br><span class="hljs-params"><span class="hljs-meta">        Output(<span class="hljs-params"><span class="hljs-string">&#x27;graph-write-resp&#x27;</span>, <span class="hljs-string">&#x27;figure&#x27;</span></span>),</span></span><br><span class="hljs-params"><span class="hljs-meta">        Output(<span class="hljs-params"><span class="hljs-string">&#x27;graph-mb-read&#x27;</span>, <span class="hljs-string">&#x27;figure&#x27;</span></span>),</span></span><br><span class="hljs-params"><span class="hljs-meta">        Output(<span class="hljs-params"><span class="hljs-string">&#x27;graph-mb-write&#x27;</span>, <span class="hljs-string">&#x27;figure&#x27;</span></span>),</span></span><br><span class="hljs-params"><span class="hljs-meta">        Output(<span class="hljs-params"><span class="hljs-string">&#x27;graph-mb&#x27;</span>, <span class="hljs-string">&#x27;figure&#x27;</span></span>),</span></span><br><span class="hljs-params"><span class="hljs-meta">        Output(<span class="hljs-params"><span class="hljs-string">&#x27;interval-update&#x27;</span>, <span class="hljs-string">&#x27;disabled&#x27;</span></span>),  <span class="hljs-comment"># 控制 Interval 组件的 disabled 属性</span></span></span><br><span class="hljs-params"><span class="hljs-meta">    ],</span></span><br><span class="hljs-params"><span class="hljs-meta">    [Input(<span class="hljs-params"><span class="hljs-string">&#x27;interval-update&#x27;</span>, <span class="hljs-string">&#x27;n_intervals&#x27;</span></span>),</span></span><br><span class="hljs-params"><span class="hljs-meta">     Input(<span class="hljs-params"><span class="hljs-string">&#x27;pause-button&#x27;</span>, <span class="hljs-string">&#x27;n_clicks&#x27;</span></span>)]  <span class="hljs-comment"># 点击暂停按钮时触发</span></span></span><br><span class="hljs-params"><span class="hljs-meta"></span>)</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">update_graphs</span>(<span class="hljs-params">n_intervals, n_clicks</span>):<br>    <span class="hljs-comment"># 判断按钮点击次数，偶数表示继续刷新，奇数表示暂停刷新</span><br>    is_paused = n_clicks % <span class="hljs-number">2</span> == <span class="hljs-number">1</span><br><br>    <span class="hljs-keyword">try</span>:<br>        <span class="hljs-comment"># 读取最新的 CSV 数据</span><br>        df = pd.read_csv(plotdata, header=<span class="hljs-number">0</span>, skiprows=[<span class="hljs-number">1</span>])<br><br>        <span class="hljs-comment"># 创建图表时不裁剪数据，直接使用整个数据集</span><br>        fig_rate = px.line(df, x=<span class="hljs-string">&#x27;Interval&#x27;</span>, y=<span class="hljs-string">&#x27;rate&#x27;</span>, title=<span class="hljs-string">&#x27;I/O Rate随时间变化趋势&#x27;</span>,<br>                           labels=&#123;<span class="hljs-string">&#x27;rate&#x27;</span>: <span class="hljs-string">&#x27;请求速率(ops/s)&#x27;</span>, <span class="hljs-string">&#x27;Interval&#x27;</span>: <span class="hljs-string">&#x27;时间间隔&#x27;</span>&#125;, markers=<span class="hljs-literal">True</span>)<br>        fig_resp = px.line(df, x=<span class="hljs-string">&#x27;Interval&#x27;</span>, y=<span class="hljs-string">&#x27;resp&#x27;</span>, title=<span class="hljs-string">&#x27;I/O response随时间变化趋势&#x27;</span>,<br>                           labels=&#123;<span class="hljs-string">&#x27;resp&#x27;</span>: <span class="hljs-string">&#x27;响应时间(ms)&#x27;</span>, <span class="hljs-string">&#x27;Interval&#x27;</span>: <span class="hljs-string">&#x27;时间间隔&#x27;</span>&#125;, markers=<span class="hljs-literal">True</span>)<br>        fig_read_rate = px.line(df, x=<span class="hljs-string">&#x27;Interval&#x27;</span>, y=<span class="hljs-string">&#x27;read_rate&#x27;</span>, title=<span class="hljs-string">&#x27;I/O 读取请求速率随时间变化趋势&#x27;</span>,<br>                                labels=&#123;<span class="hljs-string">&#x27;read_rate&#x27;</span>: <span class="hljs-string">&#x27;读取请求速率(ops/s)&#x27;</span>, <span class="hljs-string">&#x27;Interval&#x27;</span>: <span class="hljs-string">&#x27;时间间隔&#x27;</span>&#125;, markers=<span class="hljs-literal">True</span>)<br>        fig_read_resp = px.line(df, x=<span class="hljs-string">&#x27;Interval&#x27;</span>, y=<span class="hljs-string">&#x27;read_resp&#x27;</span>, title=<span class="hljs-string">&#x27;I/O 读取response随时间变化趋势&#x27;</span>,<br>                                labels=&#123;<span class="hljs-string">&#x27;read_resp&#x27;</span>: <span class="hljs-string">&#x27;响应时间(ms)&#x27;</span>, <span class="hljs-string">&#x27;Interval&#x27;</span>: <span class="hljs-string">&#x27;时间间隔&#x27;</span>&#125;, markers=<span class="hljs-literal">True</span>)<br>        fig_write_rate = px.line(df, x=<span class="hljs-string">&#x27;Interval&#x27;</span>, y=<span class="hljs-string">&#x27;write_rate&#x27;</span>, title=<span class="hljs-string">&#x27;I/O 写入请求速率随时间变化趋势&#x27;</span>,<br>                                 labels=&#123;<span class="hljs-string">&#x27;write_rate&#x27;</span>: <span class="hljs-string">&#x27;请求速率(ops/s)&#x27;</span>, <span class="hljs-string">&#x27;Interval&#x27;</span>: <span class="hljs-string">&#x27;时间间隔&#x27;</span>&#125;, markers=<span class="hljs-literal">True</span>)<br>        fig_write_resp = px.line(df, x=<span class="hljs-string">&#x27;Interval&#x27;</span>, y=<span class="hljs-string">&#x27;write_resp&#x27;</span>, title=<span class="hljs-string">&#x27;I/O 写入response随时间变化趋势&#x27;</span>,<br>                                 labels=&#123;<span class="hljs-string">&#x27;write_resp&#x27;</span>: <span class="hljs-string">&#x27;响应时间(ms)&#x27;</span>, <span class="hljs-string">&#x27;Interval&#x27;</span>: <span class="hljs-string">&#x27;时间间隔&#x27;</span>&#125;, markers=<span class="hljs-literal">True</span>)<br>        fig_mb_read = px.line(df, x=<span class="hljs-string">&#x27;Interval&#x27;</span>, y=<span class="hljs-string">&#x27;mb_read&#x27;</span>, title=<span class="hljs-string">&#x27;I/O 读取带宽随时间变化趋势&#x27;</span>,<br>                              labels=&#123;<span class="hljs-string">&#x27;mb_read&#x27;</span>: <span class="hljs-string">&#x27;带宽(MB/s)&#x27;</span>, <span class="hljs-string">&#x27;Interval&#x27;</span>: <span class="hljs-string">&#x27;时间间隔&#x27;</span>&#125;, markers=<span class="hljs-literal">True</span>)<br>        fig_mb_write = px.line(df, x=<span class="hljs-string">&#x27;Interval&#x27;</span>, y=<span class="hljs-string">&#x27;mb_write&#x27;</span>, title=<span class="hljs-string">&#x27;I/O 写入带宽随时间变化趋势&#x27;</span>,<br>                               labels=&#123;<span class="hljs-string">&#x27;mb_write&#x27;</span>: <span class="hljs-string">&#x27;带宽(MB/s)&#x27;</span>, <span class="hljs-string">&#x27;Interval&#x27;</span>: <span class="hljs-string">&#x27;时间间隔&#x27;</span>&#125;, markers=<span class="hljs-literal">True</span>)<br>        fig_mb = px.line(df, x=<span class="hljs-string">&#x27;Interval&#x27;</span>, y=<span class="hljs-string">&#x27;mb/sec&#x27;</span>, title=<span class="hljs-string">&#x27;I/O 总带宽随时间变化趋势&#x27;</span>,<br>                         labels=&#123;<span class="hljs-string">&#x27;mb/sec&#x27;</span>: <span class="hljs-string">&#x27;带宽(MB/s)&#x27;</span>, <span class="hljs-string">&#x27;Interval&#x27;</span>: <span class="hljs-string">&#x27;时间间隔&#x27;</span>&#125;, markers=<span class="hljs-literal">True</span>)<br><br>        <span class="hljs-comment"># 获取最后 60 个数据的 x 范围</span><br>        x_range = df[<span class="hljs-string">&#x27;Interval&#x27;</span>].iloc[-<span class="hljs-number">60</span>:].values<br><br>        <span class="hljs-comment"># 统一调整图表样式</span><br>        <span class="hljs-keyword">for</span> fig <span class="hljs-keyword">in</span> [fig_rate, fig_resp, fig_read_rate, fig_read_resp, fig_write_rate, fig_write_resp, fig_mb_read, fig_mb_write, fig_mb]:<br>            fig.update_yaxes(<span class="hljs-built_in">range</span>=[<span class="hljs-number">0</span>, <span class="hljs-literal">None</span>])<br>            fig.update_layout(<br>                plot_bgcolor=<span class="hljs-string">&#x27;#f5f5f5&#x27;</span>,<br>                hovermode=<span class="hljs-string">&#x27;x unified&#x27;</span>,<br>                xaxis=<span class="hljs-built_in">dict</span>(<br>                    tickmode=<span class="hljs-string">&#x27;linear&#x27;</span>,<br>                    dtick=<span class="hljs-number">1</span>,<br>                    <span class="hljs-built_in">type</span>=<span class="hljs-string">&#x27;category&#x27;</span>,  <span class="hljs-comment"># 离散类别类型</span><br>                    rangeslider=<span class="hljs-built_in">dict</span>(visible=<span class="hljs-literal">True</span>),  <span class="hljs-comment"># 启用滑动条</span><br>                    <span class="hljs-built_in">range</span>=[x_range[<span class="hljs-number">0</span>], x_range[-<span class="hljs-number">1</span>]]  <span class="hljs-comment"># 默认显示最近 60 个数据点</span><br>                )<br>            )<br>        <span class="hljs-keyword">return</span> [fig_rate, fig_resp, fig_read_rate, fig_read_resp, fig_write_rate, fig_write_resp, fig_mb_read, fig_mb_write, fig_mb, is_paused]<br><br>    <span class="hljs-keyword">except</span> Exception <span class="hljs-keyword">as</span> e:<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;读取 CSV 失败: <span class="hljs-subst">&#123;e&#125;</span>&quot;</span>)<br>        <span class="hljs-keyword">return</span> dash.no_update<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    app.run_server(debug=<span class="hljs-literal">False</span>, host=<span class="hljs-string">&#x27;0.0.0.0&#x27;</span>, port=<span class="hljs-number">8050</span>)<br></code></pre></td></tr></table></figure><h2 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment">## 配置区域</span><br>vdbench_output_dir = <span class="hljs-string">&quot;/root/vdbench/output&quot;</span><br>vdbench_bin = <span class="hljs-string">&quot;/root/vdbench/vdbench&quot;</span><br>plotdir = <span class="hljs-string">&quot;/root/plotly/output/&quot;</span><br><span class="hljs-comment">##</span><br></code></pre></td></tr></table></figure><p>就三个配置：</p><ul><li>vdbench测试输出的文件夹名称</li><li>vdbench的二进制的路径</li><li>准备存储本次测试处理数据的地方</li></ul><h2 id="运行"><a href="#运行" class="headerlink" title="运行"></a>运行</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab102 plotly]<span class="hljs-comment"># python3 vdisplay.py</span><br>执行拷贝配置文件命令...<br>配置文件内容已成功读取并保存到变量 config_content 中。<br>Dash is running on http://0.0.0.0:8050/<br><br> * Serving Flask app <span class="hljs-string">&#x27;vdisplay&#x27;</span> (lazy loading)<br> * Environment: production<br>   WARNING: This is a development server. Do not use it <span class="hljs-keyword">in</span> a production deployment.<br>   Use a production WSGI server instead.<br> * Debug mode: off<br> * Running on all addresses.<br>   WARNING: This is a development server. Do not use it <span class="hljs-keyword">in</span> a production deployment.<br> * Running on http://192.168.19.102:8050/ (Press CTRL+C to quit)<br>17:31:18.680 ParseFlat completed successfully.<br>192.168.19.101 - - [12/Feb/2025 17:31:20] <span class="hljs-string">&quot;POST /_dash-update-component HTTP/1.1&quot;</span> 200 -<br></code></pre></td></tr></table></figure><h2 id="运行效果"><a href="#运行效果" class="headerlink" title="运行效果"></a>运行效果</h2><p><img src="/images/blog/vdisplay1.png"><br><img src="/images/blog/vdisplay2.png"></p><h2 id="后续如果有优化会记录"><a href="#后续如果有优化会记录" class="headerlink" title="后续如果有优化会记录"></a>后续如果有优化会记录</h2><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>这个方便进行一些测试过程的查看，特别是需要进行存储的调试的时候，能够在大量的数据里面找到一些过高或者过低的点</p>]]></content>
    
    
    <categories>
      
      <category>系统测试</category>
      
    </categories>
    
    
    <tags>
      
      <tag>存储</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>搭建一个本地的deepseek环境</title>
    <link href="/2025/02/08/%E6%90%AD%E5%BB%BA%E4%B8%80%E4%B8%AA%E6%9C%AC%E5%9C%B0%E7%9A%84deepseek%E7%8E%AF%E5%A2%83/"/>
    <url>/2025/02/08/%E6%90%AD%E5%BB%BA%E4%B8%80%E4%B8%AA%E6%9C%AC%E5%9C%B0%E7%9A%84deepseek%E7%8E%AF%E5%A2%83/</url>
    
    <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>最近deepseek很火，性能最好的肯定是网页版本或者官方的api的版本，但是目前的服务可用性还存在一点问题，如果对数据的隐私有一定的要求可以部署本地的版本，这个操作很简单</p><h2 id="部署方法"><a href="#部署方法" class="headerlink" title="部署方法"></a>部署方法</h2><p>下载ollama</p><blockquote><p><a href="https://ollama.com/">https://ollama.com/</a><br>下载对应的版本即可，我的是mac mini m4,24G内存，这个配置跑起来还是很流畅的<br>这个工具类似一个容器，会把模型加载起来，下载好了后，去下载对应模型即可</p></blockquote><p>下载模型</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">ollama run deepseek-r1:7b<br>ollama run deepseek-r1:14b<br></code></pre></td></tr></table></figure><p>在iterm的终端运行这两条命令会自动下载这两个模型，如果命令行使用，直接执行上面的命令即可，如果需要有图形界面使用，还要下个工具</p><p>下载chatbox</p><blockquote><p><a href="https://chatboxai.app/en">https://chatboxai.app/en</a></p></blockquote><p>下载以后就可以本地运行图形界面直接使用了</p><p>配置模型提供方选择ollama api<br>模型选择下载好的模型即可<br><img src="/images/blog/chatbox.png"></p><p>7b的模型输出速度快很多<br>14b的模型稍微慢点，但是也是很快的，用起来体验感也可以</p><h2 id="使用效果"><a href="#使用效果" class="headerlink" title="使用效果"></a>使用效果</h2><p><img src="/images/blog/deepseekoutput.png"></p><p>里面的有个推理的过程还是很好的</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul><li>能用网页版本可以用网页版本，网页版本还支持联网</li><li>如果对本地的数据隐私有要求的，可以部署本地版本，大部分场景还是可以覆盖的</li></ul>]]></content>
    
    
    <categories>
      
      <category>人工智能</category>
      
    </categories>
    
    
    <tags>
      
      <tag>deepseek</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>小电视屏幕拉伸问题解决</title>
    <link href="/2024/12/31/%E5%B0%8F%E7%94%B5%E8%A7%86%E5%B1%8F%E5%B9%95%E6%8B%89%E4%BC%B8%E9%97%AE%E9%A2%98%E8%A7%A3%E5%86%B3/"/>
    <url>/2024/12/31/%E5%B0%8F%E7%94%B5%E8%A7%86%E5%B1%8F%E5%B9%95%E6%8B%89%E4%BC%B8%E9%97%AE%E9%A2%98%E8%A7%A3%E5%86%B3/</url>
    
    <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>最近在看3D打印的东西，看到一个模型不错就拿来打了</p><p><a href="https://makerworld.com.cn/zh/models/655031#profileId-597147">Mōcintosh 摸鱼小副屏|IPS面板|2.8寸|640x480|</a><br>这个是任工坊的模型，可以直接从makerworld里面下载</p><p>然后根据作者提供的链接去购买了一个2.8寸的屏幕，这个看评论里面说，macos下会出现屏幕被拉伸的问题</p><p>正好正在用的就是macos，本着遇到问题解决问题的想法，来尝试解决一下</p><h2 id="问题现象"><a href="#问题现象" class="headerlink" title="问题现象"></a>问题现象</h2><p><img src="/images/blog/20241231095252.png"></p><p>默认的分辨率为800x600</p><p><img src="/images/blog/20241231101014.png"></p><p><img src="/images/blog/20241231100856.png"></p><p>可以看到，这个时候，我们把屏幕横着摆放，屏幕内容是水平的，系统识别是横屏的，这个地方方向与系统分辨率是对不上的</p><p><img src="/images/blog/20241231101320.png"></p><p>我们操作旋转270度，操作系统显示竖屏的<br><img src="/images/blog/20241231101408.png"></p><p>这里又实际是横屏的，也就是分辨率正好方向错开了，也就出现了拉伸的情况</p><p>这个问题应该是macos识别屏幕的分辨率的方向的内部文件没有匹配上，这个地方我们需要做的就是给屏幕设置一个正确的分辨率，也就是跟系统识别到的分辨率进行一个纵向和横向的交换即可</p><h2 id="关闭SIP"><a href="#关闭SIP" class="headerlink" title="关闭SIP"></a>关闭SIP</h2><p>查看当前的SIP状态</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">zphj1987@zphj1987Macmini ~ % csrutil status<br>System Integrity Protection status: disabled.<br></code></pre></td></tr></table></figure><p>这个SIP就是系统的一个保护机制，一些操作需要关闭这个功能</p><p>就是进入Recovery模式后</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">csrutil <span class="hljs-built_in">disable</span><br></code></pre></td></tr></table></figure><p>关闭即可</p><h2 id="SwitchResX-自定义分辨率"><a href="#SwitchResX-自定义分辨率" class="headerlink" title="SwitchResX 自定义分辨率"></a>SwitchResX 自定义分辨率</h2><p>下载SwitchResX这个软件</p><p>安装</p><p><img src="/images/blog/20241231101637.png"></p><h3 id="开始配置"><a href="#开始配置" class="headerlink" title="开始配置"></a>开始配置</h3><p><img src="/images/blog/20241231143916.png"></p><p>这个设置默认的状态，注意不要旋转，之前因为提前设置了旋转，导致后面设置不成功，新增分辨率需要在默认的状态下操作</p><p><img src="/images/blog/20241231143955.png"></p><p>屏幕横过来<br>现在的分辨率是800x480  66hz<br>还可以设置960x540   60hz    记住这两个 </p><p>查看800x480的配置</p><p><img src="/images/blog/20241231144405.png"></p><p>查看960x540的配置，并且记录下来</p><p><img src="/images/blog/20241231144303.png"></p><p>把这两个数值交换</p><p>交换<br><img src="/images/blog/20241231144708.png"></p><p>添加了下面两个配置</p><p><img src="/images/blog/20241231144739.png"></p><p>关闭窗口保存配置<br>关闭系统配置窗口，然后重新再打开</p><p><img src="/images/blog/20241231144852.png"></p><p>可以看到这两个配置已经active了</p><p><img src="/images/blog/20241231145040.png"></p><p>可以看到自定义的配置已经加进去了<br><img src="/images/blog/20241231145129.png"></p><p>可以看到系统分辨率已经有了跟其它默认的一些配置大小相反的配置了，我们直接在系统设置里面设置并设置方向</p><p><img src="/images/blog/20241231145257.png"></p><p>测试可以看到960x540的这个分辨率可以全屏</p><p><img src="/images/blog/20241231145641.png"><br><img src="/images/blog/20241231145712.png"><br><img src="/images/blog/20241231145632.png"></p><h2 id="备注"><a href="#备注" class="headerlink" title="备注"></a>备注</h2><p>如果出现分辨率不对的情况，先把270度还原，然后再打开设置页面，分辨率出现后，再调整，应该是系统转动270度后，分辨率的配置不适配，需要先设置默认，再调整即可</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>问题的原因是系统识别错误，横竖弄反了，那么可以通过工具自定义分辨率的方式，把这个交换过来</p><p>重点注意需要在默认状态下设置<br>还有最好直接延用当前可以显示的，然后添加的时候会带入参数，然后修改横向纵向分辨率即可</p>]]></content>
    
    
    <categories>
      
      <category>3D打印</category>
      
    </categories>
    
    
    <tags>
      
      <tag>问题处理</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>清理macos下的citrix残留</title>
    <link href="/2024/12/20/%E6%B8%85%E7%90%86macos%E4%B8%8B%E7%9A%84citrix%E6%AE%8B%E7%95%99/"/>
    <url>/2024/12/20/%E6%B8%85%E7%90%86macos%E4%B8%8B%E7%9A%84citrix%E6%AE%8B%E7%95%99/</url>
    
    <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>这个问题困扰了很久，之前在macbook pro m1 上面就有这个不停的弹窗的问题，把系统改成宽松才绕过了，但是一直没解决</p><p>最近把系统整个复制到mac mini m4上面，这个问题一直存在</p><p>这个是很久前安装的citrix的一个远程控制系统的一个残留，一直不知道怎么解决</p><p>关键词是</p><ul><li>ReceiverHelper 将对您的电脑造成伤害</li><li>ServiceRecords 将对您的电脑造成伤害</li></ul><p><img src="/images/blog/citrixlaji.png"></p><p>处理完成后，安全策略就可以根据自己的需要进行修改了，也不会一直弹窗了</p><h2 id="处理方法"><a href="#处理方法" class="headerlink" title="处理方法"></a>处理方法</h2><p>这个是citrix workspace helper and citrix service records残留服务<br>位置在 </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">/usr/local/libexec/ReceiverHelper.app/<br>/usr/local/libexec/ServiceRecords.app/<br></code></pre></td></tr></table></figure><p>先要禁用这几个服务</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">launchctl remove com.citrix.ServiceRecords<br>launchctl remove com.citrix.ReceiverHelper<br>launchctl remove com.citrix.AuthManager_Mac<br></code></pre></td></tr></table></figure><p>然后执行 </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">rm</span> -rf /usr/local/libexec/ReceiverHelper.app/<br><span class="hljs-built_in">rm</span> -rf /usr/local/libexec/ServiceRecords.app/<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>系统管理</category>
      
    </categories>
    
    
    <tags>
      
      <tag>macos</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>tgt的chap双向认证认证逻辑</title>
    <link href="/2024/12/20/tgt%E7%9A%84chap%E5%8F%8C%E5%90%91%E8%AE%A4%E8%AF%81%E8%AE%A4%E8%AF%81%E9%80%BB%E8%BE%91/"/>
    <url>/2024/12/20/tgt%E7%9A%84chap%E5%8F%8C%E5%90%91%E8%AE%A4%E8%AF%81%E8%AE%A4%E8%AF%81%E9%80%BB%E8%BE%91/</url>
    
    <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>iscsi的单向认证和双向认证的验证</p><h2 id="验证"><a href="#验证" class="headerlink" title="验证"></a>验证</h2><h3 id="单向认证"><a href="#单向认证" class="headerlink" title="单向认证"></a>单向认证</h3><p>开启单向认证</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs bash">&lt;target iqn.2008-09.com.example:server.target1&gt;<br>    backing-store /dev/rbd0<br>    incominguser zp 123456<br>&lt;/target&gt;<br><br>[root@lab102 ~]<span class="hljs-comment"># iscsiadm -m discovery -t sendtargets -p 192.168.0.101</span><br>192.168.0.101:3260,1 iqn.2008-09.com.example:server.target1<br></code></pre></td></tr></table></figure><p>可以发现</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab102 ~]<span class="hljs-comment"># iscsiadm -m node -T iqn.2008-09.com.example:server.target1 -l</span><br>Logging <span class="hljs-keyword">in</span> to [iface: default, target: iqn.2008-09.com.example:server.target1, portal: 192.168.0.101,3260] (multiple)<br>iscsiadm: Could not login to [iface: default, target: iqn.2008-09.com.example:server.target1, portal: 192.168.0.101,3260].<br>iscsiadm: initiator reported error (24 - iSCSI login failed due to authorization failure)<br>iscsiadm: Could not <span class="hljs-built_in">log</span> into all portals<br></code></pre></td></tr></table></figure><p>但是会提示不允许连接 认证错误</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab102 ~]<span class="hljs-comment"># vim /etc/iscsi/iscsid.conf</span><br><br><span class="hljs-comment"># To set a CHAP username and password for initiator</span><br><span class="hljs-comment"># authentication by the target(s), uncomment the following lines:</span><br>node.session.auth.username = zp<br>node.session.auth.password = 123456<br></code></pre></td></tr></table></figure><p>要删除之前的连接</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab102 ~]<span class="hljs-comment"># iscsiadm -m discovery -p 192.168.0.101 -o delete</span><br>[root@lab102 ~]<span class="hljs-comment"># iscsiadm -m discovery -p 192.168.0.101 -o delete</span><br>iscsiadm: Discovery record [192.168.0.101,3260] not found!<br><br>[root@lab102 ~]<span class="hljs-comment"># iscsiadm -m node -T iqn.2008-09.com.example:server.target1 -l</span><br>Logging <span class="hljs-keyword">in</span> to [iface: default, target: iqn.2008-09.com.example:server.target1, portal: 192.168.0.101,3260] (multiple)<br>Login to [iface: default, target: iqn.2008-09.com.example:server.target1, portal: 192.168.0.101,3260] successful.<br></code></pre></td></tr></table></figure><p>连接成功</p><p>单向认证的就跑通了 </p><h3 id="双向认证的"><a href="#双向认证的" class="headerlink" title="双向认证的"></a>双向认证的</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">&lt;target iqn.2008-09.com.example:server.target1&gt;<br>    backing-store /dev/rbd0<br>    incominguser zp 123456<br>    outgoinguser admin 12345678<br>&lt;/target&gt;<br></code></pre></td></tr></table></figure><p>客户端配置</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs bash">/etc/iscsi/iscsid.conf<br><span class="hljs-comment"># To set a CHAP username and password for initiator</span><br><span class="hljs-comment"># authentication by the target(s), uncomment the following lines:</span><br>node.session.auth.username = zp<br>node.session.auth.password = 123456<br><br><span class="hljs-comment"># To set a CHAP username and password for target(s)</span><br><span class="hljs-comment"># authentication by the initiator, uncomment the following lines:</span><br><span class="hljs-comment">#node.session.auth.username_in = username_in</span><br><span class="hljs-comment">#node.session.auth.password_in = password_in</span><br></code></pre></td></tr></table></figure><p>如果没配置的话，就不校验</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs bash">int iscsi_setup_authentication(struct iscsi_session *session,<br>                               struct iscsi_auth_config *auth_cfg)<br>&#123;<br>        /* <span class="hljs-keyword">if</span> we have any incoming credentials, we insist on authenticating<br>         * the target or not logging <span class="hljs-keyword">in</span> at all<br>         */<br>        <span class="hljs-keyword">if</span> (auth_cfg-&gt;username_in[0] || auth_cfg-&gt;password_in_length) &#123;<br>                /* sanity check the config */<br>                <span class="hljs-keyword">if</span> (auth_cfg-&gt;password_length == 0) &#123;<br>                        log_warning(<span class="hljs-string">&quot;CHAP configuration has incoming &quot;</span><br>                                    <span class="hljs-string">&quot;authentication credentials but has no &quot;</span><br>                                    <span class="hljs-string">&quot;outgoing credentials configured.&quot;</span>);<br>                        <span class="hljs-built_in">return</span> EINVAL;<br>                &#125;<br>                session-&gt;bidirectional_auth = 1;<br>        &#125; <span class="hljs-keyword">else</span> &#123;<br>                /* no or 1-way authentication */<br>                session-&gt;bidirectional_auth = 0;<br>        &#125;<br><br></code></pre></td></tr></table></figure><p>iscsi-initiator-utils 也就是open-iscsi<br>上面的就是这段代码里面的</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[open-iscsi-2.0.874]<span class="hljs-comment"># vim usr/initiator_common.c </span><br></code></pre></td></tr></table></figure><p>代码里面写了，如果我们有配置这个incoming的密码相关的，我们就开启双向认证，否则就是单向认证，这个逻辑没问题</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>认证里面注意下，如果客户端的对服务的认证未配置的话，就不做双向认证的</p>]]></content>
    
    
    <categories>
      
      <category>系统管理</category>
      
    </categories>
    
    
    <tags>
      
      <tag>iscsi相关</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>centos7过期源添加</title>
    <link href="/2024/12/18/centos7%E8%BF%87%E6%9C%9F%E6%BA%90%E6%B7%BB%E5%8A%A0/"/>
    <url>/2024/12/18/centos7%E8%BF%87%E6%9C%9F%E6%BA%90%E6%B7%BB%E5%8A%A0/</url>
    
    <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>centos7的支持已经没有了，源也移动到其它的路径，还是有环境需要用</p><h2 id="源内容"><a href="#源内容" class="headerlink" title="源内容"></a>源内容</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 yum.repos.d]<span class="hljs-comment"># cat CentOS-Base.repo</span><br>[base]<br>name=CentOS-<span class="hljs-variable">$releasever</span> - Base<br>baseurl=https://mirrors.aliyun.com/centos-vault/7.9.2009/os/x86_64<br>gpgcheck=0<br><br>[updates]<br>name=CentOS-<span class="hljs-variable">$releasever</span> - Updates<br>baseurl=https://mirrors.aliyun.com/centos-vault/7.9.2009/updates/x86_64/<br>gpgcheck=0<br><br><span class="hljs-comment">#additional packages that may be useful</span><br>[extras]<br>name=CentOS-<span class="hljs-variable">$releasever</span> - Extras<br>baseurl=https://mirrors.aliyun.com/centos-vault/7.9.2009/extras/x86_64/<br>gpgcheck=0<br><br><span class="hljs-comment">#additional packages that extend functionality of existing packages</span><br>[centosplus]<br>name=CentOS-<span class="hljs-variable">$releasever</span> - Plus<br>baseurl=https://mirrors.aliyun.com/centos-vault/7.9.2009/extras/x86_64/<br>gpgcheck=1<br>enabled=0<br>gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7<br><br>[epel]<br>name=epel<br>baseurl=https://mirrors.aliyun.com/epel-archive/7/x86_64<br>gpgcheck=0<br>enabled=1<br></code></pre></td></tr></table></figure><p>替换成上面的即可</p><p>ceph的老版本源</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs bash">[ceph]<br>name=ceph<br>baseurl=https://mirrors.tuna.tsinghua.edu.cn/ceph/rpm-15.2.17/el7/x86_64/<br>gpgcheck=0<br>enabled=1<br><br>[ceph-noarch]<br>name=ceph-noarch<br>baseurl=https://mirrors.tuna.tsinghua.edu.cn/ceph/rpm-15.2.17/el7/noarch/<br>gpgcheck=0<br>enabled=1<br></code></pre></td></tr></table></figure><p>下载deploy</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">wget https://mirrors.aliyun.com/ceph/rpm-luminous/el7/noarch/ceph-deploy-2.0.1-0.noarch.rpm<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>系统管理</category>
      
    </categories>
    
    
    <tags>
      
      <tag>操作系统</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>nvme电源控制引起的nvme磁盘离线</title>
    <link href="/2024/12/17/nvme%E8%83%BD%E6%BA%90%E6%8E%A7%E5%88%B6%E5%BC%95%E8%B5%B7%E7%9A%84nvme%E7%A3%81%E7%9B%98%E7%A6%BB%E7%BA%BF/"/>
    <url>/2024/12/17/nvme%E8%83%BD%E6%BA%90%E6%8E%A7%E5%88%B6%E5%BC%95%E8%B5%B7%E7%9A%84nvme%E7%A3%81%E7%9B%98%E7%A6%BB%E7%BA%BF/</url>
    
    <content type="html"><![CDATA[<h2 id="故障信息"><a href="#故障信息" class="headerlink" title="故障信息"></a>故障信息</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs bash">Nov 26 07:20:12 node03 kernel: nvme nvme0: I/O 24 QID 0 <span class="hljs-built_in">timeout</span>, reset controller<br>Nov 26 07:20:22 node03 kernel: nvme nvme0: I/O 146 QID 4 <span class="hljs-built_in">timeout</span>, aborting<br>Nov 26 07:20:25 node03 kernel: nvme nvme0: I/O 719 QID 1 <span class="hljs-built_in">timeout</span>, reset controller<br>Nov 26 07:21:37 node03 kernel: nvme nvme0: Device not ready; aborting reset<br>Nov 26 07:21:37 node03 kernel: nvme nvme0: Abort status: 0x7<br>Nov 26 07:21:37 node03 kernel: nvme nvme0: Abort status: 0x7<br>Nov 26 07:21:57 node03 kernel: nvme nvme0: Device not ready; aborting reset<br>Nov 26 07:21:57 node03 kernel: nvme nvme0: Removing after probe failure status: -19<br>Nov 26 07:21:57 node03 kernel: nvme0n1: detected capacity change from 4000787030016 to 0<br>Nov 26 07:21:59 node03 kernel: nvme nvme0: failed to <span class="hljs-built_in">set</span> APST feature (-19)<br></code></pre></td></tr></table></figure><p>故障信息就是这个磁盘离线了，最后面有个信息是  APST feature 无法设置</p><h2 id="相关资料"><a href="#相关资料" class="headerlink" title="相关资料"></a>相关资料</h2><p>某些 NVMe 设备可能会出现与省电 (APST) 相关的问题。这是 Kingston A2000 [8]（自固件S5Z42105起）的已知问题，之前已在 Samsung NVMe 驱动器（Linux v4.10）[9] [10]上报告过。某些 WesternDigital&#x2F;Sandisk 设备也报告过此问题[11]。<br><a href="https://bugs.launchpad.net/ubuntu/+source/linux/+bug/1678184">https://bugs.launchpad.net/ubuntu/+source/linux/+bug/1678184</a><br><a href="https://askubuntu.com/questions/905710/ext4-fs-error-after-ubuntu-17-04-upgrade/906105#906105">https://askubuntu.com/questions/905710/ext4-fs-error-after-ubuntu-17-04-upgrade/906105#906105</a><br>相关的bug<br>自 2021 年 3 月起，金士顿推出了固件更新9。由于金士顿仅支持 Windows，因此可以通过heise.de或github找到 Linux 的下载。预计只要内核解决方法到位，固件更新就不会有太大作用，因为无论如何都不会达到最深的省电状态。</p><p>参考这个做处理<br><a href="https://wiki.archlinux.org/title/Solid_state_drive/NVMe#Troubleshooting">https://wiki.archlinux.org/title/Solid_state_drive/NVMe#Troubleshooting</a></p><p>重点信息</p><p><img src="/images/blog/nvmeerror.png"></p><p>这个就是有几个触发条件</p><ul><li>三星nvme ssd （其它可能也会，这个确实是出现了）</li><li>4.x的内核</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@node27 ~]<span class="hljs-comment"># uname -a</span><br>Linux node27 4.14.113-1.el7.x86_64 <span class="hljs-comment">#1</span><br></code></pre></td></tr></table></figure><p>三星990 pro的nvme ssd</p><p>这两个条件都具备，并且显示的信息一致</p><h2 id="处理方法"><a href="#处理方法" class="headerlink" title="处理方法"></a>处理方法</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs bash">vim drivers/nvme/host/core.c<br>static int nvme_configure_apst(struct nvme_ctrl *ctrl)<br>&#123;<br>        /*<br>         * APST (Autonomous Power State Transition) lets us program a<br>         * table of power state transitions that the controller will<br>         * perform automatically.  We configure it with a simple<br>         * heuristic: we are willing to spend at most 2% of the time<br>         * transitioning between power states.  Therefore, when running<br>         * <span class="hljs-keyword">in</span> any given state, we will enter the next lower-power<br>         * non-operational state after waiting 50 * (enlat + exlat)<br>         * microseconds, as long as that state<span class="hljs-string">&#x27;s exit latency is under</span><br><span class="hljs-string">         * the requested maximum latency.</span><br><span class="hljs-string">         *</span><br><span class="hljs-string">         * We will not autonomously enter any non-operational state for</span><br><span class="hljs-string">         * which the total latency exceeds ps_max_latency_us.  Users</span><br><span class="hljs-string">         * can set ps_max_latency_us to zero to turn off APST.</span><br><span class="hljs-string">         */</span><br></code></pre></td></tr></table></figure><p>可以看到可以通过设置ps_max_latency_us 为0 来关闭APST</p><p>关闭这个电源控制的地方</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@node65 ~]<span class="hljs-comment"># cat  /sys/module/nvme_core/parameters/default_ps_max_latency_us</span><br>100000<br></code></pre></td></tr></table></figure><p>默认是这个值，我们需要改成0</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@node65 ~]<span class="hljs-comment"># nvme get-feature -f 0x0c -H /dev/nvme0n1|head -n 20</span><br>get-feature:0xc (Autonomous Power State Transition), Current value:0x000001<br>    Autonomous Power State Transition Enable (APSTE): Enabled<br>    Auto PST Entries    .................<br>    Entry[ 0]<br>    .................<br>    Idle Time Prior to Transition (ITPT): 200 ms<br>    Idle Transition Power State   (ITPS): 3<br>    .................<br>    Entry[ 1]<br>    .................<br>    Idle Time Prior to Transition (ITPT): 200 ms<br>    Idle Transition Power State   (ITPS): 3<br>    .................<br>    Entry[ 2]<br>    .................<br>    Idle Time Prior to Transition (ITPT): 200 ms<br>    Idle Transition Power State   (ITPS): 3<br>    .................<br>    Entry[ 3]<br>    .................<br></code></pre></td></tr></table></figure><p>这个也可以查询到</p><p>调整grub文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">linuxefi /vmlinuz-4.14.113-1.el7.x86_64 root=/dev/mapper/centos-root ro crashkernel=auto rd.lvm.lv=centos/root rd.lvm.lv=centos/swap rhgb quiet LANG=en_US.UTF-8 nvme_core.default_ps_max_latency_us=0<br></code></pre></td></tr></table></figure><p>nvme_core.default_ps_max_latency_us&#x3D;0<br>调整为0后重启</p><h3 id="检查"><a href="#检查" class="headerlink" title="检查"></a>检查</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@node27 ~]<span class="hljs-comment"># cat /proc/cmdline</span><br>BOOT_IMAGE=/vmlinuz-4.14.113-1.el7.x86_64 root=/dev/mapper/centos-root ro crashkernel=auto rd.lvm.lv=centos/root rd.lvm.lv=centos/swap rhgb quiet LANG=en_US.UTF-8 nvme_core.default_ps_max_latency_us=0<br></code></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@node27 ~]<span class="hljs-comment"># cat  /sys/module/nvme_core/parameters/default_ps_max_latency_us</span><br>0<br></code></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@node27 ~]<span class="hljs-comment"># nvme get-feature -f 0x0c -H /dev/nvme0n1|head -n 20</span><br>get-feature:0xc (Autonomous Power State Transition), Current value:00000000<br>    Autonomous Power State Transition Enable (APSTE): Disabled<br>    Auto PST Entries    .................<br>    Entry[ 0]<br>    .................<br>    Idle Time Prior to Transition (ITPT): 0 ms<br>    Idle Transition Power State   (ITPS): 0<br>    .................<br>    Entry[ 1]<br>    .................<br>    Idle Time Prior to Transition (ITPT): 0 ms<br>    Idle Transition Power State   (ITPS): 0<br>    .................<br></code></pre></td></tr></table></figure><p>上面的任意一种方式查询都可以</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>就是固件的电源管理这块，跟内核的功能不匹配，固件也不知道什么版本能解决，这个地方直接在内核里面禁用掉这个功能即可，网上可以搜到很多相关的问题，处理方式这样是最简单的 </p>]]></content>
    
    
    <categories>
      
      <category>系统管理</category>
      
    </categories>
    
    
    <tags>
      
      <tag>磁盘相关</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>esxi克隆虚拟机方法</title>
    <link href="/2024/12/16/esxi%E5%85%8B%E9%9A%86%E8%99%9A%E6%8B%9F%E6%9C%BA%E6%96%B9%E6%B3%95/"/>
    <url>/2024/12/16/esxi%E5%85%8B%E9%9A%86%E8%99%9A%E6%8B%9F%E6%9C%BA%E6%96%B9%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>内网搭建了一套esxi做测试的机器，没有用vcenter，管理平台没有克隆的操作的地方</p><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><p>最开始使用的是平台的存储浏览的复制功能<br>这个里面有个问题是，复制的很慢，并且精简配置的属性没有保留，占用了过多的空间<br>这个地方可以后台通过命令行操作，也比较简单</p><h3 id="后台操作"><a href="#后台操作" class="headerlink" title="后台操作"></a>后台操作</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@nucesxi:/vmfs/volumes/6730898e-54870331-6a0a-1c697a6078e1] <span class="hljs-built_in">ls</span> -ahl centos7模板机器/<br>total 2051720<br>drwxr-xr-x    1 root     root       76.0K Dec 16 03:26 .<br>drwxr-xr-t    1 root     root       76.0K Dec 16 01:55 ..<br>-rw-r--r--    1 root     root        7.2K Dec 16 03:21 centos7模板机器-1.scoreboard<br>-rw-r--r--    1 root     root        7.2K Dec 16 03:24 centos7模板机器-2.scoreboard<br>-rw-r--r--    1 root     root        6.9K Dec 16 03:26 centos7模板机器-3.scoreboard<br>-rw-------    1 root     root       80.0G Dec 16 03:24 centos7模板机器-flat.vmdk<br>-rw-------    1 root     root        8.5K Dec 16 03:21 centos7模板机器.nvram<br>-rw-r--r--    1 root     root        6.9K Dec 16 03:26 centos7模板机器.scoreboard<br>-rw-------    1 root     root         541 Dec 16 03:21 centos7模板机器.vmdk<br>-rw-r--r--    1 root     root           0 Dec 16 01:55 centos7模板机器.vmsd<br>-rwxr-xr-x    1 root     root        3.7K Dec 16 03:26 centos7模板机器.vmx<br>-rw-------    1 root     root      120.0G Dec 16 01:55 centos7模板机器_1-flat.vmdk<br>-rw-------    1 root     root         543 Dec 16 03:20 centos7模板机器_1.vmdk<br>-rw-r--r--    1 root     root      205.7K Dec 16 03:21 vmware-1.log<br>-rw-r--r--    1 root     root      152.9K Dec 16 03:24 vmware-2.log<br>-rw-r--r--    1 root     root       59.3K Dec 16 03:26 vmware-3.log<br>-rw-r--r--    1 root     root       59.1K Dec 16 03:26 vmware.log<br>[root@nucesxi:/vmfs/volumes/6730898e-54870331-6a0a-1c697a6078e1] <span class="hljs-built_in">du</span> -sh  centos7模板机器/<br>2.0G    centos7模板机器/<br></code></pre></td></tr></table></figure><p>可以看到整个大小其实就是2G的大小，如果用平台去复制很慢,这里我们后台可能不到十秒</p><p>拷贝文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">mkdir</span> ceph15<br>vmkfstools -i centos7模板机器/centos7模板机器.vmdk ceph15/centos7模板机器.vmdk -d thin<br>vmkfstools -i centos7模板机器/centos7模板机器_1.vmdk ceph15/centos7模板机器_1.vmdk -d thin<br><span class="hljs-built_in">cp</span> centos7模板机器/centos7模板机器.vmx ceph15/<br></code></pre></td></tr></table></figure><p>解释一下，上面两个是拷贝的磁盘文件，这个环境是两个磁盘的，最后一个拷贝的是虚拟机的配置文件，这样我们就不用重新配置了</p><p>然后去管理平台里面，浏览存储里面，找到centos7模板机器.vmx，然后右键选择注册虚拟机即可</p><p>注册完成后，关联平台里面有两个同名的虚拟机，我们把最新的那个虚拟机改成我们需要的名称即可，不确定的话可以通过虚拟机的配置里面看下存储路径</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>上面的操作后，可以很快的复制一台新的虚拟机</p>]]></content>
    
    
    <categories>
      
      <category>虚拟化</category>
      
    </categories>
    
    
    <tags>
      
      <tag>exsi</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>rgw追加写功能测试验证</title>
    <link href="/2024/12/13/rgw%E8%BF%BD%E5%8A%A0%E5%86%99%E5%8A%9F%E8%83%BD%E6%B5%8B%E8%AF%95%E9%AA%8C%E8%AF%81/"/>
    <url>/2024/12/13/rgw%E8%BF%BD%E5%8A%A0%E5%86%99%E5%8A%9F%E8%83%BD%E6%B5%8B%E8%AF%95%E9%AA%8C%E8%AF%81/</url>
    
    <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>aws的s3以前是不支持追加写这个功能的</p><p><img src="/images/blog/Pasted%20image%2020241213100200.png"></p><p>这个存储类型大概是2023年发布的，这个文章是2024年11月21日发布的</p><h3 id="找到功能的发布大概时间"><a href="#找到功能的发布大概时间" class="headerlink" title="找到功能的发布大概时间"></a>找到功能的发布大概时间</h3><p>我们看下boto3的工具是什么时候集成进去的就知道这个功能发布的大概的时间</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">WriteOffsetBytes<br></code></pre></td></tr></table></figure><p>aws里面使用这个参数去控制追加写的偏移量的，我们根据这个关键字去找</p><p>找到botcore的代码，使用git下载下来</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">cd</span>  botocore/botocore/data/s3/2006-03-01<br></code></pre></td></tr></table></figure><p>进入到这个目录，这个目录里面是aws的s3的接口文档，也就是不管是用python的sdk还是使用aws进行操作的时候，命令行的一些接口都是通过这个目录里面的json文件去判断的</p><p>找到这个文件的所有的历史提交</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">git <span class="hljs-built_in">log</span> -p service-2.json<br></code></pre></td></tr></table></figure><p>找到了这个地方的提交代码</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs bash">@@ -9579,6 +9667,12 @@<br>           <span class="hljs-string">&quot;location&quot;</span>:<span class="hljs-string">&quot;uri&quot;</span>,<br>           <span class="hljs-string">&quot;locationName&quot;</span>:<span class="hljs-string">&quot;Key&quot;</span><br>         &#125;,<br>+        <span class="hljs-string">&quot;WriteOffsetBytes&quot;</span>:&#123;<br>+          <span class="hljs-string">&quot;shape&quot;</span>:<span class="hljs-string">&quot;WriteOffsetBytes&quot;</span>,<br>+          <span class="hljs-string">&quot;documentation&quot;</span>:<span class="hljs-string">&quot;&lt;p&gt; Specifies the offset for appending data to existing objects in bytes. The offset must be equal to the size of the existing object being appended to. If no object exists, setting this header to 0 will create a new object. &lt;/p&gt; &lt;note&gt; &lt;p&gt;This functionality is only supported for objects in the Amazon S3 Express One Zone storage class in directory buckets.&lt;/p&gt; &lt;/note&gt;&quot;</span>,<br>+          <span class="hljs-string">&quot;location&quot;</span>:<span class="hljs-string">&quot;header&quot;</span>,<br>+          <span class="hljs-string">&quot;locationName&quot;</span>:<span class="hljs-string">&quot;x-amz-write-offset-bytes&quot;</span><br>+        &#125;,<br></code></pre></td></tr></table></figure><p>找到5c8efd314eee44949f6a9e1fb8af522c654ded15这个提交</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">commit 5c8efd314eee44949f6a9e1fb8af522c654ded15<br>Author: aws-sdk-python-automation &lt;github-aws-sdk-python-automation@amazon.com&gt;<br>Date:   Fri Nov 22 00:52:00 2024 +0000<br><br>    Update to latest models<br></code></pre></td></tr></table></figure><p>因为这个模型的提交很频繁，所以这个提交里面也不会单独说明这个功能</p><p>查看这次提交</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">git  show 5c8efd314eee44949f6a9e1fb8af522c654ded15  service-2.json<br></code></pre></td></tr></table></figure><p>确认就是这个提交里面增加的</p><p>也就是上面的2024年的11月21日，跟上面的发布时间基本对上了，也就是那边发布了接口文档，这边适配几个关键字就可以使用相关的功能了，这个松耦合做的非常好</p><h2 id="ceph这边的适配"><a href="#ceph这边的适配" class="headerlink" title="ceph这边的适配"></a>ceph这边的适配</h2><p>我们看下官方文档</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">https://docs.ceph.com/en/reef/radosgw/s3/objectops/<br></code></pre></td></tr></table></figure><p>返回的关键字是</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">x-rgw-next-append-position<br></code></pre></td></tr></table></figure><p>那我们查询这个关键字就知道什么时候合入的这个功能的</p><p><img src="/images/blog/Pasted%20image%2020241213103133.png"></p><p>最开始在2018年就提出来了，这个是阿里云的oss支持这个功能，然后ceph这边做了适配，这个地方从时间线上面aws官方这个发布的时间有点晚</p><p><img src="/images/blog/Pasted%20image%2020241213103407.png"></p><p>在2019年的2月就合入了这个功能了，也就是基本上2019年2月后面的主发行版本应该都是支持这个功能的</p><h2 id="功能的验证"><a href="#功能的验证" class="headerlink" title="功能的验证"></a>功能的验证</h2><p>官方的说明</p><blockquote><p><a href="https://github.com/ceph/ceph/blob/main/examples/rgw/boto3/README.md">https://github.com/ceph/ceph/blob/main/examples/rgw/boto3/README.md</a></p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">This directory contains examples on how to use AWS CLI/boto3 to exercise the RadosGW extensions to the S3 API. This is an extension to the [AWS SDK](https://github.com/boto/botocore/blob/develop/botocore/data/s3/2006-03-01/service-2.json).<br><br>For the standard client to support these extensions, the `service-2.sdk-extras.json` file should be added. You can place it under the default folder `~/.aws/models/s3/2006-03-01/` or create a custom one `/path/to/custom/folder/models/s3/2006-03-01/` and add it to `AWS_DATA_PATH` environment variable. For more information see [here](https://github.com/boto/botocore/blob/develop/botocore/loaders.py<span class="hljs-comment">#L33).</span><br></code></pre></td></tr></table></figure><p>这个写的是，这个目录下面有怎样使用aws的客户端和boto3的例子，以及相关的api的扩展，一般兼容性s3接口的做法是基于s3的原生标准接口再做扩展操作，这个里面就是相关的api的文件，我们需要把ceph官方提供的service-2.sdk-extras.json，添加到正在使用的boto3的sdk里面去，具体操作下面会写</p><h2 id="确定boto3的位置"><a href="#确定boto3的位置" class="headerlink" title="确定boto3的位置"></a>确定boto3的位置</h2><p>这个地方很容易出问题，因为可能环境里面装了不同版本的boto3，可能重新安装就按照了新的版本，也有可能通过不同的命令安装了不同的版本，我们需要确定好我们正在使用的boto3的库在哪里</p><p>安装boto3</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 site-packages]<span class="hljs-comment"># pip3 install boto3==1.35.79  botocore==1.35.79  -i https://mirrors.aliyun.com/pypi/simple</span><br></code></pre></td></tr></table></figure><p>这里我指定了安装的版本，因为aws对这个botocore的版本有要求，然后隔一天boto重新安装就升级了，可以通过上面的命令来按照指定的版本即可</p><p>确定路径</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 site-packages]<span class="hljs-comment"># python3</span><br>Python 3.9.6 (default, Dec 12 2024, 16:15:26)<br>[GCC 4.8.5 20150623 (Red Hat 4.8.5-44)] on linux<br>Type <span class="hljs-string">&quot;help&quot;</span>, <span class="hljs-string">&quot;copyright&quot;</span>, <span class="hljs-string">&quot;credits&quot;</span> or <span class="hljs-string">&quot;license&quot;</span> <span class="hljs-keyword">for</span> more information.<br>&gt;&gt;&gt; import botocore<br>&gt;&gt;&gt; <span class="hljs-built_in">print</span>(botocore)<br>&lt;module <span class="hljs-string">&#x27;botocore&#x27;</span> from <span class="hljs-string">&#x27;/usr/local/lib/python3.9/site-packages/botocore/__init__.py&#x27;</span>&gt;<br></code></pre></td></tr></table></figure><p>可以看到路径在</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">/usr/local/lib/python3.9/site-packages/botocore<br></code></pre></td></tr></table></figure><p>注意下我们的模型是在这个下面的botocore里面的</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 2006-03-01]<span class="hljs-comment"># ll /usr/local/lib/python3.9/site-packages/botocore/data/s3/2006-03-01/</span><br>total 248<br>-rw-r--r-- 1 root root  18305 Dec 13 10:48 endpoint-rule-set-1.json.gz<br>-rw-r--r-- 1 root root  57596 Dec 13 10:48 examples-1.json<br>-rw-r--r-- 1 root root   1837 Dec 13 10:48 paginators-1.json<br>-rw-r--r-- 1 root root    856 Dec 13 10:48 paginators-1.sdk-extras.json<br>-rw-r--r-- 1 root root 154059 Dec 13 10:48 service-2.json.gz<br>-rw-r--r-- 1 root root     98 Dec 13 10:48 service-2.sdk-extras.json<br>-rw-r--r-- 1 root root   1436 Dec 13 10:48 waiters-2.json<br></code></pre></td></tr></table></figure><p>查看这个文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 2006-03-01]<span class="hljs-comment"># cat /usr/local/lib/python3.9/site-packages/botocore/data/s3/2006-03-01/service-2.sdk-extras.json</span><br>&#123;<br>  <span class="hljs-string">&quot;version&quot;</span>: 1.0,<br>  <span class="hljs-string">&quot;merge&quot;</span>: &#123;<br>    <span class="hljs-string">&quot;shapes&quot;</span>: &#123;<br>      <span class="hljs-string">&quot;Expires&quot;</span>:&#123;<span class="hljs-string">&quot;type&quot;</span>:<span class="hljs-string">&quot;timestamp&quot;</span>&#125;<br>    &#125;<br>  &#125;<br>&#125;<br></code></pre></td></tr></table></figure><p>可以看到这个文件是没有什么内容的，我们需要把ceph的扩展放到这个里面 </p><p>这个去自己使用的ceph版本里面找，跨版本有可能出现当前版本没支持，后面版本支持的情况，所以尽量使用当前版本</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">https://github.com/ceph/ceph/blob/v15.2.17/examples/boto3/service-2.sdk-extras.json<br></code></pre></td></tr></table></figure><p>比如上面这个是我要测试的版本的，下面是当前最新版本的</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">https://github.com/ceph/ceph/blob/main/examples/rgw/boto3/service-2.sdk-extras.json<br></code></pre></td></tr></table></figure><p>可以看到路径发生了变化，把boto3的放到rgw下面去了，这个找下即可</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ~]<span class="hljs-comment">#wget  https://raw.githubusercontent.com/ceph/ceph/refs/heads/main/examples/rgw/boto3/service-2.sdk-extras.json</span><br>[root@lab101 ~]<span class="hljs-comment"># cp service-2.sdk-extras.json /usr/local/lib/python3.9/site-packages/botocore/data/s3/2006-03-01/service-2.sdk-extras.json</span><br><span class="hljs-built_in">cp</span>: overwrite ‘/usr/local/lib/python3.9/site-packages/botocore/data/s3/2006-03-01/service-2.sdk-extras.json’? y<br></code></pre></td></tr></table></figure><p>替换</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 2006-03-01]<span class="hljs-comment"># cat service-2.sdk-extras.json |grep Append</span><br>                <span class="hljs-string">&quot;AppendPosition&quot;</span>: &#123;<br>                    <span class="hljs-string">&quot;shape&quot;</span>:<span class="hljs-string">&quot;AppendPosition&quot;</span>,<br>                <span class="hljs-string">&quot;Append&quot;</span>: &#123;<br>                    <span class="hljs-string">&quot;shape&quot;</span>:<span class="hljs-string">&quot;Append&quot;</span>,<br>                    <span class="hljs-string">&quot;documentation&quot;</span>:<span class="hljs-string">&quot;&lt;p&gt;Append Object&lt;/p&gt;&quot;</span>,<br>        <span class="hljs-string">&quot;Append&quot;</span>: &#123;<span class="hljs-string">&quot;type&quot;</span>:<span class="hljs-string">&quot;boolean&quot;</span>&#125;,<br>        <span class="hljs-string">&quot;AppendPosition&quot;</span>:&#123;<span class="hljs-string">&quot;type&quot;</span>:<span class="hljs-string">&quot;integer&quot;</span>&#125;,<br>                <span class="hljs-string">&quot;AppendPosition&quot;</span>: &#123;<br>                    <span class="hljs-string">&quot;shape&quot;</span>:<span class="hljs-string">&quot;AppendPosition&quot;</span>,<br></code></pre></td></tr></table></figure><p>检查确认下相关的关键字已经放入了，上面的就是确认有了</p><h3 id="验证追加写功能"><a href="#验证追加写功能" class="headerlink" title="验证追加写功能"></a>验证追加写功能</h3><blockquote><p><a href="https://github.com/ceph/ceph/blob/v15.2.17/examples/boto3/append_object.py">https://github.com/ceph/ceph/blob/v15.2.17/examples/boto3/append_object.py</a></p></blockquote><p>这个是ceph官方提供的例子</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-meta">#!/usr/bin/python</span><br>from __future__ import print_function<br><br>import boto3<br>import sys<br>import json<br><br>def js_print(arg):<br>    <span class="hljs-built_in">print</span>(json.dumps(arg, indent=2))<br><br><span class="hljs-keyword">if</span> len(sys.argv) != 3:<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Usage: &#x27;</span> + sys.argv[0] + <span class="hljs-string">&#x27; &lt;bucket&gt; &lt;key&gt;&#x27;</span>)<br>    sys.exit(1)<br><br><span class="hljs-comment"># bucket name as first argument</span><br>bucketname = sys.argv[1]<br>keyname = sys.argv[2]<br><span class="hljs-comment"># endpoint and keys from vstart</span><br>endpoint = <span class="hljs-string">&#x27;http://127.0.0.1:8000&#x27;</span><br>access_key=<span class="hljs-string">&#x27;0555b35654ad1656d804&#x27;</span><br>secret_key=<span class="hljs-string">&#x27;h7GhxuBLTrlhVUyxSPUKUV8r/2EI4ngqJxD7iBdBYLhwluN30JaT3Q==&#x27;</span><br><br>client = boto3.client(<span class="hljs-string">&#x27;s3&#x27;</span>,<br>        endpoint_url=endpoint,<br>        aws_access_key_id=access_key,<br>        aws_secret_access_key=secret_key)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;deleting object first&#x27;</span>)<br>js_print(client.delete_object(Bucket=bucketname, Key=keyname))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;appending at position 0&#x27;</span>)<br>resp = client.put_object(Bucket=bucketname, Key=keyname,<br>                         Append=True,<br>                         AppendPosition=0,<br>                         Body=<span class="hljs-string">&#x27;8letters&#x27;</span>)<br><br>js_print(resp)<br>append_pos = resp[<span class="hljs-string">&#x27;AppendPosition&#x27;</span>]<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;appending at position %d&#x27;</span> % append_pos)<br>js_print(client.put_object(Bucket=bucketname, Key=keyname,<br>                           Append=True,<br>                           AppendPosition=append_pos,<br>                           Body=<span class="hljs-string">&#x27;8letters&#x27;</span>))<br></code></pre></td></tr></table></figure><p>可以看下大概的内容</p><ul><li>先删除指定的名称的对象</li><li>然后在0的位置写入8letters字符</li><li>然后在获取返回的AppendPosition</li><li>基于AppendPosition写入下一个8letters字符</li></ul><p>看下aws的官方的例子</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">s3.put_object(Bucket=<span class="hljs-string">&#x27;amzn-s3-demo-bucket--use2-az2--x-s3&#x27;</span>, Key=<span class="hljs-string">&#x27;2024-11-05-sdk-test&#x27;</span>, Body=b<span class="hljs-string">&#x27;123456789&#x27;</span>, WriteOffsetBytes=9)<br></code></pre></td></tr></table></figure><p>大部分是一样的，就是WriteOffsetBytes对应的是ceph的AppendPosition</p><p>修改成自己可以用的脚本</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ceph]<span class="hljs-comment"># cat test.py</span><br><span class="hljs-comment">#!/usr/bin/python</span><br>from __future__ import print_function<br><br>import boto3<br>import sys<br>import json<br><br>def js_print(arg):<br>    <span class="hljs-built_in">print</span>(json.dumps(arg, indent=2))<br><br><span class="hljs-keyword">if</span> len(sys.argv) != 3:<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Usage: &#x27;</span> + sys.argv[0] + <span class="hljs-string">&#x27; &lt;bucket&gt; &lt;key&gt;&#x27;</span>)<br>    sys.exit(1)<br><br><span class="hljs-comment"># bucket name as first argument</span><br>bucketname = sys.argv[1]<br>keyname = sys.argv[2]<br><span class="hljs-comment"># endpoint and keys from vstart</span><br>endpoint = <span class="hljs-string">&#x27;http://192.168.0.101:7481&#x27;</span><br>access_key=<span class="hljs-string">&#x27;test1&#x27;</span><br>secret_key=<span class="hljs-string">&#x27;test1&#x27;</span><br><br>client = boto3.client(<span class="hljs-string">&#x27;s3&#x27;</span>,<br>        endpoint_url=endpoint,<br>        aws_access_key_id=access_key,<br>        aws_secret_access_key=secret_key)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;deleting object first&#x27;</span>)<br>js_print(client.delete_object(Bucket=bucketname, Key=keyname))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;appending at position 0&#x27;</span>)<br>resp = client.put_object(Bucket=bucketname, Key=keyname,<br>                         Append=True,<br>                         AppendPosition=0,<br>                         Body=<span class="hljs-string">&#x27;8letters&#x27;</span>)<br><br>js_print(resp)<br>append_pos = resp[<span class="hljs-string">&#x27;AppendPosition&#x27;</span>]<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;appending at position %d&#x27;</span> % append_pos)<br>js_print(client.put_object(Bucket=bucketname, Key=keyname,<br>                           Append=True,<br>                           AppendPosition=append_pos,<br>                           Body=<span class="hljs-string">&#x27;8letters&#x27;</span>))<br></code></pre></td></tr></table></figure><p>运行脚本</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ceph]<span class="hljs-comment"># python3 test.py testbucket myappend.txt</span><br>deleting object first<br>&#123;<br>  <span class="hljs-string">&quot;ResponseMetadata&quot;</span>: &#123;<br>    <span class="hljs-string">&quot;RequestId&quot;</span>: <span class="hljs-string">&quot;tx000001eb3924743c3404f-00675ba47a-37bd-default&quot;</span>,<br>    <span class="hljs-string">&quot;HostId&quot;</span>: <span class="hljs-string">&quot;&quot;</span>,<br>    <span class="hljs-string">&quot;HTTPStatusCode&quot;</span>: 204,<br>    <span class="hljs-string">&quot;HTTPHeaders&quot;</span>: &#123;<br>      <span class="hljs-string">&quot;x-amz-request-id&quot;</span>: <span class="hljs-string">&quot;tx000001eb3924743c3404f-00675ba47a-37bd-default&quot;</span>,<br>      <span class="hljs-string">&quot;date&quot;</span>: <span class="hljs-string">&quot;Fri, 13 Dec 2024 03:05:30 GMT&quot;</span><br>    &#125;,<br>    <span class="hljs-string">&quot;RetryAttempts&quot;</span>: 0<br>  &#125;<br>&#125;<br>appending at position 0<br>&#123;<br>  <span class="hljs-string">&quot;ResponseMetadata&quot;</span>: &#123;<br>    <span class="hljs-string">&quot;RequestId&quot;</span>: <span class="hljs-string">&quot;tx00000952a1aea29f3ea2f-00675ba47a-37bd-default&quot;</span>,<br>    <span class="hljs-string">&quot;HostId&quot;</span>: <span class="hljs-string">&quot;&quot;</span>,<br>    <span class="hljs-string">&quot;HTTPStatusCode&quot;</span>: 200,<br>    <span class="hljs-string">&quot;HTTPHeaders&quot;</span>: &#123;<br>      <span class="hljs-string">&quot;content-length&quot;</span>: <span class="hljs-string">&quot;0&quot;</span>,<br>      <span class="hljs-string">&quot;etag&quot;</span>: <span class="hljs-string">&quot;\&quot;4310bc7480e027b4b5aede530e4ee9df\&quot;&quot;</span>,<br>      <span class="hljs-string">&quot;accept-ranges&quot;</span>: <span class="hljs-string">&quot;bytes&quot;</span>,<br>      <span class="hljs-string">&quot;x-rgw-next-append-position&quot;</span>: <span class="hljs-string">&quot;8&quot;</span>,<br>      <span class="hljs-string">&quot;x-amz-request-id&quot;</span>: <span class="hljs-string">&quot;tx00000952a1aea29f3ea2f-00675ba47a-37bd-default&quot;</span>,<br>      <span class="hljs-string">&quot;date&quot;</span>: <span class="hljs-string">&quot;Fri, 13 Dec 2024 03:05:30 GMT&quot;</span><br>    &#125;,<br>    <span class="hljs-string">&quot;RetryAttempts&quot;</span>: 0<br>  &#125;,<br>  <span class="hljs-string">&quot;ETag&quot;</span>: <span class="hljs-string">&quot;\&quot;4310bc7480e027b4b5aede530e4ee9df\&quot;&quot;</span>,<br>  <span class="hljs-string">&quot;AppendPosition&quot;</span>: 8<br>&#125;<br>appending at position 8<br>&#123;<br>  <span class="hljs-string">&quot;ResponseMetadata&quot;</span>: &#123;<br>    <span class="hljs-string">&quot;RequestId&quot;</span>: <span class="hljs-string">&quot;tx00000bb44851eb1f0253d-00675ba47a-37bd-default&quot;</span>,<br>    <span class="hljs-string">&quot;HostId&quot;</span>: <span class="hljs-string">&quot;&quot;</span>,<br>    <span class="hljs-string">&quot;HTTPStatusCode&quot;</span>: 200,<br>    <span class="hljs-string">&quot;HTTPHeaders&quot;</span>: &#123;<br>      <span class="hljs-string">&quot;content-length&quot;</span>: <span class="hljs-string">&quot;0&quot;</span>,<br>      <span class="hljs-string">&quot;etag&quot;</span>: <span class="hljs-string">&quot;\&quot;4310bc7480e027b4b5aede530e4ee9df\&quot;&quot;</span>,<br>      <span class="hljs-string">&quot;accept-ranges&quot;</span>: <span class="hljs-string">&quot;bytes&quot;</span>,<br>      <span class="hljs-string">&quot;x-rgw-next-append-position&quot;</span>: <span class="hljs-string">&quot;16&quot;</span>,<br>      <span class="hljs-string">&quot;x-amz-request-id&quot;</span>: <span class="hljs-string">&quot;tx00000bb44851eb1f0253d-00675ba47a-37bd-default&quot;</span>,<br>      <span class="hljs-string">&quot;date&quot;</span>: <span class="hljs-string">&quot;Fri, 13 Dec 2024 03:05:30 GMT&quot;</span><br>    &#125;,<br>    <span class="hljs-string">&quot;RetryAttempts&quot;</span>: 0<br>  &#125;,<br>  <span class="hljs-string">&quot;ETag&quot;</span>: <span class="hljs-string">&quot;\&quot;4310bc7480e027b4b5aede530e4ee9df\&quot;&quot;</span>,<br>  <span class="hljs-string">&quot;AppendPosition&quot;</span>: 16<br>&#125;<br></code></pre></td></tr></table></figure><p>参数是一个bucket名，一个对象的名称</p><p>查看运行的效果</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ceph]<span class="hljs-comment"># aws --endpoint=http://192.168.0.101:7481 s3 cp  s3://testbucket/myappend.txt  myappend.txt</span><br>download: s3://testbucket/myappend.txt to ./myappend.txt<br>[root@lab101 ceph]<span class="hljs-comment"># cat myappend.txt</span><br>8letters8letters[root@lab101 ceph]<span class="hljs-comment">#</span><br></code></pre></td></tr></table></figure><p>可以看到追加写的功能成功了,确实有16个字符了</p><p>上面的是使用的boto3的sdk做的python的测试用例，看下aws的怎样使用</p><h2 id="aws的追加写方法"><a href="#aws的追加写方法" class="headerlink" title="aws的追加写方法"></a>aws的追加写方法</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">https://docs.aws.amazon.com/AmazonS3/latest/userguide/directory-buckets-objects-append.html<br></code></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">aws s3api put-object --bucket amzn-s3-demo-bucket--azid--x-s3 --key sampleinput/file001.bin --body bucket-seed/file001.bin --write-offset-bytes size-of-sampleinput/file001.bin<br></code></pre></td></tr></table></figure><p>这个是aws这边的推荐的方法，这个里面有个write-offset-bytes的关键字，这个就是对象的偏移量<br>规则是：<br>指定bucket名称 key指定对象的名称 body指定本地的文件 write-offset-bytes 这个指定偏移量的，也就是存在的对象的大小</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 2006-03-01]<span class="hljs-comment"># cat service-2.json |grep WriteOffsetBytes</span><br>        <span class="hljs-string">&quot;WriteOffsetBytes&quot;</span>:&#123;<br>          <span class="hljs-string">&quot;shape&quot;</span>:<span class="hljs-string">&quot;WriteOffsetBytes&quot;</span>,<br>    <span class="hljs-string">&quot;WriteOffsetBytes&quot;</span>:&#123;<br></code></pre></td></tr></table></figure><p>这个地方WriteOffsetBytes会在命令行这边解析为 –write-offset-bytes<br>规则就是大写字符和小写字符之间会插入一个-</p><p>这个是aws的这边的解析，在ceph这边是按AppendPosition和Append来解析的，那么我们可以尝试命令行用这个参数测试</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ~]<span class="hljs-comment"># cat zpappend.txt</span><br>aaaaaaa<br>[root@lab101 ~]<span class="hljs-comment"># aws    s3api put-object   --endpoint=http://192.168.0.101:7481   --bucket testbucket --key newappend.txt --body zpappend.txt --append   --append-position 0</span><br>&#123;<br>    <span class="hljs-string">&quot;ETag&quot;</span>: <span class="hljs-string">&quot;\&quot;15fe514867dd5b4a1abf91ea35ff9e22\&quot;&quot;</span>,<br>    <span class="hljs-string">&quot;AppendPosition&quot;</span>: 8<br>&#125;<br></code></pre></td></tr></table></figure><p>下载下来看看</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ceph]<span class="hljs-comment"># aws --endpoint=http://192.168.0.101:7481 s3 cp s3://testbucket/newappend.txt newappend.txt</span><br>download: s3://testbucket/newappend.txt to ./newappend.txt<br>[root@lab101 ceph]<span class="hljs-comment"># cat newappend.txt</span><br>aaaaaaa<br></code></pre></td></tr></table></figure><p>再追加</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ~]<span class="hljs-comment"># aws    s3api put-object   --endpoint=http://192.168.0.101:7481   --bucket testbucket --key newappend.txt --body zpappend.txt --append   --append-position 8</span><br>&#123;<br>    <span class="hljs-string">&quot;ETag&quot;</span>: <span class="hljs-string">&quot;\&quot;15fe514867dd5b4a1abf91ea35ff9e22\&quot;&quot;</span>,<br>    <span class="hljs-string">&quot;AppendPosition&quot;</span>: 16<br>&#125;<br></code></pre></td></tr></table></figure><p>再下载</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ceph]<span class="hljs-comment"># aws --endpoint=http://192.168.0.101:7481 s3 cp s3://testbucket/newappend.txt newappend.txt</span><br>download: s3://testbucket/newappend.txt to ./newappend.txt<br>[root@lab101 ceph]<span class="hljs-comment"># cat newappend.txt</span><br>aaaaaaa<br>aaaaaaa<br></code></pre></td></tr></table></figure><p>再测试</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ~]<span class="hljs-comment"># aws    s3api put-object   --endpoint=http://192.168.0.101:7481   --bucket testbucket --key newappend.txt --body zpappend.txt --append   --append-position 8</span><br><br>An error occurred (PositionNotEqualToLength) when calling the PutObject operation: Unknown<br>[root@lab101 ~]<span class="hljs-comment"># aws    s3api put-object   --endpoint=http://192.168.0.101:7481   --bucket testbucket --key newappend.txt --body zpappend.txt --append   --append-position 16</span><br>&#123;<br>    <span class="hljs-string">&quot;ETag&quot;</span>: <span class="hljs-string">&quot;\&quot;15fe514867dd5b4a1abf91ea35ff9e22\&quot;&quot;</span>,<br>    <span class="hljs-string">&quot;AppendPosition&quot;</span>: 24<br>&#125;<br></code></pre></td></tr></table></figure><p>可以看到，追加写的时候必须正确的指定已经存在的对象的大小位置，也就是偏移量，从哪里开始追加写</p><p>返回的位置就是最后的对象的大小</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ceph]<span class="hljs-comment"># aws --endpoint=http://192.168.0.101:7481 s3 ls s3://testbucket/newappend.txt</span><br>2024-12-13 11:22:10         24 newappend.txt<br></code></pre></td></tr></table></figure><p>可以看到追加写成功了</p><h2 id="restapi追加写的功能"><a href="#restapi追加写的功能" class="headerlink" title="restapi追加写的功能"></a>restapi追加写的功能</h2><p>下面是正常的上传的代码，不是追加写的功能</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ceph]<span class="hljs-comment"># cat test1.py</span><br><span class="hljs-comment">#! /usr/bin/python3</span><br><span class="hljs-comment"># -*- coding:utf-8 -*-</span><br>import requests<br>import hashlib<br>import hmac<br>import datetime<br>import <span class="hljs-built_in">base64</span><br><br><span class="hljs-comment"># 配置 Ceph RGW 的访问信息</span><br>ACCESS_KEY = <span class="hljs-string">&quot;test1&quot;</span><br>SECRET_KEY = <span class="hljs-string">&quot;test1&quot;</span><br>RGW_ENDPOINT = <span class="hljs-string">&quot;http://192.168.0.101:7481&quot;</span>  <span class="hljs-comment"># Ceph RADOS Gateway 的地址</span><br>BUCKET_NAME = <span class="hljs-string">&quot;testbucket&quot;</span><br>OBJECT_KEY = <span class="hljs-string">&quot;testrest.txt&quot;</span><br><br><span class="hljs-comment"># 计算 AWS S3 签名</span><br>def generate_signature(method, bucket, object_key, content_type, <span class="hljs-built_in">date</span>, secret_key):<br>    string_to_sign = f<span class="hljs-string">&quot;&#123;method&#125;\n\n&#123;content_type&#125;\n&#123;date&#125;\n/&#123;bucket&#125;/&#123;object_key&#125;&quot;</span><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;String to sign:&quot;</span>, string_to_sign)  <span class="hljs-comment"># 调试信息</span><br>    signature = hmac.new(secret_key.encode(<span class="hljs-string">&#x27;utf-8&#x27;</span>), string_to_sign.encode(<span class="hljs-string">&#x27;utf-8&#x27;</span>), hashlib.sha1).digest()<br>    <span class="hljs-built_in">return</span> base64.b64encode(signature).decode(<span class="hljs-string">&#x27;utf-8&#x27;</span>)<br><br><span class="hljs-comment"># 上传对象到 Ceph</span><br>def upload_object(file_path):<br>    with open(file_path, <span class="hljs-string">&quot;rb&quot;</span>) as f:<br>        file_data = f.read()<br><br>    content_type = <span class="hljs-string">&quot;application/octet-stream&quot;</span><br>    <span class="hljs-built_in">date</span> = datetime.datetime.utcnow().strftime(<span class="hljs-string">&quot;%a, %d %b %Y %H:%M:%S GMT&quot;</span>)<br><br>    signature = generate_signature(<br>        method=<span class="hljs-string">&quot;PUT&quot;</span>,<br>        bucket=BUCKET_NAME,<br>        object_key=OBJECT_KEY,<br>        content_type=content_type,<br>        <span class="hljs-built_in">date</span>=<span class="hljs-built_in">date</span>,<br>        secret_key=SECRET_KEY<br>    )<br><br>    headers = &#123;<br>        <span class="hljs-string">&quot;Content-Type&quot;</span>: content_type,<br>        <span class="hljs-string">&quot;Date&quot;</span>: <span class="hljs-built_in">date</span>,<br>        <span class="hljs-string">&quot;Authorization&quot;</span>: f<span class="hljs-string">&quot;AWS &#123;ACCESS_KEY&#125;:&#123;signature&#125;&quot;</span><br>    &#125;<br><br>    url = f<span class="hljs-string">&quot;&#123;RGW_ENDPOINT&#125;/&#123;BUCKET_NAME&#125;/&#123;OBJECT_KEY&#125;&quot;</span><br><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Request URL:&quot;</span>, url)  <span class="hljs-comment"># 调试信息</span><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Headers:&quot;</span>, headers)  <span class="hljs-comment"># 调试信息</span><br><br>    response = requests.put(url, headers=headers, data=file_data)<br><br>    <span class="hljs-keyword">if</span> response.status_code == 200:<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Upload successful!&quot;</span>)<br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-built_in">print</span>(f<span class="hljs-string">&quot;Upload failed! Status code: &#123;response.status_code&#125;, Response: &#123;response.text&#125;&quot;</span>)<br><br><span class="hljs-comment"># 使用示例</span><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    upload_object(<span class="hljs-string">&quot;zp.txt&quot;</span>)<br></code></pre></td></tr></table></figure><p>我们再来一个追加写的restapi</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ceph]<span class="hljs-comment"># cat test4.py</span><br><span class="hljs-comment">#! /usr/bin/python3</span><br><span class="hljs-comment"># -*- coding:utf-8 -*-</span><br>import requests<br>import hashlib<br>import hmac<br>import datetime<br>import <span class="hljs-built_in">base64</span><br>import sys<br><br><span class="hljs-comment"># 配置 Ceph RGW 的访问信息</span><br>ACCESS_KEY = <span class="hljs-string">&quot;test1&quot;</span><br>SECRET_KEY = <span class="hljs-string">&quot;test1&quot;</span><br>RGW_ENDPOINT = <span class="hljs-string">&quot;http://192.168.0.101:7481&quot;</span>  <span class="hljs-comment"># Ceph RADOS Gateway 的地址</span><br>BUCKET_NAME = <span class="hljs-string">&quot;testbucket&quot;</span><br><br><span class="hljs-comment"># 计算 AWS S3 签名</span><br>def generate_signature(method, bucket, object_key, content_type, <span class="hljs-built_in">date</span>, secret_key):<br>    string_to_sign = f<span class="hljs-string">&quot;&#123;method&#125;\n\n&#123;content_type&#125;\n&#123;date&#125;\n/&#123;bucket&#125;/&#123;object_key&#125;&quot;</span><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;String to sign:&quot;</span>, string_to_sign)  <span class="hljs-comment"># 调试信息</span><br>    signature = hmac.new(secret_key.encode(<span class="hljs-string">&#x27;utf-8&#x27;</span>), string_to_sign.encode(<span class="hljs-string">&#x27;utf-8&#x27;</span>), hashlib.sha1).digest()<br>    <span class="hljs-built_in">return</span> base64.b64encode(signature).decode(<span class="hljs-string">&#x27;utf-8&#x27;</span>)<br><br><span class="hljs-comment"># 上传对象到 Ceph</span><br>def upload_object(object_key, file_path, position):<br>    with open(file_path, <span class="hljs-string">&quot;rb&quot;</span>) as f:<br>        file_data = f.read()<br><br>    content_type = <span class="hljs-string">&quot;application/octet-stream&quot;</span><br>    <span class="hljs-built_in">date</span> = datetime.datetime.utcnow().strftime(<span class="hljs-string">&quot;%a, %d %b %Y %H:%M:%S GMT&quot;</span>)<br><br>    signature = generate_signature(<br>        method=<span class="hljs-string">&quot;PUT&quot;</span>,<br>        bucket=BUCKET_NAME,<br>        object_key=object_key,<br>        content_type=content_type,<br>        <span class="hljs-built_in">date</span>=<span class="hljs-built_in">date</span>,<br>        secret_key=SECRET_KEY<br>    )<br><br>    headers = &#123;<br>        <span class="hljs-string">&quot;Content-Type&quot;</span>: content_type,<br>        <span class="hljs-string">&quot;Date&quot;</span>: <span class="hljs-built_in">date</span>,<br>        <span class="hljs-string">&quot;Authorization&quot;</span>: f<span class="hljs-string">&quot;AWS &#123;ACCESS_KEY&#125;:&#123;signature&#125;&quot;</span><br>    &#125;<br><br>    url = f<span class="hljs-string">&quot;&#123;RGW_ENDPOINT&#125;/&#123;BUCKET_NAME&#125;/&#123;object_key&#125;?position=&#123;position&#125;&amp;append=true&quot;</span><br><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Request URL:&quot;</span>, url)  <span class="hljs-comment"># 调试信息</span><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Headers:&quot;</span>, headers)  <span class="hljs-comment"># 调试信息</span><br><br>    response = requests.put(url, headers=headers, data=file_data)<br><br>    <span class="hljs-keyword">if</span> response.status_code == 200:<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Append successful!&quot;</span>)<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Response headers:&quot;</span>, response.headers)<br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-built_in">print</span>(f<span class="hljs-string">&quot;Append failed! Status code: &#123;response.status_code&#125;, Response: &#123;response.text&#125;&quot;</span>)<br><br><span class="hljs-comment"># 使用示例</span><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    <span class="hljs-keyword">if</span> len(sys.argv) != 4:<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Usage: python3 test4.py &lt;object_key&gt; &lt;file_path&gt; &lt;position&gt;&quot;</span>)<br>        sys.exit(1)<br><br>    object_key = sys.argv[1]<br>    file_path = sys.argv[2]<br>    position = sys.argv[3]<br><br>    upload_object(object_key, file_path, position)<br></code></pre></td></tr></table></figure><p>可以看到签名的地方并没有改变，改变的就是请求的questsring发生了变化</p><p>看下运行效果</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ceph]<span class="hljs-comment"># python3 test4.py  restaaa zp.txt 0</span><br>String to sign: PUT<br><br>application/octet-stream<br>Fri, 13 Dec 2024 07:13:33 GMT<br>/testbucket/restaaa<br>Request URL: http://192.168.0.101:7481/testbucket/restaaa?position=0&amp;append=<span class="hljs-literal">true</span><br>Headers: &#123;<span class="hljs-string">&#x27;Content-Type&#x27;</span>: <span class="hljs-string">&#x27;application/octet-stream&#x27;</span>, <span class="hljs-string">&#x27;Date&#x27;</span>: <span class="hljs-string">&#x27;Fri, 13 Dec 2024 07:13:33 GMT&#x27;</span>, <span class="hljs-string">&#x27;Authorization&#x27;</span>: <span class="hljs-string">&#x27;AWS test1:3J3okCbKD2WVsNpfngjMPqBmvVU=&#x27;</span>&#125;<br>Append successful!<br>Response headers: &#123;<span class="hljs-string">&#x27;Content-Length&#x27;</span>: <span class="hljs-string">&#x27;0&#x27;</span>, <span class="hljs-string">&#x27;ETag&#x27;</span>: <span class="hljs-string">&#x27;&quot;0a2806dcc91865a811d19150084f6de7&quot;&#x27;</span>, <span class="hljs-string">&#x27;Accept-Ranges&#x27;</span>: <span class="hljs-string">&#x27;bytes&#x27;</span>, <span class="hljs-string">&#x27;x-rgw-next-append-position&#x27;</span>: <span class="hljs-string">&#x27;16&#x27;</span>, <span class="hljs-string">&#x27;x-amz-request-id&#x27;</span>: <span class="hljs-string">&#x27;tx00000ec3108225d59c5e1-00675bde9d-37bd-default&#x27;</span>, <span class="hljs-string">&#x27;Date&#x27;</span>: <span class="hljs-string">&#x27;Fri, 13 Dec 2024 07:13:33 GMT&#x27;</span>, <span class="hljs-string">&#x27;Connection&#x27;</span>: <span class="hljs-string">&#x27;Keep-Alive&#x27;</span>&#125;<br></code></pre></td></tr></table></figure><p>可以看到返回的x-rgw-next-append-position为16，也就是下个请求应该从16开始</p><p>查看文件大小</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 site-packages]<span class="hljs-comment"># aws --endpoint=http://192.168.0.101:7481 s3 ls s3://testbucket/restaaa</span><br>2024-12-13 15:13:33         16 restaaa<br></code></pre></td></tr></table></figure><p>再次请求</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ceph]<span class="hljs-comment"># python3 test4.py  restaaa zp.txt 16</span><br>String to sign: PUT<br><br>application/octet-stream<br>Fri, 13 Dec 2024 07:14:29 GMT<br>/testbucket/restaaa<br>Request URL: http://192.168.0.101:7481/testbucket/restaaa?position=16&amp;append=<span class="hljs-literal">true</span><br>Headers: &#123;<span class="hljs-string">&#x27;Content-Type&#x27;</span>: <span class="hljs-string">&#x27;application/octet-stream&#x27;</span>, <span class="hljs-string">&#x27;Date&#x27;</span>: <span class="hljs-string">&#x27;Fri, 13 Dec 2024 07:14:29 GMT&#x27;</span>, <span class="hljs-string">&#x27;Authorization&#x27;</span>: <span class="hljs-string">&#x27;AWS test1:PsYEAg3YnDb8PVBvRWSKL7nPhy0=&#x27;</span>&#125;<br>Append successful!<br>Response headers: &#123;<span class="hljs-string">&#x27;Content-Length&#x27;</span>: <span class="hljs-string">&#x27;0&#x27;</span>, <span class="hljs-string">&#x27;ETag&#x27;</span>: <span class="hljs-string">&#x27;&quot;0a2806dcc91865a811d19150084f6de7&quot;&#x27;</span>, <span class="hljs-string">&#x27;Accept-Ranges&#x27;</span>: <span class="hljs-string">&#x27;bytes&#x27;</span>, <span class="hljs-string">&#x27;x-rgw-next-append-position&#x27;</span>: <span class="hljs-string">&#x27;32&#x27;</span>, <span class="hljs-string">&#x27;x-amz-request-id&#x27;</span>: <span class="hljs-string">&#x27;tx00000cbe7cf038ff40327-00675bded5-37bd-default&#x27;</span>, <span class="hljs-string">&#x27;Date&#x27;</span>: <span class="hljs-string">&#x27;Fri, 13 Dec 2024 07:14:29 GMT&#x27;</span>, <span class="hljs-string">&#x27;Connection&#x27;</span>: <span class="hljs-string">&#x27;Keep-Alive&#x27;</span>&#125;<br></code></pre></td></tr></table></figure><p>再次查看</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 site-packages]<span class="hljs-comment"># aws --endpoint=http://192.168.0.101:7481 s3 ls s3://testbucket/restaaa</span><br>2024-12-13 15:14:29         32 restaaa<br></code></pre></td></tr></table></figure><p>可以看到文件追加写进去了 </p><p>上面的例子是用的v2认证做的例子，v2认证比v4要简单一些</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本篇介绍了ceph这边的rgw的追加写的功能验证，可以看到跟aws类似，关键字做一点改变即可，本篇从三个工具讲述了ceph里面的追加写的功能，Python的boto的sdk的方法，aws的工具的方法，以及restapi的方法，从三个地方讲述了怎样发起追加写的请求，整体上还是比较简单就是对着指定的位置发起请求即可，就是请求的时候带上一个偏移量</p>]]></content>
    
    
    <categories>
      
      <category>存储系统</category>
      
    </categories>
    
    
    <tags>
      
      <tag>ceph</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>纠删码中间对象属性丢失引起osd的崩溃</title>
    <link href="/2024/12/11/%E7%BA%A0%E5%88%A0%E7%A0%81%E4%B8%AD%E9%97%B4%E5%AF%B9%E8%B1%A1%E5%B1%9E%E6%80%A7%E4%B8%A2%E5%A4%B1%E5%BC%95%E8%B5%B7osd%E7%9A%84%E5%B4%A9%E6%BA%83/"/>
    <url>/2024/12/11/%E7%BA%A0%E5%88%A0%E7%A0%81%E4%B8%AD%E9%97%B4%E5%AF%B9%E8%B1%A1%E5%B1%9E%E6%80%A7%E4%B8%A2%E5%A4%B1%E5%BC%95%E8%B5%B7osd%E7%9A%84%E5%B4%A9%E6%BA%83/</url>
    
    <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>迁移的时候出现osd的崩溃，然后进行pg的备份的时候出现了无法获取属性的情况，本篇记录问题和解决的方法</p><h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs bash">Error getting attr on : 2.7s2_head,2<span class="hljs-comment">#2:f7d032a7:::rbd_data.1.101a6b8b4567.00000000000000a1:head#f6, (61) No data available</span><br>Error getting attr on : 2.7s2_head,2<span class="hljs-comment">#2:fecb9c0c:::rbd_data.1.101a6b8b4567.0000000000000089:head#46, (61) No data available</span><br>Error getting attr on : 2.7s2_head,2<span class="hljs-comment">#2:fecb9c0c:::rbd_data.1.101a6b8b4567.0000000000000089:head#48, (61) No data available</span><br>Error getting attr on : 2.7s2_head,2<span class="hljs-comment">#2:fecb9c0c:::rbd_data.1.101a6b8b4567.0000000000000089:head#ed, (61) No data available</span><br>[<span class="hljs-string">&quot;2.7s2&quot;</span>,&#123;<span class="hljs-string">&quot;oid&quot;</span>:<span class="hljs-string">&quot;rbd_data.1.101a6b8b4567.0000000000000089&quot;</span>,<span class="hljs-string">&quot;key&quot;</span>:<span class="hljs-string">&quot;&quot;</span>,<span class="hljs-string">&quot;snapid&quot;</span>:-2,<span class="hljs-string">&quot;hash&quot;</span>:809096063,<span class="hljs-string">&quot;max&quot;</span>:0,<span class="hljs-string">&quot;pool&quot;</span>:2,<span class="hljs-string">&quot;namespace&quot;</span>:<span class="hljs-string">&quot;&quot;</span>,<span class="hljs-string">&quot;generation&quot;</span>:120,<span class="hljs-string">&quot;shard_id&quot;</span>:2,<span class="hljs-string">&quot;max&quot;</span>:0&#125;]<br>[<span class="hljs-string">&quot;2.7s2&quot;</span>,&#123;<span class="hljs-string">&quot;oid&quot;</span>:<span class="hljs-string">&quot;rbd_data.1.101a6b8b4567.0000000000000089&quot;</span>,<span class="hljs-string">&quot;key&quot;</span>:<span class="hljs-string">&quot;&quot;</span>,<span class="hljs-string">&quot;snapid&quot;</span>:-2,<span class="hljs-string">&quot;hash&quot;</span>:809096063,<span class="hljs-string">&quot;max&quot;</span>:0,<span class="hljs-string">&quot;pool&quot;</span>:2,<span class="hljs-string">&quot;namespace&quot;</span>:<span class="hljs-string">&quot;&quot;</span>,<span class="hljs-string">&quot;generation&quot;</span>:157,<span class="hljs-string">&quot;shard_id&quot;</span>:2,<span class="hljs-string">&quot;max&quot;</span>:0&#125;]<br>[<span class="hljs-string">&quot;2.7s2&quot;</span>,&#123;<span class="hljs-string">&quot;oid&quot;</span>:<span class="hljs-string">&quot;rbd_data.1.101a6b8b4567.0000000000000089&quot;</span>,<span class="hljs-string">&quot;key&quot;</span>:<span class="hljs-string">&quot;&quot;</span>,<span class="hljs-string">&quot;snapid&quot;</span>:-2,<span class="hljs-string">&quot;hash&quot;</span>:809096063,<span class="hljs-string">&quot;max&quot;</span>:0,<span class="hljs-string">&quot;pool&quot;</span>:2,<span class="hljs-string">&quot;namespace&quot;</span>:<span class="hljs-string">&quot;&quot;</span>,<span class="hljs-string">&quot;generation&quot;</span>:194,<span class="hljs-string">&quot;shard_id&quot;</span>:2,<span class="hljs-string">&quot;max&quot;</span>:0&#125;]<br>[<span class="hljs-string">&quot;2.7s2&quot;</span>,&#123;<span class="hljs-string">&quot;oid&quot;</span>:<span class="hljs-string">&quot;rbd_data.1.101a6b8b4567.0000000000000089&quot;</span>,<span class="hljs-string">&quot;key&quot;</span>:<span class="hljs-string">&quot;&quot;</span>,<span class="hljs-string">&quot;snapid&quot;</span>:-2,<span class="hljs-string">&quot;hash&quot;</span>:809096063,<span class="hljs-string">&quot;max&quot;</span>:0,<span class="hljs-string">&quot;pool&quot;</span>:2,<span class="hljs-string">&quot;namespace&quot;</span>:<span class="hljs-string">&quot;&quot;</span>,<span class="hljs-string">&quot;generation&quot;</span>:287,<span class="hljs-string">&quot;shard_id&quot;</span>:2,<span class="hljs-string">&quot;max&quot;</span>:0&#125;]<br>[<span class="hljs-string">&quot;2.7s2&quot;</span>,&#123;<span class="hljs-string">&quot;oid&quot;</span>:<span class="hljs-string">&quot;rbd_data.1.101a6b8b4567.0000000000000089&quot;</span>,<span class="hljs-string">&quot;key&quot;</span>:<span class="hljs-string">&quot;&quot;</span>,<span class="hljs-string">&quot;snapid&quot;</span>:-2,<span class="hljs-string">&quot;hash&quot;</span>:809096063,<span class="hljs-string">&quot;max&quot;</span>:0,<span class="hljs-string">&quot;pool&quot;</span>:2,<span class="hljs-string">&quot;namespace&quot;</span>:<span class="hljs-string">&quot;&quot;</span>,<span class="hljs-string">&quot;generation&quot;</span>:326,<span class="hljs-string">&quot;shard_id&quot;</span>:2,<span class="hljs-string">&quot;max&quot;</span>:0&#125;]<br>[<span class="hljs-string">&quot;2.7s2&quot;</span>,&#123;<span class="hljs-string">&quot;oid&quot;</span>:<span class="hljs-string">&quot;rbd_data.1.101a6b8b4567.0000000000000089&quot;</span>,<span class="hljs-string">&quot;key&quot;</span>:<span class="hljs-string">&quot;&quot;</span>,<span class="hljs-string">&quot;snapid&quot;</span>:-2,<span class="hljs-string">&quot;hash&quot;</span>:809096063,<span class="hljs-string">&quot;max&quot;</span>:0,<span class="hljs-string">&quot;pool&quot;</span>:2,<span class="hljs-string">&quot;namespace&quot;</span>:<span class="hljs-string">&quot;&quot;</span>,<span class="hljs-string">&quot;shard_id&quot;</span>:2,<span class="hljs-string">&quot;max&quot;</span>:0&#125;]<br>[root@lab103 mnt]<span class="hljs-comment"># ceph-objectstore-tool --data /var/lib/ceph/osd/ceph-2 --pgid 2.7s2 --op list|grep rbd_data.1.101a6b8b4567.0000000000000089</span><br></code></pre></td></tr></table></figure><p>做list或者export的时候会报错</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab103 mnt]<span class="hljs-comment"># ceph-objectstore-tool --data /var/lib/ceph/osd/ceph-2 --pgid 2.7s2 --op export --file zp</span><br>Exporting 2.7s2<br>Read 2<span class="hljs-comment">#2:e0384c0e:::rbd_data.1.101a6b8b4567.0000000000000072:head#3e</span><br>getattr failure object_info -61<br>export_files error -61<br></code></pre></td></tr></table></figure><p>然后这个情况下在做backfill的时候osd 就崩溃了</p><h2 id="问题原因"><a href="#问题原因" class="headerlink" title="问题原因"></a>问题原因</h2><p>存储系统的盘出现了故障，造成了一些属性没有写上去，有的对象没删除，就出现这种中间状态了，这种一般是阵列卡引起或者磁盘问题，出现后，就可能出现卡pg的状态了，必须修复才能恢复环境</p><p>出现问题的时候，开始理解错了，上面的2.7s2_head,2#2:fecb9c0c:::rbd_data.1.101a6b8b4567.0000000000000089:head#ed这个结尾的编号跟快照的编号一样的<br>误认为这个地方是快照的，这个地方实际上是纠删码的覆盖写过程中的中间对象的，正常情况下会自动删除了，但是没删除的时候，就出现问题了</p><h2 id="模拟出这个问题"><a href="#模拟出这个问题" class="headerlink" title="模拟出这个问题"></a>模拟出这个问题</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs bash">vim ./ceph-xxxx/src/osd/PGBackend.cc<br>void PGBackend::rollback_extents(<br>  version_t gen,<br>  const vector&lt;pair&lt;uint64_t, uint64_t&gt; &gt; &amp;extents,<br>  const hobject_t &amp;hoid,<br>  ObjectStore::Transaction *t) &#123;<br>  auto shard = get_parent()-&gt;whoami_shard().shard;<br>  <span class="hljs-keyword">for</span> (auto &amp;&amp;extent: extents) &#123;<br>    t-&gt;clone_range(<br>      coll,<br>      ghobject_t(hoid, gen, shard),<br>      ghobject_t(hoid, ghobject_t::NO_GEN, shard),<br>      extent.first,<br>      extent.second,<br>      extent.first);<br>  &#125;<br>//  t-&gt;remove(<br>//    coll,<br>//    ghobject_t(hoid, gen, shard));<br>&#125;<br><br>void PGBackend::trim_rollback_object(<br>  const hobject_t &amp;hoid,<br>  version_t old_version,<br>  ObjectStore::Transaction *t) &#123;<br>  assert(!hoid.is_temp());<br>//  t-&gt;remove(<br>//    coll, ghobject_t(hoid, old_version, get_parent()-&gt;whoami_shard().shard));<br>&#125;<br></code></pre></td></tr></table></figure><h3 id="触发故障"><a href="#触发故障" class="headerlink" title="触发故障"></a>触发故障</h3><p>屏蔽掉两个删除中间对象的地方<br>然后配置一个纠删码的集群，然后对着rbd进行覆盖写的操作，然后list对象</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">ceph-objectstore-tool --data /var/lib/ceph/osd/ceph-2 --pgid 2.7s2 <span class="hljs-string">&#x27;&#123;&quot;oid&quot;:&quot;rbd_data.1.101a6b8b4567.00000000000000b3&quot;,&quot;key&quot;:&quot;&quot;,&quot;snapid&quot;:-2,&quot;hash&quot;:3266086655,&quot;max&quot;:0,&quot;pool&quot;:2,&quot;namespace&quot;:&quot;&quot;,&quot;generation&quot;:127,&quot;shard_id&quot;:2,&quot;max&quot;:0&#125;&#x27;</span> rm-attr snapset<br>ceph-objectstore-tool --data /var/lib/ceph/osd/ceph-2 --pgid 2.7s2 <span class="hljs-string">&#x27;&#123;&quot;oid&quot;:&quot;rbd_data.1.101a6b8b4567.00000000000000b3&quot;,&quot;key&quot;:&quot;&quot;,&quot;snapid&quot;:-2,&quot;hash&quot;:3266086655,&quot;max&quot;:0,&quot;pool&quot;:2,&quot;namespace&quot;:&quot;&quot;,&quot;generation&quot;:127,&quot;shard_id&quot;:2,&quot;max&quot;:0&#125;&#x27;</span> rm-attr _<br></code></pre></td></tr></table></figure><p>执行完这个以后，就可以发现，对象无法删除了，模拟出了问题的现象</p><h2 id="问题解决方式"><a href="#问题解决方式" class="headerlink" title="问题解决方式"></a>问题解决方式</h2><p>问题比较清晰了，就是中间对象的扩展属性丢失了，我们需要处理这种情况，通过上面的模拟，我们找到了对象的命名规则<br>这个地方的snapid跟原始对象一样，就是generation这个地方是编号的16进制转10进制</p><h3 id="设置snapset属性"><a href="#设置snapset属性" class="headerlink" title="设置snapset属性"></a>设置snapset属性</h3><p>从其它正常的对象上面获取到snapset的属性</p><p>然后通过</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">ceph-objectstore-tool --data /var/lib/ceph/osd/ceph-2 --pgid 2.7s2 <span class="hljs-string">&#x27;&#123;&quot;oid&quot;:&quot;rbd_data.1.101a6b8b4567.00000000000000b3&quot;,&quot;key&quot;:&quot;&quot;,&quot;snapid&quot;:-2,&quot;hash&quot;:3266086655,&quot;max&quot;:0,&quot;pool&quot;:2,&quot;namespace&quot;:&quot;&quot;,&quot;generation&quot;:127,&quot;shard_id&quot;:2,&quot;max&quot;:0&#125;&#x27;</span> set-attr napset &lt; snapset<br></code></pre></td></tr></table></figure><p>设置这个属性后才能删除这个对象,attr _这个属性可以不设置，也可以删除对象，缺这个snapset的属性是不能删除的</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">ceph-objectstore-tool --data /var/lib/ceph/osd/ceph-2 --pgid 2.7s2 <span class="hljs-string">&#x27;&#123;&quot;oid&quot;:&quot;rbd_data.1.101a6b8b4567.00000000000000b3&quot;,&quot;key&quot;:&quot;&quot;,&quot;snapid&quot;:-2,&quot;hash&quot;:3266086655,&quot;max&quot;:0,&quot;pool&quot;:2,&quot;namespace&quot;:&quot;&quot;,&quot;generation&quot;:127,&quot;shard_id&quot;:2,&quot;max&quot;:0&#125;&#x27;</span> remove<br></code></pre></td></tr></table></figure><p>同样的方法把其它的垃圾对象处理掉</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>问题就是ec的情况下出现了中间对象的扩展属性丢失的情况，扩展属性丢失无法删除对象，无法export，backfill也崩溃，多个问题<br>处理问题的思路就是把对象给构造回去然后再删除</p><p>这个地方跟之前的快照对象存在，原始对象缺失造成的崩溃有点类似，但是这个地方难点是需要知道中间对象命名规则</p><p>这个问题一般出现在硬件出问题或者掉电情况下，场景比较限定，使用纠删场景，并且使用rbd，还触发了覆盖写，正好碰上了磁盘数据没法删除产生垃圾文件的情况</p>]]></content>
    
    
    <categories>
      
      <category>存储系统</category>
      
    </categories>
    
    
    <tags>
      
      <tag>ceph</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>快照原始对象缺失引起的osd崩溃</title>
    <link href="/2024/12/11/%E5%BF%AB%E7%85%A7%E5%8E%9F%E5%A7%8B%E5%AF%B9%E8%B1%A1%E7%BC%BA%E5%A4%B1%E5%BC%95%E8%B5%B7%E7%9A%84osd%E5%B4%A9%E6%BA%83/"/>
    <url>/2024/12/11/%E5%BF%AB%E7%85%A7%E5%8E%9F%E5%A7%8B%E5%AF%B9%E8%B1%A1%E7%BC%BA%E5%A4%B1%E5%BC%95%E8%B5%B7%E7%9A%84osd%E5%B4%A9%E6%BA%83/</url>
    
    <content type="html"><![CDATA[<h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p><img src="/images/blog/unexpectedclone.png" alt="unexpectedclone"></p><p>修复pg的时候出现了 unexpected clone </p><p>可以看到这个对象后面有编号，这个编号有两种情况</p><ul><li>快照的对象(snapid)</li><li>纠删对象的中间版本（对应是generation）</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">&#123;<span class="hljs-string">&quot;oid&quot;</span>:<span class="hljs-string">&quot;rbd_data.1.101a6b8b4567.00000000000000b3&quot;</span>,<span class="hljs-string">&quot;key&quot;</span>:<span class="hljs-string">&quot;&quot;</span>,<span class="hljs-string">&quot;snapid&quot;</span>:26,<span class="hljs-string">&quot;hash&quot;</span>:3266086655,<span class="hljs-string">&quot;max&quot;</span>:0,<span class="hljs-string">&quot;pool&quot;</span>:2,<span class="hljs-string">&quot;namespace&quot;</span>:<span class="hljs-string">&quot;&quot;</span>,<span class="hljs-string">&quot;generation&quot;</span>:251,<span class="hljs-string">&quot;shard_id&quot;</span>:2,<span class="hljs-string">&quot;max&quot;</span>:0&#125;<br></code></pre></td></tr></table></figure><p>这个地方需要根据自己的使用情况分析是哪种数据</p><p>通过在osd上面进行object list操作的时候发现<br>只有快照对象，没有原始对象</p><p>这个说明这个对象其实是做了删除的，但是快照对象没删除掉，遗留在这里了，所以出现了这个情况<br>这个情况正常情况是无法出现的，所以这里osd.12上面的磁盘做删除的时候没有删除成功,一般是出现在磁盘出现了异常的时候</p><h2 id="问题复现方法"><a href="#问题复现方法" class="headerlink" title="问题复现方法"></a>问题复现方法</h2><p>模拟一个删除了快照和对象<br>一个只删除了对象，快照数据都留着的，然后进行一次scrub的模拟 ，这个对象完全就没有了的 </p><p>osd.0 是对象和快照都删除<br>osd.1 是删除对象，不删除快照对象 （需要加参数，内部正常不会出现这个情况）<br>原始对象是snapid -2<br>快照对象就是快照的编号id</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ceph]<span class="hljs-comment"># ceph-objectstore-tool --data /var/lib/ceph/osd/ceph-1/ --pgid 1.75 --op list</span><br>Error getting attr on : 1.75_head,<span class="hljs-comment">#1:ae000000::::head#, (61) No data available</span><br>[<span class="hljs-string">&quot;1.75&quot;</span>,&#123;<span class="hljs-string">&quot;oid&quot;</span>:<span class="hljs-string">&quot;&quot;</span>,<span class="hljs-string">&quot;key&quot;</span>:<span class="hljs-string">&quot;&quot;</span>,<span class="hljs-string">&quot;snapid&quot;</span>:-2,<span class="hljs-string">&quot;hash&quot;</span>:117,<span class="hljs-string">&quot;max&quot;</span>:0,<span class="hljs-string">&quot;pool&quot;</span>:1,<span class="hljs-string">&quot;namespace&quot;</span>:<span class="hljs-string">&quot;&quot;</span>,<span class="hljs-string">&quot;max&quot;</span>:0&#125;]<br>[<span class="hljs-string">&quot;1.75&quot;</span>,&#123;<span class="hljs-string">&quot;oid&quot;</span>:<span class="hljs-string">&quot;rbd_data.606c9027f4e3.0000000000000117&quot;</span>,<span class="hljs-string">&quot;key&quot;</span>:<span class="hljs-string">&quot;&quot;</span>,<span class="hljs-string">&quot;snapid&quot;</span>:-2,<span class="hljs-string">&quot;hash&quot;</span>:4088145525,<span class="hljs-string">&quot;max&quot;</span>:0,<span class="hljs-string">&quot;pool&quot;</span>:1,<span class="hljs-string">&quot;namespace&quot;</span>:<span class="hljs-string">&quot;&quot;</span>,<span class="hljs-string">&quot;max&quot;</span>:0&#125;]<br>[<span class="hljs-string">&quot;1.75&quot;</span>,&#123;<span class="hljs-string">&quot;oid&quot;</span>:<span class="hljs-string">&quot;rbd_data.606c9027f4e3.0000000000000014&quot;</span>,<span class="hljs-string">&quot;key&quot;</span>:<span class="hljs-string">&quot;&quot;</span>,<span class="hljs-string">&quot;snapid&quot;</span>:18,<span class="hljs-string">&quot;hash&quot;</span>:2027662965,<span class="hljs-string">&quot;max&quot;</span>:0,<span class="hljs-string">&quot;pool&quot;</span>:1,<span class="hljs-string">&quot;namespace&quot;</span>:<span class="hljs-string">&quot;&quot;</span>,<span class="hljs-string">&quot;max&quot;</span>:0&#125;]<br>[<span class="hljs-string">&quot;1.75&quot;</span>,&#123;<span class="hljs-string">&quot;oid&quot;</span>:<span class="hljs-string">&quot;rbd_data.606c9027f4e3.0000000000000014&quot;</span>,<span class="hljs-string">&quot;key&quot;</span>:<span class="hljs-string">&quot;&quot;</span>,<span class="hljs-string">&quot;snapid&quot;</span>:-2,<span class="hljs-string">&quot;hash&quot;</span>:2027662965,<span class="hljs-string">&quot;max&quot;</span>:0,<span class="hljs-string">&quot;pool&quot;</span>:1,<span class="hljs-string">&quot;namespace&quot;</span>:<span class="hljs-string">&quot;&quot;</span>,<span class="hljs-string">&quot;max&quot;</span>:0&#125;]<br>[<span class="hljs-string">&quot;1.75&quot;</span>,&#123;<span class="hljs-string">&quot;oid&quot;</span>:<span class="hljs-string">&quot;rbd_data.606c9027f4e3.0000000000000064&quot;</span>,<span class="hljs-string">&quot;key&quot;</span>:<span class="hljs-string">&quot;&quot;</span>,<span class="hljs-string">&quot;snapid&quot;</span>:16,<span class="hljs-string">&quot;hash&quot;</span>:2791802613,<span class="hljs-string">&quot;max&quot;</span>:0,<span class="hljs-string">&quot;pool&quot;</span>:1,<span class="hljs-string">&quot;namespace&quot;</span>:<span class="hljs-string">&quot;&quot;</span>,<span class="hljs-string">&quot;max&quot;</span>:0&#125;]<br>[<span class="hljs-string">&quot;1.75&quot;</span>,&#123;<span class="hljs-string">&quot;oid&quot;</span>:<span class="hljs-string">&quot;rbd_data.606c9027f4e3.0000000000000064&quot;</span>,<span class="hljs-string">&quot;key&quot;</span>:<span class="hljs-string">&quot;&quot;</span>,<span class="hljs-string">&quot;snapid&quot;</span>:18,<span class="hljs-string">&quot;hash&quot;</span>:2791802613,<span class="hljs-string">&quot;max&quot;</span>:0,<span class="hljs-string">&quot;pool&quot;</span>:1,<span class="hljs-string">&quot;namespace&quot;</span>:<span class="hljs-string">&quot;&quot;</span>,<span class="hljs-string">&quot;max&quot;</span>:0&#125;]<br>[<span class="hljs-string">&quot;1.75&quot;</span>,&#123;<span class="hljs-string">&quot;oid&quot;</span>:<span class="hljs-string">&quot;rbd_data.606c9027f4e3.0000000000000064&quot;</span>,<span class="hljs-string">&quot;key&quot;</span>:<span class="hljs-string">&quot;&quot;</span>,<span class="hljs-string">&quot;snapid&quot;</span>:-2,<span class="hljs-string">&quot;hash&quot;</span>:2791802613,<span class="hljs-string">&quot;max&quot;</span>:0,<span class="hljs-string">&quot;pool&quot;</span>:1,<span class="hljs-string">&quot;namespace&quot;</span>:<span class="hljs-string">&quot;&quot;</span>,<span class="hljs-string">&quot;max&quot;</span>:0&#125;]<br>[<span class="hljs-string">&quot;1.75&quot;</span>,&#123;<span class="hljs-string">&quot;oid&quot;</span>:<span class="hljs-string">&quot;rbd_data.606c9027f4e3.0000000000000057&quot;</span>,<span class="hljs-string">&quot;key&quot;</span>:<span class="hljs-string">&quot;&quot;</span>,<span class="hljs-string">&quot;snapid&quot;</span>:18,<span class="hljs-string">&quot;hash&quot;</span>:250437109,<span class="hljs-string">&quot;max&quot;</span>:0,<span class="hljs-string">&quot;pool&quot;</span>:1,<span class="hljs-string">&quot;namespace&quot;</span>:<span class="hljs-string">&quot;&quot;</span>,<span class="hljs-string">&quot;max&quot;</span>:0&#125;]<br>[<span class="hljs-string">&quot;1.75&quot;</span>,&#123;<span class="hljs-string">&quot;oid&quot;</span>:<span class="hljs-string">&quot;rbd_data.606c9027f4e3.0000000000000057&quot;</span>,<span class="hljs-string">&quot;key&quot;</span>:<span class="hljs-string">&quot;&quot;</span>,<span class="hljs-string">&quot;snapid&quot;</span>:-2,<span class="hljs-string">&quot;hash&quot;</span>:250437109,<span class="hljs-string">&quot;max&quot;</span>:0,<span class="hljs-string">&quot;pool&quot;</span>:1,<span class="hljs-string">&quot;namespace&quot;</span>:<span class="hljs-string">&quot;&quot;</span>,<span class="hljs-string">&quot;max&quot;</span>:0&#125;]<br>[root@lab101 ceph]<span class="hljs-comment">#</span><br>[root@lab101 ceph]<span class="hljs-comment"># ceph-objectstore-tool --data /var/lib/ceph/osd/ceph-0/ --pgid 1.75 --op list</span><br>Error getting attr on : 1.75_head,<span class="hljs-comment">#1:ae000000::::head#, (61) No data available</span><br>[<span class="hljs-string">&quot;1.75&quot;</span>,&#123;<span class="hljs-string">&quot;oid&quot;</span>:<span class="hljs-string">&quot;&quot;</span>,<span class="hljs-string">&quot;key&quot;</span>:<span class="hljs-string">&quot;&quot;</span>,<span class="hljs-string">&quot;snapid&quot;</span>:-2,<span class="hljs-string">&quot;hash&quot;</span>:117,<span class="hljs-string">&quot;max&quot;</span>:0,<span class="hljs-string">&quot;pool&quot;</span>:1,<span class="hljs-string">&quot;namespace&quot;</span>:<span class="hljs-string">&quot;&quot;</span>,<span class="hljs-string">&quot;max&quot;</span>:0&#125;]<br>[<span class="hljs-string">&quot;1.75&quot;</span>,&#123;<span class="hljs-string">&quot;oid&quot;</span>:<span class="hljs-string">&quot;rbd_data.606c9027f4e3.0000000000000117&quot;</span>,<span class="hljs-string">&quot;key&quot;</span>:<span class="hljs-string">&quot;&quot;</span>,<span class="hljs-string">&quot;snapid&quot;</span>:-2,<span class="hljs-string">&quot;hash&quot;</span>:4088145525,<span class="hljs-string">&quot;max&quot;</span>:0,<span class="hljs-string">&quot;pool&quot;</span>:1,<span class="hljs-string">&quot;namespace&quot;</span>:<span class="hljs-string">&quot;&quot;</span>,<span class="hljs-string">&quot;max&quot;</span>:0&#125;]<br>[<span class="hljs-string">&quot;1.75&quot;</span>,&#123;<span class="hljs-string">&quot;oid&quot;</span>:<span class="hljs-string">&quot;rbd_data.606c9027f4e3.0000000000000014&quot;</span>,<span class="hljs-string">&quot;key&quot;</span>:<span class="hljs-string">&quot;&quot;</span>,<span class="hljs-string">&quot;snapid&quot;</span>:18,<span class="hljs-string">&quot;hash&quot;</span>:2027662965,<span class="hljs-string">&quot;max&quot;</span>:0,<span class="hljs-string">&quot;pool&quot;</span>:1,<span class="hljs-string">&quot;namespace&quot;</span>:<span class="hljs-string">&quot;&quot;</span>,<span class="hljs-string">&quot;max&quot;</span>:0&#125;]<br>[<span class="hljs-string">&quot;1.75&quot;</span>,&#123;<span class="hljs-string">&quot;oid&quot;</span>:<span class="hljs-string">&quot;rbd_data.606c9027f4e3.0000000000000014&quot;</span>,<span class="hljs-string">&quot;key&quot;</span>:<span class="hljs-string">&quot;&quot;</span>,<span class="hljs-string">&quot;snapid&quot;</span>:-2,<span class="hljs-string">&quot;hash&quot;</span>:2027662965,<span class="hljs-string">&quot;max&quot;</span>:0,<span class="hljs-string">&quot;pool&quot;</span>:1,<span class="hljs-string">&quot;namespace&quot;</span>:<span class="hljs-string">&quot;&quot;</span>,<span class="hljs-string">&quot;max&quot;</span>:0&#125;]<br>[<span class="hljs-string">&quot;1.75&quot;</span>,&#123;<span class="hljs-string">&quot;oid&quot;</span>:<span class="hljs-string">&quot;rbd_data.606c9027f4e3.0000000000000064&quot;</span>,<span class="hljs-string">&quot;key&quot;</span>:<span class="hljs-string">&quot;&quot;</span>,<span class="hljs-string">&quot;snapid&quot;</span>:16,<span class="hljs-string">&quot;hash&quot;</span>:2791802613,<span class="hljs-string">&quot;max&quot;</span>:0,<span class="hljs-string">&quot;pool&quot;</span>:1,<span class="hljs-string">&quot;namespace&quot;</span>:<span class="hljs-string">&quot;&quot;</span>,<span class="hljs-string">&quot;max&quot;</span>:0&#125;]<br>[<span class="hljs-string">&quot;1.75&quot;</span>,&#123;<span class="hljs-string">&quot;oid&quot;</span>:<span class="hljs-string">&quot;rbd_data.606c9027f4e3.0000000000000064&quot;</span>,<span class="hljs-string">&quot;key&quot;</span>:<span class="hljs-string">&quot;&quot;</span>,<span class="hljs-string">&quot;snapid&quot;</span>:18,<span class="hljs-string">&quot;hash&quot;</span>:2791802613,<span class="hljs-string">&quot;max&quot;</span>:0,<span class="hljs-string">&quot;pool&quot;</span>:1,<span class="hljs-string">&quot;namespace&quot;</span>:<span class="hljs-string">&quot;&quot;</span>,<span class="hljs-string">&quot;max&quot;</span>:0&#125;]<br>[<span class="hljs-string">&quot;1.75&quot;</span>,&#123;<span class="hljs-string">&quot;oid&quot;</span>:<span class="hljs-string">&quot;rbd_data.606c9027f4e3.0000000000000064&quot;</span>,<span class="hljs-string">&quot;key&quot;</span>:<span class="hljs-string">&quot;&quot;</span>,<span class="hljs-string">&quot;snapid&quot;</span>:-2,<span class="hljs-string">&quot;hash&quot;</span>:2791802613,<span class="hljs-string">&quot;max&quot;</span>:0,<span class="hljs-string">&quot;pool&quot;</span>:1,<span class="hljs-string">&quot;namespace&quot;</span>:<span class="hljs-string">&quot;&quot;</span>,<span class="hljs-string">&quot;max&quot;</span>:0&#125;]<br>[<span class="hljs-string">&quot;1.75&quot;</span>,&#123;<span class="hljs-string">&quot;oid&quot;</span>:<span class="hljs-string">&quot;rbd_data.606c9027f4e3.0000000000000057&quot;</span>,<span class="hljs-string">&quot;key&quot;</span>:<span class="hljs-string">&quot;&quot;</span>,<span class="hljs-string">&quot;snapid&quot;</span>:18,<span class="hljs-string">&quot;hash&quot;</span>:250437109,<span class="hljs-string">&quot;max&quot;</span>:0,<span class="hljs-string">&quot;pool&quot;</span>:1,<span class="hljs-string">&quot;namespace&quot;</span>:<span class="hljs-string">&quot;&quot;</span>,<span class="hljs-string">&quot;max&quot;</span>:0&#125;]<br>[<span class="hljs-string">&quot;1.75&quot;</span>,&#123;<span class="hljs-string">&quot;oid&quot;</span>:<span class="hljs-string">&quot;rbd_data.606c9027f4e3.0000000000000057&quot;</span>,<span class="hljs-string">&quot;key&quot;</span>:<span class="hljs-string">&quot;&quot;</span>,<span class="hljs-string">&quot;snapid&quot;</span>:-2,<span class="hljs-string">&quot;hash&quot;</span>:250437109,<span class="hljs-string">&quot;max&quot;</span>:0,<span class="hljs-string">&quot;pool&quot;</span>:1,<span class="hljs-string">&quot;namespace&quot;</span>:<span class="hljs-string">&quot;&quot;</span>,<span class="hljs-string">&quot;max&quot;</span>:0&#125;]<br><br><br><br><br>[root@lab101 ceph]<span class="hljs-comment"># ceph-objectstore-tool --data /var/lib/ceph/osd/ceph-0/ --pgid 1.75 &#x27;&#123;&quot;oid&quot;:&quot;rbd_data.606c9027f4e3.0000000000000064&quot;,&quot;key&quot;:&quot;&quot;,&quot;snapid&quot;:-2,&quot;hash&quot;:2791802613,&quot;max&quot;:0,&quot;pool&quot;:1,&quot;namespace&quot;:&quot;&quot;,&quot;max&quot;:0&#125;&#x27; remove</span><br>Clones are present, use removeall to delete everything<br>[root@lab101 ceph]<span class="hljs-comment"># ceph-objectstore-tool --data /var/lib/ceph/osd/ceph-0/ --pgid 1.75 &#x27;&#123;&quot;oid&quot;:&quot;rbd_data.606c9027f4e3.0000000000000064&quot;,&quot;key&quot;:&quot;&quot;,&quot;snapid&quot;:-2,&quot;hash&quot;:2791802613,&quot;max&quot;:0,&quot;pool&quot;:1,&quot;namespace&quot;:&quot;&quot;,&quot;max&quot;:0&#125;&#x27; removeall</span><br>remove <span class="hljs-built_in">clone</span> <span class="hljs-comment">#1:af61e665:::rbd_data.606c9027f4e3.0000000000000064:10#</span><br>remove <span class="hljs-built_in">clone</span> <span class="hljs-comment">#1:af61e665:::rbd_data.606c9027f4e3.0000000000000064:12#</span><br>remove <span class="hljs-comment">#1:af61e665:::rbd_data.606c9027f4e3.0000000000000064:head#</span><br><br><br><br>[root@lab101 ceph]<span class="hljs-comment"># ceph-objectstore-tool --data /var/lib/ceph/osd/ceph-1/ --pgid 1.75 &#x27;&#123;&quot;oid&quot;:&quot;rbd_data.606c9027f4e3.0000000000000064&quot;,&quot;key&quot;:&quot;&quot;,&quot;snapid&quot;:-2,&quot;hash&quot;:2791802613,&quot;max&quot;:0,&quot;pool&quot;:1,&quot;namespace&quot;:&quot;&quot;,&quot;max&quot;:0&#125;&#x27; remove</span><br>Clones are present, use removeall to delete everything<br>[root@lab101 ceph]<span class="hljs-comment"># ceph-objectstore-tool --data /var/lib/ceph/osd/ceph-1/ --pgid 1.75 &#x27;&#123;&quot;oid&quot;:&quot;rbd_data.606c9027f4e3.0000000000000064&quot;,&quot;key&quot;:&quot;&quot;,&quot;snapid&quot;:-2,&quot;hash&quot;:2791802613,&quot;max&quot;:0,&quot;pool&quot;:1,&quot;namespace&quot;:&quot;&quot;,&quot;max&quot;:0&#125;&#x27; remove --force</span><br>WARNING: only removing <span class="hljs-built_in">head</span> with clones present<br>remove <span class="hljs-comment">#1:af61e665:::rbd_data.606c9027f4e3.0000000000000064:head#</span><br><br><br>[root@lab101 ceph]<span class="hljs-comment"># ceph-objectstore-tool --data /var/lib/ceph/osd/ceph-0/ --pgid 1.75 --op list</span><br>Error getting attr on : 1.75_head,<span class="hljs-comment">#1:ae000000::::head#, (61) No data available</span><br>[<span class="hljs-string">&quot;1.75&quot;</span>,&#123;<span class="hljs-string">&quot;oid&quot;</span>:<span class="hljs-string">&quot;&quot;</span>,<span class="hljs-string">&quot;key&quot;</span>:<span class="hljs-string">&quot;&quot;</span>,<span class="hljs-string">&quot;snapid&quot;</span>:-2,<span class="hljs-string">&quot;hash&quot;</span>:117,<span class="hljs-string">&quot;max&quot;</span>:0,<span class="hljs-string">&quot;pool&quot;</span>:1,<span class="hljs-string">&quot;namespace&quot;</span>:<span class="hljs-string">&quot;&quot;</span>,<span class="hljs-string">&quot;max&quot;</span>:0&#125;]<br>[<span class="hljs-string">&quot;1.75&quot;</span>,&#123;<span class="hljs-string">&quot;oid&quot;</span>:<span class="hljs-string">&quot;rbd_data.606c9027f4e3.0000000000000117&quot;</span>,<span class="hljs-string">&quot;key&quot;</span>:<span class="hljs-string">&quot;&quot;</span>,<span class="hljs-string">&quot;snapid&quot;</span>:-2,<span class="hljs-string">&quot;hash&quot;</span>:4088145525,<span class="hljs-string">&quot;max&quot;</span>:0,<span class="hljs-string">&quot;pool&quot;</span>:1,<span class="hljs-string">&quot;namespace&quot;</span>:<span class="hljs-string">&quot;&quot;</span>,<span class="hljs-string">&quot;max&quot;</span>:0&#125;]<br>[<span class="hljs-string">&quot;1.75&quot;</span>,&#123;<span class="hljs-string">&quot;oid&quot;</span>:<span class="hljs-string">&quot;rbd_data.606c9027f4e3.0000000000000014&quot;</span>,<span class="hljs-string">&quot;key&quot;</span>:<span class="hljs-string">&quot;&quot;</span>,<span class="hljs-string">&quot;snapid&quot;</span>:18,<span class="hljs-string">&quot;hash&quot;</span>:2027662965,<span class="hljs-string">&quot;max&quot;</span>:0,<span class="hljs-string">&quot;pool&quot;</span>:1,<span class="hljs-string">&quot;namespace&quot;</span>:<span class="hljs-string">&quot;&quot;</span>,<span class="hljs-string">&quot;max&quot;</span>:0&#125;]<br>[<span class="hljs-string">&quot;1.75&quot;</span>,&#123;<span class="hljs-string">&quot;oid&quot;</span>:<span class="hljs-string">&quot;rbd_data.606c9027f4e3.0000000000000014&quot;</span>,<span class="hljs-string">&quot;key&quot;</span>:<span class="hljs-string">&quot;&quot;</span>,<span class="hljs-string">&quot;snapid&quot;</span>:-2,<span class="hljs-string">&quot;hash&quot;</span>:2027662965,<span class="hljs-string">&quot;max&quot;</span>:0,<span class="hljs-string">&quot;pool&quot;</span>:1,<span class="hljs-string">&quot;namespace&quot;</span>:<span class="hljs-string">&quot;&quot;</span>,<span class="hljs-string">&quot;max&quot;</span>:0&#125;]<br>[<span class="hljs-string">&quot;1.75&quot;</span>,&#123;<span class="hljs-string">&quot;oid&quot;</span>:<span class="hljs-string">&quot;rbd_data.606c9027f4e3.0000000000000057&quot;</span>,<span class="hljs-string">&quot;key&quot;</span>:<span class="hljs-string">&quot;&quot;</span>,<span class="hljs-string">&quot;snapid&quot;</span>:18,<span class="hljs-string">&quot;hash&quot;</span>:250437109,<span class="hljs-string">&quot;max&quot;</span>:0,<span class="hljs-string">&quot;pool&quot;</span>:1,<span class="hljs-string">&quot;namespace&quot;</span>:<span class="hljs-string">&quot;&quot;</span>,<span class="hljs-string">&quot;max&quot;</span>:0&#125;]<br>[<span class="hljs-string">&quot;1.75&quot;</span>,&#123;<span class="hljs-string">&quot;oid&quot;</span>:<span class="hljs-string">&quot;rbd_data.606c9027f4e3.0000000000000057&quot;</span>,<span class="hljs-string">&quot;key&quot;</span>:<span class="hljs-string">&quot;&quot;</span>,<span class="hljs-string">&quot;snapid&quot;</span>:-2,<span class="hljs-string">&quot;hash&quot;</span>:250437109,<span class="hljs-string">&quot;max&quot;</span>:0,<span class="hljs-string">&quot;pool&quot;</span>:1,<span class="hljs-string">&quot;namespace&quot;</span>:<span class="hljs-string">&quot;&quot;</span>,<span class="hljs-string">&quot;max&quot;</span>:0&#125;]<br>[root@lab101 ceph]<span class="hljs-comment"># ceph-objectstore-tool --data /var/lib/ceph/osd/ceph-1/ --pgid 1.75 --op list</span><br>Error getting attr on : 1.75_head,<span class="hljs-comment">#1:ae000000::::head#, (61) No data available</span><br>[<span class="hljs-string">&quot;1.75&quot;</span>,&#123;<span class="hljs-string">&quot;oid&quot;</span>:<span class="hljs-string">&quot;&quot;</span>,<span class="hljs-string">&quot;key&quot;</span>:<span class="hljs-string">&quot;&quot;</span>,<span class="hljs-string">&quot;snapid&quot;</span>:-2,<span class="hljs-string">&quot;hash&quot;</span>:117,<span class="hljs-string">&quot;max&quot;</span>:0,<span class="hljs-string">&quot;pool&quot;</span>:1,<span class="hljs-string">&quot;namespace&quot;</span>:<span class="hljs-string">&quot;&quot;</span>,<span class="hljs-string">&quot;max&quot;</span>:0&#125;]<br>[<span class="hljs-string">&quot;1.75&quot;</span>,&#123;<span class="hljs-string">&quot;oid&quot;</span>:<span class="hljs-string">&quot;rbd_data.606c9027f4e3.0000000000000117&quot;</span>,<span class="hljs-string">&quot;key&quot;</span>:<span class="hljs-string">&quot;&quot;</span>,<span class="hljs-string">&quot;snapid&quot;</span>:-2,<span class="hljs-string">&quot;hash&quot;</span>:4088145525,<span class="hljs-string">&quot;max&quot;</span>:0,<span class="hljs-string">&quot;pool&quot;</span>:1,<span class="hljs-string">&quot;namespace&quot;</span>:<span class="hljs-string">&quot;&quot;</span>,<span class="hljs-string">&quot;max&quot;</span>:0&#125;]<br>[<span class="hljs-string">&quot;1.75&quot;</span>,&#123;<span class="hljs-string">&quot;oid&quot;</span>:<span class="hljs-string">&quot;rbd_data.606c9027f4e3.0000000000000014&quot;</span>,<span class="hljs-string">&quot;key&quot;</span>:<span class="hljs-string">&quot;&quot;</span>,<span class="hljs-string">&quot;snapid&quot;</span>:18,<span class="hljs-string">&quot;hash&quot;</span>:2027662965,<span class="hljs-string">&quot;max&quot;</span>:0,<span class="hljs-string">&quot;pool&quot;</span>:1,<span class="hljs-string">&quot;namespace&quot;</span>:<span class="hljs-string">&quot;&quot;</span>,<span class="hljs-string">&quot;max&quot;</span>:0&#125;]<br>[<span class="hljs-string">&quot;1.75&quot;</span>,&#123;<span class="hljs-string">&quot;oid&quot;</span>:<span class="hljs-string">&quot;rbd_data.606c9027f4e3.0000000000000014&quot;</span>,<span class="hljs-string">&quot;key&quot;</span>:<span class="hljs-string">&quot;&quot;</span>,<span class="hljs-string">&quot;snapid&quot;</span>:-2,<span class="hljs-string">&quot;hash&quot;</span>:2027662965,<span class="hljs-string">&quot;max&quot;</span>:0,<span class="hljs-string">&quot;pool&quot;</span>:1,<span class="hljs-string">&quot;namespace&quot;</span>:<span class="hljs-string">&quot;&quot;</span>,<span class="hljs-string">&quot;max&quot;</span>:0&#125;]<br>[<span class="hljs-string">&quot;1.75&quot;</span>,&#123;<span class="hljs-string">&quot;oid&quot;</span>:<span class="hljs-string">&quot;rbd_data.606c9027f4e3.0000000000000064&quot;</span>,<span class="hljs-string">&quot;key&quot;</span>:<span class="hljs-string">&quot;&quot;</span>,<span class="hljs-string">&quot;snapid&quot;</span>:16,<span class="hljs-string">&quot;hash&quot;</span>:2791802613,<span class="hljs-string">&quot;max&quot;</span>:0,<span class="hljs-string">&quot;pool&quot;</span>:1,<span class="hljs-string">&quot;namespace&quot;</span>:<span class="hljs-string">&quot;&quot;</span>,<span class="hljs-string">&quot;max&quot;</span>:0&#125;]<br>[<span class="hljs-string">&quot;1.75&quot;</span>,&#123;<span class="hljs-string">&quot;oid&quot;</span>:<span class="hljs-string">&quot;rbd_data.606c9027f4e3.0000000000000064&quot;</span>,<span class="hljs-string">&quot;key&quot;</span>:<span class="hljs-string">&quot;&quot;</span>,<span class="hljs-string">&quot;snapid&quot;</span>:18,<span class="hljs-string">&quot;hash&quot;</span>:2791802613,<span class="hljs-string">&quot;max&quot;</span>:0,<span class="hljs-string">&quot;pool&quot;</span>:1,<span class="hljs-string">&quot;namespace&quot;</span>:<span class="hljs-string">&quot;&quot;</span>,<span class="hljs-string">&quot;max&quot;</span>:0&#125;]<br>[<span class="hljs-string">&quot;1.75&quot;</span>,&#123;<span class="hljs-string">&quot;oid&quot;</span>:<span class="hljs-string">&quot;rbd_data.606c9027f4e3.0000000000000057&quot;</span>,<span class="hljs-string">&quot;key&quot;</span>:<span class="hljs-string">&quot;&quot;</span>,<span class="hljs-string">&quot;snapid&quot;</span>:18,<span class="hljs-string">&quot;hash&quot;</span>:250437109,<span class="hljs-string">&quot;max&quot;</span>:0,<span class="hljs-string">&quot;pool&quot;</span>:1,<span class="hljs-string">&quot;namespace&quot;</span>:<span class="hljs-string">&quot;&quot;</span>,<span class="hljs-string">&quot;max&quot;</span>:0&#125;]<br>[<span class="hljs-string">&quot;1.75&quot;</span>,&#123;<span class="hljs-string">&quot;oid&quot;</span>:<span class="hljs-string">&quot;rbd_data.606c9027f4e3.0000000000000057&quot;</span>,<span class="hljs-string">&quot;key&quot;</span>:<span class="hljs-string">&quot;&quot;</span>,<span class="hljs-string">&quot;snapid&quot;</span>:-2,<span class="hljs-string">&quot;hash&quot;</span>:250437109,<span class="hljs-string">&quot;max&quot;</span>:0,<span class="hljs-string">&quot;pool&quot;</span>:1,<span class="hljs-string">&quot;namespace&quot;</span>:<span class="hljs-string">&quot;&quot;</span>,<span class="hljs-string">&quot;max&quot;</span>:0&#125;]<br><br></code></pre></td></tr></table></figure><p>都启动了</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ceph]<span class="hljs-comment"># rados -p rbd ls|grep rbd_data.606c9027f4e3.0000000000000064</span><br></code></pre></td></tr></table></figure><p>找不到相关的对象，这个正常的</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ceph]<span class="hljs-comment"># ceph -s</span><br>  cluster:<br>    <span class="hljs-built_in">id</span>:     40edb456-3006-4189-aced-68abcdac0f72<br>    health: HEALTH_WARN<br>            mon is allowing insecure global_id reclaim<br>            noout flag(s) <span class="hljs-built_in">set</span><br>            1 pool(s) <span class="hljs-keyword">do</span> not have an application enabled<br>            3 daemons have recently crashed<br>            OSD count 2 &lt; osd_pool_default_size 3<br><br>  services:<br>    mon: 1 daemons, quorum lab101 (age 44m)<br>    mgr: lab101(active, since 44m)<br>    osd: 2 osds: 2 up (since 61s), 2 <span class="hljs-keyword">in</span> (since 6d)<br>         flags noout<br><br>  data:<br>    pools:   1 pools, 128 pgs<br>    objects: 435 objects, 1.6 GiB<br>    usage:   3.2 GiB used, 442 GiB / 445 GiB avail<br>    pgs:     128 active+clean<br><br></code></pre></td></tr></table></figure><p>也正常的,我们来一次scrub</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ceph]<span class="hljs-comment"># ceph pg deep-scrub 1.75</span><br>instructing pg 1.75 on osd.1 to deep-scrub<br><br><br>        flags noout<br><br>  data:<br>    pools:   1 pools, 128 pgs<br>    objects: 435 objects, 1.6 GiB<br>    usage:   3.2 GiB used, 442 GiB / 445 GiB avail<br>    pgs:     127 active+clean<br>             1   active+clean+inconsistent<br><br></code></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash"> [root@lab101 ceph]<span class="hljs-comment"># rados list-inconsistent-obj 1.75</span><br>&#123;<span class="hljs-string">&quot;epoch&quot;</span>:704,<span class="hljs-string">&quot;inconsistents&quot;</span>:[&#123;<span class="hljs-string">&quot;object&quot;</span>:&#123;<span class="hljs-string">&quot;name&quot;</span>:<span class="hljs-string">&quot;rbd_data.606c9027f4e3.0000000000000064&quot;</span>,<span class="hljs-string">&quot;nspace&quot;</span>:<span class="hljs-string">&quot;&quot;</span>,<span class="hljs-string">&quot;locator&quot;</span>:<span class="hljs-string">&quot;&quot;</span>,<span class="hljs-string">&quot;snap&quot;</span>:16,<span class="hljs-string">&quot;version&quot;</span>:28&#125;,<span class="hljs-string">&quot;errors&quot;</span>:[],<span class="hljs-string">&quot;union_shard_errors&quot;</span>:[<span class="hljs-string">&quot;missing&quot;</span>],<span class="hljs-string">&quot;selected_object_info&quot;</span>:&#123;<span class="hljs-string">&quot;oid&quot;</span>:&#123;<span class="hljs-string">&quot;oid&quot;</span>:<span class="hljs-string">&quot;rbd_data.606c9027f4e3.0000000000000064&quot;</span>,<span class="hljs-string">&quot;key&quot;</span>:<span class="hljs-string">&quot;&quot;</span>,<span class="hljs-string">&quot;snapid&quot;</span>:16,<span class="hljs-string">&quot;hash&quot;</span>:2791802613,<span class="hljs-string">&quot;max&quot;</span>:0,<span class="hljs-string">&quot;pool&quot;</span>:1,<span class="hljs-string">&quot;namespace&quot;</span>:<span class="hljs-string">&quot;&quot;</span>&#125;,<span class="hljs-string">&quot;version&quot;</span>:<span class="hljs-string">&quot;688&#x27;29&quot;</span>,<span class="hljs-string">&quot;prior_version&quot;</span>:<span class="hljs-string">&quot;535&#x27;28&quot;</span>,<span class="hljs-string">&quot;last_reqid&quot;</span>:<span class="hljs-string">&quot;client.24688.0:849&quot;</span>,<span class="hljs-string">&quot;user_version&quot;</span>:28,<span class="hljs-string">&quot;size&quot;</span>:131072,<span class="hljs-string">&quot;mtime&quot;</span>:<span class="hljs-string">&quot;2024-11-01T00:26:29.890995+0800&quot;</span>,<span class="hljs-string">&quot;local_mtime&quot;</span>:<span class="hljs-string">&quot;2024-11-01T00:26:29.908788+0800&quot;</span>,<span class="hljs-string">&quot;lost&quot;</span>:0,<span class="hljs-string">&quot;flags&quot;</span>:[<span class="hljs-string">&quot;dirty&quot;</span>,<span class="hljs-string">&quot;data_digest&quot;</span>],<span class="hljs-string">&quot;truncate_seq&quot;</span>:0,<span class="hljs-string">&quot;truncate_size&quot;</span>:0,<span class="hljs-string">&quot;data_digest&quot;</span>:<span class="hljs-string">&quot;0x3fdefe12&quot;</span>,<span class="hljs-string">&quot;omap_digest&quot;</span>:<span class="hljs-string">&quot;0xffffffff&quot;</span>,<span class="hljs-string">&quot;expected_object_size&quot;</span>:0,<span class="hljs-string">&quot;expected_write_size&quot;</span>:0,<span class="hljs-string">&quot;alloc_hint_flags&quot;</span>:0,<span class="hljs-string">&quot;manifest&quot;</span>:&#123;<span class="hljs-string">&quot;type&quot;</span>:0&#125;,<span class="hljs-string">&quot;watchers&quot;</span>:&#123;&#125;&#125;,<span class="hljs-string">&quot;shards&quot;</span>:[&#123;<span class="hljs-string">&quot;osd&quot;</span>:0,<span class="hljs-string">&quot;primary&quot;</span>:<span class="hljs-literal">false</span>,<span class="hljs-string">&quot;errors&quot;</span>:[<span class="hljs-string">&quot;missing&quot;</span>]&#125;,&#123;<span class="hljs-string">&quot;osd&quot;</span>:1,<span class="hljs-string">&quot;primary&quot;</span>:<span class="hljs-literal">true</span>,<span class="hljs-string">&quot;errors&quot;</span>:[],<span class="hljs-string">&quot;size&quot;</span>:131072,<span class="hljs-string">&quot;omap_digest&quot;</span>:<span class="hljs-string">&quot;0xffffffff&quot;</span>,<span class="hljs-string">&quot;data_digest&quot;</span>:<span class="hljs-string">&quot;0x3fdefe12&quot;</span>&#125;]&#125;,&#123;<span class="hljs-string">&quot;object&quot;</span>:&#123;<span class="hljs-string">&quot;name&quot;</span>:<span class="hljs-string">&quot;rbd_data.606c9027f4e3.0000000000000064&quot;</span>,<span class="hljs-string">&quot;nspace&quot;</span>:<span class="hljs-string">&quot;&quot;</span>,<span class="hljs-string">&quot;locator&quot;</span>:<span class="hljs-string">&quot;&quot;</span>,<span class="hljs-string">&quot;snap&quot;</span>:18,<span class="hljs-string">&quot;version&quot;</span>:31&#125;,<span class="hljs-string">&quot;errors&quot;</span>:[],<span class="hljs-string">&quot;union_shard_errors&quot;</span>:[<span class="hljs-string">&quot;missing&quot;</span>],<span class="hljs-string">&quot;selected_object_info&quot;</span>:&#123;<span class="hljs-string">&quot;oid&quot;</span>:&#123;<span class="hljs-string">&quot;oid&quot;</span>:<span class="hljs-string">&quot;rbd_data.606c9027f4e3.0000000000000064&quot;</span>,<span class="hljs-string">&quot;key&quot;</span>:<span class="hljs-string">&quot;&quot;</span>,<span class="hljs-string">&quot;snapid&quot;</span>:18,<span class="hljs-string">&quot;hash&quot;</span>:2791802613,<span class="hljs-string">&quot;max&quot;</span>:0,<span class="hljs-string">&quot;pool&quot;</span>:1,<span class="hljs-string">&quot;namespace&quot;</span>:<span class="hljs-string">&quot;&quot;</span>&#125;,<span class="hljs-string">&quot;version&quot;</span>:<span class="hljs-string">&quot;697&#x27;43&quot;</span>,<span class="hljs-string">&quot;prior_version&quot;</span>:<span class="hljs-string">&quot;688&#x27;31&quot;</span>,<span class="hljs-string">&quot;last_reqid&quot;</span>:<span class="hljs-string">&quot;client.24688.0:1028&quot;</span>,<span class="hljs-string">&quot;user_version&quot;</span>:31,<span class="hljs-string">&quot;size&quot;</span>:4194304,<span class="hljs-string">&quot;mtime&quot;</span>:<span class="hljs-string">&quot;2024-11-01T00:37:32.918239+0800&quot;</span>,<span class="hljs-string">&quot;local_mtime&quot;</span>:<span class="hljs-string">&quot;2024-11-01T00:37:32.922857+0800&quot;</span>,<span class="hljs-string">&quot;lost&quot;</span>:0,<span class="hljs-string">&quot;flags&quot;</span>:[<span class="hljs-string">&quot;dirty&quot;</span>,<span class="hljs-string">&quot;data_digest&quot;</span>],<span class="hljs-string">&quot;truncate_seq&quot;</span>:0,<span class="hljs-string">&quot;truncate_size&quot;</span>:0,<span class="hljs-string">&quot;data_digest&quot;</span>:<span class="hljs-string">&quot;0x2dea776f&quot;</span>,<span class="hljs-string">&quot;omap_digest&quot;</span>:<span class="hljs-string">&quot;0xffffffff&quot;</span>,<span class="hljs-string">&quot;expected_object_size&quot;</span>:0,<span class="hljs-string">&quot;expected_write_size&quot;</span>:0,<span class="hljs-string">&quot;alloc_hint_flags&quot;</span>:0,<span class="hljs-string">&quot;manifest&quot;</span>:&#123;<span class="hljs-string">&quot;type&quot;</span>:0&#125;,<span class="hljs-string">&quot;watchers&quot;</span>:&#123;&#125;&#125;,<span class="hljs-string">&quot;shards&quot;</span>:[&#123;<span class="hljs-string">&quot;osd&quot;</span>:0,<span class="hljs-string">&quot;primary&quot;</span>:<span class="hljs-literal">false</span>,<span class="hljs-string">&quot;errors&quot;</span>:[<span class="hljs-string">&quot;missing&quot;</span>]&#125;,&#123;<span class="hljs-string">&quot;osd&quot;</span>:1,<span class="hljs-string">&quot;primary&quot;</span>:<span class="hljs-literal">true</span>,<span class="hljs-string">&quot;errors&quot;</span>:[],<span class="hljs-string">&quot;size&quot;</span>:4194304,<span class="hljs-string">&quot;omap_digest&quot;</span>:<span class="hljs-string">&quot;0xffffffff&quot;</span>,<span class="hljs-string">&quot;data_digest&quot;</span>:<span class="hljs-string">&quot;0x2dea776f&quot;</span>&#125;]&#125;]&#125;[root@lab101 ceph]<span class="hljs-comment">#</span><br><br></code></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ceph]<span class="hljs-comment"># ceph pg repair 1.75</span><br>instructing pg 1.75 on osd.1 to repair<br></code></pre></td></tr></table></figure><p>尝试修复</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs bash">data:<br>    pools:   1 pools, 128 pgs<br>    objects: 432 objects, 1.6 GiB<br>    usage:   3.2 GiB used, 442 GiB / 445 GiB avail<br>    pgs:     127 active+clean<br>             1   active+clean+inconsistent+failed_repair<br><br><br><br>  2024-11-01T00:46:21.640+0800 7f7ae0d5a700 -1 log_channel(cluster) <span class="hljs-built_in">log</span> [ERR] : 1.75 shard 0 soid 1:af61e665:::rbd_data.606c9027f4e3.0000000000000064:12 : data_digest 0x43d61c5d != data_digest 0x2dea776f from shard 1, data_digest 0x43d61c5d != data_digest 0x2dea776f from auth oi 1:af61e<br>665:::rbd_data.606c9027f4e3.0000000000000064:12(697<span class="hljs-string">&#x27;43 client.24688.0:1028 dirty|data_digest s 4194304 uv 31 dd 2dea776f alloc_hint [0 0 0])</span><br><span class="hljs-string">2024-11-01T00:46:21.640+0800 7f7ae0d5a700 -1 log_channel(cluster) log [ERR] : deep-scrub 1.75 1:af61e665:::rbd_data.606c9027f4e3.0000000000000064:12 : is an unexpected clone</span><br><span class="hljs-string">2024-11-01T00:46:21.640+0800 7f7ae0d5a700 -1 log_channel(cluster) log [ERR] : deep-scrub 1.75 1:af61e665:::rbd_data.606c9027f4e3.0000000000000064:10 : is an unexpected clone</span><br><span class="hljs-string"></span><br></code></pre></td></tr></table></figure><p>查看日志 ，可以看到一模一样了</p><h2 id="问题根因"><a href="#问题根因" class="headerlink" title="问题根因"></a>问题根因</h2><h3 id="情况一："><a href="#情况一：" class="headerlink" title="情况一："></a>情况一：</h3><p>集群内需要删除数据的时候，在osd上面无法操作删除，同时也看到了日志有read error，这种读都读不出来的情况，说明磁盘出了问题，同时也有这个删除无法删除的情况，就是磁盘上面的数据已经不能正常的处理请求了，数据混乱状态了</p><h3 id="情况二："><a href="#情况二：" class="headerlink" title="情况二："></a>情况二：</h3><p>原始对象丢失了，只剩下快照对象，从现场的情况看，快照499还在，对象还在，说明并没有删除快照的操作，然后这个原始对象没了，那么丢失的原始对象的可能性比较大<br>这个丢失什么情况造成的就没法判断了</p><p>测试验证了一个情况，就是只删除主本的原始对象，这种情况是可以修复的，那么现场的情况，应该是修复也无法修复了<br>说明还是应该有删除的操作，或者就是这个对象做快照的时候应该就删除了，但是这个异常节点并没有删除，做快照的时候就出现了快照对象，然后原始对象又没有了，就是出现了这种主本没有的情况了</p><h2 id="解决办法"><a href="#解决办法" class="headerlink" title="解决办法"></a>解决办法</h2><p>通过这个删除相关的对象，进行启动</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">ceph-objectstore-tool --data /var/lib/ceph/osd/ceph-1/ --pgid 1.75 <span class="hljs-string">&#x27;&#123;&quot;oid&quot;:&quot;rbd_data.606c9027f4e3.0000000000000064&quot;,&quot;key&quot;:&quot;&quot;,&quot;snapid&quot;:-2,&quot;hash&quot;:2791802613,&quot;max&quot;:0,&quot;pool&quot;:1,&quot;namespace&quot;:&quot;&quot;,&quot;max&quot;:0&#125;&#x27;</span> remove<br></code></pre></td></tr></table></figure><p>如果环境多个osd 出现down了。也需要按这个删除对象，避免osd 无法启动了<br>这种有快照对象没原始对象的就是异常对象的 ,删除的时候需要找到所有节点的这种情况进行pg内的快照对象的删除，否则无法启动osd</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>这个属于ceph里面中间垃圾数据引起的异常的情况，这个一般处理方法就是要么删除垃圾数据，要么有的情况无法删除，就构造对象后再删除,避免无法启动osd的情况造成pg的异常</p>]]></content>
    
    
    <categories>
      
      <category>存储系统</category>
      
    </categories>
    
    
    <tags>
      
      <tag>ceph</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>为什么只坏了一个盘集群无法读写</title>
    <link href="/2024/12/04/%E4%B8%BA%E4%BB%80%E4%B9%88%E5%8F%AA%E5%9D%8F%E4%BA%86%E4%B8%80%E4%B8%AA%E7%9B%98%E9%9B%86%E7%BE%A4%E6%97%A0%E6%B3%95%E8%AF%BB%E5%86%99/"/>
    <url>/2024/12/04/%E4%B8%BA%E4%BB%80%E4%B9%88%E5%8F%AA%E5%9D%8F%E4%BA%86%E4%B8%80%E4%B8%AA%E7%9B%98%E9%9B%86%E7%BE%A4%E6%97%A0%E6%B3%95%E8%AF%BB%E5%86%99/</url>
    
    <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>我们拿到故障环境，看到环境就坏一个osd，但是环境还是处于卡着的状态，这个时候客户肯定会问，怎么就坏一个盘，还无法用了，不是都配置了冗余么</p><p>这个地方我们来分析下这个问题的原因，坏一个osd只是结果，不是过程，我们看下过程发生了哪些状况</p><h2 id="中间过程"><a href="#中间过程" class="headerlink" title="中间过程"></a>中间过程</h2><p>这个中间过程我们用图来说过程比较清晰一些</p><p><img src="/images/blog/downosd.png" alt="存储阶段"></p><p>上面是时间线上的三个阶段的情况</p><ul><li><p>阶段一：<br>数据完整，没有任何问题，数据写三份，分布到三台机器</p></li><li><p>阶段二：<br>主机1sas卡出问题了，这个时候集群处于降级状态，数据只写了两份写到主机2，主机3，这个时候过了一段时间主机3突然又坏了，我们这个时候同时也发现主机1之前坏了，这个时候主机1启动起来，主机3是不可启动状态（磁盘异常无法恢复）</p></li><li><p>阶段三：<br>ceph集群判断状态正常是需要至少能恢复的数据，也就是如果是三份（ec2+1），至少需要两份数据是完整的，这个时候集群才能读能写，到这个阶段我们可以看到，中间黄色部分的数据，因为主机3现在无法恢复，主机1之前故障了，这个只有一份数据2还在，三份数据只有1份了，这个时候集群自己是没法让自己正常的，因为主机3如果能启动起来，那么数据就还是完整的，集群内部也无法知道主机3是否能启动（也就是看到的pg的提示建议标记lost或者启动）<br>这个时候集群就处于异常了，需要人工处理</p></li></ul><p>上面就是最终体现的现场的情况了，最终看到的是down一个osd，其实是因为down osd 之前，集群已经处于不正常状态了（注：现场发生了整机osddown和另外一个osddown ，发生的先后关系不影响上面的过程）</p><p><img src="/images/blog/incomplete.png" alt="incomplete"></p><p>官方的解释就是，无法有完整的pg做恢复的时候 就是这个状态，集群中间状态又出第二个问题的时候，就会有这种情况出现了 </p><h2 id="处理方法"><a href="#处理方法" class="headerlink" title="处理方法"></a>处理方法</h2><p>人工处理</p><ul><li>方法一：<br>1、最好的办法是把主机3的最后坏的那个osd启动起来，能拉起来，那么数据还是完整的，写入的数据就都不丢</li><li>方法二：<br>另外一个办法就是，主机1和主机2强制标记状态，那么中间那部分黄色的数据就掉了，这个数据破坏多大，就是这个中间的时间差写的数据了</li><li>方法三（操作不确定性比较大，不建议）：<br>看下主机3的pg是否可以全部导出，如果可以导出，可以导入到最新映射的地方，这样数据也能恢复</li></ul>]]></content>
    
    
    <categories>
      
      <category>存储相关</category>
      
    </categories>
    
    
    <tags>
      
      <tag>ceph</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>device_health_metrics正确的删除方法</title>
    <link href="/2024/11/21/device-health-metrics%E6%AD%A3%E7%A1%AE%E7%9A%84%E5%88%A0%E9%99%A4%E6%96%B9%E6%B3%95/"/>
    <url>/2024/11/21/device-health-metrics%E6%AD%A3%E7%A1%AE%E7%9A%84%E5%88%A0%E9%99%A4%E6%96%B9%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<h2 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab103 ceph]<span class="hljs-comment"># ceph -s</span><br>  cluster:<br>    <span class="hljs-built_in">id</span>:     4f5ce868-8389-489a-bd96-f2754ed6fa2f<br>    health: HEALTH_ERR<br>            mon is allowing insecure global_id reclaim<br>            Module <span class="hljs-string">&#x27;devicehealth&#x27;</span> has failed: [errno 2] RADOS object not found (error opening pool <span class="hljs-string">&#x27;b&#x27;</span>device_health_metrics<span class="hljs-string">&#x27;&#x27;</span>)<br>            1 pool(s) <span class="hljs-keyword">do</span> not have an application enabled<br>            1 pool(s) have no replicas configured<br><br>  services:<br>    mon: 1 daemons, quorum lab103 (age 11m)<br>    mgr: lab103(active, since 11m)<br>    mds:  1 up:standby<br>    osd: 1 osds: 1 up (since 22m), 1 <span class="hljs-keyword">in</span> (since 22m)<br><br>  data:<br>    pools:   1 pools, 32 pgs<br>    objects: 100 objects, 400 MiB<br>    usage:   1.4 GiB used, 3.6 TiB / 3.6 TiB avail<br>    pgs:     32 active+clean<br></code></pre></td></tr></table></figure><p>集群的状态如上面的</p><p>这个是因为之前操作过</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">ceph osd pool delete device_health_metrics  device_health_metrics --yes-i-really-really-mean-it<br></code></pre></td></tr></table></figure><p>删除了记录磁盘相关smart信息的存储池</p><p>那么这个时候如果恢复集群的状态</p><h2 id="操作方法"><a href="#操作方法" class="headerlink" title="操作方法"></a>操作方法</h2><p>我们删除了这个存储一般有两个情况</p><ul><li>一个是误删除了</li><li>真不想要这个存储池</li></ul><p>那么就是有两种情况，一种是需要关闭，一种是恢复原状，那么这里有两个处理方式</p><h3 id="恢复原状"><a href="#恢复原状" class="headerlink" title="恢复原状"></a>恢复原状</h3><p>恢复原状很简单，重启ceph-mgr即可，这个会自动创建这个存储池</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">ceph mgr fail <br></code></pre></td></tr></table></figure><p>用这个即可,这个会自动创建device_health_metrics,警告也会自动消失，需要稍微等一会</p><h3 id="真的要删除"><a href="#真的要删除" class="headerlink" title="真的要删除"></a>真的要删除</h3><p>真的要删除也要先让存储恢复，然后再按操作顺序删除，否则提示会一直在<br>也就是</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">ceph mgr fail<br></code></pre></td></tr></table></figure><p>等待恢复，告警消失后<br>禁用告警</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">ceph config  <span class="hljs-built_in">set</span> mgr   mgr/devicehealth/enable_monitoring <span class="hljs-literal">false</span><br></code></pre></td></tr></table></figure><p>然后再删除存储池</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">ceph osd pool delete device_health_metrics  device_health_metrics --yes-i-really-really-mean-it<br></code></pre></td></tr></table></figure><p>这样系统内部就不会找不到存储磁盘监控信息的对象了，也就不会提示异常了  </p>]]></content>
    
    
    <categories>
      
      <category>存储相关</category>
      
    </categories>
    
    
    <tags>
      
      <tag>ceph</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>earlyoom预防机器卡死</title>
    <link href="/2024/11/14/earlyoom%E9%A2%84%E9%98%B2%E6%9C%BA%E5%99%A8%E5%8D%A1%E6%AD%BB/"/>
    <url>/2024/11/14/earlyoom%E9%A2%84%E9%98%B2%E6%9C%BA%E5%99%A8%E5%8D%A1%E6%AD%BB/</url>
    
    <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>机器还没来得及oom，机器就出现挂死的状态，swap无法交换出，或者直接挂死</p><p>这个问题比较好复现<br>在机器上面一直进行内存的申请即可</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">python3 memory.py 100 40000<br></code></pre></td></tr></table></figure><p>这个在x86上面做测试的时候，系统能够比较快的oom，但是这个板卡的系统盘本身慢，这个就可能出现卡顿的情况了</p><p>系统的oom，需要进行一些计算和系统处理，并且有个问题是，很多进程都不杀，因为都是系统进程，很多是-1000</p><p>优先级又很低，oom并不能释放太多内存，无法及时释放内存就会卡死机器了</p><h2 id="参数调整"><a href="#参数调整" class="headerlink" title="参数调整"></a>参数调整</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">vm.min_free_kbytes=512144<br></code></pre></td></tr></table></figure><p>这个我们采用512MB来作为系统的保证内存，这个是oom的判断条件，默认的16MB太小了，并不足以保证运行的环境，这个后面可以根据实际情况再调整<br>其它参数维持系统之前的参数（系统上面看到是这个）</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">vm.overcommit_memory = 1<br></code></pre></td></tr></table></figure><p>这个0是默认的，1就是无限分配内存，这个压测更极端的情况</p><p>earlyoom配置参数</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs bash">root@myserver:~<span class="hljs-comment"># cat /etc/default/earlyoom</span><br><span class="hljs-comment"># Default settings for earlyoom. This file is sourced by /bin/sh from</span><br><span class="hljs-comment"># /etc/init.d/earlyoom.</span><br><br><span class="hljs-comment"># Options to pass to earlyoom</span><br>EARLYOOM_ARGS=<span class="hljs-string">&quot;&quot;</span><br><br><span class="hljs-comment"># Examples:</span><br><br><span class="hljs-comment"># Available minimum memory 5%</span><br><span class="hljs-comment"># EARLYOOM_ARGS=&quot;-m 5&quot;</span><br><br><span class="hljs-comment"># Available minimum memory 15% and free minimum swap 5%</span><br>EARLYOOM_ARGS=<span class="hljs-string">&quot;-m 5 -s 4&quot;</span><br><br><span class="hljs-comment"># Use kernel oom killer</span><br><span class="hljs-comment"># EARLYOOM_ARGS=&quot;-k&quot;</span><br><br><span class="hljs-comment"># See more at `earlyoom -h&#x27;</span><br><br></code></pre></td></tr></table></figure><p>其中的-m 5 -s 4<br>是物理内存5%  swap %4<br>对应到794 MiB<br>swap剩余327 MiB<br>这个就是杀进程的判断点，系统剩余多少内存的时候介入，这两个加起来是1G左右，操作系统的判断的时候没有区分swap和物理内存，这个地方会更精细，这个设置的可以后面再调整更小，也可以根据系统需要的内存来看，这个总共留1G也不算太大</p><p>下面可以看到</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs bash">root@myserver:~<span class="hljs-comment"># systemctl status earlyoom</span><br>● earlyoom.service - Early OOM Daemon<br>     Loaded: loaded (/lib/systemd/system/earlyoom.service; enabled; vendor preset: enabled)<br>     Active: active (running) since Thu 2024-11-14 14:56:57 +08; 2s ago<br>       Docs: https://github.com/rfjakob/earlyoom<br>   Main PID: 41953 (earlyoom)<br>      Tasks: 1 (<span class="hljs-built_in">limit</span>: 9830)<br>     Memory: 1.0M<br>     CGroup: /system.slice/earlyoom.service<br>             └─41953 /usr/bin/earlyoom -m 5 -s 4<br><br>11月 14 14:56:57 myserver earlyoom[41953]: earlyoom v0.12<br>11月 14 14:56:57 myserver earlyoom[41953]: mem total: 15895 MiB, min: 794 MiB (5 %)<br>11月 14 14:56:57 myserver earlyoom[41953]: swap total: 8191 MiB, min: 327 MiB (4 %)<br><br></code></pre></td></tr></table></figure><p>运行效果可以看到，实际物理内存已经基本耗尽了，但是系统自带的oom并没有触发，我们用的这个earlyoom也会有一点滞后，但是并没卡死系统</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>1、调整下系统最小保留内存<br>2、设置earlyoom避免内存耗尽<br>3、板卡的内存耗尽到oom之间应该存在滞后，这个early可以避免没来得及oom的情况</p><p>验证大内存压力情况下的稳定性<br>剩余的内存的地方可以根据实际情况进行调整，上面是预估的保留内存（可以测试验证看下情况）</p><h2 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h2><p>占用内存脚本 memory.py</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-meta">#! /usr/bin/python3</span><br>import time<br>import sys<br><br>def allocate_memory(allocation_size_mb, total_memory_mb):<br>    <span class="hljs-comment"># 将 MB 转换为字节</span><br>    allocation_size_bytes = allocation_size_mb * 1024 * 1024<br>    total_memory_bytes = total_memory_mb * 1024 * 1024<br><br>    <span class="hljs-comment"># 初始化用于存储分配数据的列表</span><br>    allocated_memory = []<br><br>    <span class="hljs-comment"># 追踪已分配的总内存量</span><br>    allocated_total = 0<br><br>    try:<br>        <span class="hljs-keyword">while</span> allocated_total &lt; total_memory_bytes:<br>            <span class="hljs-comment"># 每次分配内存</span><br>            allocated_memory.append(bytearray(allocation_size_bytes))<br>            allocated_total += allocation_size_bytes<br>            <span class="hljs-built_in">print</span>(f<span class="hljs-string">&quot;Allocated: &#123;allocated_total / (1024 ** 2):.2f&#125; MB&quot;</span>)<br>            time.sleep(0.5)  <span class="hljs-comment"># 使内存增长速度更缓慢，便于观察</span><br>    except MemoryError:<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Memory allocation failed. The system ran out of memory.&quot;</span>)<br><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Memory allocation complete. Holding memory...&quot;</span>)<br><br>    <span class="hljs-comment"># 无限循环，保持进程不退出</span><br>    <span class="hljs-keyword">while</span> True:<br>        time.sleep(1)<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    <span class="hljs-comment"># 获取命令行参数</span><br>    <span class="hljs-keyword">if</span> len(sys.argv) != 3:<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Usage: python script.py &lt;allocation_size_mb&gt; &lt;total_memory_mb&gt;&quot;</span>)<br>        sys.exit(1)<br><br>    <span class="hljs-comment"># 解析参数</span><br>    allocation_size_mb = int(sys.argv[1])<br>    total_memory_mb = int(sys.argv[2])<br><br>    <span class="hljs-comment"># 调用内存分配函数</span><br>    allocate_memory(allocation_size_mb, total_memory_mb)<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>系统管理</category>
      
    </categories>
    
    
    <tags>
      
      <tag>内存管理</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>docker配置代理下载镜像</title>
    <link href="/2024/11/11/docker%E9%85%8D%E7%BD%AE%E4%BB%A3%E7%90%86%E4%B8%8B%E8%BD%BD%E9%95%9C%E5%83%8F/"/>
    <url>/2024/11/11/docker%E9%85%8D%E7%BD%AE%E4%BB%A3%E7%90%86%E4%B8%8B%E8%BD%BD%E9%95%9C%E5%83%8F/</url>
    
    <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>需要下载镜像，但是无法下载</p><h2 id="处理方法"><a href="#处理方法" class="headerlink" title="处理方法"></a>处理方法</h2><h3 id="在macos上面开启端口转发"><a href="#在macos上面开启端口转发" class="headerlink" title="在macos上面开启端口转发"></a>在macos上面开启端口转发</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">socat TCP4-LISTEN:25433,fork TCP4:127.0.0.1:25432<br></code></pre></td></tr></table></figure><h3 id="在需要下载的机器上面配置docker代理"><a href="#在需要下载的机器上面配置docker代理" class="headerlink" title="在需要下载的机器上面配置docker代理"></a>在需要下载的机器上面配置docker代理</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs bash">vim /lib/systemd/system/docker.service<br><br>[Service]<br>Environment=<span class="hljs-string">&quot;HTTP_PROXY=http://192.168.0.225:25433&quot;</span><br>Environment=<span class="hljs-string">&quot;HTTPS_PROXY=http://192.168.0.225:25433&quot;</span><br><br></code></pre></td></tr></table></figure><p>ip为代理机器的ip</p><h3 id="重启服务"><a href="#重启服务" class="headerlink" title="重启服务"></a>重启服务</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">systemctl daemon-reload<br>systemctl restart docker<br></code></pre></td></tr></table></figure><p>检查代理配置</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">systemctl show --property=Environment docker<br></code></pre></td></tr></table></figure><p>然后就可以开始使用了</p><p>不用的时候就还原回来</p>]]></content>
    
    
    <categories>
      
      <category>系统服务</category>
      
    </categories>
    
    
    <tags>
      
      <tag>docker</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>cephfs统计稀疏文件大小的脚本</title>
    <link href="/2024/11/07/cephfs%E7%BB%9F%E8%AE%A1%E7%A8%80%E7%96%8F%E6%96%87%E4%BB%B6%E5%A4%A7%E5%B0%8F%E7%9A%84%E8%84%9A%E6%9C%AC/"/>
    <url>/2024/11/07/cephfs%E7%BB%9F%E8%AE%A1%E7%A8%80%E7%96%8F%E6%96%87%E4%BB%B6%E5%A4%A7%E5%B0%8F%E7%9A%84%E8%84%9A%E6%9C%AC/</url>
    
    <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>cephfs的df看到的是真实的容量的占用的，ll是看到文件的元数据大小的，du一般是统计文件的真实占用的大小的，但是cephfs并没有记录文件内的占用的情况,所以du无法统计到真实占用</p><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><p>我们先拿到全部的inode编号</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">ls</span> -i -R  /mnt &gt; inode.list<br></code></pre></td></tr></table></figure><p>拿到全部的对象名称</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">rados -p rbd <span class="hljs-built_in">ls</span> &gt; object.list<br></code></pre></td></tr></table></figure><p>写一个脚本</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-meta">#! /bin/bash</span><br><br><span class="hljs-built_in">cat</span> inode.list|<span class="hljs-keyword">while</span> <span class="hljs-built_in">read</span> -r line;<span class="hljs-keyword">do</span><br><br><span class="hljs-keyword">if</span> [[ <span class="hljs-variable">$line</span> == *:* ]];<span class="hljs-keyword">then</span><br>        <span class="hljs-built_in">echo</span> <span class="hljs-variable">$line</span><br><span class="hljs-keyword">elif</span> [[ -z <span class="hljs-variable">$line</span> ]];<span class="hljs-keyword">then</span><br><br>   <span class="hljs-built_in">echo</span> <span class="hljs-string">&quot; &quot;</span><br><br><span class="hljs-keyword">else</span><br>        inode=`<span class="hljs-built_in">echo</span> <span class="hljs-variable">$line</span>|awk <span class="hljs-string">&#x27;&#123;print $1&#125;&#x27;</span>`<br>        filename=`<span class="hljs-built_in">echo</span> <span class="hljs-variable">$line</span>|awk <span class="hljs-string">&#x27;&#123;print $2&#125;&#x27;</span>`<br>        objectpre=$(<span class="hljs-built_in">printf</span> <span class="hljs-string">&quot;%x&quot;</span> <span class="hljs-string">&quot;<span class="hljs-variable">$inode</span>&quot;</span> )<br>        objnum=`<span class="hljs-built_in">cat</span> object.list|grep <span class="hljs-variable">$objectpre</span>|<span class="hljs-built_in">wc</span> -l`<br>        result=$(<span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;scale=3; <span class="hljs-variable">$objnum</span>*4 / 1000&quot;</span>|bc)<br>        result=`<span class="hljs-built_in">printf</span> <span class="hljs-string">&quot;%.3f\n&quot;</span> <span class="hljs-variable">$result</span>`<br>        <span class="hljs-built_in">echo</span> <span class="hljs-variable">$&#123;result&#125;</span>GB <span class="hljs-variable">$filename</span><br><span class="hljs-keyword">fi</span><br><br><span class="hljs-keyword">done</span><br><br></code></pre></td></tr></table></figure><p>这里我们只需要的是大概的占用情况,直接就是预估的单个对象4M的，这个属于估算的，如果想到非常精确的，那么就需要用rados -p data stat 拿到每一个对象的大小，然后在后面做计算的时候，把每个的大小加起来，这个是可以拿到精准的大小的占用的<br>我们这里因为文件都是大文件，只需要预估即可</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>通过对象的统计，可以计算出文件的真实大小占用</p>]]></content>
    
    
    <categories>
      
      <category>存储相关</category>
      
    </categories>
    
    
    <tags>
      
      <tag>ceph</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>iis无法访问samba的文件的问题</title>
    <link href="/2024/11/07/iis%E6%97%A0%E6%B3%95%E8%AE%BF%E9%97%AEsamba%E7%9A%84%E6%96%87%E4%BB%B6%E7%9A%84%E9%97%AE%E9%A2%98/"/>
    <url>/2024/11/07/iis%E6%97%A0%E6%B3%95%E8%AE%BF%E9%97%AEsamba%E7%9A%84%E6%96%87%E4%BB%B6%E7%9A%84%E9%97%AE%E9%A2%98/</url>
    
    <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>配置iis的数据目录为samba的共享目录，目录可以访问，文件不能访问</p><h2 id="处理方法"><a href="#处理方法" class="headerlink" title="处理方法"></a>处理方法</h2><p>iis在访问samba的文件的时候，默认把文件全部转换成大写的路径去发送的请求，而linux是区分大小写的，访问的时候就无法访问到这个小写的文件</p><p>所以需要配置samba忽略大小写</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-keyword">case</span> sensitive = <span class="hljs-built_in">yes</span><br></code></pre></td></tr></table></figure><p>改成</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-keyword">case</span> sensitive = no<br></code></pre></td></tr></table></figure><p>然后重启samba即可</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>大小写是否需要设置忽略我们可以根据实际场景进行配置，不同场景的需求不一样</p>]]></content>
    
    
    <categories>
      
      <category>存储相关</category>
      
    </categories>
    
    
    <tags>
      
      <tag>samba</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>修改cephmon的ip</title>
    <link href="/2024/11/04/%E4%BF%AE%E6%94%B9cephmon%E7%9A%84ip/"/>
    <url>/2024/11/04/%E4%BF%AE%E6%94%B9cephmon%E7%9A%84ip/</url>
    
    <content type="html"><![CDATA[<h2 id="需求"><a href="#需求" class="headerlink" title="需求"></a>需求</h2><p>修改mon的ip</p><h2 id="操作步骤"><a href="#操作步骤" class="headerlink" title="操作步骤"></a>操作步骤</h2><h3 id="卸载客户端挂载的服务"><a href="#卸载客户端挂载的服务" class="headerlink" title="卸载客户端挂载的服务"></a>卸载客户端挂载的服务</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab103 ~]<span class="hljs-comment"># umount /mnt</span><br></code></pre></td></tr></table></figure><h3 id="停掉mds的服务"><a href="#停掉mds的服务" class="headerlink" title="停掉mds的服务"></a>停掉mds的服务</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab103 ~]<span class="hljs-comment"># systemctl stop ceph-mds@lab103</span><br></code></pre></td></tr></table></figure><h3 id="停掉osd的服务"><a href="#停掉osd的服务" class="headerlink" title="停掉osd的服务"></a>停掉osd的服务</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab103 ~]<span class="hljs-comment"># systemctl stop ceph-osd.target</span><br></code></pre></td></tr></table></figure><h3 id="停掉管理服务"><a href="#停掉管理服务" class="headerlink" title="停掉管理服务"></a>停掉管理服务</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab103 ~]<span class="hljs-comment"># systemctl stop ceph-mgr@lab103</span><br>[root@lab103 ~]<span class="hljs-comment"># systemctl stop ceph-mon@lab103</span><br></code></pre></td></tr></table></figure><h3 id="备份mon的数据"><a href="#备份mon的数据" class="headerlink" title="备份mon的数据"></a>备份mon的数据</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># cp -ra /var/lib/ceph/mon/ceph-lab103/ /opt/ceph-lab103bk</span><br></code></pre></td></tr></table></figure><h3 id="编辑monmap"><a href="#编辑monmap" class="headerlink" title="编辑monmap"></a>编辑monmap</h3><p>获取monmap</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab103 mon]<span class="hljs-comment"># ceph-mon -i lab103 --extract-monmap ./monmap</span><br>2024-11-04 11:11:11.046678 7f5f75107000 -1 wrote monmap to ./monmap<br>[root@lab103 mon]<span class="hljs-comment"># monmaptool --print monmap</span><br>monmaptool: monmap file monmap<br>epoch 1<br>fsid 4f5ce868-8389-489a-bd96-f2754ed6fa2f<br>last_changed 2024-10-18 14:45:39.259449<br>created 2024-10-18 14:45:39.259449<br>0: 192.168.19.103:6789/0 mon.lab103<br></code></pre></td></tr></table></figure><p>从monmap删除旧的节点信息</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab103 mon]<span class="hljs-comment"># monmaptool ./monmap --rm   lab103</span><br>monmaptool: monmap file ./monmap<br>monmaptool: removing lab103<br>monmaptool: writing epoch 1 to ./monmap (0 monitors)<br></code></pre></td></tr></table></figure><p>添加新的mon的信息</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab103 mon]<span class="hljs-comment"># monmaptool ./monmap  --add lab103 192.168.19.105:6789</span><br>monmaptool: monmap file ./monmap<br>monmaptool: writing epoch 1 to ./monmap (1 monitors)<br></code></pre></td></tr></table></figure><p>查看monmap的信息</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab103 mon]<span class="hljs-comment"># monmaptool --print ./monmap</span><br>monmaptool: monmap file ./monmap<br>epoch 1<br>fsid 4f5ce868-8389-489a-bd96-f2754ed6fa2f<br>last_changed 2024-10-18 14:45:39.259449<br>created 2024-10-18 14:45:39.259449<br>0: 192.168.19.105:6789/0 mon.lab103<br></code></pre></td></tr></table></figure><h3 id="导入monmap到mon的数据"><a href="#导入monmap到mon的数据" class="headerlink" title="导入monmap到mon的数据"></a>导入monmap到mon的数据</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab103 mon]<span class="hljs-comment"># ceph-mon -i lab103 --inject-monmap ./monmap</span><br></code></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab103 mon]<span class="hljs-comment"># chown ceph:ceph -R /var/lib/ceph/mon/ceph-lab103/</span><br></code></pre></td></tr></table></figure><h3 id="修改配置文件"><a href="#修改配置文件" class="headerlink" title="修改配置文件"></a>修改配置文件</h3><p>修改为当前机器新的对应的主机名和ip<br>修改&#x2F;etc&#x2F;ceph&#x2F;ceph.conf</p><h3 id="启动mon"><a href="#启动mon" class="headerlink" title="启动mon"></a>启动mon</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab103 mon]<span class="hljs-comment"># systemctl start ceph-mon@lab103</span><br>[root@lab103 mon]<span class="hljs-comment"># systemctl start  ceph-osd.target</span><br></code></pre></td></tr></table></figure><h3 id="启动mds"><a href="#启动mds" class="headerlink" title="启动mds"></a>启动mds</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab103 mon]<span class="hljs-comment"># systemctl start  ceph-mds@lab103</span><br></code></pre></td></tr></table></figure><h3 id="启动客户端服务"><a href="#启动客户端服务" class="headerlink" title="启动客户端服务"></a>启动客户端服务</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab103 mon]<span class="hljs-comment">#  mount -t ceph 192.168.19.105:/ /mnt</span><br></code></pre></td></tr></table></figure><p>以上的操作后就完成了mon的ip的替换工作</p>]]></content>
    
    
    <categories>
      
      <category>存储相关</category>
      
    </categories>
    
    
    <tags>
      
      <tag>ceph</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>systemd病毒定位和处理</title>
    <link href="/2024/09/24/systemd%E7%97%85%E6%AF%92%E5%AE%9A%E4%BD%8D%E5%92%8C%E5%A4%84%E7%90%86/"/>
    <url>/2024/09/24/systemd%E7%97%85%E6%AF%92%E5%AE%9A%E4%BD%8D%E5%92%8C%E5%A4%84%E7%90%86/</url>
    
    <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>内网测试环境出现一台机器上面的systemd返回值异常，其它都正常，具体的现象如下</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">systemctl restart smb;<span class="hljs-built_in">echo</span> $?<br></code></pre></td></tr></table></figure><p>这个返回的是1，正常执行完毕应该是0</p><h2 id="问题定位"><a href="#问题定位" class="headerlink" title="问题定位"></a>问题定位</h2><p>开始的时候以为是网络的问题，通过执行本地的命令发现，任何systemd的执行命令返回的都是1，但是实际成功了<br>开始并没有往病毒方面想，系统没有定时任务，没有异常的cpu负载</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab103 ~]<span class="hljs-comment"># strace systemctl restart rsyslogd</span><br></code></pre></td></tr></table></figure><p>后面是通过这个命令定位到出现异常了，我们复现下病毒的操作</p><p>通过把systemctl拷贝一份到本地的另外一个路径</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab103 ~]<span class="hljs-comment"># cp -ra /usr/bin/systemctl /usr/bin/sys</span><br></code></pre></td></tr></table></figure><p>使用一个新的脚本替换，这里我用的脚本，实际是二进制，看不到内容，效果差不多</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab103 ~]<span class="hljs-comment"># cat /usr/bin/systemctl</span><br><span class="hljs-comment">#! /bin/bash</span><br><span class="hljs-built_in">echo</span> a &gt; /tmp/test<br>/usr/bin/sys <span class="hljs-variable">$1</span> <span class="hljs-variable">$2</span> <span class="hljs-variable">$3</span> <span class="hljs-variable">$4</span> <span class="hljs-variable">$5</span> <span class="hljs-variable">$6</span> <span class="hljs-variable">$7</span><br><span class="hljs-built_in">exit</span> 1<br></code></pre></td></tr></table></figure><p>上面的exit 1是模拟的病毒执行的命令返回值，这个地方应该是失败了，返回的1，但是命令本身执行没有问题，所以不容易发现</p><p>我们看下现在的strace的值</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs bash">strace systemctl  restart rsyslog<br><span class="hljs-built_in">read</span>(3, <span class="hljs-string">&quot;#! /bin/bash\necho a &gt; /tmp/test\n&quot;</span>..., 80) = 73<br>lseek(3, 0, SEEK_SET)                   = 0<br>getrlimit(RLIMIT_NOFILE, &#123;rlim_cur=1024, rlim_max=4*1024&#125;) = 0<br>fcntl(255, F_GETFD)                     = -1 EBADF (Bad file descriptor)<br>dup2(3, 255)                            = 255<br>close(3)                                = 0<br>fcntl(255, F_SETFD, FD_CLOEXEC)         = 0<br></code></pre></td></tr></table></figure><p>这里我们是抓到了一个异常的路径，实际环境是捕获到一个&#x2F;usr&#x2F;bin&#x2F;sys文件执行</p><p>这里基本可以定位到环境已经被替换了systemctl，这个地方确定病毒后，还发现了一些问题</p><p>lsattr命令被删除了<br>chattr也被替换了</p><p>上面都是常规的搞法，我们需要进行环境恢复，其实最开始的时候发现做dnf 升级systemd的时候就无法升级，没想到是命令被替换并锁定了，以为是库本身有问题</p><h2 id="环境恢复"><a href="#环境恢复" class="headerlink" title="环境恢复"></a>环境恢复</h2><p>首先要恢复lsattr和chattr命令</p><p>下载软件包e2fsprogs<br>解压拿到二进制</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">rpm2cpio  e2fsprogs-xxx-xxx.rpm|cpio -div<br></code></pre></td></tr></table></figure><p>拿到后使用命令解除锁定</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">chattr -ia  filename<br></code></pre></td></tr></table></figure><p>解除锁定以后就可以进行替换的操作，把文件还原了</p><p>systemctl直接用&#x2F;usr&#x2F;bin&#x2F;sys进行还原即可，这个检查下二进制的md5值确认下就行</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>环境出现问题后，可以排查下环境是不是有下面的异常来大概判断环境是不是有上面的这种病毒情况</p><ul><li>检查systemctl的md5值</li><li>检查lsattr命令的md5值</li><li>检查chattr命令的md5值</li><li>检查&#x2F;usr&#x2F;bin&#x2F;sys路径</li></ul><p>上面的问题可能出现各种不同的路径或者命令，这个可以类似的进行排查即可</p>]]></content>
    
    
    <categories>
      
      <category>系统管理</category>
      
    </categories>
    
    
    <tags>
      
      <tag>异常处理</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>获取docker的镜像的方法</title>
    <link href="/2024/09/23/%E8%8E%B7%E5%8F%96docker%E7%9A%84%E9%95%9C%E5%83%8F%E7%9A%84%E6%96%B9%E6%B3%95/"/>
    <url>/2024/09/23/%E8%8E%B7%E5%8F%96docker%E7%9A%84%E9%95%9C%E5%83%8F%E7%9A%84%E6%96%B9%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>需要下载一个mysql的docker的镜像，但是遇到了问题，这里把相关的方法记录下</p><h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab103 data]<span class="hljs-comment"># docker pull mysql:8.0</span><br>Trying to pull repository docker.io/library/mysql ...<br>Get https://registry-1.docker.io/v2/: net/http: request canceled <span class="hljs-keyword">while</span> waiting <span class="hljs-keyword">for</span> connection (Client.Timeout exceeded <span class="hljs-keyword">while</span> awaiting headers)<br></code></pre></td></tr></table></figure><p>直接下载的时候无法下载，这个需要给这个做个代理，但是机器可能是内网的机器，那么我们可以下载下来</p><h2 id="处理方法"><a href="#处理方法" class="headerlink" title="处理方法"></a>处理方法</h2><p>我的mac本身可以直接联通上面的外网地址，那么可以通过下载一个docker客户端，然后给客户端设置代理，然后下载，把镜像保存好即可</p><p>具体操作</p><h3 id="下载docker桌面"><a href="#下载docker桌面" class="headerlink" title="下载docker桌面"></a>下载docker桌面</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">https://www.docker.com/products/docker-desktop/<br></code></pre></td></tr></table></figure><p>在官方下载即可</p><h3 id="配置代理"><a href="#配置代理" class="headerlink" title="配置代理"></a>配置代理</h3><p><img src="/images/blog/2024-09-23-15-48-20.png"></p><p>这个填写自己本地的代理地址即可</p><p>上面配置代理后，就可以下载了</p><h3 id="搜索下载的镜像"><a href="#搜索下载的镜像" class="headerlink" title="搜索下载的镜像"></a>搜索下载的镜像</h3><p><img src="/images/blog/2024-09-23-15-49-28.png"></p><p>这个地方下载的是arm64架构的，我们需要x86的就需要命令行下载</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">docker pull --platform amd64 mysql:8.0<br></code></pre></td></tr></table></figure><h3 id="查询导出"><a href="#查询导出" class="headerlink" title="查询导出"></a>查询导出</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">zphj1987@zphj1987deMacBook-Pro docker % docker image <span class="hljs-built_in">ls</span><br>REPOSITORY   TAG       IMAGE ID       CREATED        SIZE<br>mysql        8.0       f5da8fc4b539   2 months ago   573MB<br>zphj1987@zphj1987deMacBook-Pro docker % docker save  f5da8fc4b539 &gt;  mysql8.0.tar<br></code></pre></td></tr></table></figure><h3 id="导入"><a href="#导入" class="headerlink" title="导入"></a>导入</h3><p>传输到需要用的机器上面</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab103 data]<span class="hljs-comment"># docker  load -i  mysql8.0.tar</span><br>sha256:d0e2993cf4498a437735cd6b74253e1e541da55198240567a37570d51ee4599a<br>[root@lab103 data]<span class="hljs-comment"># docker image ls</span><br>REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE<br>&lt;none&gt;              &lt;none&gt;              d0e2993cf449        5 seconds ago       607 MB<br>[root@lab103 data]<span class="hljs-comment"># docker tag d0e2993cf449 mysql:8.0</span><br>[root@lab103 data]<span class="hljs-comment"># docker image ls</span><br>REPOSITORY          TAG                 IMAGE ID            CREATED              SIZE<br>mysql               8.0                 d0e2993cf449        About a minute ago   607 MB<br></code></pre></td></tr></table></figure><p>上面的就完成了导入和重命名的操作</p><p>注意，上面的导出命令用save，导入命令使用load，这个会保留image里面的执行命令，也就是跟pull的是相同的<br>如果用export和import，这个启动的时候会有问题，提示命令找不到</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab103 backup]<span class="hljs-comment"># docker run -p 3307:3306 --name mysql8 -v /data/mysql/data:/var/lib/mysql  -e MYSQL_ROOT_PASSWORD=123456 -d mysql:8.0</span><br>/usr/bin/docker-current: Error response from daemon: No <span class="hljs-built_in">command</span> specified.<br>See <span class="hljs-string">&#x27;/usr/bin/docker-current run --help&#x27;</span>.<br></code></pre></td></tr></table></figure><h2 id="备注"><a href="#备注" class="headerlink" title="备注"></a>备注</h2><p>查询容器启动的执行命令</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">docker ps -a --no-trunc<br></code></pre></td></tr></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>上面的操作就完成了下载和导入的操作</p>]]></content>
    
    
    <categories>
      
      <category>系统管理</category>
      
    </categories>
    
    
    <tags>
      
      <tag>docker</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>使用qemu构建一个loongarch64虚拟机</title>
    <link href="/2024/09/11/%E4%BD%BF%E7%94%A8qemu%E6%9E%84%E5%BB%BA%E4%B8%80%E4%B8%AAloongarch64%E8%99%9A%E6%8B%9F%E6%9C%BA/"/>
    <url>/2024/09/11/%E4%BD%BF%E7%94%A8qemu%E6%9E%84%E5%BB%BA%E4%B8%80%E4%B8%AAloongarch64%E8%99%9A%E6%8B%9F%E6%9C%BA/</url>
    
    <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>国产化的平台目前主要用到的有飞腾和龙芯，飞腾的是arm64架构，这个使用的比较多，龙芯的架构之前是mips架构，最新的硬件的架构已经发生了改变，现在是loongarch64架构，这个算比较新的架构，所以硬件就比较难获得，如果又有适配的需求，这个时候通过虚拟机启动环境是一个比较好的方式</p><p>虚拟机有两种方式：</p><ul><li>一种是在相同架构下开虚拟机，这个比较简单，一般的操作系统内都带了kvm的相关的软件，直接使用qemu-kvm启动虚拟机就可以了，这个方式的前提是你得有一台这个架构的物理机器，才能在上面启动kvm虚拟机，还存在资源共用的问题了</li><li>另外一种方式就是通过qemu完全软件来进行启动这个虚拟机，qemu是在软件内实现了硬件的模拟，一般硬件厂商会给这个做一些适配工作，然后就支持虚拟厂商的硬件了，这个对宿主机环境没有要求</li></ul><p>上面的方式二在之前尝试过，可能当时厂商对qemu还没开发完整，所以整个软件链路我自己当时是没有跑起来的，最近因为软件适配的问题，再次尝试，能够启动成功了，本篇就是把这个过程记录下来</p><h2 id="准备的资源"><a href="#准备的资源" class="headerlink" title="准备的资源"></a>准备的资源</h2><ul><li>准备一个x86的主机，性能尽量好点，这样虚拟机也能快一点</li><li>准备一个操作系统，这个操作系统最好系统版本高一点，centos8或者8以上的stream都可以，因为新版本的qemu里面用到的库的版本比较高，所以操作系统尽量高一点，这个依赖就比较好找了</li><li>qemu的软件，这个用最新版本的即可</li><li>loongarch64的qemu的一些运行的二进制文件</li><li>一个支持loongarch64的操作系统，这个国产操作系统基本都做了适配，这个我直接下载的操作系统，没有走iso安装的步骤，这个后面再研究，如果不好安装，也可以在kvm下把系统安装好了以后，把系统文件弄出来给qemu使用即可，这里直接下载官方提供的，本次使用的是opencloudos的龙芯操作系统</li></ul><h3 id="操作系统"><a href="#操作系统" class="headerlink" title="操作系统"></a>操作系统</h3><p>这里为了方便，主机的操作系统我选择的是opencloudos的x86系统</p><blockquote><p><a href="https://mirrors.opencloudos.tech/opencloudos-stream/releases/23/images/x86_64/OpenCloudOS-Stream-23-20240304-minimal-x86_64.iso">https://mirrors.opencloudos.tech/opencloudos-stream/releases/23/images/x86_64/OpenCloudOS-Stream-23-20240304-minimal-x86_64.iso</a></p></blockquote><p>下载的是minimal的，后面根据需要安装包即可</p><h2 id="配置方法"><a href="#配置方法" class="headerlink" title="配置方法"></a>配置方法</h2><h3 id="编译qemu"><a href="#编译qemu" class="headerlink" title="编译qemu"></a>编译qemu</h3><p>安装后面会用到的依赖包</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ~]<span class="hljs-comment"># dnf install wget gcc python3-sphinx python3-sphinx_rtd_theme ninja-build glib2-devel usbredir-devel libusbx-devel git bridge-utils net-tools</span><br></code></pre></td></tr></table></figure><p>有个libusbredirparser-0.5包冲突处理下</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ~]<span class="hljs-comment"># rpm -e libusbx --nodeps</span><br>[root@lab101 ~]<span class="hljs-comment"># dnf install libusb1-devel</span><br></code></pre></td></tr></table></figure><p>下载qemu源码包</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ~]<span class="hljs-comment"># wget https://download.qemu.org/qemu-9.0.2.tar.xz</span><br></code></pre></td></tr></table></figure><p>解压</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ~]<span class="hljs-comment"># tar -xvf qemu-9.0.2.tar.xz</span><br></code></pre></td></tr></table></figure><p>编译</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ~]<span class="hljs-comment"># cd qemu-9.0.2</span><br>[root@lab101 qemu-9.0.2]<span class="hljs-comment"># ./configure --disable-rdma --prefix=/usr             --target-list=&quot;loongarch64-softmmu&quot;             --disable-libiscsi --disable-libnfs --disable-libpmem             --disable-glusterfs --enable-libusb --enable-usb-redir             --disable-opengl --disable-xen --disable-spice             --enable-debug --disable-capstone --disable-kvm</span><br>[root@lab101 qemu-9.0.2]<span class="hljs-comment"># make -j24</span><br></code></pre></td></tr></table></figure><p>完成后检查输出</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 qemu-9.0.2]<span class="hljs-comment"># /root/qemu-9.0.2/build/qemu-system-loongarch64 -M ?</span><br>Supported machines are:<br>none                 empty machine<br>virt                 Loongson-3A5000 LS7A1000 machine (default)<br></code></pre></td></tr></table></figure><p>可以看到支持模拟的是 Loongson-3A5000</p><h3 id="配置网桥"><a href="#配置网桥" class="headerlink" title="配置网桥"></a>配置网桥</h3><p>宿主机配置一个网桥</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ~]<span class="hljs-comment"># cat br0.sh</span><br>ifconfig p786p1 down<br>ifconfig p786p1 0.0.0.0<br>brctl addbr br0<br>brctl addif br0 p786p1<br>ifconfig br0 192.168.19.101/16 up<br>brctl stp br0 off<br>route add default gw 192.168.1.1 br0<br></code></pre></td></tr></table></figure><h3 id="下载固件和镜像"><a href="#下载固件和镜像" class="headerlink" title="下载固件和镜像"></a>下载固件和镜像</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ~]<span class="hljs-comment"># mkdir /vm</span><br>[root@lab101 ~]<span class="hljs-comment"># cd /vm/</span><br>[root@lab101 vm]<span class="hljs-comment"># wget https://mirrors.opencloudos.tech/opencloudos-stream/releases/23/images/loongarch64/OpenCloudOS-Stream-23-20240808-loongarch64-preview.qcow2</span><br>[root@lab101 vm]<span class="hljs-comment"># wget https://github.com/yangxiaojuan-loongson/qemu-binary/releases/download/2024-05-30/QEMU_EFI.fd</span><br></code></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 vm]<span class="hljs-comment"># mkdir -p /root/qemu-9.0.2/build/qemu-bundle/etc/qemu/</span><br>[root@lab101 vm]<span class="hljs-comment"># vi /root/qemu-9.0.2/build/qemu-bundle/etc/qemu/bridge.conf</span><br></code></pre></td></tr></table></figure><p>添加</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">allow br0<br></code></pre></td></tr></table></figure><p>启动虚拟机</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">/root/qemu-9.0.2/build/qemu-system-loongarch64 -M virt -m 12G -cpu la464 -smp 12 -bios /vm/QEMU_EFI.fd -drive file=/vm/OpenCloudOS-Stream-23-20240808-loongarch64-preview.qcow2  -net nic  -net bridge,br=br0  --nographic<br></code></pre></td></tr></table></figure><h3 id="修改root密码"><a href="#修改root密码" class="headerlink" title="修改root密码"></a>修改root密码</h3><p>通过用 rw init&#x3D;&#x2F;sysroot&#x2F;bin&#x2F;sh 参数替换内核中的 ro 语句以单用户模式启动</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">linux (<span class="hljs-variable">$root</span>)/boot/vmlinuz-6.6.34-9.ocs23.loongarch64 root=UUID=69ed94f1-41\|<br> |53-48c0-8f9a-937959022d1d ro i8042.noaux quiet console=ttyS0,115200 console\|<br> |=tty0 crashkernel=512M-4G:192M,4G-64G:256M,64G-128G:512M,128G-:768M biosdev\|<br> |name=0 net.ifnames=0<br></code></pre></td></tr></table></figure><p>改成</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">linux (<span class="hljs-variable">$root</span>)/boot/vmlinuz-6.6.34-9.ocs23.loongarch64 root=UUID=69ed94f1-41\|<br> |53-48c0-8f9a-937959022d1d rw init=/sysroot/bin/sh i8042.noaux quiet console=ttyS0,115200 console\|<br> |=tty0 crashkernel=512M-4G:192M,4G-64G:256M,64G-128G:512M,128G-:768M biosdev\|<br> |name=0 net.ifnames=0<br></code></pre></td></tr></table></figure><p>改好了后按F10启动，这个如果知道root密码就不用改</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs bash">Generating <span class="hljs-string">&quot;/run/initramfs/rdsosreport.txt&quot;</span><br><br><br>Entering emergency mode. Exit the shell to <span class="hljs-built_in">continue</span>.<br>Type <span class="hljs-string">&quot;journalctl&quot;</span> to view system logs.<br>You might want to save <span class="hljs-string">&quot;/run/initramfs/rdsosreport.txt&quot;</span> to a USB stick or /boot<br>after mounting them and attach it to a bug report.<br><br><br>Press Enter <span class="hljs-keyword">for</span> maintenance<br>(or press Control-D to <span class="hljs-built_in">continue</span>):<br></code></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs bash">:/root<span class="hljs-comment"># /sysroot/sbin/chroot /sysroot/</span><br>:/<span class="hljs-comment"># passwd root</span><br>Changing password <span class="hljs-keyword">for</span> user root.<br>New password:<br>BAD PASSWORD: The password is shorter than 8 characters<br>Retype new password:<br>passwd: all authentication tokens updated successfully.<br></code></pre></td></tr></table></figure><p>改好后，重启退出，再启动</p><p><img src="/images/blog/2024-09-11-18-46-31.png"></p><p>输入用户名密码登录即可</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@192 ~]<span class="hljs-comment"># cat /proc/cpuinfo |grep &quot;Model\|CPU Revision&quot;|sort -u</span><br>CPU Revision: 0x10<br>Model Name: Loongson-3A5000<br></code></pre></td></tr></table></figure><h3 id="启动成功界面"><a href="#启动成功界面" class="headerlink" title="启动成功界面"></a>启动成功界面</h3><p><img src="/images/blog/2024-09-11-18-52-12.png"></p><p>可以看到，通过qemu，我们完成了虚拟机的模拟启动了</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本篇介绍了使用qemu来模拟一个龙芯的环境，并且安装了opencloudos的镜像，其它镜像也是类似的操作</p>]]></content>
    
    
    <categories>
      
      <category>系统管理</category>
      
    </categories>
    
    
    <tags>
      
      <tag>虚拟化</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>ceph-radosgw配置bucket的policy</title>
    <link href="/2024/09/05/ceph-radosgw%E9%85%8D%E7%BD%AEbucket%E7%9A%84policy/"/>
    <url>/2024/09/05/ceph-radosgw%E9%85%8D%E7%BD%AEbucket%E7%9A%84policy/</url>
    
    <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>ceph通过radosgw对外提供了s3功能，这个能够提供一个s3接口供外部使用，s3的功能选项很多，本篇记录的是policy的功能配置</p><h2 id="配置方法"><a href="#配置方法" class="headerlink" title="配置方法"></a>配置方法</h2><p>配置policy可以通过s3cmd，或者windows的s3客户端都可以，这里我们使用s3cmd进行配置</p><h3 id="安装配置s3cmd"><a href="#安装配置s3cmd" class="headerlink" title="安装配置s3cmd"></a>安装配置s3cmd</h3><p>下载客户端</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ceph]<span class="hljs-comment"># yum install s3cmd</span><br></code></pre></td></tr></table></figure><p>配置s3cmd</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ceph]<span class="hljs-comment"># s3cmd --configure</span><br><br>Enter new values or accept defaults <span class="hljs-keyword">in</span> brackets with Enter.<br>Refer to user manual <span class="hljs-keyword">for</span> detailed description of all options.<br><br>Access key and Secret key are your identifiers <span class="hljs-keyword">for</span> Amazon S3. Leave them empty <span class="hljs-keyword">for</span> using the <span class="hljs-built_in">env</span> variables.<br>Access Key: test1<br>Secret Key: test1<br>Default Region [US]: <br><br>Use <span class="hljs-string">&quot;s3.amazonaws.com&quot;</span> <span class="hljs-keyword">for</span> S3 Endpoint and not modify it to the target Amazon S3.<br>S3 Endpoint [s3.amazonaws.com]: 192.168.0.101:7481<br><br>Use <span class="hljs-string">&quot;%(bucket)s.s3.amazonaws.com&quot;</span> to the target Amazon S3. <span class="hljs-string">&quot;%(bucket)s&quot;</span> and <span class="hljs-string">&quot;%(location)s&quot;</span> vars can be used<br><span class="hljs-keyword">if</span> the target S3 system supports dns based buckets.<br>DNS-style bucket+hostname:port template <span class="hljs-keyword">for</span> accessing a bucket [%(bucket)s.s3.amazonaws.com]: %(bucket).192.168.0.101:7481<br><br>Encryption password is used to protect your files from reading<br>by unauthorized persons <span class="hljs-keyword">while</span> <span class="hljs-keyword">in</span> transfer to S3<br>Encryption password: <br>Path to GPG program [/usr/bin/gpg]: <br><br>When using secure HTTPS protocol all communication with Amazon S3<br>servers is protected from 3rd party eavesdropping. This method is<br>slower than plain HTTP, and can only be proxied with Python 2.7 or newer<br>Use HTTPS protocol [Yes]: no<br><br>On some networks all internet access must go through a HTTP proxy.<br>Try setting it here <span class="hljs-keyword">if</span> you can<span class="hljs-string">&#x27;t connect to S3 directly</span><br><span class="hljs-string">HTTP Proxy server name: </span><br><span class="hljs-string"></span><br><span class="hljs-string">New settings:</span><br><span class="hljs-string">  Access Key: test1</span><br><span class="hljs-string">  Secret Key: test1</span><br><span class="hljs-string">  Default Region: US</span><br><span class="hljs-string">  S3 Endpoint: 192.168.0.101:7481</span><br><span class="hljs-string">  DNS-style bucket+hostname:port template for accessing a bucket: %(bucket).192.168.0.101:7481</span><br><span class="hljs-string">  Encryption password: </span><br><span class="hljs-string">  Path to GPG program: /usr/bin/gpg</span><br><span class="hljs-string">  Use HTTPS protocol: False</span><br><span class="hljs-string">  HTTP Proxy server name: </span><br><span class="hljs-string">  HTTP Proxy server port: 0</span><br><span class="hljs-string"></span><br><span class="hljs-string">Test access with supplied credentials? [Y/n] Y</span><br><span class="hljs-string">Please wait, attempting to list all buckets...</span><br><span class="hljs-string">Success. Your access key and secret key worked fine :-)</span><br><span class="hljs-string"></span><br><span class="hljs-string">Now verifying that encryption works...</span><br><span class="hljs-string">Not configured. Never mind.</span><br><span class="hljs-string"></span><br><span class="hljs-string">Save settings? [y/N] y</span><br><span class="hljs-string">Configuration saved to &#x27;</span>/root/.s3cfg<span class="hljs-string">&#x27;</span><br></code></pre></td></tr></table></figure><p>如果有多个用户操作的需求，可以指定配置文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">s3cmd --config s3cfguser1<br></code></pre></td></tr></table></figure><h3 id="编写policy规则"><a href="#编写policy规则" class="headerlink" title="编写policy规则"></a>编写policy规则</h3><p>首先需要写一个policy规则</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs bash">&#123;<br>  <span class="hljs-string">&quot;Version&quot;</span>:  <span class="hljs-string">&quot;2012-10-17&quot;</span>,<br>  <span class="hljs-string">&quot;Id&quot;</span>:  <span class="hljs-string">&quot;bucketname-write&quot;</span>,<br>  <span class="hljs-string">&quot;Statement&quot;</span>:  [<br>    &#123;<br>      <span class="hljs-string">&quot;Sid&quot;</span>:  <span class="hljs-string">&quot;bucketname-write&quot;</span>,<br>      <span class="hljs-string">&quot;Effect&quot;</span>:  <span class="hljs-string">&quot;Allow&quot;</span>,<br>      <span class="hljs-string">&quot;Principal&quot;</span>:  &#123;<br>        <span class="hljs-string">&quot;AWS&quot;</span>:  [<br>           <span class="hljs-string">&quot;arn:aws:iam:::user/test2&quot;</span><br>        ]<br>      &#125;,<br>      <span class="hljs-string">&quot;Action&quot;</span>:  [<br>        <span class="hljs-string">&quot;s3:ListBucket&quot;</span>,<br>        <span class="hljs-string">&quot;s3:PutObject&quot;</span>,<br>        <span class="hljs-string">&quot;s3:DeleteObject&quot;</span>,<br>        <span class="hljs-string">&quot;s3:GetObject&quot;</span><br>      ],<br>      <span class="hljs-string">&quot;Resource&quot;</span>:  [<br>        <span class="hljs-string">&quot;arn:aws:s3:::mybucket1/*&quot;</span>,<br>        <span class="hljs-string">&quot;arn:aws:s3:::mybucket1&quot;</span><br>      ]<br>    &#125;<br>  ]<br>&#125;<br></code></pre></td></tr></table></figure><ul><li>Version : 这个是policy的规则的版本，这个地方只有两个固定日期可以写，2012-10-17 和2008-10-17 ,写其它日期后台会无法解析</li><li>Id: 这个就是这个policy的id，用于区分不同的policy的</li><li>Statement: 就是主体的配置</li><li>Sid: 就是statement内部的不同配置的标识</li><li>Effect: 这个就是配置允许，还是禁止的，有Allow和Deny</li><li>Principal: 这个里面就是控制对哪个角色进行的配置</li><li>Action: 这个就是有哪些操作</li><li>Resource: 这个就是对哪些资源的配置</li></ul><p>上面的配置就是允许test2的用户对mybucket1的一些操作</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ~]<span class="hljs-comment"># s3cmd setpolicy w.json s3://mybucket1</span><br>s3://mybucket1/: Policy updated<br></code></pre></td></tr></table></figure><p>查询当前的policy的命令</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ~]<span class="hljs-comment"># s3cmd info s3://mybucket1</span><br>s3://mybucket1/ (bucket):<br>   Location:  default<br>   Payer:     BucketOwner<br>   Expiration Rule: none<br>   Policy:    &#123;<br>  <span class="hljs-string">&quot;Version&quot;</span>:  <span class="hljs-string">&quot;2012-10-17&quot;</span>,<br>  <span class="hljs-string">&quot;Id&quot;</span>:  <span class="hljs-string">&quot;bucketname-write&quot;</span>,<br>  <span class="hljs-string">&quot;Statement&quot;</span>:  [<br>    &#123;<br>      <span class="hljs-string">&quot;Sid&quot;</span>:  <span class="hljs-string">&quot;bucketname-write&quot;</span>,<br>      <span class="hljs-string">&quot;Effect&quot;</span>:  <span class="hljs-string">&quot;Allow&quot;</span>,<br>      <span class="hljs-string">&quot;Principal&quot;</span>:  &#123;<br>        <span class="hljs-string">&quot;AWS&quot;</span>:  [<br>           <span class="hljs-string">&quot;arn:aws:iam:::user/test2&quot;</span><br>        ]<br>      &#125;,<br>      <span class="hljs-string">&quot;Action&quot;</span>:  [<br>        <span class="hljs-string">&quot;s3:ListBucket&quot;</span>,<br>        <span class="hljs-string">&quot;s3:PutObject&quot;</span>,<br>        <span class="hljs-string">&quot;s3:DeleteObject&quot;</span>,<br>        <span class="hljs-string">&quot;s3:GetObject&quot;</span><br>      ],<br>      <span class="hljs-string">&quot;Resource&quot;</span>:  [<br>        <span class="hljs-string">&quot;arn:aws:s3:::mybucket1/*&quot;</span>,<br>        <span class="hljs-string">&quot;arn:aws:s3:::mybucket1&quot;</span><br>      ]<br>    &#125;<br>  ]<br>&#125;<br><br>   CORS:      none<br>   ACL:       test1: FULL_CONTROL<br></code></pre></td></tr></table></figure><p>删除policy的命令</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ~]<span class="hljs-comment"># s3cmd delpolicy  s3://mybucket1</span><br>s3://mybucket1/: Policy deleted<br></code></pre></td></tr></table></figure><h2 id="附加"><a href="#附加" class="headerlink" title="附加"></a>附加</h2><p>作为集群的维护者，我们需要掌握更多的信息，这个policy是在客户端进行设置，如果客户认为设置跟自己想象的有区别，那么我们如何去查看这些policy的信息，或者说，是否存在设置不正确，权限过大的情况</p><p>ceph本身没有命令直接查询这个policy的，这个是作为一个xattr扩展属性存储在对象里面的，我们看下怎么处理这个</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ~]<span class="hljs-comment">#  rados -p default.rgw.meta ls --all</span><br>users.keys  test2<br>users.keys  test3<br>root    .bucket.meta.mybucket2:27ff3ab2-6caf-43b1-9281-af0d05a57319.24123.2<br>root    mybucket3<br>users.uid   test2.buckets<br>root    .bucket.meta.mybucket3:27ff3ab2-6caf-43b1-9281-af0d05a57319.24160.1<br>users.uid   test1.buckets<br>users.keys  test1<br>users.uid   test2<br>root    .bucket.meta.mybucket1:27ff3ab2-6caf-43b1-9281-af0d05a57319.24123.1<br>users.swift test1:test3<br>users.uid   test1<br>root    mybucket2<br>root    mybucket1<br>users.keys  INMJZ9W82AFSJYA9T5Z4<br></code></pre></td></tr></table></figure><p>对象是存储在root命令空间里面</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ~]<span class="hljs-comment"># rados -p default.rgw.meta --namespace=root ls</span><br>.bucket.meta.mybucket2:27ff3ab2-6caf-43b1-9281-af0d05a57319.24123.2<br>mybucket3<br>.bucket.meta.mybucket3:27ff3ab2-6caf-43b1-9281-af0d05a57319.24160.1<br>.bucket.meta.mybucket1:27ff3ab2-6caf-43b1-9281-af0d05a57319.24123.1<br>mybucket2<br>mybucket1<br></code></pre></td></tr></table></figure><p>上面的bucket meta里面就是存储的这个policy的信息的，设置了才有没有设置就没有</p><p>看下没设置的情况</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ~]<span class="hljs-comment"># rados -p default.rgw.meta --namespace=root  listxattr  .bucket.meta.mybucket1:27ff3ab2-6caf-43b1-9281-af0d05a57319.24123.1</span><br>ceph.objclass.version<br>user.rgw.acl<br></code></pre></td></tr></table></figure><p>设置以后的情况</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ~]<span class="hljs-comment"># rados -p default.rgw.meta --namespace=root  listxattr  .bucket.meta.mybucket1:27ff3ab2-6caf-43b1-9281-af0d05a57319.24123.1</span><br>ceph.objclass.version<br>user.rgw.acl<br>user.rgw.iam-policy<br></code></pre></td></tr></table></figure><p>也就是如果设置了policy就会多一个user.rgw.iam-policy</p><p>我们看下内容</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ~]<span class="hljs-comment"># rados -p default.rgw.meta --namespace=root  getxattr  .bucket.meta.mybucket1:27ff3ab2-6caf-43b1-9281-af0d05a57319.24123.1 user.rgw.iam-policy</span><br>&#123;<br>  <span class="hljs-string">&quot;Version&quot;</span>:  <span class="hljs-string">&quot;2008-10-17&quot;</span>,<br>  <span class="hljs-string">&quot;Id&quot;</span>:  <span class="hljs-string">&quot;bucketname-write&quot;</span>,<br>  <span class="hljs-string">&quot;Statement&quot;</span>:  [<br>    &#123;<br>      <span class="hljs-string">&quot;Sid&quot;</span>:  <span class="hljs-string">&quot;bucketname-write&quot;</span>,<br>      <span class="hljs-string">&quot;Effect&quot;</span>:  <span class="hljs-string">&quot;Allow&quot;</span>,<br>      <span class="hljs-string">&quot;Principal&quot;</span>:  &#123;<br>        <span class="hljs-string">&quot;AWS&quot;</span>:  [<br>           <span class="hljs-string">&quot;arn:aws:iam:::user/test2&quot;</span><br>        ]<br>      &#125;,<br>      <span class="hljs-string">&quot;Action&quot;</span>:  [<br>        <span class="hljs-string">&quot;s3:ListBucket&quot;</span>,<br>        <span class="hljs-string">&quot;s3:ListAllMyBuckets&quot;</span>,<br>        <span class="hljs-string">&quot;s3:PutObject&quot;</span>,<br>        <span class="hljs-string">&quot;s3:DeleteObject&quot;</span>,<br>        <span class="hljs-string">&quot;s3:GetObject&quot;</span><br>      ],<br>      <span class="hljs-string">&quot;Resource&quot;</span>:  [<br>        <span class="hljs-string">&quot;arn:aws:s3:::mybucket1/*&quot;</span>,<br>        <span class="hljs-string">&quot;arn:aws:s3:::mybucket1&quot;</span><br>      ]<br>    &#125;<br>  ]<br>&#125;<br></code></pre></td></tr></table></figure><p>这个是明文的json的没有进行序列化的，那么我们就可以从底层查看到哪些设置了哪些没有设置</p><p>这个地方还可以从底层进行修改</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ~]<span class="hljs-comment"># rados -p default.rgw.meta --namespace=root  getxattr  .bucket.meta.mybucket1:27ff3ab2-6caf-43b1-9281-af0d05a57319.24123.1 user.rgw.iam-policy &gt; user.rgw.iam-policy.json</span><br>[root@lab101 ~]<span class="hljs-comment"># rados -p default.rgw.meta --namespace=root  setxattr  .bucket.meta.mybucket1:27ff3ab2-6caf-43b1-9281-af0d05a57319.24123.1 user.rgw.iam-policy &lt; user.rgw.iam-policy.json</span><br>[root@lab101 ~]<span class="hljs-comment"># rados -p default.rgw.meta --namespace=root  getxattr  .bucket.meta.mybucket1:27ff3ab2-6caf-43b1-9281-af0d05a57319.24123.1 user.rgw.iam-policy</span><br></code></pre></td></tr></table></figure><p>但是这个地方有个问题，设置了后，底层是马上更新了，但是客户端那边</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ~]<span class="hljs-comment"># s3cmd info s3://mybucket1</span><br></code></pre></td></tr></table></figure><p>这个并没有更新</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">2024-09-05T11:42:18.074+0800 7f15435db700 20 get_system_obj_state: rctx=0x7f15435d1988 obj=default.rgw.meta:users.uid:test1 state=0x55a9b8284040 s-&gt;prefetch_data=0<br>2024-09-05T11:42:18.074+0800 7f15435db700 10 cache get: name=default.rgw.meta+users.uid+test1 : hit (requested=0x6, cached=0x17)<br>2024-09-05T11:42:18.074+0800 7f15435db700 20 get_system_obj_state: s-&gt;obj_tag was <span class="hljs-built_in">set</span> empty<br></code></pre></td></tr></table></figure><p>日志看是命中了缓存，说明这个信息在rgw这边缓存了，这个重启下rgw的进程就刷新了，所以这个底层的操作并不适合频繁的去设置</p><p>这个地方查看检查还是可以的，也就是我们哪些bucket设置了policy，设置了什么policy，这个是可以拿到的，设置的通过前端设置即可，或者有比较特殊的批量的需求的时候，再考虑在底层设置</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本篇记录了policy的设置以及从底层查询设置的规则方法</p>]]></content>
    
    
    <categories>
      
      <category>存储相关</category>
      
    </categories>
    
    
    <tags>
      
      <tag>ceph</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>questdb在opencloudos下的打包总结</title>
    <link href="/2024/08/28/questdb%E5%9C%A8opencloudos%E4%B8%8B%E7%9A%84%E6%89%93%E5%8C%85%E6%80%BB%E7%BB%93/"/>
    <url>/2024/08/28/questdb%E5%9C%A8opencloudos%E4%B8%8B%E7%9A%84%E6%89%93%E5%8C%85%E6%80%BB%E7%BB%93/</url>
    
    <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>最近看到opencloudos社区有一些公开的任务可以做，尝试了做其中的一个打包的任务，整个任务的执行周期超过预期，但是中间还是掌握了一些新的东西<br>本篇文章就是总结这些新的知识点</p><h2 id="具体过程"><a href="#具体过程" class="headerlink" title="具体过程"></a>具体过程</h2><h3 id="打包环境问题"><a href="#打包环境问题" class="headerlink" title="打包环境问题"></a>打包环境问题</h3><p>以前的打包经验是从官网找到source的rpm包，然后本地解压，然后对着spec文件进行rpmbuild -bb即可生成当前系统的包<br>但是这个环境是需要使用mock跑一遍的,这个之前确实没接触过，只是在centos下包的时候，某些包的下载链接有koji这个地址的<br>这个是一套完整的环境</p><p>我们看下这个地方是怎么运行的</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@myserver ~]<span class="hljs-comment"># cat mock.cfg</span><br><span class="hljs-comment"># Auto-generated by the Koji build system</span><br><br>config_opts[<span class="hljs-string">&#x27;basedir&#x27;</span>] = <span class="hljs-string">&#x27;/var/lib/mock&#x27;</span><br>config_opts[<span class="hljs-string">&#x27;chroot_setup_cmd&#x27;</span>] = <span class="hljs-string">&#x27;groupinstall server-product-environment development&#x27;</span><br><span class="hljs-comment">#config_opts[&#x27;chroot_setup_cmd&#x27;] = &#x27;groupinstall Server&#x27;</span><br><span class="hljs-comment">#config_opts[&#x27;chroot_setup_cmd&#x27;] = &#x27;install rpm-build shadow-utils systemd&#x27;</span><br><span class="hljs-comment">#config_opts[&#x27;chroot_setup_cmd&#x27;] = &#x27;groupinstall development&#x27;</span><br>config_opts[<span class="hljs-string">&#x27;chroothome&#x27;</span>] = <span class="hljs-string">&#x27;/builddir&#x27;</span><br>config_opts[<span class="hljs-string">&#x27;dnf_warning&#x27;</span>] = False<br>config_opts[<span class="hljs-string">&#x27;package_manager&#x27;</span>] = <span class="hljs-string">&#x27;dnf&#x27;</span><br>config_opts[<span class="hljs-string">&#x27;root&#x27;</span>] = <span class="hljs-string">&#x27;dist-ocs23-build-repo_latest&#x27;</span><br>config_opts[<span class="hljs-string">&#x27;rpmbuild_networking&#x27;</span>] = False<br>config_opts[<span class="hljs-string">&#x27;rpmbuild_timeout&#x27;</span>] = 86400<br>config_opts[<span class="hljs-string">&#x27;target_arch&#x27;</span>] = <span class="hljs-string">&#x27;x86_64&#x27;</span><br>config_opts[<span class="hljs-string">&#x27;use_host_resolv&#x27;</span>] = False<br><span class="hljs-comment">#config_opts[&#x27;yum.conf&#x27;] = &#x27;[main]\ncachedir=/var/cache/yum\ndebuglevel=1\nlogfile=/var/log/yum.log\nreposdir=/dev/null\nretries=20\nobsoletes=1\ngpgcheck=0\nassumeyes=1\nkeepcache=1\ninstall_weak_deps=0\nstrict=1\n\n# repos\n\n[build]\nname=build\nbaseurl=https://build.stream.opencloudos.tech/kojifiles/repos/dist-ocs23-build/latest/x86_64\n&#x27;</span><br>config_opts[<span class="hljs-string">&#x27;yum.conf&#x27;</span>] = <span class="hljs-string">&#x27;[main]\ncachedir=/var/cache/yum\ndebuglevel=1\nlogfile=/var/log/yum.log\nexclude= kernel-core*\nexclude=kmod-kvdo*\nexclude==vdo*\nexclude=kernel-modules*\nexclude=kernel-6.6.6*\nreposdir=/dev/null\nretries=20\nobsoletes=1\ngpgcheck=0\nassumeyes=1\nkeepcache=1\ninstall_weak_deps=0\nstrict=1\n\n# repos\n\n[build]\nname=build\nbaseurl=http://192.168.0.208/BaseOS/\n\n[Appstream]\nname=appstream\nbaseurl=http://192.168.0.208/AppStream/ &#x27;</span><br><br>config_opts[<span class="hljs-string">&#x27;plugin_conf&#x27;</span>][<span class="hljs-string">&#x27;ccache_enable&#x27;</span>] = False<br>config_opts[<span class="hljs-string">&#x27;plugin_conf&#x27;</span>][<span class="hljs-string">&#x27;root_cache_enable&#x27;</span>] = False<br>config_opts[<span class="hljs-string">&#x27;plugin_conf&#x27;</span>][<span class="hljs-string">&#x27;yum_cache_enable&#x27;</span>] = False<br><br>config_opts[<span class="hljs-string">&#x27;macros&#x27;</span>][<span class="hljs-string">&#x27;%_host&#x27;</span>] = <span class="hljs-string">&#x27;x86_64-koji-linux-gnu&#x27;</span><br>config_opts[<span class="hljs-string">&#x27;macros&#x27;</span>][<span class="hljs-string">&#x27;%_host_cpu&#x27;</span>] = <span class="hljs-string">&#x27;x86_64&#x27;</span><br>config_opts[<span class="hljs-string">&#x27;macros&#x27;</span>][<span class="hljs-string">&#x27;%_rpmfilename&#x27;</span>] = <span class="hljs-string">&#x27;%%&#123;NAME&#125;-%%&#123;VERSION&#125;-%%&#123;RELEASE&#125;.%%&#123;ARCH&#125;.rpm&#x27;</span><br>config_opts[<span class="hljs-string">&#x27;macros&#x27;</span>][<span class="hljs-string">&#x27;%_topdir&#x27;</span>] = <span class="hljs-string">&#x27;/builddir/build&#x27;</span><br>config_opts[<span class="hljs-string">&#x27;macros&#x27;</span>][<span class="hljs-string">&#x27;%dist&#x27;</span>] = <span class="hljs-string">&#x27;.ocs23&#x27;</span><br>config_opts[<span class="hljs-string">&#x27;macros&#x27;</span>][<span class="hljs-string">&#x27;%distribution&#x27;</span>] = <span class="hljs-string">&#x27;Koji Testing&#x27;</span><br>config_opts[<span class="hljs-string">&#x27;macros&#x27;</span>][<span class="hljs-string">&#x27;%packager&#x27;</span>] = <span class="hljs-string">&#x27;Koji&#x27;</span><br>config_opts[<span class="hljs-string">&#x27;macros&#x27;</span>][<span class="hljs-string">&#x27;%vendor&#x27;</span>] = <span class="hljs-string">&#x27;Koji&#x27;</span><br></code></pre></td></tr></table></figure><p>这个是我的本地的mock配置文件，官方提供的是使用的官方的源，官方源是需要通过公网网络去下载rpm包，然后在本地构建了一个类似lxc或者docker的环境，然后在这个环境内打包的<br>这个环境的基础环境是通过上面的一些地方做控制的<br>yum.conf那里就是写的dnf的配置文件，这个地方决定了rpm包从哪里来的<br>chroot_setup_cmd那里就是安装的包环境的，需要多少就自己装就行，官方是用了一个build的group，这个包里面有多少内容就是官方去控制的，这个地方我们尽量去模拟一个跟官方类似的环境即可<br>上面的环境我自己是把cdrom直接mount本地<br>然后做了一个本机的nginx的源，然后dnf指定到这个源，这个可以根据自己的需要进行处理即可，方法很多，怎么方便怎么来即可</p><p>运行mock打包</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">mock -r mock.cfg --config-opts mirrored=False  --rebuild ~/rpmbuild/SRPMS/questdb-8.1.0-1.ocs23.src.rpm<br></code></pre></td></tr></table></figure><p>这个跑起来就可以了，出错会提示是什么问题引起的</p><p>执行这个命令就会跑起自动的流程</p><h3 id="mvn-打包"><a href="#mvn-打包" class="headerlink" title="mvn 打包"></a>mvn 打包</h3><p>mvn是java打包的一个工具套件，这个之前也没用过，打包过程会提示很多依赖，然后自动下载，这个跟之前做go的一些打包有点类似，会自动取下一些依赖包，然后放到本地的目录，下次用的时候就不用下了</p><p>那么这个地方我们需要离线打包的话，需要把依赖都弄下来</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">mvn  package -DskipTests -P build-web-console  -Dmaven.repo.local=./repository/<br></code></pre></td></tr></table></figure><p>这个就是一个打包的过程，打包过程会把依赖包都下载到.&#x2F;repository里面，那么下次再打包的时候就不用下载，我们就是利用这个来进行离线打包，其它mvn打包的都可以类似处理</p><h3 id="spec的规范性问题"><a href="#spec的规范性问题" class="headerlink" title="spec的规范性问题"></a>spec的规范性问题</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@myserver questdb]<span class="hljs-comment"># cat questdb.spec</span><br>%define debug_package %&#123;nil&#125;<br><br>Summary:        QuestDB is the fastest growing open-source time-series database offering blazingly fast, high throughput ingestion and dynamic, low-latency SQL queries.<br>Name:           questdb<br>Version:        8.1.0<br>Release:        1%&#123;?dist&#125;<br>License:        Apache License 2.0<br>URL:            https://github.com/questdb/questdb/<br>Source0:        https://github.com/questdb/questdb/archive/refs/tags/%&#123;version&#125;.tar.gz<br>Source1:        repository.tar.gz<br>Source2:        questdb.service<br>Source3:        web-console.tgz<br><span class="hljs-comment"># mvn buid use many jar need download from maven center</span><br><span class="hljs-comment"># build on local server use command</span><br><span class="hljs-comment"># mvn clean package -DskipTests -P build-web-console,build-binaries -Dmaven.repo.local=./repository/</span><br><span class="hljs-comment"># tar the repository to repository.tar.gz</span><br><span class="hljs-comment"># rpmbuild can use mvn build local not need network or rpm</span><br><br><span class="hljs-comment"># web-console.tgz</span><br><span class="hljs-comment"># Source3 wget from https://registry.npmjs.org/@questdb/web-console/-/web-console-0.5.1.tgz</span><br><span class="hljs-comment">#</span><br><br>BuildRequires:  maven-local<br>BuildRequires:  java-17-konajdk-jmods<br>BuildRequires:  git<br>BuildRequires: systemd-rpm-macros<br>Requires: java<br>%description<br>QuestDB is the fastest growing open-source time-series database offering blazingly fast, high throughput ingestion and dynamic, low-latency SQL queries. The entire high-performance codebase is built from the ground up <span class="hljs-keyword">in</span> Java, C++ and Rust with no dependencies and zero garbage collection.<br><br>%prep<br>%autosetup<br>tar -xvf  %&#123;SOURCE1&#125;<br><span class="hljs-built_in">cp</span> -ra %&#123;SOURCE2&#125; ./<br><span class="hljs-built_in">mkdir</span> -p core/target/site/<br><span class="hljs-built_in">cp</span> -ra %&#123;SOURCE3&#125; core/target/site/<br><br>%build<br><span class="hljs-comment"># skip 10000+ unit test</span><br><span class="hljs-comment">#%%mvn_build</span><br><span class="hljs-comment">#mvn clean package -Dmaven.test.skip=true -Dmaven.repo.local=./repository/</span><br>mvn  package -DskipTests -P build-web-console  -Dmaven.repo.local=./repository/<br><br>%install<br>install -m 0755 -D core/target/questdb-8.1.0.jar %&#123;buildroot&#125;%&#123;_bindir&#125;/questdb.jar<br>install -m 0755 -D core/src/main/bin/env.sh %&#123;buildroot&#125;%&#123;_bindir&#125;/env.sh<br>install -m 0755 -D core/src/main/bin/print-hello.sh %&#123;buildroot&#125;%&#123;_bindir&#125;/print-hello.sh<br>install -m 0755 -D core/src/main/bin/questdb.sh %&#123;buildroot&#125;%&#123;_bindir&#125;/questdb.sh<br>install -m 0644 -D ./questdb.service  %&#123;buildroot&#125;%&#123;_unitdir&#125;/questdb.service<br><br><br>%files<br>%license core/LICENSE.txt<br>%&#123;_bindir&#125;/questdb.jar<br>%&#123;_bindir&#125;/env.sh<br>%&#123;_bindir&#125;/print-hello.sh<br>%&#123;_bindir&#125;/questdb.sh<br>%&#123;_unitdir&#125;/questdb.service<br><br><br>%changelog<br>* Tue Aug 6 2024 zphj1987 &lt;199383004@qq.com&gt; - 8.1.0-1<br>- [Type] other<br>- [DESC] Initial build of questdb.<br></code></pre></td></tr></table></figure><h4 id="字段的语法规则"><a href="#字段的语法规则" class="headerlink" title="字段的语法规则"></a>字段的语法规则</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">S -&gt; N -&gt;  V - &gt;  R -&gt;  L -&gt;  U<br></code></pre></td></tr></table></figure><p>按照这个顺序处理</p><h4 id="license的放置问题"><a href="#license的放置问题" class="headerlink" title="license的放置问题"></a>license的放置问题</h4><p>这个按标准的放置</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">%license core/LICENSE.txt<br></code></pre></td></tr></table></figure><p>不用install，这个直接会在源码里面取，并且放到了标准的位置</p><h3 id="install路径问题"><a href="#install路径问题" class="headerlink" title="install路径问题"></a>install路径问题</h3><p>之前计划把jar放到一个独立的&#x2F;opt下面，后面发现启动里面还判断了java的路径，如果放在&#x2F;usr&#x2F;bin下面<br>就不会提示java_home没设置的问题，这个按标准处理即可</p><h3 id="打包过程的缺包问题"><a href="#打包过程的缺包问题" class="headerlink" title="打包过程的缺包问题"></a>打包过程的缺包问题</h3><blockquote><p>java-17-konajdk-jmods-17.0.11-1.ocs23.x86_64</p></blockquote><p>打包过程发现 <code>Module java.management</code> 缺失的问题,这个就是缺上面的包，应该是漏放到源里面了</p><p>这个地方我的处理是直接拿源码按当前的java版本重新打了上面的这个rpm包，安装后就通过了</p><h3 id="编译无web的问题"><a href="#编译无web的问题" class="headerlink" title="编译无web的问题"></a>编译无web的问题</h3><p>最开始编译的时候，安装完没有web</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">mvn  package -DskipTests  -Dmaven.repo.local=./repository/<br></code></pre></td></tr></table></figure><p>修改为</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">mvn  package -DskipTests -P build-web-console  -Dmaven.repo.local=./repository/<br></code></pre></td></tr></table></figure><p>就可以了，这个github官方的repo里面有说明编译不同的东西</p><h3 id="service问题"><a href="#service问题" class="headerlink" title="service问题"></a>service问题</h3><p>官方没提供service</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@myserver questdb]<span class="hljs-comment"># cat questdb.service</span><br>[Unit]<br>Description=QuestDB  Service<br>After=network-online.target<br>Wants=network-online.target<br><br>[Service]<br>LimitNOFILE=1048576<br>LimitNPROC=1048576<br>Type=forking<br>Environment=QDB_DEFAULT_ROOT=/etc/questdb<br>ExecStart=/usr/bin/questdb.sh start -d <span class="hljs-variable">$QDB_DEFAULT_ROOT</span><br>ExecReload=/usr/bin/kill -HUP <span class="hljs-variable">$MAINPID</span><br>KillSignal=SIGQUIT<br>TimeoutStopSec=5<br>KillMode=mixed<br>PrivateTmp=<span class="hljs-literal">true</span><br><br>[Install]<br>WantedBy=multi-user.target<br></code></pre></td></tr></table></figure><p>这个就是service启动里面，我们对配置文件的目录做了一个控制，还有系统限制也可以做下控制</p><h3 id="编译过程下载的问题"><a href="#编译过程下载的问题" class="headerlink" title="编译过程下载的问题"></a>编译过程下载的问题</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">wget https://registry.npmjs.org/@questdb/web-console/-/web-console-0.5.1.tgz<br><span class="hljs-built_in">cp</span> -ra web-console-0.5.1.tgz /root/rpmbuild/BUILD/questdb-8.1.0/core/target/site/web-console.tgz<br></code></pre></td></tr></table></figure><p>编译带web的包过程中需要下载上面的web代码，然后这个网不通畅，离线也不好编译，我们处理方式是，下载下来作为一个源码包<br>在spec里面复制进去，检测到了这个包存在，就不会下载了</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>上面就是打包的整个过程，由于这个打包过程依赖太多在线下载的东西，可能存在不可控的因素比较多，这个可以直接下载官方提供的二进制或者自己打包一下即可<br>本篇就是记录整个打包过程，后续其它的打包的可以做一些参考</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>海量文件的rsync同步方案</title>
    <link href="/2024/08/23/%E6%B5%B7%E9%87%8F%E6%96%87%E4%BB%B6%E7%9A%84rsync%E5%90%8C%E6%AD%A5%E6%96%B9%E6%A1%88/"/>
    <url>/2024/08/23/%E6%B5%B7%E9%87%8F%E6%96%87%E4%BB%B6%E7%9A%84rsync%E5%90%8C%E6%AD%A5%E6%96%B9%E6%A1%88/</url>
    
    <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>如果一个环境需要对文件系统进行系统备份，文件系统内的文件数目是10亿级别的，那么直接通过一个rsync进行同步肯定是不太好的，如果出现中断，那么再次启动后的遍历的时间成本太高了</p><h2 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h2><p>rsync的同步原理是，启动同步的时候，会对源端进行一个全量的扫描，构建一个incremental file list，然后开始同步，如果是系统本地的目录进行同步我们可以看到三个进程<br>一个是生成器，一个是发送方，一个是接收方，因为是在一台机器上面运行，所以可以看到这三个进程的，这个之前还误以为是并发三，实际还是单进程模式的</p><p>我们很多情况下，存储环境都是集群模式的，集群模式就是文件系统有多个入口，可以多个并发同时去操作，比如我们原始集群有2个网关，我们新集群有6个网关，那么我们实际上是可以1个原始网关对3个新网关的方式去处理数据的</p><p>由于我们无法去判断原始的目录结构，并且即使能够获取到，也不太好去做均分，比如有6亿文件，根目录下面一个目录5亿，其它5个目录1亿，这种情况就不好去分平分目录了，还有个情况是如果按容量区分，也是不太好取到怎么去区分目录的，所以这个地方想到的一个方案是获取到所有的文件的列表，然后对列表做拆分，然后分配任务的方式</p><h2 id="如何实现"><a href="#如何实现" class="headerlink" title="如何实现"></a>如何实现</h2><p>如果能够拿到完整的文件列表，然后再对列表拆分就很好拆分了，比如9亿文件，我可以拆分成1000w一个任务，分成90份，然后平分给6个机器上面去，都是很好去做拆分的，拆大拆小都比较自由</p><h3 id="获取文件的完整列表"><a href="#获取文件的完整列表" class="headerlink" title="获取文件的完整列表"></a>获取文件的完整列表</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">time rsync -av --dry-run /source/ /target/  &gt; file_list.txt<br></code></pre></td></tr></table></figure><p>通过上面的命令可以完整的拿到文件的列表,&#x2F;target目录是空目录</p><p>拿到的文件的格式如下</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs bash">sending incremental file list<br>./<br>file1<br>···<br>zp/xag<br><br>sent 3,169,627 bytes  received 600,058 bytes  2,513,123.33 bytes/sec<br>total size is 819,200,131  speedup is 217.31 (DRY RUN)<br></code></pre></td></tr></table></figure><p>也就是这个文件我们需要删掉文件的第一行和文件的倒数两行还有目录<br>因为这个扫描会把目录扫描出来，我们是要文件同步，如果是包含目录，那么会出现重复的文件，所有的文件同步了，目录自然也是同步的，并且同步的时候目录rsync会自己处理好<br>如果这个文件是巨大的，那么我们处理文本文件的时候还是要注意下，本篇会考虑这种情况</p><h3 id="拆分文件"><a href="#拆分文件" class="headerlink" title="拆分文件"></a>拆分文件</h3><p>拆分文件就是根据我们需要的任务数，然后拆分文件，首先需要统计文件的行数</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@myserver home]<span class="hljs-comment"># time wc -l file_list.txt </span><br>918411499 file_list.txt<br>real    1m31.319s<br>user    0m7.154s<br>sys     0m15.842s<br></code></pre></td></tr></table></figure><p>统计这个9个亿的文件列表的行数，需要90秒</p><p>看下文件的大小</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@myserver home]<span class="hljs-comment"># ll file_list.txt </span><br>-rw-r--r-- 1 root root 49173415850 Aug 23 00:46 file_list.txt<br></code></pre></td></tr></table></figure><p>9亿文件列表的大小为45G，这里建议拆分成2G左右一个文件，或者更小，我们拆分2G就是24份，用9亿除以24得到大概得文件数目，然后往高取整得到38267146这个值，也就是单个文件存储大概3800w条数据</p><p>我们使用slit命令拆分文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@myserver zpdisk]<span class="hljs-comment"># time split -l 38267146 file_list.txt</span><br><br>real    9m18.176s<br>user    0m6.718s<br>sys     0m31.634s<br></code></pre></td></tr></table></figure><p>拆分的时间大概在10min左右，得到的文件大概2G</p><p>我们拆分完文件以后还要处理文件</p><h3 id="处理文件列表"><a href="#处理文件列表" class="headerlink" title="处理文件列表"></a>处理文件列表</h3><p>拆分出来的文件第一个文件的开头要去掉，最后一个文件的结尾去掉两行，中间每个文件都需要去掉目录</p><p>这里我们需要处理三个地方</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@myserver zpdisk]<span class="hljs-comment"># time sed -i &#x27;1d&#x27; xaa</span><br><br>real    0m51.695s<br>user    0m10.834s<br>sys     0m36.708s<br></code></pre></td></tr></table></figure><p>处理开头的一行，这个需要大概1min</p><p>删除最后一行</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@myserver zpdisk]<span class="hljs-comment"># time sed -i &#x27;$d&#x27; xax</span><br><br>real    0m50.757s<br>user    0m11.149s<br>sys     0m35.621s<br></code></pre></td></tr></table></figure><p>删除最后一行需要1min，要操作两次</p><p>删除列表里面的目录</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@myserver zpdisk]<span class="hljs-comment"># time sed -i &#x27;/\/$/d&#x27; xaa </span><br>real    0m59.833s<br>user    0m19.544s<br>sys     0m36.030s<br></code></pre></td></tr></table></figure><p>这里一个文件需要处理大概1min，一共24个也就是总共时间24min</p><p>处理完成后我们就得到一个完整的文件列表了</p><h3 id="使用列表"><a href="#使用列表" class="headerlink" title="使用列表"></a>使用列表</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">rsync -av /source/ /target/  --files-from=xaa  --log-file=/var/log/xaa.log<br></code></pre></td></tr></table></figure><p>一个列表对应一个命令，然后这个具体要并发开几个，这个就是很简单的事情了，一条命令对一个列表即可，剩下的事情就很简单了</p><h3 id="补充"><a href="#补充" class="headerlink" title="补充"></a>补充</h3><p>上面是同步的所有文件的情况，还有个情况是原始目录里面有可能有很多空目录，这个需要取一下列表</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@jenkins filelist]<span class="hljs-comment"># cat checkkong.py</span><br><span class="hljs-comment">#! /bin/env python</span><br><span class="hljs-comment"># -*- coding:utf-8 -*-</span><br><br><span class="hljs-comment"># 比较方法如下</span><br><span class="hljs-comment">#判断当前行和上一行</span><br><span class="hljs-comment">#如果当前行为文件的，那么上一行不管文件还是目录，上一行肯定不为空</span><br><span class="hljs-comment">#如果当前行为目录的，上一行为目录的</span><br><span class="hljs-comment">#   如果上一行的目录的字符串包含在当前行里面，那么上一行肯定不为空</span><br><span class="hljs-comment">#   如果上一行的目录字符串不包含在当前行的，那么上一行就是空目录了</span><br><br><span class="hljs-comment">#按这个取空目录的列表</span><br><br><br>import sys<br>def compare_lines_in_file(filename):<br>    with open(filename, <span class="hljs-string">&#x27;r&#x27;</span>) as file:<br>        previous_line = None<br><br>        <span class="hljs-keyword">for</span> current_line <span class="hljs-keyword">in</span> file:<br>            current_line = current_line.strip()  <span class="hljs-comment"># 去除行末尾的换行符或空白符</span><br><br>            <span class="hljs-keyword">if</span> previous_line is not None:<br>                <span class="hljs-keyword">if</span> current_line.endswith(<span class="hljs-string">&#x27;/&#x27;</span>) and previous_line.endswith(<span class="hljs-string">&#x27;/&#x27;</span>):<br>                    <span class="hljs-keyword">if</span> previous_line <span class="hljs-keyword">in</span>  current_line :<br>                        pass<br>                    <span class="hljs-keyword">else</span>:<br>                        <span class="hljs-built_in">print</span>(f<span class="hljs-string">&quot;&#123;previous_line&#125;&quot;</span>)<br><br>            <span class="hljs-comment"># 将当前行设为下一次比较的前一行</span><br>            previous_line = current_line<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    <span class="hljs-keyword">if</span> len(sys.argv) &lt; 2:<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Usage: python checkkong.py &lt;filename&gt;&quot;</span>)<br>    <span class="hljs-keyword">else</span>:<br>        filename = sys.argv[1]  <span class="hljs-comment"># 从命令行获取文件名参数</span><br>        compare_lines_in_file(filename)<br></code></pre></td></tr></table></figure><p>执行方法</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">time  python3 checkkong.py  filelistfile.txt &gt; kong.txt<br></code></pre></td></tr></table></figure><p>删除第一行的<code>./</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">sed -i <span class="hljs-string">&#x27;1d&#x27;</span>  kong.txt<br></code></pre></td></tr></table></figure><p>然后同步这个空目录列表即可</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">rsync -av  --files-from=kongdir.list   /zp/source/ /zp/target/<br></code></pre></td></tr></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>总结下操作流程</p><ul><li>拿列表</li><li>拆分列表</li><li>处理列表</li><li>使用列表</li></ul><p>整体上的步骤就这四步了</p><h2 id="时间和空间数据"><a href="#时间和空间数据" class="headerlink" title="时间和空间数据"></a>时间和空间数据</h2><p>上面是基于一个9亿文件的测试环境做的测试，有一些结果可以供参考</p><table><thead><tr><th align="center">测试事项</th><th align="center">时间&#x2F;容量</th><th align="center">单项计算时间</th></tr></thead><tbody><tr><td align="center">9亿文件的获取列表时间</td><td align="center">563min</td><td align="center">1亿文件需要62min</td></tr><tr><td align="center">9亿文件的本地文本占用</td><td align="center">45G</td><td align="center">1亿文件需要占用5G</td></tr><tr><td align="center">9亿文件的本地文件统计行数</td><td align="center">90s</td><td align="center"></td></tr><tr><td align="center">9亿文件拆分成24个文件（3800w单文件）</td><td align="center">558s</td><td align="center"></td></tr><tr><td align="center">3800w文件本地占用</td><td align="center">2G</td><td align="center"></td></tr><tr><td align="center">删除2G文本文件的开头一行</td><td align="center">51s</td><td align="center"></td></tr><tr><td align="center">删除2G文本文件的结尾一行</td><td align="center">51s</td><td align="center"></td></tr><tr><td align="center">删除2G文本文件的内的目录项</td><td align="center">59s</td><td align="center"></td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>存储相关</category>
      
    </categories>
    
    
    <tags>
      
      <tag>数据管理</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>定位读写的扇区的编号</title>
    <link href="/2024/08/19/%E5%AE%9A%E4%BD%8D%E8%AF%BB%E5%86%99%E7%9A%84%E6%89%87%E5%8C%BA%E7%9A%84%E7%BC%96%E5%8F%B7/"/>
    <url>/2024/08/19/%E5%AE%9A%E4%BD%8D%E8%AF%BB%E5%86%99%E7%9A%84%E6%89%87%E5%8C%BA%E7%9A%84%E7%BC%96%E5%8F%B7/</url>
    
    <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>最近有碰到扇区的错误，引起osd的无法启动，经过处理，系统可以恢复了，但是扇区的错误为什么导致了osd down，怎么去模拟这个删除的问题，就需要做一些定位的工作了</p><p>我们需要模拟扇区的问题，就需要去精确的模拟到损坏的扇区，也就是比如我需要模拟某个文件的扇区损坏，那么就需要定位到这个文件属于哪些扇区</p><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><h3 id="定位扇区"><a href="#定位扇区" class="headerlink" title="定位扇区"></a>定位扇区</h3><p>通过blktrace进行模拟，这个操作是去读取文件就行，”C”是完成的请求，下面是过滤的，避免太多其它信息干扰</p><p>对磁盘的文件进行读取的请求 </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">blktrace -d /dev/sdb -o - | blkparse -i - | awk <span class="hljs-string">&#x27;$6 == &quot;C&quot; &#123;print $1, $2, $6, $7, $8, $9, $10&#125;&#x27;</span><br></code></pre></td></tr></table></figure><p>输出类似下面的信息</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs bash">8,80 8 C R 17152 + 512<br>8,80 8 C R 17664 + 512<br>8,80 8 C R 18176 + 512<br>8,80 8 C R 18688 + 512<br>8,80 8 C R 19200 + 512<br>8,80 8 C R 19712 + 512<br>8,80 8 C R 20224 + 512<br>8,80 8 C R 20736 + 512<br></code></pre></td></tr></table></figure><h3 id="模拟坏扇区"><a href="#模拟坏扇区" class="headerlink" title="模拟坏扇区"></a>模拟坏扇区</h3><p>后面接的就是扇区的编号</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">我们尝试破坏比较后面的扇区试试 20737<br>[root@lab101 ~]<span class="hljs-comment">#  hdparm --yes-i-know-what-i-am-doing --make-bad-sector  20737 /dev/sdf</span><br></code></pre></td></tr></table></figure><p>系统应该会报</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># blk_update_request: critical medium error, dev sdf, sector 20737</span><br><span class="hljs-comment"># print_seq_error: critical medium error, dev sdf, sector 20737</span><br></code></pre></td></tr></table></figure><p>不同版本的内核有不同的输出，3.10是blk_update_request，4.14是print_seq_error</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ~]<span class="hljs-comment"># dmesg </span><br>[  527.575584] sd 0:0:20:0: [sdc] tag<span class="hljs-comment">#8 FAILED Result: hostbyte=DID_OK driverbyte=DRIVER_SENSE</span><br>[  527.575598] sd 0:0:20:0: [sdc] tag<span class="hljs-comment">#8 Sense Key : Medium Error [current] </span><br>[  527.575602] sd 0:0:20:0: [sdc] tag<span class="hljs-comment">#8 Add. Sense: Unrecovered read error</span><br>[  527.575609] sd 0:0:20:0: [sdc] tag<span class="hljs-comment">#8 CDB: Read(16) 88 00 00 00 00 00 00 00 50 20 00 00 02 00 00 00</span><br>[  527.575614] print_req_error: critical medium error, dev sdc, sector 20737<br></code></pre></td></tr></table></figure><p>完整信息</p><p>然后观察现象即可</p><h3 id="恢复扇区"><a href="#恢复扇区" class="headerlink" title="恢复扇区"></a>恢复扇区</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ~]<span class="hljs-comment">#  hdparm --yes-i-know-what-i-am-doing --repair-sector  20737 /dev/sdf</span><br></code></pre></td></tr></table></figure><p>都测试完成后，进行环境的恢复，避免后面触发其它的问题 </p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本篇记录了模拟破坏指定扇区的方法，触发一些问题，并寻找解决方法</p>]]></content>
    
    
    <categories>
      
      <category>问题处理</category>
      
    </categories>
    
    
    <tags>
      
      <tag>存储相关</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>udp服务器和客户端</title>
    <link href="/2024/08/14/udp%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%92%8C%E5%AE%A2%E6%88%B7%E7%AB%AF/"/>
    <url>/2024/08/14/udp%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%92%8C%E5%AE%A2%E6%88%B7%E7%AB%AF/</url>
    
    <content type="html"><![CDATA[<h2 id="需求背景"><a href="#需求背景" class="headerlink" title="需求背景"></a>需求背景</h2><p>线上的项目有使用lvs作为端口转发，使用的是udp的端口转发服务，想在测试环境下面模拟一个高并发的udp的服务器和客户端的环境，现成的没有相关的软件，那么就用两个脚本进行实现</p><p>要实现这个服务，需要准备一个服务端的脚本，和客户端的脚本，服务端使用用本地文件对外提供服务，客户端请求数据流并保存到本地</p><h2 id="需求分析"><a href="#需求分析" class="headerlink" title="需求分析"></a>需求分析</h2><p>考虑高并发，所以需要对软件进行一下限速，然后尽量高的并发的，一个端口响应一个请求，并发的取读取udp的请求</p><h2 id="需求实现"><a href="#需求实现" class="headerlink" title="需求实现"></a>需求实现</h2><h3 id="python实现"><a href="#python实现" class="headerlink" title="python实现"></a>python实现</h3><p>这个实现是最开始实现的一个版本，服务端和客户端都采用的python的，一个python进程启动20个端口，然后并发启动进程，这个发现一个问题，因为python的进程占用cpu很容易100%，这个对系统的资源有点大，造成无法处理很大的并发，客户端也是，这个如果小批量的测试还是可以的，也记录下</p><h4 id="服务端实现"><a href="#服务端实现" class="headerlink" title="服务端实现"></a>服务端实现</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 tftpboot]<span class="hljs-comment"># cat server2-1.py</span><br><span class="hljs-comment">#! /usr/bin/env python3</span><br><span class="hljs-comment"># -*- coding:utf-8 -*-</span><br>import socket<br>import os<br>import time<br>import threading<br><br>def start_udp_file_server(host, port, buffer_size=1024, file_path=<span class="hljs-string">&#x27;video.mp4&#x27;</span>, target_speed_mbps=2):<br>    <span class="hljs-comment"># 创建UDP套接字</span><br>    server_socket = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)<br><br>    <span class="hljs-comment"># 绑定到指定的地址和端口</span><br>    server_socket.bind((host, port))<br>    <span class="hljs-built_in">print</span>(f<span class="hljs-string">&quot;UDP 文件服务器启动，监听 &#123;host&#125;:&#123;port&#125;...&quot;</span>)<br><br>    <span class="hljs-comment"># 计算每MB需要的时间（秒）</span><br>    target_speed_bps = target_speed_mbps * 1024 * 1024<br>    time_per_chunk = buffer_size / target_speed_bps<br><br>    <span class="hljs-keyword">while</span> True:<br>        try:<br>            <span class="hljs-comment"># 接收客户端的请求</span><br>            data, addr = server_socket.recvfrom(buffer_size)<br>            request = data.decode()<br><br>            <span class="hljs-built_in">print</span>(f<span class="hljs-string">&quot;接收到来自 &#123;addr&#125; 的请求: &#123;request&#125;&quot;</span>)<br><br>            <span class="hljs-comment"># 判断请求是否为“GET”并且文件存在</span><br>            <span class="hljs-keyword">if</span> request == <span class="hljs-string">&#x27;GET&#x27;</span> and os.path.exists(file_path):<br>                start_time = time.time()<br>                total_sent = 0<br><br>                with open(file_path, <span class="hljs-string">&#x27;rb&#x27;</span>) as file:<br>                    <span class="hljs-keyword">while</span> True:<br>                        chunk = file.read(buffer_size)<br>                        <span class="hljs-keyword">if</span> not chunk:<br>                            <span class="hljs-built_in">break</span><br>                        <span class="hljs-comment"># 发送数据块到客户端</span><br>                        server_socket.sendto(chunk, addr)<br>                        total_sent += len(chunk)<br><br>                        <span class="hljs-comment"># 计算并显示发送速度</span><br>                        elapsed_time = time.time() - start_time<br>                        speed = total_sent / (1024 * 1024) / elapsed_time  <span class="hljs-comment"># MB/s</span><br>                 <span class="hljs-comment">#       print(f&quot;已发送 &#123;total_sent / (1024 * 1024):.2f&#125; MB, 速度: &#123;speed:.2f&#125; MB/s&quot;)</span><br><br>                        <span class="hljs-comment"># 控制发送速度</span><br>                        time.sleep(time_per_chunk)<br><br>                <span class="hljs-comment"># 发送文件结束标志</span><br>                server_socket.sendto(b<span class="hljs-string">&#x27;END&#x27;</span>, addr)<br>                <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;文件已发送完毕&quot;</span>)<br>            <span class="hljs-keyword">else</span>:<br>                server_socket.sendto(b<span class="hljs-string">&#x27;FILE_NOT_FOUND&#x27;</span>, addr)<br>                <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;文件未找到或请求无效&quot;</span>)<br>        except Exception as e:<br>            <span class="hljs-built_in">print</span>(f<span class="hljs-string">&quot;发生错误: &#123;e&#125;&quot;</span>)<br><br>def run_servers_on_multiple_ports(host=<span class="hljs-string">&#x27;192.167.19.101&#x27;</span>, ports=[30000,30001,30002,30003,30004,30005,30006,30007,30008,30009,30010,30011,30012,30013,30014,30015,30016,30017,30018,30019,30020], buffer_size=1024, file_path=<span class="hljs-string">&#x27;video.mp4&#x27;</span>, target_speed_mbps=2):<br>    threads = []<br>    <span class="hljs-keyword">for</span> port <span class="hljs-keyword">in</span> ports:<br>        thread = threading.Thread(target=start_udp_file_server, args=(host, port, buffer_size, file_path, target_speed_mbps))<br>        thread.start()<br>        threads.append(thread)<br>    <span class="hljs-keyword">for</span> thread <span class="hljs-keyword">in</span> threads:<br>        thread.join()<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    run_servers_on_multiple_ports()<br></code></pre></td></tr></table></figure><p>使用方法 </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">python server.py <br></code></pre></td></tr></table></figure><h4 id="客户端实现"><a href="#客户端实现" class="headerlink" title="客户端实现"></a>客户端实现</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab104 udp]<span class="hljs-comment"># cat test.py</span><br><span class="hljs-comment">#! /usr/bin/env python3</span><br><span class="hljs-comment"># -*- coding:utf-8 -*-</span><br><br>import socket<br>import sys<br>def udp_file_client(server_ip, server_port, request_message=<span class="hljs-string">&#x27;GET&#x27;</span>, output_file=<span class="hljs-string">&#x27;received_video.mp4&#x27;</span>, buffer_size=1024):<br>    <span class="hljs-comment"># 创建UDP套接字</span><br>    client_socket = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)<br><br>    <span class="hljs-comment"># 发送请求到服务器</span><br>    client_socket.sendto(request_message.encode(), (server_ip, server_port))<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;请求已发送&quot;</span>)<br><br>    with open(output_file, <span class="hljs-string">&#x27;wb&#x27;</span>) as file:<br>        <span class="hljs-keyword">while</span> True:<br>            <span class="hljs-comment"># 接收数据</span><br>            data, _ = client_socket.recvfrom(buffer_size)<br>            <span class="hljs-keyword">if</span> data == b<span class="hljs-string">&#x27;END&#x27;</span>:<br>                <span class="hljs-built_in">break</span><br>            <span class="hljs-keyword">elif</span> data == b<span class="hljs-string">&#x27;FILE_NOT_FOUND&#x27;</span>:<br>                <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;文件未找到&quot;</span>)<br>                <span class="hljs-built_in">break</span><br>            <span class="hljs-keyword">else</span>:<br>                file.write(data)<br><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;文件接收完毕&quot;</span>)<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    ip = sys.argv[1]<br>    port = int(sys.argv[2])<br>    udp_file_client(ip, port)<br></code></pre></td></tr></table></figure><p>使用方法</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab104 udp]<span class="hljs-comment"># cat test1.sh</span><br><span class="hljs-keyword">for</span> a <span class="hljs-keyword">in</span> `<span class="hljs-built_in">seq</span> 30001 30200`<br><span class="hljs-keyword">do</span><br><br>python test.py 192.168.3.235 <span class="hljs-variable">$a</span> &amp;<br><span class="hljs-built_in">sleep</span> 0.5<br><span class="hljs-keyword">done</span><br></code></pre></td></tr></table></figure><p>这样就可以并发请求了</p><h3 id="go的实现"><a href="#go的实现" class="headerlink" title="go的实现"></a>go的实现</h3><p>go的好处是比python的占用小，并且处处可运行，无需基础环境</p><h4 id="服务端的实现"><a href="#服务端的实现" class="headerlink" title="服务端的实现"></a>服务端的实现</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab103 udp]<span class="hljs-comment"># cat server1.go</span><br>package main<br><br>import (<br><span class="hljs-string">&quot;fmt&quot;</span><br><span class="hljs-string">&quot;net&quot;</span><br><span class="hljs-string">&quot;os&quot;</span><br><span class="hljs-string">&quot;strconv&quot;</span><br><span class="hljs-string">&quot;sync&quot;</span><br><span class="hljs-string">&quot;time&quot;</span><br>)<br><br>func startUDPFileServer(wg *sync.WaitGroup, host string, port int, bufferSize int, filePath string, targetSpeedMbps int) &#123;<br>defer wg.Done()<br><br>addr := fmt.Sprintf(<span class="hljs-string">&quot;%s:%d&quot;</span>, host, port)<br>udpAddr, err := net.ResolveUDPAddr(<span class="hljs-string">&quot;udp&quot;</span>, addr)<br><span class="hljs-keyword">if</span> err != nil &#123;<br>fmt.Printf(<span class="hljs-string">&quot;Failed to resolve address: %v\n&quot;</span>, err)<br><span class="hljs-built_in">return</span><br>&#125;<br><br>conn, err := net.ListenUDP(<span class="hljs-string">&quot;udp&quot;</span>, udpAddr)<br><span class="hljs-keyword">if</span> err != nil &#123;<br>fmt.Printf(<span class="hljs-string">&quot;Failed to listen on %s: %v\n&quot;</span>, addr, err)<br><span class="hljs-built_in">return</span><br>&#125;<br>defer conn.Close()<br><br>fmt.Printf(<span class="hljs-string">&quot;UDP File server started, listening on %s...\n&quot;</span>, addr)<br><br>targetSpeedBps := targetSpeedMbps * 1024 * 1024<br><br>buffer := make([]byte, bufferSize)<br><br><span class="hljs-keyword">for</span> &#123;<br>n, clientAddr, err := conn.ReadFromUDP(buffer)<br><span class="hljs-keyword">if</span> err != nil &#123;<br>fmt.Printf(<span class="hljs-string">&quot;Error receiving data: %v\n&quot;</span>, err)<br><span class="hljs-built_in">continue</span><br>&#125;<br>request := string(buffer[:n])<br>fmt.Printf(<span class="hljs-string">&quot;Received request from %v: %s\n&quot;</span>, clientAddr, request)<br><br><span class="hljs-keyword">if</span> request == <span class="hljs-string">&quot;GET&quot;</span> &#123;<br><span class="hljs-keyword">if</span> _, err := os.Stat(filePath); os.IsNotExist(err) &#123;<br>conn.WriteToUDP([]byte(<span class="hljs-string">&quot;FILE_NOT_FOUND&quot;</span>), clientAddr)<br>fmt.Println(<span class="hljs-string">&quot;File not found&quot;</span>)<br><span class="hljs-built_in">continue</span><br>&#125;<br><br>file, err := os.Open(filePath)<br><span class="hljs-keyword">if</span> err != nil &#123;<br>fmt.Printf(<span class="hljs-string">&quot;Failed to open file: %v\n&quot;</span>, err)<br><span class="hljs-built_in">continue</span><br>&#125;<br>defer file.Close()<br><br>totalSent := 0<br>startTime := time.Now()<br><br><span class="hljs-keyword">for</span> &#123;<br>n, err := file.Read(buffer)<br><span class="hljs-keyword">if</span> err != nil &#123;<br><span class="hljs-built_in">break</span><br>&#125;<br>_, err = conn.WriteToUDP(buffer[:n], clientAddr)<br><span class="hljs-keyword">if</span> err != nil &#123;<br>fmt.Printf(<span class="hljs-string">&quot;Error sending data: %v\n&quot;</span>, err)<br><span class="hljs-built_in">break</span><br>&#125;<br>totalSent += n<br><br>// 控制发送速度<br>timeElapsed := time.Since(startTime).Seconds()<br>expectedTime := float64(totalSent) / float64(targetSpeedBps)<br><span class="hljs-keyword">if</span> timeElapsed &lt; expectedTime &#123;<br>time.Sleep(time.Duration(expectedTime-timeElapsed) * time.Second)<br>&#125;<br>&#125;<br><br>conn.WriteToUDP([]byte(<span class="hljs-string">&quot;END&quot;</span>), clientAddr)<br>fmt.Println(<span class="hljs-string">&quot;File sent successfully&quot;</span>)<br>&#125; <span class="hljs-keyword">else</span> &#123;<br>conn.WriteToUDP([]byte(<span class="hljs-string">&quot;INVALID_REQUEST&quot;</span>), clientAddr)<br>fmt.Println(<span class="hljs-string">&quot;Invalid request&quot;</span>)<br>&#125;<br>&#125;<br>&#125;<br><br>func runServersOnMultiplePorts(host string, startPort, endPort int, bufferSize int, filePath string, targetSpeedMbps int) &#123;<br>var wg sync.WaitGroup<br><br><span class="hljs-keyword">for</span> port := startPort; port &lt;= endPort; port++ &#123;<br>wg.Add(1)<br>go startUDPFileServer(&amp;wg, host, port, bufferSize, filePath, targetSpeedMbps)<br>&#125;<br><br>wg.Wait()<br>&#125;<br><br>func <span class="hljs-function"><span class="hljs-title">main</span></span>() &#123;<br><span class="hljs-keyword">if</span> len(os.Args) != 4 &#123;<br>fmt.Println(<span class="hljs-string">&quot;Usage: go run server.go &lt;host&gt; &lt;start_port&gt; &lt;end_port&gt;&quot;</span>)<br><span class="hljs-built_in">return</span><br>&#125;<br><br>host := os.Args[1]<br>startPort, err1 := strconv.Atoi(os.Args[2])<br>endPort, err2 := strconv.Atoi(os.Args[3])<br><br><span class="hljs-keyword">if</span> err1 != nil || err2 != nil &#123;<br>fmt.Println(<span class="hljs-string">&quot;Invalid port number&quot;</span>)<br><span class="hljs-built_in">return</span><br>&#125;<br><br>runServersOnMultiplePorts(host, startPort, endPort, 1024, <span class="hljs-string">&quot;video.mp4&quot;</span>, 2)<br>&#125;<br></code></pre></td></tr></table></figure><p>使用方法</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab103 udp]<span class="hljs-comment"># ./server1 192.168.19.103 30001  30050</span><br></code></pre></td></tr></table></figure><h4 id="客户端的实现"><a href="#客户端的实现" class="headerlink" title="客户端的实现"></a>客户端的实现</h4><h5 id="版本一：存储文件的版本"><a href="#版本一：存储文件的版本" class="headerlink" title="版本一：存储文件的版本"></a>版本一：存储文件的版本</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab104 udp]<span class="hljs-comment"># cat client1.go</span><br>package main<br><br>import (<br><span class="hljs-string">&quot;fmt&quot;</span><br><span class="hljs-string">&quot;net&quot;</span><br><span class="hljs-string">&quot;os&quot;</span><br><span class="hljs-string">&quot;strconv&quot;</span><br><span class="hljs-string">&quot;sync&quot;</span><br><span class="hljs-string">&quot;time&quot;</span><br>)<br><br>func udpFileClient(serverIP string, port int, requestMessage string, outputFile string, bufferSize int, wg *sync.WaitGroup) &#123;<br>defer wg.Done()<br><br>// 创建UDP地址<br>serverAddr := &amp;net.UDPAddr&#123;<br>IP:   net.ParseIP(serverIP),<br>Port: port,<br>&#125;<br><br>// 创建UDP套接字<br>conn, err := net.DialUDP(<span class="hljs-string">&quot;udp&quot;</span>, nil, serverAddr)<br><span class="hljs-keyword">if</span> err != nil &#123;<br>fmt.Printf(<span class="hljs-string">&quot;端口 %d: 无法创建UDP套接字: %v\n&quot;</span>, port, err)<br><span class="hljs-built_in">return</span><br>&#125;<br>defer conn.Close()<br><br>// 发送请求到服务器<br>_, err = conn.Write([]byte(requestMessage))<br><span class="hljs-keyword">if</span> err != nil &#123;<br>fmt.Printf(<span class="hljs-string">&quot;端口 %d: 发送请求失败: %v\n&quot;</span>, port, err)<br><span class="hljs-built_in">return</span><br>&#125;<br>fmt.Printf(<span class="hljs-string">&quot;端口 %d: 请求已发送\n&quot;</span>, port)<br><br>// 打开文件用于写入接收的数据<br>file, err := os.Create(fmt.Sprintf(<span class="hljs-string">&quot;%s_%d.mp4&quot;</span>, outputFile, port))<br><span class="hljs-keyword">if</span> err != nil &#123;<br>fmt.Printf(<span class="hljs-string">&quot;端口 %d: 无法创建文件: %v\n&quot;</span>, port, err)<br><span class="hljs-built_in">return</span><br>&#125;<br>defer file.Close()<br><br>buffer := make([]byte, bufferSize)<br><span class="hljs-keyword">for</span> &#123;<br>conn.SetReadDeadline(time.Now().Add(5 * time.Second)) // 设置超时时间<br>n, _, err := conn.ReadFromUDP(buffer)<br><span class="hljs-keyword">if</span> err != nil &#123;<br><span class="hljs-keyword">if</span> netErr, ok := err.(net.Error); ok &amp;&amp; netErr.<span class="hljs-function"><span class="hljs-title">Timeout</span></span>() &#123;<br>fmt.Printf(<span class="hljs-string">&quot;端口 %d: 接收超时\n&quot;</span>, port)<br><span class="hljs-built_in">break</span><br>&#125;<br>fmt.Printf(<span class="hljs-string">&quot;端口 %d: 接收数据失败: %v\n&quot;</span>, port, err)<br><span class="hljs-built_in">return</span><br>&#125;<br><br>data := buffer[:n]<br><span class="hljs-keyword">if</span> string(data) == <span class="hljs-string">&quot;END&quot;</span> &#123;<br><span class="hljs-built_in">break</span><br>&#125; <span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span> string(data) == <span class="hljs-string">&quot;FILE_NOT_FOUND&quot;</span> &#123;<br>fmt.Printf(<span class="hljs-string">&quot;端口 %d: 文件未找到\n&quot;</span>, port)<br><span class="hljs-built_in">return</span><br>&#125;<br><br>_, err = file.Write(data)<br><span class="hljs-keyword">if</span> err != nil &#123;<br>fmt.Printf(<span class="hljs-string">&quot;端口 %d: 写入文件失败: %v\n&quot;</span>, port, err)<br><span class="hljs-built_in">return</span><br>&#125;<br>&#125;<br><br>fmt.Printf(<span class="hljs-string">&quot;端口 %d: 文件接收完毕\n&quot;</span>, port)<br>&#125;<br><br>func <span class="hljs-function"><span class="hljs-title">main</span></span>() &#123;<br><span class="hljs-keyword">if</span> len(os.Args) != 4 &#123;<br>fmt.Println(<span class="hljs-string">&quot;用法: go run client.go &lt;IP&gt; &lt;起始端口&gt; &lt;结束端口&gt;&quot;</span>)<br><span class="hljs-built_in">return</span><br>&#125;<br><br>ip := os.Args[1]<br>startPort, err := strconv.Atoi(os.Args[2])<br><span class="hljs-keyword">if</span> err != nil &#123;<br>fmt.Println(<span class="hljs-string">&quot;起始端口无效&quot;</span>)<br><span class="hljs-built_in">return</span><br>&#125;<br>endPort, err := strconv.Atoi(os.Args[3])<br><span class="hljs-keyword">if</span> err != nil &#123;<br>fmt.Println(<span class="hljs-string">&quot;结束端口无效&quot;</span>)<br><span class="hljs-built_in">return</span><br>&#125;<br><br><span class="hljs-keyword">if</span> startPort &gt; endPort &#123;<br>fmt.Println(<span class="hljs-string">&quot;起始端口不能大于结束端口&quot;</span>)<br><span class="hljs-built_in">return</span><br>&#125;<br><br>var wg sync.WaitGroup<br><span class="hljs-keyword">for</span> port := startPort; port &lt;= endPort; port++ &#123;<br>wg.Add(1)<br>go udpFileClient(ip, port, <span class="hljs-string">&quot;GET&quot;</span>, <span class="hljs-string">&quot;received_video&quot;</span>, 1024, &amp;wg)<br>&#125;<br><br>wg.Wait()<br>&#125;<br></code></pre></td></tr></table></figure><p>使用方法</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">./client1 192.168.19.103 30001 30050<br></code></pre></td></tr></table></figure><h5 id="版本二：存储内存的版本"><a href="#版本二：存储内存的版本" class="headerlink" title="版本二：存储内存的版本"></a>版本二：存储内存的版本</h5><p>上面的客户端的版本是本地要存储实际文件的，这个可能本地磁盘性能也会有影响，我们可以给一个丢内存的版本</p><p>客户端只请求，数据丢内存的版本</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab104 udp]<span class="hljs-comment"># cat client2.go</span><br>package main<br><br>import (<br><span class="hljs-string">&quot;bytes&quot;</span><br><span class="hljs-string">&quot;fmt&quot;</span><br><span class="hljs-string">&quot;net&quot;</span><br><span class="hljs-string">&quot;os&quot;</span><br><span class="hljs-string">&quot;strconv&quot;</span><br><span class="hljs-string">&quot;sync&quot;</span><br><span class="hljs-string">&quot;time&quot;</span><br>)<br><br>func udpFileClient(serverIP string, port int, requestMessage string, bufferSize int, wg *sync.WaitGroup) &#123;<br>defer wg.Done()<br><br>// 创建UDP地址<br>serverAddr := &amp;net.UDPAddr&#123;<br>IP:   net.ParseIP(serverIP),<br>Port: port,<br>&#125;<br><br>// 创建UDP套接字<br>conn, err := net.DialUDP(<span class="hljs-string">&quot;udp&quot;</span>, nil, serverAddr)<br><span class="hljs-keyword">if</span> err != nil &#123;<br>fmt.Printf(<span class="hljs-string">&quot;端口 %d: 无法创建UDP套接字: %v\n&quot;</span>, port, err)<br><span class="hljs-built_in">return</span><br>&#125;<br>defer conn.Close()<br><br>// 发送请求到服务器<br>_, err = conn.Write([]byte(requestMessage))<br><span class="hljs-keyword">if</span> err != nil &#123;<br>fmt.Printf(<span class="hljs-string">&quot;端口 %d: 发送请求失败: %v\n&quot;</span>, port, err)<br><span class="hljs-built_in">return</span><br>&#125;<br>fmt.Printf(<span class="hljs-string">&quot;端口 %d: 请求已发送\n&quot;</span>, port)<br><br>// 使用 bytes.Buffer 存储接收到的数据<br>var buffer bytes.Buffer<br>dataBuffer := make([]byte, bufferSize)<br><br><span class="hljs-keyword">for</span> &#123;<br>conn.SetReadDeadline(time.Now().Add(5 * time.Second)) // 设置超时时间<br>n, _, err := conn.ReadFromUDP(dataBuffer)<br><span class="hljs-keyword">if</span> err != nil &#123;<br><span class="hljs-keyword">if</span> netErr, ok := err.(net.Error); ok &amp;&amp; netErr.<span class="hljs-function"><span class="hljs-title">Timeout</span></span>() &#123;<br>fmt.Printf(<span class="hljs-string">&quot;端口 %d: 接收超时\n&quot;</span>, port)<br><span class="hljs-built_in">break</span><br>&#125;<br>fmt.Printf(<span class="hljs-string">&quot;端口 %d: 接收数据失败: %v\n&quot;</span>, port, err)<br><span class="hljs-built_in">return</span><br>&#125;<br><br>data := dataBuffer[:n]<br><span class="hljs-keyword">if</span> string(data) == <span class="hljs-string">&quot;END&quot;</span> &#123;<br><span class="hljs-built_in">break</span><br>&#125; <span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span> string(data) == <span class="hljs-string">&quot;FILE_NOT_FOUND&quot;</span> &#123;<br>fmt.Printf(<span class="hljs-string">&quot;端口 %d: 文件未找到\n&quot;</span>, port)<br><span class="hljs-built_in">return</span><br>&#125;<br><br>_, err = buffer.Write(data)<br><span class="hljs-keyword">if</span> err != nil &#123;<br>fmt.Printf(<span class="hljs-string">&quot;端口 %d: 写入内存失败: %v\n&quot;</span>, port, err)<br><span class="hljs-built_in">return</span><br>&#125;<br>&#125;<br><br>fmt.Printf(<span class="hljs-string">&quot;端口 %d: 文件接收完毕，总数据大小: %d bytes\n&quot;</span>, port, buffer.Len())<br>// 如果需要，你可以在这里进一步处理 buffer 中的数据<br>&#125;<br><br>func <span class="hljs-function"><span class="hljs-title">main</span></span>() &#123;<br><span class="hljs-keyword">if</span> len(os.Args) != 4 &#123;<br>fmt.Println(<span class="hljs-string">&quot;用法: go run client.go &lt;IP&gt; &lt;起始端口&gt; &lt;结束端口&gt;&quot;</span>)<br><span class="hljs-built_in">return</span><br>&#125;<br><br>ip := os.Args[1]<br>startPort, err := strconv.Atoi(os.Args[2])<br><span class="hljs-keyword">if</span> err != nil &#123;<br>fmt.Println(<span class="hljs-string">&quot;起始端口无效&quot;</span>)<br><span class="hljs-built_in">return</span><br>&#125;<br>endPort, err := strconv.Atoi(os.Args[3])<br><span class="hljs-keyword">if</span> err != nil &#123;<br>fmt.Println(<span class="hljs-string">&quot;结束端口无效&quot;</span>)<br><span class="hljs-built_in">return</span><br>&#125;<br><br><span class="hljs-keyword">if</span> startPort &gt; endPort &#123;<br>fmt.Println(<span class="hljs-string">&quot;起始端口不能大于结束端口&quot;</span>)<br><span class="hljs-built_in">return</span><br>&#125;<br><br>var wg sync.WaitGroup<br><span class="hljs-keyword">for</span> port := startPort; port &lt;= endPort; port++ &#123;<br>wg.Add(1)<br>go udpFileClient(ip, port, <span class="hljs-string">&quot;GET&quot;</span>, 1024, &amp;wg)<br>&#125;<br><br>wg.Wait()<br>&#125;<br></code></pre></td></tr></table></figure><p>使用方法</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab104 udp]<span class="hljs-comment"># ./client2 192.168.19.103 30001 30050</span><br></code></pre></td></tr></table></figure><h5 id="版本三：存储内存并控制大小的版本"><a href="#版本三：存储内存并控制大小的版本" class="headerlink" title="版本三：存储内存并控制大小的版本"></a>版本三：存储内存并控制大小的版本</h5><p>上面的版本有个问题是文件大小多大会占用多大内存，没有处理内存的问题，我们需要控制内存，设置一个缓冲区，缓冲区满了就释放内存，下面的版本就是设置的20MB的单个文件的缓冲区，如果是2MB&#x2F;s，差不多可以缓冲10s左右的数据，这个可以自己修改具体的</p><p>20个并发占用的内存大概在1G左右，这个内存大小还行</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab104 udp]<span class="hljs-comment"># cat client3.go</span><br>package main<br><br>import (<br><span class="hljs-string">&quot;fmt&quot;</span><br><span class="hljs-string">&quot;net&quot;</span><br><span class="hljs-string">&quot;os&quot;</span><br><span class="hljs-string">&quot;strconv&quot;</span><br><span class="hljs-string">&quot;sync&quot;</span><br><span class="hljs-string">&quot;time&quot;</span><br>)<br><br>const maxBufferSize = 20 * 1024 * 1024 // 20 MB<br><br>func udpFileClient(serverIP string, port int, requestMessage string, bufferSize int, wg *sync.WaitGroup) &#123;<br>defer wg.Done()<br><br>// 创建UDP地址<br>serverAddr := &amp;net.UDPAddr&#123;<br>IP:   net.ParseIP(serverIP),<br>Port: port,<br>&#125;<br><br>// 创建UDP套接字<br>conn, err := net.DialUDP(<span class="hljs-string">&quot;udp&quot;</span>, nil, serverAddr)<br><span class="hljs-keyword">if</span> err != nil &#123;<br>fmt.Printf(<span class="hljs-string">&quot;端口 %d: 无法创建UDP套接字: %v\n&quot;</span>, port, err)<br><span class="hljs-built_in">return</span><br>&#125;<br>defer conn.Close()<br><br>// 发送请求到服务器<br>_, err = conn.Write([]byte(requestMessage))<br><span class="hljs-keyword">if</span> err != nil &#123;<br>fmt.Printf(<span class="hljs-string">&quot;端口 %d: 发送请求失败: %v\n&quot;</span>, port, err)<br><span class="hljs-built_in">return</span><br>&#125;<br>fmt.Printf(<span class="hljs-string">&quot;端口 %d: 请求已发送\n&quot;</span>, port)<br><br>// 使用固定大小的缓冲区存储接收到的数据<br>dataBuffer := make([]byte, bufferSize)<br>totalData := make([]byte, 0, maxBufferSize)<br><br><span class="hljs-keyword">for</span> &#123;<br>conn.SetReadDeadline(time.Now().Add(5 * time.Second)) // 设置超时时间<br>n, _, err := conn.ReadFromUDP(dataBuffer)<br><span class="hljs-keyword">if</span> err != nil &#123;<br><span class="hljs-keyword">if</span> netErr, ok := err.(net.Error); ok &amp;&amp; netErr.<span class="hljs-function"><span class="hljs-title">Timeout</span></span>() &#123;<br>fmt.Printf(<span class="hljs-string">&quot;端口 %d: 接收超时\n&quot;</span>, port)<br><span class="hljs-built_in">break</span><br>&#125;<br>fmt.Printf(<span class="hljs-string">&quot;端口 %d: 接收数据失败: %v\n&quot;</span>, port, err)<br><span class="hljs-built_in">return</span><br>&#125;<br><br>data := dataBuffer[:n]<br><span class="hljs-keyword">if</span> string(data) == <span class="hljs-string">&quot;END&quot;</span> &#123;<br><span class="hljs-built_in">break</span><br>&#125; <span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span> string(data) == <span class="hljs-string">&quot;FILE_NOT_FOUND&quot;</span> &#123;<br>fmt.Printf(<span class="hljs-string">&quot;端口 %d: 文件未找到\n&quot;</span>, port)<br><span class="hljs-built_in">return</span><br>&#125;<br><br>// 如果总数据量超过了最大缓冲区大小，则丢弃之前的数据<br><span class="hljs-keyword">if</span> len(totalData)+len(data) &gt; maxBufferSize &#123;<br>totalData = totalData[len(data):]<br>&#125;<br><br>totalData = append(totalData, data...)<br>&#125;<br><br>fmt.Printf(<span class="hljs-string">&quot;端口 %d: 数据接收完毕，总数据大小: %d bytes\n&quot;</span>, port, len(totalData))<br>// 在这里处理 totalData，如进行统计或分析<br>&#125;<br><br>func <span class="hljs-function"><span class="hljs-title">main</span></span>() &#123;<br><span class="hljs-keyword">if</span> len(os.Args) != 4 &#123;<br>fmt.Println(<span class="hljs-string">&quot;用法: go run client.go &lt;IP&gt; &lt;起始端口&gt; &lt;结束端口&gt;&quot;</span>)<br><span class="hljs-built_in">return</span><br>&#125;<br><br>ip := os.Args[1]<br>startPort, err := strconv.Atoi(os.Args[2])<br><span class="hljs-keyword">if</span> err != nil &#123;<br>fmt.Println(<span class="hljs-string">&quot;起始端口无效&quot;</span>)<br><span class="hljs-built_in">return</span><br>&#125;<br>endPort, err := strconv.Atoi(os.Args[3])<br><span class="hljs-keyword">if</span> err != nil &#123;<br>fmt.Println(<span class="hljs-string">&quot;结束端口无效&quot;</span>)<br><span class="hljs-built_in">return</span><br>&#125;<br><br><span class="hljs-keyword">if</span> startPort &gt; endPort &#123;<br>fmt.Println(<span class="hljs-string">&quot;起始端口不能大于结束端口&quot;</span>)<br><span class="hljs-built_in">return</span><br>&#125;<br><br>var wg sync.WaitGroup<br><span class="hljs-keyword">for</span> port := startPort; port &lt;= endPort; port++ &#123;<br>wg.Add(1)<br>go udpFileClient(ip, port, <span class="hljs-string">&quot;GET&quot;</span>, 1024, &amp;wg)<br>&#125;<br><br>wg.Wait()<br>&#125;<br></code></pre></td></tr></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>使用上面的就实现了一个udp的服务端和客户端的模拟程序</p>]]></content>
    
    
    <categories>
      
      <category>系统服务</category>
      
    </categories>
    
    
    <tags>
      
      <tag>测试相关</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>限制rbd只能在一台机器上面挂载</title>
    <link href="/2024/07/01/%E9%99%90%E5%88%B6rbd%E5%8F%AA%E8%83%BD%E5%9C%A8%E4%B8%80%E5%8F%B0%E6%9C%BA%E5%99%A8%E4%B8%8A%E9%9D%A2%E6%8C%82%E8%BD%BD/"/>
    <url>/2024/07/01/%E9%99%90%E5%88%B6rbd%E5%8F%AA%E8%83%BD%E5%9C%A8%E4%B8%80%E5%8F%B0%E6%9C%BA%E5%99%A8%E4%B8%8A%E9%9D%A2%E6%8C%82%E8%BD%BD/</url>
    
    <content type="html"><![CDATA[<h2 id="需求"><a href="#需求" class="headerlink" title="需求"></a>需求</h2><p>rbd是ceph的块接口，我们可以把设备map到服务器上面，然后当磁盘进行访问，有的软件能够基于块设备再做上层的高可用的写入的，所以，从软件来说，同一个设备在两台机器同时挂载，没有什么问题，但是如果应用层不加锁，随意的写入就可能破坏了数据，造成磁盘无法访问，那么这里就是利用ceph本身的功能来实现排它的挂载</p><p>也就是同一时刻，同一个rbd，只能被一个设备挂载</p><h2 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h2><p>查看rbd的属性</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ~]<span class="hljs-comment"># rbd info gamebase</span><br>rbd image <span class="hljs-string">&#x27;gamebase&#x27;</span>:<br>size 30 GiB <span class="hljs-keyword">in</span> 7680 objects<br>order 22 (4 MiB objects)<br>snapshot_count: 1<br><span class="hljs-built_in">id</span>: 5e6552f9f0e2<br>block_name_prefix: rbd_data.5e6552f9f0e2<br>format: 2<br>features: layering, exclusive-lock, object-map, fast-diff<br></code></pre></td></tr></table></figure><p>我们进行挂载测试</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ~]<span class="hljs-comment"># rbd-nbd map gamebase</span><br>/dev/nbd1<br>[root@lab101 ~]<span class="hljs-comment"># rbd-nbd map gamebase</span><br>/dev/nbd2<br>[root@lab101 ~]<span class="hljs-comment"># rbd-nbd map gamebase</span><br>/dev/nbd3<br>[root@lab101 ~]<span class="hljs-comment"># rbd status gamebase</span><br>Watchers:<br>watcher=192.168.0.101:0/2358144113 client.44873 cookie=139788367701840<br>watcher=192.168.0.101:0/1355685081 client.44900 cookie=139976943609808<br>watcher=192.168.0.101:0/136931942 client.44896 cookie=140146057947088<br>watcher=192.168.0.101:0/643167227 client.44893 cookie=140482743117776<br></code></pre></td></tr></table></figure><p>可以看到不加限制的时候，同一个设备在同一台机器也是可以多次挂载，多台机器更不用说了</p><h3 id="加选项后挂载测试"><a href="#加选项后挂载测试" class="headerlink" title="加选项后挂载测试"></a>加选项后挂载测试</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs bash"><br>[root@lab101 ~]<span class="hljs-comment"># rbd-nbd map gamebase  --exclusive</span><br>/dev/nbd0<br>[root@lab101 ~]<span class="hljs-comment"># rbd-nbd map gamebase  --exclusive</span><br>rbd-nbd: failed to acquire exclusive lock: 2024-07-01T18:18:26.548+0800 7f32c49ddb80 -1 librbd: failed to request exclusive lock: (30) Read-only file system<br>(30) Read-only file system<br><br>[root@lab101 ~]<span class="hljs-comment"># rbd status gamebase</span><br>Watchers:<br>watcher=192.168.0.101:0/2977136172 client.44914 cookie=139788166375248<br></code></pre></td></tr></table></figure><p>可以看到通过添加exclusive就可以独占的挂载，也就是挂载前，系统会自动的检查有没有被其它地方挂载，挂载了自然会上锁，本身挂载后是有这个watcher的，而加上exclusive这个参数后，会检查系统内部一个特殊的标签，这个内部就是这样实现挂载的排它的</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>通过增加参数能够实现排它挂载，也就是唯一的挂载</p>]]></content>
    
    
    <categories>
      
      <category>存储相关</category>
      
    </categories>
    
    
    <tags>
      
      <tag>ceph</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>利用Clonezilla进行windows的系统复制</title>
    <link href="/2024/06/27/%E5%88%A9%E7%94%A8Clonezilla%E8%BF%9B%E8%A1%8Cwindows%E7%9A%84%E7%B3%BB%E7%BB%9F%E5%A4%8D%E5%88%B6/"/>
    <url>/2024/06/27/%E5%88%A9%E7%94%A8Clonezilla%E8%BF%9B%E8%A1%8Cwindows%E7%9A%84%E7%B3%BB%E7%BB%9F%E5%A4%8D%E5%88%B6/</url>
    
    <content type="html"><![CDATA[<h2 id="需求"><a href="#需求" class="headerlink" title="需求"></a>需求</h2><p>之前利用傲梅轻松备份这个软件做过系统复制，这个是需要运行在windows下面的，可以利用winpe运行，然后进行系统复制，需要手动点击操作，而在需要自动化的场景，这个就不太适合了，那么有一款软件Clonezilla，这个复制的系统启动正常</p><h2 id="使用方式"><a href="#使用方式" class="headerlink" title="使用方式"></a>使用方式</h2><p>官方提供了iso的和zip文件的两种方式的，iso的可以直接启动的，然后进行系统的复制，而zip里面包括一个完整的系统，可以在ipxe环境下使用，本篇就记录这个ipxe下的使用</p><h3 id="配置方法"><a href="#配置方法" class="headerlink" title="配置方法"></a>配置方法</h3><p>下载解压得到启动文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@pxe101 nbi_img]<span class="hljs-comment"># unzip -j clonezilla-live-3.1.2-22-amd64.zip  live/vmlinuz live/initrd.img live/filesystem.squashfs  -d ./</span><br>Archive:  clonezilla-live-3.1.2-22-amd64.zip<br>  inflating: ./initrd.img<br>  inflating: ./filesystem.squashfs<br>  inflating: ./vmlinuz<br></code></pre></td></tr></table></figure><p>完整的系统已经在里面了</p><p>ipxe的脚本的写法</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs bash">:CloneBoot<br><br><span class="hljs-built_in">echo</span> Starting CentOS Install <span class="hljs-variable">$&#123;archl&#125;</span> <span class="hljs-keyword">for</span> <span class="hljs-variable">$&#123;initiator-iqn&#125;</span><br>cpuid --ext 29 &amp;&amp; <span class="hljs-built_in">set</span> <span class="hljs-built_in">arch</span> amd64 || <span class="hljs-built_in">set</span> <span class="hljs-built_in">arch</span> x86<br>sanhook --drive 0x81 iscsi:192.168.19.103::::iqn.2017-05.net.vlnb:tgt<br><span class="hljs-built_in">set</span> base-url http://<span class="hljs-variable">$&#123;SAN_IP&#125;</span>/nbi_img<br>kernel <span class="hljs-variable">$&#123;base-url&#125;</span>/vmlinuz<br>initrd <span class="hljs-variable">$&#123;base-url&#125;</span>/initrd.img<br><span class="hljs-comment"># ocs_live_batch =no 就是选择模式，yes就是直接进入了程序了(sda 是本地盘)</span><br><span class="hljs-comment">#imgargs vmlinuz **initrd=initrd.img** boot=live username=user union=overlay fetch=$&#123;base-url&#125;/filesystem.squashfs   config components quiet noswap  edd=on nomodeset nodmraid locales= keyboard-layouts=us  net.ifnames=0   nosplash noprompt  ocs_daemonon=&quot;ssh&quot; ocs_prerun=&quot;iscsiadm -m discovery -t sendtargets -p 192.168.19.103&quot; ocs_prerun1=&quot;iscsiadm -m node -T iqn.2017-05.net.vlnb:tgt -p 192.168.19.103 -l&quot;  ocs_live_run=&quot;ocs-live-general&quot;  ocs_live_extra_param=&quot;&quot; ocs_live_batch=no</span><br>imgargs vmlinuz **initrd=initrd.img** boot=live username=user union=overlay fetch=<span class="hljs-variable">$&#123;base-url&#125;</span>/filesystem.squashfs   config components quiet noswap  edd=on nomodeset nodmraid locales=keyboard-layouts=us  net.ifnames=0   nosplash noprompt  ocs_daemonon=<span class="hljs-string">&quot;ssh&quot;</span>  ocs_prerun=<span class="hljs-string">&quot;ip link set down eth0&quot;</span> ocs_prerun1=<span class="hljs-string">&quot;ifconfig eth5 192.168.19.245 netmask 255.255.0.0&quot;</span>  ocs_prerun2=<span class="hljs-string">&quot;iscsiadm -m discovery -t sendtargets -p 192.168.19.103&quot;</span> ocs_prerun3=<span class="hljs-string">&quot;iscsiadm -m node -T iqn.2017-05.net.vlnb:tgt -p 192.168.19.103 -l&quot;</span>  ocs_live_run=<span class="hljs-string">&quot;/usr/sbin/ocs-onthefly-batch -g auto -e1 auto -e2 -r -j2 -fsck-y -k0 -p choose -f sdb -d sda&quot;</span>  ocs_live_extra_param=<span class="hljs-string">&quot;&quot;</span> ocs_live_batch=<span class="hljs-built_in">yes</span><br>boot || goto failed<br>goto start<br></code></pre></td></tr></table></figure><p>上面注释掉的imgargs里面是交互模式的，也就是进去选择怎么操作的，一般不自动化的场景用交互模式就行了，如果要自动化的场景就用后面的那个就行，这个环境还用到了iscsi的无盘的，有其它需要的在里面进行配置即可，网络也可以配置</p><h2 id="复制性能"><a href="#复制性能" class="headerlink" title="复制性能"></a>复制性能</h2><p>这个软件进行了一些优化，复制的性能还可以，我的环境是</p><ul><li>从ssd到iscsi是8GB&#x2F;min</li><li>从iscsi到ssd是16GB&#x2F;s</li></ul><p>这个不同的环境性能会不同的，整体上来说软件使用很简单</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>记录下这个windows的复制软件 </p>]]></content>
    
    
    <categories>
      
      <category>操作系统</category>
      
    </categories>
    
    
    <tags>
      
      <tag>windows</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>rbd做快照克隆以后的容量相关问题</title>
    <link href="/2024/06/26/rbd%E5%81%9A%E5%BF%AB%E7%85%A7%E5%85%8B%E9%9A%86%E4%BB%A5%E5%90%8E%E7%9A%84%E5%AE%B9%E9%87%8F%E7%9B%B8%E5%85%B3%E9%97%AE%E9%A2%98/"/>
    <url>/2024/06/26/rbd%E5%81%9A%E5%BF%AB%E7%85%A7%E5%85%8B%E9%9A%86%E4%BB%A5%E5%90%8E%E7%9A%84%E5%AE%B9%E9%87%8F%E7%9B%B8%E5%85%B3%E9%97%AE%E9%A2%98/</url>
    
    <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>rbd的做快照，然后克隆，原始设备可以变成两个设备供客户端使用，占用的空间为1份，这个是在没有新写入情况下的容量占用情况<br>那么如果有新写入的的数据之后，这个地方容量是怎么去看的，以及在出现写入又删除的情况下，rbd的实际占用空间又是如何释放的</p><h2 id="相关测试操作"><a href="#相关测试操作" class="headerlink" title="相关测试操作"></a>相关测试操作</h2><h3 id="快照后的容量占用"><a href="#快照后的容量占用" class="headerlink" title="快照后的容量占用"></a>快照后的容量占用</h3><p>我们先准备一个rbd的设备，并且格式化为ntfs的文件系统，然后写入一个大文件为2.4G的<br>我们看下容量占用情况</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ~]<span class="hljs-comment"># rbd du gamebase</span><br>NAME      PROVISIONED  USED<br>gamebase       30 GiB  2.4 GiB<br><br>[root@lab101 ~]<span class="hljs-comment"># ceph df</span><br>--- POOLS ---<br>POOL  ID  PGS  STORED   OBJECTS  USED     %USED  MAX AVAIL<br>rbd    9   32  2.4 GiB      631  2.4 GiB   3.22     73 GiB<br></code></pre></td></tr></table></figure><p>占用就是这么多，我们再做一个快照</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ~]<span class="hljs-comment"># rbd snap create --image gamebase --snap gamesnap1</span><br>[root@lab101 ~]<span class="hljs-comment"># rbd snap protect --image gamebase --snap gamesnap1</span><br></code></pre></td></tr></table></figure><p>再看占用情况</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ~]<span class="hljs-comment"># rbd du gamebase</span><br>NAME                PROVISIONED  USED<br>gamebase@gamesnap1       30 GiB  2.4 GiB<br>gamebase                 30 GiB      0 B<br>&lt;TOTAL&gt;                  30 GiB  2.4 GiB<br>[root@lab101 ~]<span class="hljs-comment"># ceph df</span><br>--- POOLS ---<br>POOL  ID  PGS  STORED   OBJECTS  USED     %USED  MAX AVAIL<br>rbd    9   32  2.4 GiB      632  2.4 GiB   3.22     73 GiB<br></code></pre></td></tr></table></figure><p>可以看到gamebase那里的显示是0，因为做了快照，那么做快照的那一刻以后再写入的数据，就是gamebase的写入，做了快照的部分是不变化的了，因为快照的内容是可以恢复的，所以这里肯定就是固定住的</p><p>这里我们使用ceph的内部的一个清理0空间的命令做一个操作看下</p><h3 id="sparsify操作引起的显示偏差"><a href="#sparsify操作引起的显示偏差" class="headerlink" title="sparsify操作引起的显示偏差"></a>sparsify操作引起的显示偏差</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ~]<span class="hljs-comment"># rbd sparsify rbd/gamebase</span><br>Image sparsify: 100% complete...done.<br></code></pre></td></tr></table></figure><p>我们查看下容量</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ~]<span class="hljs-comment"># rbd du gamebase</span><br>NAME                PROVISIONED  USED<br>gamebase@gamesnap1       30 GiB  2.4 GiB<br>gamebase                 30 GiB  2.4 GiB<br>&lt;TOTAL&gt;                  30 GiB  4.9 GiB<br>[root@lab101 ~]<span class="hljs-comment"># rbd du gamebase --exact</span><br>NAME                PROVISIONED  USED<br>gamebase@gamesnap1       30 GiB  2.4 GiB<br>gamebase                 30 GiB   20 MiB<br>&lt;TOTAL&gt;                  30 GiB  2.4 GiB<br></code></pre></td></tr></table></figure><p>可以看到，gamebase里面统计的值出现了很大的情况，这个值，直接就是把快照的值基本一样</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ~]<span class="hljs-comment"># ceph df</span><br>--- POOLS ---<br>POOL  ID  PGS  STORED   OBJECTS  USED     %USED  MAX AVAIL<br>rbd    9   32  2.4 GiB      640  2.4 GiB   3.23     73 GiB<br></code></pre></td></tr></table></figure><p>但是实际并没有对集群进行空间的占用，所以这里注意下，这个sparsify除非底层真的有0空间的对象，否则的话没必要做，会造成这个地方的显示差别，有参数可以排除这个差异，这里知道操作能够引起这个显示的差别即可，这个地方有正常的统计的方法即可，里面涉及到底层的一些不同的计算逻辑，我们继续后面的，环境先恢复为没有做sparsify前</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ~]<span class="hljs-comment"># rbd snap unprotect --image gamebase --snap gamesnap1</span><br>[root@lab101 ~]<span class="hljs-comment"># rbd snap rm --image gamebase --snap gamesnap1</span><br>Removing snap: 100% complete...done.<br>[root@lab101 ~]<span class="hljs-comment"># rbd snap create --image gamebase --snap gamesnap1</span><br>[root@lab101 ~]<span class="hljs-comment"># rbd snap protect --image gamebase --snap gamesnap1</span><br></code></pre></td></tr></table></figure><h3 id="windows的trim触发进行空间回收"><a href="#windows的trim触发进行空间回收" class="headerlink" title="windows的trim触发进行空间回收"></a>windows的trim触发进行空间回收</h3><p>我们克隆一个镜像</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ~]<span class="hljs-comment"># rbd clone --image gamebase --snap gamesnap1  gameclone1</span><br></code></pre></td></tr></table></figure><p>我们在windows下面直接使用这个镜像，通过windows的rbd的功能</p><p>启用.NET Franework 3.5功能</p><p>windows下面使用wnbd要注意（powershell操作）</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">Confirm-SecureBootUEFI<br>bcdedit.exe /set testsigning <span class="hljs-built_in">yes</span><br></code></pre></td></tr></table></figure><p>一个确认关闭了安全启动<br>一个是进入测试模式，也就是关闭了签名认证类的，否则安装报错</p><p>安装包的地址</p><blockquote><p><a href="https://cloudba.se/ceph-win-latest-pacific">https://cloudba.se/ceph-win-latest-pacific</a></p></blockquote><p>win10版本也注意下，我测试了几个版本不行，需要用win10 22H2版本的才能正常安装</p><p>windows的ceph配置文件<br>配置文件的路径 C:\ProgramData\Ceph\ceph.conf</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs bash">[global]<br>    <span class="hljs-built_in">log</span> to stderr = <span class="hljs-literal">true</span><br>auth_cluster_required = none<br>auth_service_required = none<br>auth_client_required = none<br>    ; Uncomment the following <span class="hljs-keyword">in</span> order to use the Windows Event Log<br>    ; <span class="hljs-built_in">log</span> to syslog = <span class="hljs-literal">true</span><br><br>    run <span class="hljs-built_in">dir</span> = C:/ProgramData/Ceph/out<br>    crash <span class="hljs-built_in">dir</span> = C:/ProgramData/Ceph/out<br><br>    ; Use the following to change the cephfs client <span class="hljs-built_in">log</span> level<br>    ; debug client = 2<br>[client]<br>   ； keyring = C:/ProgramData/Ceph/keyring<br>    ; <span class="hljs-built_in">log</span> file = C:/ProgramData/ceph/out/<span class="hljs-variable">$name</span>.<span class="hljs-variable">$pid</span>.<span class="hljs-built_in">log</span><br>    ；admin socket = C:/ProgramData/Ceph/out/<span class="hljs-variable">$name</span>.<span class="hljs-variable">$pid</span>.asok<br><br>    ; client_permissions = <span class="hljs-literal">true</span><br>    ; client_mount_uid = 1000<br>    ; client_mount_gid = 1000<br>[global]<br>    mon host = 192.168.0.101<br></code></pre></td></tr></table></figure><p>挂载rbd的命令</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">rbd-wnbd.exe -c C:\ProgramData\Ceph\ceph.conf map rbd/gameclone1<br></code></pre></td></tr></table></figure><p>卸载rbd的命令</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">rbd-wnbd.exe -c C:\ProgramData\Ceph\ceph.conf unmap rbd/gameclone1<br></code></pre></td></tr></table></figure><p>注意操作后去磁盘管理里面看看，进行联机操作后，盘符可以识别</p><p>检查下容量</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ~]<span class="hljs-comment"># rbd du gamebase</span><br>NAME                PROVISIONED  USED<br>gamebase@gamesnap1       30 GiB  2.4 GiB<br>gamebase                 30 GiB      0 B<br>&lt;TOTAL&gt;                  30 GiB  2.4 GiB<br>[root@lab101 ~]<span class="hljs-comment"># rbd du gameclone1</span><br>NAME        PROVISIONED  USED<br>gameclone1       30 GiB  32 MiB<br></code></pre></td></tr></table></figure><p>我们往gamebase和gameclone里面都写入一个400MB文件，看下容量占用情况</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ~]<span class="hljs-comment"># rbd du gameclone1</span><br>NAME        PROVISIONED  USED<br>gameclone1       30 GiB  448 MiB<br>[root@lab101 ~]<span class="hljs-comment"># rbd du gamebase</span><br>NAME                PROVISIONED  USED<br>gamebase@gamesnap1       30 GiB  2.4 GiB<br>gamebase                 30 GiB  436 MiB<br>&lt;TOTAL&gt;                  30 GiB  2.9 GiB<br>[root@lab101 ~]<span class="hljs-comment"># ceph df</span><br>--- POOLS ---<br>POOL  ID  PGS  STORED   OBJECTS  USED     %USED  MAX AVAIL<br>rbd    9   32  3.2 GiB      856  3.2 GiB   4.31     72 GiB<br></code></pre></td></tr></table></figure><p>可以看到容量都是正常的，我们再删除刚刚写入的文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ~]<span class="hljs-comment"># rbd du gamebase</span><br>NAME                PROVISIONED  USED<br>gamebase@gamesnap1       30 GiB  2.4 GiB<br>gamebase                 30 GiB  436 MiB<br>&lt;TOTAL&gt;                  30 GiB  2.9 GiB<br>[root@lab101 ~]<span class="hljs-comment"># rbd du gameclone1</span><br>NAME        PROVISIONED  USED<br>gameclone1       30 GiB  448 MiB<br>[root@lab101 ~]<span class="hljs-comment"># ceph df</span><br>--- POOLS ---<br>POOL  ID  PGS  STORED   OBJECTS  USED     %USED  MAX AVAIL<br>rbd    9   32  3.2 GiB      856  3.2 GiB   4.31     72 GiB<br></code></pre></td></tr></table></figure><p>可以看到容量没有发生任何变化，这里我们忽略了windows的一个问题，问题有回收站，所以回收站需要清理文件才是真正的删除，否则只是隐藏了而已</p><p><img src="/images/blog/2024-06-26-14-42-24.png"></p><p>清空回收站后再看容量</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ~]<span class="hljs-comment"># rbd du gamebase</span><br>NAME                PROVISIONED  USED<br>gamebase@gamesnap1       30 GiB  2.4 GiB<br>gamebase                 30 GiB   32 MiB<br>&lt;TOTAL&gt;                  30 GiB  2.5 GiB<br>[root@lab101 ~]<span class="hljs-comment"># rbd du gamebase --exact</span><br>NAME                PROVISIONED  USED<br>gamebase@gamesnap1       30 GiB  2.4 GiB<br>gamebase                 30 GiB  2.5 MiB<br>&lt;TOTAL&gt;                  30 GiB  2.4 GiB<br>[root@lab101 ~]<span class="hljs-comment"># rbd du gameclone1</span><br>NAME        PROVISIONED  USED<br>gameclone1       30 GiB  448 MiB<br>[root@lab101 ~]<span class="hljs-comment"># rbd du gameclone1 --exact</span><br>NAME        PROVISIONED  USED<br>gameclone1       30 GiB  27 MiB<br>[root@lab101 ~]<span class="hljs-comment"># ceph df</span><br>--- RAW STORAGE ---<br>CLASS  SIZE    AVAIL   USED     RAW USED  %RAW USED<br>hdd    80 GiB  77 GiB  2.5 GiB   3.5 GiB       4.37<br>TOTAL  80 GiB  77 GiB  2.5 GiB   3.5 GiB       4.37<br><br>--- POOLS ---<br>POOL  ID  PGS  STORED   OBJECTS  USED     %USED  MAX AVAIL<br>rbd    9   32  2.4 GiB      755  2.4 GiB   3.24     73 GiB<br></code></pre></td></tr></table></figure><p>可以看到，gameclone1和gamebase写入后有删除的文件，在后台都得到了空间释放，这个地方是windows自动做了trim操作，并且我们可以看到gameclone1这里不带参数的查询，显示的还是之前的那个文件大小448MB，看上去没有释放，但是用exact查询和ceph df可以看到容量其实是真正释放了</p><h3 id="全盘trim会引起底层空对象"><a href="#全盘trim会引起底层空对象" class="headerlink" title="全盘trim会引起底层空对象"></a>全盘trim会引起底层空对象</h3><p>windows的trim触发默认是文件级别的触发的，可以看到，我们再调用下磁盘级别的trim看下变化</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs bash">PS C:\Windows\system32&gt; Optimize-Volume -DriveLetter G -ReTrim -Verbose<br>详细信息: 正在调用 新加卷 (G:) 上的 重新剪裁...<br>详细信息: 正在执行传递 1:<br>详细信息: 重新剪裁:  0% 完成...<br>详细信息: 重新剪裁:  7% 完成...<br>详细信息: 重新剪裁:  10% 完成...<br>详细信息: 重新剪裁:  16% 完成...<br>详细信息: 重新剪裁:  33% 完成...<br>详细信息: 重新剪裁:  50% 完成...<br>详细信息: 重新剪裁:  66% 完成...<br>详细信息: 重新剪裁:  86% 完成...<br>详细信息: 重新剪裁:  100% 完成。<br>详细信息:<br>Post Defragmentation Report:<br>详细信息:<br> 卷信息:<br>详细信息:   卷大小                 = 29.98 GB<br>详细信息:   簇大小                 = 4 KB<br>详细信息:   已用空间               = 2.43 GB<br>详细信息:   可用空间               = 27.54 GB<br>详细信息:<br> 重新剪裁:<br>详细信息:   支持的分配          = 30<br>详细信息:   已整理的分配         = 27<br>详细信息:   已整理的空间总计         = 26.54 GB<br></code></pre></td></tr></table></figure><p>检查容量情况</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ~]<span class="hljs-comment"># rbd du gameclone1</span><br>NAME        PROVISIONED  USED<br>gameclone1       30 GiB  27 GiB<br>[root@lab101 ~]<span class="hljs-comment"># rbd du gameclone1 --exact</span><br>NAME        PROVISIONED  USED<br>gameclone1       30 GiB  38 MiB<br>[root@lab101 ~]<span class="hljs-comment"># ceph df</span><br>--- POOLS ---<br>POOL  ID  PGS  STORED   OBJECTS  USED     %USED  MAX AVAIL<br>rbd    9   32  2.5 GiB    7.50k  2.4 GiB   3.26     72 GiB<br></code></pre></td></tr></table></figure><p>可以看到那个使用USED的地方变成了27GB，这个地方是全盘的trim触发的，相当于用0空间去全部占用了，然后又全部进行了统计了，那个exact的就是会真实去计算空间占用，这个地方还有个地方变化比较大，可以看到objects变成了7.5K，也就是这个镜像全部的对象都进行了分配并且是0空间的</p><p>查看windows是否开启了trim的命令</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">PS C:\Windows\system32&gt; fsutil behavior query DisableDeleteNotify<br>NTFS DisableDeleteNotify = 0  (已禁用)<br>ReFS DisableDeleteNotify = 0  (已禁用)<br></code></pre></td></tr></table></figure><p>显示为上面的情况就是开启了的，默认是开启了trim的</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>上面做了一些测试，我们来进行一下总结</p><ul><li>1、如果image做了快照，那么容量显示就会转移到快照那个地方，image本身归0，后写入的会计算进去</li><li>2、如果对做了快照的镜像做sparsify，会在显示上显示镜像使用很大的空间，这个地方通过exact参数来检查真实占用</li><li>3、windows自带了trim，注意回收站及时清理，才能释放，自带的trim是文件级别的触发的，对底层友好</li><li>4、如果对全盘做trim的操作，那么底层的会用0空间的对象去填充这个image，底层会产生很多空对象</li><li>5、sparsify是清理的占0空间的写入，比如zero写入的，可以清理，写入实际文件又删除的情况不是sparsify去处理的，是trim处理的</li></ul><p>再精简总结就是：</p><ul><li>1、查询真实用量就加上exact参数</li><li>2、不要做sparsify操作和windows的全盘trim，让系统的trim自己处理</li></ul>]]></content>
    
    
    <categories>
      
      <category>存储相关</category>
      
    </categories>
    
    
    <tags>
      
      <tag>ceph</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>制作一个支持中文的winpeiso</title>
    <link href="/2024/05/09/%E5%88%B6%E4%BD%9C%E4%B8%80%E4%B8%AA%E6%94%AF%E6%8C%81%E4%B8%AD%E6%96%87%E7%9A%84winpeiso/"/>
    <url>/2024/05/09/%E5%88%B6%E4%BD%9C%E4%B8%80%E4%B8%AA%E6%94%AF%E6%8C%81%E4%B8%AD%E6%96%87%E7%9A%84winpeiso/</url>
    
    <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>网上有很多的winpe，有的时候确实方便，但是又怕里面放入了一些第三方的软件什么的，想要一个自定使用的纯净版的iso，本篇就是自己打的iso</p><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">下载地址<br>https://learn.microsoft.com/zh-cn/windows-hardware/get-started/adk-install<br></code></pre></td></tr></table></figure><p>选择版本2004<br><img src="/images/blog/2024-05-09-15-47-24.png"></p><p>下载链接</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">https://go.microsoft.com/fwlink/?linkid=2120254<br>https://go.microsoft.com/fwlink/?linkid=2120253<br></code></pre></td></tr></table></figure><p>安装的是ADK2004 版本</p><p>这个版本的winpe 在vmware下面进行安装的时候驱动会正常一些<br>注意如果需要安装的系统是中文版本的，那么winpe一定要是中文版本的，否则会提示驱动无法安装</p><h2 id="定制winpe"><a href="#定制winpe" class="headerlink" title="定制winpe"></a>定制winpe</h2><p><img src="/images/blog/2024-05-09-15-48-03.png"></p><p>运行这个部署和映像工具环境，就会打开一个终端<br>主要操作目录在这个下面</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">C:\Program Files (x86)\Windows Kits\10\Assessment and Deployment Kit\Windows Preinstallation Environment\<br></code></pre></td></tr></table></figure><h3 id="创建一个挂载目录"><a href="#创建一个挂载目录" class="headerlink" title="创建一个挂载目录"></a>创建一个挂载目录</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">md C:\winpe_amd64_cn\mount<br></code></pre></td></tr></table></figure><h3 id="挂载镜像"><a href="#挂载镜像" class="headerlink" title="挂载镜像"></a>挂载镜像</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">C:\Program Files (x86)\Windows Kits\10\Assessment and Deployment Kit\Windows Preinstallation Environment\amd64&gt;Dism /Mount-Image /ImageFile:<span class="hljs-string">&quot;en-us\winpe.wim&quot;</span> /index:1 /MountDir:<span class="hljs-string">&quot;C:\winpe_amd64_cn\mount&quot;</span><br></code></pre></td></tr></table></figure><h3 id="增加语言包"><a href="#增加语言包" class="headerlink" title="增加语言包"></a>增加语言包</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">Dism /Add-Package /Image:<span class="hljs-string">&quot;C:\winpe_amd64_cn\mount&quot;</span> /PackagePath:<span class="hljs-string">&quot;C:\Program Files (x86)\Windows Kits\10\Assessment and Deployment Kit\Windows Preinstallation Environment\amd64\WinPE_OCs\zh-cn\lp.cab&quot;</span><br>Dism /Add-Package /Image:<span class="hljs-string">&quot;C:\winpe_amd64_cn\mount&quot;</span> /PackagePath:<span class="hljs-string">&quot;C:\Program Files (x86)\Windows Kits\10\Assessment and Deployment Kit\Windows Preinstallation Environment\amd64\WinPE_OCs\WinPE-HTA.cab&quot;</span><br>Dism /Add-Package /Image:<span class="hljs-string">&quot;C:\winpe_amd64_cn\mount&quot;</span> /PackagePath:<span class="hljs-string">&quot;C:\Program Files (x86)\Windows Kits\10\Assessment and Deployment Kit\Windows Preinstallation Environment\amd64\WinPE_OCs\zh-cn\WinPE-HTA_zh-cn.cab&quot;</span><br><br></code></pre></td></tr></table></figure><h3 id="添加中文字体支持"><a href="#添加中文字体支持" class="headerlink" title="添加中文字体支持"></a>添加中文字体支持</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">Dism /Add-Package /Image:<span class="hljs-string">&quot;C:\winpe_amd64_cn\mount&quot;</span> /PackagePath:<span class="hljs-string">&quot;C:\Program Files (x86)\Windows Kits\10\Assessment and Deployment Kit\Windows Preinstallation Environment\amd64\WinPE_OCs\WinPE-FontSupport-ZH-CN.cab&quot;</span><br></code></pre></td></tr></table></figure><h3 id="增加存储相关的支持"><a href="#增加存储相关的支持" class="headerlink" title="增加存储相关的支持"></a>增加存储相关的支持</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs bash">Dism /Add-Package /Image:<span class="hljs-string">&quot;C:\winpe_amd64_cn\mount&quot;</span> /PackagePath:<span class="hljs-string">&quot;C:\Program Files (x86)\Windows Kits\10\Assessment and Deployment Kit\Windows Preinstallation Environment\amd64\WinPE_OCs\WinPE-WMI.cab&quot;</span><br>Dism /Add-Package /Image:<span class="hljs-string">&quot;C:\winpe_amd64_cn\mount&quot;</span> /PackagePath:<span class="hljs-string">&quot;C:\Program Files (x86)\Windows Kits\10\Assessment and Deployment Kit\Windows Preinstallation Environment\amd64\WinPE_OCs\zh-cn\WinPE-WMI_zh-cn.cab&quot;</span><br>Dism /Add-Package /Image:<span class="hljs-string">&quot;C:\winpe_amd64_cn\mount&quot;</span> /PackagePath:<span class="hljs-string">&quot;C:\Program Files (x86)\Windows Kits\10\Assessment and Deployment Kit\Windows Preinstallation Environment\amd64\WinPE_OCs\WinPE-NetFx.cab&quot;</span><br>Dism /Add-Package /Image:<span class="hljs-string">&quot;C:\winpe_amd64_cn\mount&quot;</span> /PackagePath:<span class="hljs-string">&quot;C:\Program Files (x86)\Windows Kits\10\Assessment and Deployment Kit\Windows Preinstallation Environment\amd64\WinPE_OCs\zh-cn\WinPE-NetFx_zh-cn.cab&quot;</span><br>Dism /Add-Package /Image:<span class="hljs-string">&quot;C:\winpe_amd64_cn\mount&quot;</span> /PackagePath:<span class="hljs-string">&quot;C:\Program Files (x86)\Windows Kits\10\Assessment and Deployment Kit\Windows Preinstallation Environment\amd64\WinPE_OCs\WinPE-Scripting.cab&quot;</span><br>Dism /Add-Package /Image:<span class="hljs-string">&quot;C:\winpe_amd64_cn\mount&quot;</span> /PackagePath:<span class="hljs-string">&quot;C:\Program Files (x86)\Windows Kits\10\Assessment and Deployment Kit\Windows Preinstallation Environment\amd64\WinPE_OCs\zh-cn\WinPE-Scripting_zh-cn.cab&quot;</span><br>Dism /Add-Package /Image:<span class="hljs-string">&quot;C:\winpe_amd64_cn\mount&quot;</span> /PackagePath:<span class="hljs-string">&quot;C:\Program Files (x86)\Windows Kits\10\Assessment and Deployment Kit\Windows Preinstallation Environment\amd64\WinPE_OCs\WinPE-PowerShell.cab&quot;</span><br>Dism /Add-Package /Image:<span class="hljs-string">&quot;C:\winpe_amd64_cn\mount&quot;</span> /PackagePath:<span class="hljs-string">&quot;C:\Program Files (x86)\Windows Kits\10\Assessment and Deployment Kit\Windows Preinstallation Environment\amd64\WinPE_OCs\zh-cn\WinPE-PowerShell_zh-cn.cab&quot;</span><br>Dism /Add-Package /Image:<span class="hljs-string">&quot;C:\winpe_amd64_cn\mount&quot;</span> /PackagePath:<span class="hljs-string">&quot;C:\Program Files (x86)\Windows Kits\10\Assessment and Deployment Kit\Windows Preinstallation Environment\amd64\WinPE_OCs\WinPE-StorageWMI.cab&quot;</span><br>Dism /Add-Package /Image:<span class="hljs-string">&quot;C:\winpe_amd64_cn\mount&quot;</span> /PackagePath:<span class="hljs-string">&quot;C:\Program Files (x86)\Windows Kits\10\Assessment and Deployment Kit\Windows Preinstallation Environment\amd64\WinPE_OCs\zh-cn\WinPE-StorageWMI_zh-cn.cab&quot;</span><br></code></pre></td></tr></table></figure><p>注意增加包是有依赖关系的，所以一定按顺序操作</p><h3 id="查看包"><a href="#查看包" class="headerlink" title="查看包"></a>查看包</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">Dism /Get-Packages /Image:<span class="hljs-string">&quot;C:\winpe_amd64_cn\mount&quot;</span><br></code></pre></td></tr></table></figure><h3 id="卸载包（如果有需求）"><a href="#卸载包（如果有需求）" class="headerlink" title="卸载包（如果有需求）"></a>卸载包（如果有需求）</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">Dism /Image:<span class="hljs-string">&quot;C:\winpe_amd64_cn\mount&quot;</span> /Remove-Package /PackageName:WinPE-Setup-Package~31bf3856ad364e35~amd64~zh-CN~10.0.19041.1<br></code></pre></td></tr></table></figure><h3 id="winpe语言设置"><a href="#winpe语言设置" class="headerlink" title="winpe语言设置"></a>winpe语言设置</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">Dism /Set-AllIntl:zh-CN /Image:<span class="hljs-string">&quot;C:\winpe_amd64_cn\mount&quot;</span><br></code></pre></td></tr></table></figure><p>设置为中文的</p><h3 id="提交并保存"><a href="#提交并保存" class="headerlink" title="提交并保存"></a>提交并保存</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">Dism /Unmount-Image /MountDir:C:\winpe_amd64_cn\mount /Commit<br></code></pre></td></tr></table></figure><p>到这里就完成了镜像的定制了</p><h2 id="制作WINPE的ISO"><a href="#制作WINPE的ISO" class="headerlink" title="制作WINPE的ISO"></a>制作WINPE的ISO</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">copype amd64 C:\winpe_cn<br>MakeWinPEmedia /ISO C:\winpe_cn  C:\winpe_cn\winpe_cn.iso<br></code></pre></td></tr></table></figure><p>得到的就是一个支持存储，支持中文的winpe iso了，版本是2004版本的</p><h2 id="备注"><a href="#备注" class="headerlink" title="备注"></a>备注</h2><p>如果出现windows无法挂载镜像，就去这个注册表里面删除相关的挂载信息</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">计算机\HKEY_LOCAL_MACHINE\SOFTWARE\Microsoft\WIMMount\Mounted Images\<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>系统配置</category>
      
    </categories>
    
    
    <tags>
      
      <tag>windows</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>lvmetad内存泄露问题处理</title>
    <link href="/2024/05/09/lvmetad%E5%86%85%E5%AD%98%E6%B3%84%E9%9C%B2%E9%97%AE%E9%A2%98%E5%A4%84%E7%90%86/"/>
    <url>/2024/05/09/lvmetad%E5%86%85%E5%AD%98%E6%B3%84%E9%9C%B2%E9%97%AE%E9%A2%98%E5%A4%84%E7%90%86/</url>
    
    <content type="html"><![CDATA[<h2 id="问题背景"><a href="#问题背景" class="headerlink" title="问题背景"></a>问题背景</h2><p>lvmetad是操作系统上面的lvm的缓存服务，基本上所有的操作系统上面都有，我们使用的操作系统版本为</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@node232 ~]<span class="hljs-comment"># lsb_release -a</span><br>LSB Version:    :core-4.1-amd64:core-4.1-noarch<br>Distributor ID: CentOS<br>Description:    CentOS Linux release 7.6.1810 (Core) <br>Release:        7.6.1810<br>Codename:       Core<br>[root@node232 ~]<span class="hljs-comment"># uname -a</span><br>Linux node232 4.14.113-1.el7.x86_64 <span class="hljs-comment">#1 SMP Wed Sep 9 17:22:41 CST 2020 x86_64 x86_64 x86_64 GNU/Linux</span><br></code></pre></td></tr></table></figure><p>lvm版本为</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@node232 ~]<span class="hljs-comment"># rpm -qa|grep lvm</span><br>lvm2-libs-2.02.180-8.el7.x86_64<br>lvm2-2.02.180-8.el7.x86_64<br></code></pre></td></tr></table></figure><p>如果环境为上面的版本，重点关注一下，通过检查下top看下内存是否有下面的问题</p><p><img src="/images/blog/2024-05-09-09-53-54.png"></p><h2 id="相关问题"><a href="#相关问题" class="headerlink" title="相关问题"></a>相关问题</h2><blockquote><p><a href="https://sourceware.org/git/?p=lvm2.git;a=commit;h=bcf9556b8fcd16ad8997f80cc92785f295c66701">https://sourceware.org/git/?p=lvm2.git;a=commit;h=bcf9556b8fcd16ad8997f80cc92785f295c66701</a></p></blockquote><p><img src="/images/blog/2024-05-09-09-54-56.png"></p><p><img src="/images/blog/2024-05-09-09-55-20.png"></p><p><img src="/images/blog/2024-05-09-09-55-33.png"></p><p>确认当前使用版本存在这个问题</p><h2 id="复现步骤"><a href="#复现步骤" class="headerlink" title="复现步骤"></a>复现步骤</h2><h3 id="反复查询触发问题"><a href="#反复查询触发问题" class="headerlink" title="反复查询触发问题"></a>反复查询触发问题</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab103 ~]<span class="hljs-comment"># cat lvmleak.sh </span><br><span class="hljs-comment">#!/bin/bash</span><br><span class="hljs-keyword">while</span> :<br><span class="hljs-keyword">do</span><br>    pvs<br>    <span class="hljs-built_in">sleep</span> 1<br><span class="hljs-keyword">done</span><br></code></pre></td></tr></table></figure><p>执行一个记录内存的脚本</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab103 ~]<span class="hljs-comment"># cat jilu.sh </span><br><span class="hljs-comment">#! /bin/bash</span><br><span class="hljs-keyword">while</span> :<br>    <span class="hljs-keyword">do</span><br>mydate=`<span class="hljs-built_in">date</span>`<br>mymem=`<span class="hljs-built_in">cat</span> /proc/7194/status |grep VmRSS`<br><span class="hljs-built_in">echo</span> <span class="hljs-variable">$mydate</span>  <span class="hljs-variable">$mymem</span><br>        <span class="hljs-built_in">sleep</span> 5<br><span class="hljs-keyword">done</span><br></code></pre></td></tr></table></figure><p>7194为lvmetad的pid</p><p><img src="/images/blog/2024-05-09-09-58-11.png"><br>可以看到内存是持续增加的，每次一点点</p><h2 id="处理方法"><a href="#处理方法" class="headerlink" title="处理方法"></a>处理方法</h2><h3 id="临时释放内存"><a href="#临时释放内存" class="headerlink" title="临时释放内存"></a>临时释放内存</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab103 ~]<span class="hljs-comment"># systemctl restart lvm2-lvmetad</span><br></code></pre></td></tr></table></figure><p>如果环境是不好传包的，就直接重启下，这个内存累积的还比较慢，可以先重启处理掉<br>如果卡住，就再换一个终端，kill掉lvmetad进程即可</p><p>升级lvm解决问题<br>比较代码<br><img src="/images/blog/2024-05-09-09-58-51.png"></p><p>可以看到后面的版本已经解决了这个问题了</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs bash">device-mapper-1.02.170-6.el7_9.5.x86_64.rpm       <br>device-mapper-event-libs-1.02.170-6.el7_9.5.x86_64.rpm  <br>lvm2-2.02.187-6.el7_9.5.x86_64.rpm<br>device-mapper-event-1.02.170-6.el7_9.5.x86_64.rpm  <br>device-mapper-libs-1.02.170-6.el7_9.5.x86_64.rpm        <br>lvm2-libs-2.02.187-6.el7_9.5.x86_64.rpm<br></code></pre></td></tr></table></figure><p>一共需要更新6个包</p><blockquote><p>wget <a href="http://mirror.centos.org/centos/7/updates/x86_64/Packages/">http://mirror.centos.org/centos/7/updates/x86_64/Packages/</a></p></blockquote><p>这个路径里面下载</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">rpm -Uvh *.rpm<br></code></pre></td></tr></table></figure><p>默认情况下会保留之前的lvm配置文件，不会进行替换，所以没有问题，不需要其它操作</p><p><img src="/images/blog/2024-05-09-09-59-30.png"></p><p>重启进程生效</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab103 ~]<span class="hljs-comment"># systemctl restart lvm2-lvmetad</span><br></code></pre></td></tr></table></figure><p>如果卡住，就再换一个终端，kill掉lvmetad进程即可<br>升级后<br><img src="/images/blog/2024-05-09-10-00-21.png"></p><p>可以看到问题解决</p>]]></content>
    
    
    <categories>
      
      <category>问题处理</category>
      
    </categories>
    
    
    <tags>
      
      <tag>lvm</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>ceph-mon卡死故障分析处理</title>
    <link href="/2024/04/30/ceph-mon%E5%8D%A1%E6%AD%BB%E6%95%85%E9%9A%9C%E5%88%86%E6%9E%90%E5%A4%84%E7%90%86/"/>
    <url>/2024/04/30/ceph-mon%E5%8D%A1%E6%AD%BB%E6%95%85%E9%9A%9C%E5%88%86%E6%9E%90%E5%A4%84%E7%90%86/</url>
    
    <content type="html"><![CDATA[<h2 id="问题现象"><a href="#问题现象" class="headerlink" title="问题现象"></a>问题现象</h2><p>ceph -s命令无返回了，查看mon的日志输出内容如下</p><p><img src="/images/blog/2024-04-30-15-34-38.png" alt="globalid"></p><p>看到的现象是 failed to assign global_id</p><p>然后尝试预分配更多的 global_id </p><blockquote><p>mon_globalid_prealloc&#x3D;1111110000</p></blockquote><p>这个地方调整后，只是延缓了id的无法分配，没有解决问题，这个地方并不是消息id的问题，而是mon此时出现了问题，无法去正常的回收和响应消息，从而耗尽了id，出现了上面的现象，出现问题以后，mon实际无法响应请求了</p><p>把mon的进程直接放到前台运行</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">/usr/bin/ceph-mon -f --cluster ceph --<span class="hljs-built_in">id</span> MYMON1 --setuser ceph --setgroup ceph<br>tcmalloc: large alloc 1073741824 bytes == 0x5568d6ac6000 @  0x7fb651ad24ef 0x7fb651af1bd6 0x556892b6c049 0x556892b6c08b 0x556892b6c940 0x7fb64f4e36f6 0x7fb64f4e7e66 0x7fb64f4debb5 0x556892b7a87d 0x556892ba9ba8 0x556892badb35 0x556892baefef 0x556892bdc6c6 0x556892bd8c86 0x7fb6531d8d69 0x7fb6532861ed 0x7fb64fd9dea5 0x7fb64ec60b0d<br>tcmalloc: large alloc 2147483648 bytes == 0x556916b3e000 @  0x7fb651ad24ef 0x7fb651af1bd6 0x556892b6c049 0x556892b6c08b 0x556892b6c940 0x7fb64f4e36f6 0x7fb64f4e7e66 0x7fb64f4debb5 0x556892b7a87d 0x556892ba9ba8 0x556892badb35 0x556892baefef 0x556892bdc6c6 0x556892bd8c86 0x7fb6531d8d69 0x7fb6532861ed 0x7fb64fd9dea5 0x7fb64ec60b0d<br>tcmalloc: large alloc 4294967296 bytes == 0x556996b3e000 @  0x7fb651ad24ef 0x7fb651af1bd6 0x556892b6c049 0x556892b6c08b 0x556892b6c940 0x7fb64f4e36f6 0x7fb64f4e7e66 0x7fb64f4debb5 0x556892b7a87d 0x556892ba9ba8 0x556892badb35 0x556892baefef 0x556892bdc6c6 0x556892bd8c86 0x7fb6531d8d69 0x7fb6532861ed 0x7fb64fd9dea5 0x7fb64ec60b0d<br>tcmalloc: large alloc 8589934592 bytes == (nil) @  0x7fb651ad24ef 0x7fb651af1bd6 0x556892b6c049 0x556892b6c08b 0x556892b6c940 0x7fb64f4e36f6 0x7fb64f4e7e66 0x7fb64f4debb5 0x556892b7a87d 0x556892ba9ba8 0x556892badb35 0x556892baefef 0x556892bdc6c6 0x556892bd8c86 0x7fb6531d8d69 0x7fb6532861ed 0x7fb64fd9dea5 0x7fb64ec60b0d<br></code></pre></td></tr></table></figure><p>可以看到一直在大量的使用内存，这个地方会出现需要tcmalloc: large alloc 8589934592 bytes很大的问题</p><p><img src="/images/blog/2024-04-30-15-37-23.png"></p><p>可以看到cpu占用很高，内存占用很多</p><p>mon的现象就是上面的持续的100%的情况，但是实际没做什么</p><h2 id="临时恢复的方法"><a href="#临时恢复的方法" class="headerlink" title="临时恢复的方法"></a>临时恢复的方法</h2><p>通过增加一个这个参数进行了问题的绕过</p><blockquote><p>mon_sync_max_payload_size&#x3D;4096</p></blockquote><p>mon恢复了正常<br>这个在mon出现消息异常的时候，能够解决一些问题，这个地方我们继续分析相关的问题</p><h2 id="问题定位"><a href="#问题定位" class="headerlink" title="问题定位"></a>问题定位</h2><p>故障的原因是在响应ceph -s的时候产生了数百万个&#x3D;的打印，这个纯属一个bug</p><blockquote><p><a href="https://tracker.ceph.com/issues/50587">https://tracker.ceph.com/issues/50587</a></p></blockquote><p>这个issue里面比较清楚，并且15版本里面本身就完整的去掉了这一块，所以15版本没有问题，只有这个14版本有这个问题</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs bash">https://github.com/ceph/ceph/blob/v14.2.11/src/mon/Monitor.cc<br>    auto&amp; pem = mgrstatmon()-&gt;get_progress_events();<br>    <span class="hljs-keyword">if</span> (!pem.empty()) &#123;<br>      ss &lt;&lt; <span class="hljs-string">&quot;\n \n  progress:\n&quot;</span>;<br>      <span class="hljs-keyword">for</span> (auto&amp; i : pem) &#123;<br>ss &lt;&lt; <span class="hljs-string">&quot;    &quot;</span> &lt;&lt; <span class="hljs-string">i.second.message &lt;&lt; &quot;\n&quot;;</span><br><span class="hljs-string">ss &lt;&lt; &quot;      [&quot;;</span><br><span class="hljs-string">unsigned j;</span><br><span class="hljs-string">for (j = 0; j &lt; (unsigned)(i</span>.second.progress * 30.0); ++j) &#123;<br>  ss &lt;&lt; <span class="hljs-string">&#x27;=&#x27;</span>;<br>&#125;<br><span class="hljs-keyword">for</span> (; j &lt; 30; ++j) &#123;<br>  ss &lt;&lt; <span class="hljs-string">&#x27;.&#x27;</span>;<br>&#125;<br>ss &lt;&lt; <span class="hljs-string">&quot;]\n&quot;</span>;<br>      &#125;<br>    &#125;<br>    ss &lt;&lt; <span class="hljs-string">&quot;\n &quot;</span>;<br>  &#125;<br></code></pre></td></tr></table></figure><p>就是这块代码的问题</p><p>社区相关的pr已经解决了</p><blockquote><p><a href="https://github.com/ceph/ceph/pull/41098">https://github.com/ceph/ceph/pull/41098</a></p></blockquote><p><img src="/images/blog/2024-04-30-15-39-20.png"></p><p>按这样修改以后进行验证</p><p>这个问题触发的情况是在osd出现频繁的上下线或者out以后，计算里面的状态值里面有问题，然后执行ceph -s的时候就触发了大量的内存分配的情况了</p><p>这个问题拿的社区版本的14.2.11 复现出来了这个问题，这个复现需要使用异常的mon的数据进行复现，按照上面的在14.2.11上面进行修改后，重新打包rpm，进行复测</p><p>可以看到14.2.22版本确认已经解决了这个问题了<br><img src="/images/blog/2024-04-30-15-40-05.png"></p><p>出现问题后可以采用两个方式验证这个问题是否解决</p><ul><li>1、直接用14.2.11进行pr的修改，验证是否解决</li><li>2、不改代码，直接用官方的14.2.22，验证是否解决，升级是不是有其它问题（启动看看有没有问题）</li></ul><p>这个需要看根据实际项目来看是原包升级还是整体升级，原版本改动小一点，回退也方便</p><h2 id="离线mon数据的加载复现方法"><a href="#离线mon数据的加载复现方法" class="headerlink" title="离线mon数据的加载复现方法"></a>离线mon数据的加载复现方法</h2><p>准备了一台虚拟机，主机名和ip都设置为跟现场一样的，ceph版本也安装一样的版本，把mon的目录拷贝到虚拟机的环境里面</p><p>进行单mon的处理</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab103 ~]<span class="hljs-comment">#ceph-mon -i MYMON1 --extract-monmap /tmp/monmap</span><br>[root@lab103 ~]<span class="hljs-comment">#monmaptool --rm  MYMON2 /tmp/monmap</span><br>[root@lab103 ~]<span class="hljs-comment">#monmaptool --rm  MYMON3 /tmp/monmap</span><br>[root@lab103 ~]<span class="hljs-comment">#ceph-mon -i  MYMON1  --inject-monmap /tmp/monmap</span><br></code></pre></td></tr></table></figure><h3 id="mon数据的属性问题"><a href="#mon数据的属性问题" class="headerlink" title="mon数据的属性问题"></a>mon数据的属性问题</h3><p>一般上面就可以了，但是这个有个版本的属性不匹配，需要多做下面的操作</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab103 ~]<span class="hljs-comment">#monmaptool /tmp/monmap --feature-list parseable</span><br>monmaptool: monmap file /tmp/monmap<br>monmap:persistent:[kraken(1),luminous(2),mimic(4),osdmap-prune(8),nautilus(16),unknown(128)]<br>monmap:optional:[none]<br>monmap:required:[kraken(1),luminous(2),mimic(4),osdmap-prune(8),nautilus(16),unknown(128)]<br>available:supported:[kraken(1),luminous(2),mimic(4),osdmap-prune(8),nautilus(16)]<br>available:persistent:[kraken(1),luminous(2),mimic(4),osdmap-prune(8),nautilus(16)]<br></code></pre></td></tr></table></figure><p>关闭相关的属性</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab103 ~]<span class="hljs-comment">#monmaptool /tmp/monmap --feature-unset 128 --persistent</span><br>[root@lab103 ~]<span class="hljs-comment">#monmaptool /tmp/monmap --feature-list parseable</span><br>monmaptool: monmap file /tmp/monmap<br>monmap:persistent:[kraken(1),luminous(2),mimic(4),osdmap-prune(8),nautilus(16)]<br>monmap:optional:[none]<br>monmap:required:[kraken(1),luminous(2),mimic(4),osdmap-prune(8),nautilus(16)]<br>available:supported:[kraken(1),luminous(2),mimic(4),osdmap-prune(8),nautilus(16)]<br>available:persistent:[kraken(1),luminous(2),mimic(4),osdmap-prune(8),nautilus(16)]  <br></code></pre></td></tr></table></figure><h3 id="打包问题处理"><a href="#打包问题处理" class="headerlink" title="打包问题处理"></a>打包问题处理</h3><p>打包14.2.11过程出现了一个问题，按这个修改后打包</p><blockquote><p><a href="https://tracker.ceph.com/issues/52891">https://tracker.ceph.com/issues/52891</a></p></blockquote><h2 id="问题总结"><a href="#问题总结" class="headerlink" title="问题总结"></a>问题总结</h2><p>这个就是14版本早期版本的bug，在后期上下线osd的时候会触发，碰到了话，就按上面的先恢复后修复的方式处理即可，重点关注14版本的</p>]]></content>
    
    
    <categories>
      
      <category>存储相关</category>
      
    </categories>
    
    
    <tags>
      
      <tag>ceph</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>磁盘出现sector_error的修复</title>
    <link href="/2024/04/30/%E7%A3%81%E7%9B%98%E5%87%BA%E7%8E%B0sector-error%E7%9A%84%E4%BF%AE%E5%A4%8D/"/>
    <url>/2024/04/30/%E7%A3%81%E7%9B%98%E5%87%BA%E7%8E%B0sector-error%E7%9A%84%E4%BF%AE%E5%A4%8D/</url>
    
    <content type="html"><![CDATA[<h2 id="坏快的模拟操作"><a href="#坏快的模拟操作" class="headerlink" title="坏快的模拟操作"></a>坏快的模拟操作</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab102 ~]<span class="hljs-comment"># hdparm --yes-i-know-what-i-am-doing --make-bad-sector 5555  /dev/sdb</span><br><br>/dev/sdb:<br>Corrupting sector 5555 (WRITE_UNC_EXT as pseudo): succeeded<br></code></pre></td></tr></table></figure><p>注意这个操作如果操作了后面记得恢复，不然留在磁盘上面忘记处理，后面就是坏快在那里的，这个是人为的注入一个坏块的操作的</p><h3 id="检查坏快的情况"><a href="#检查坏快的情况" class="headerlink" title="检查坏快的情况"></a>检查坏快的情况</h3><p>（我的环境一个block &#x3D; 2个sector）</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab102 ~]<span class="hljs-comment"># badblocks -v -s /dev/sdb 3333</span><br>Checking blocks 0 to 3333<br>Checking <span class="hljs-keyword">for</span> bad blocks (read-only <span class="hljs-built_in">test</span>): 277629% <span class="hljs-keyword">done</span>, 0:06 elapsed. (0/0/0 errors)<br>277732% <span class="hljs-keyword">done</span>, 0:08 elapsed. (1/0/0 errors)<br>277835% <span class="hljs-keyword">done</span>, 0:10 elapsed. (2/0/0 errors)<br>277938% <span class="hljs-keyword">done</span>, 0:12 elapsed. (3/0/0 errors)<br><span class="hljs-keyword">done</span><br>Pass completed, 4 bad blocks found. (4/0/0 errors)<br></code></pre></td></tr></table></figure><p>用这个检测，发现了四个error ，正好是4个block，8个sector的损坏</p><p>可以看到报错的</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs bash">[20831.254978] blk_update_request: critical medium error, dev sdb, sector 5552<br>[20831.257006] Buffer I/O error on dev sdb, logical block 694, async page <span class="hljs-built_in">read</span><br>[20833.271865] sd 0:0:18:0: [sdb] tag<span class="hljs-comment">#0 FAILED Result: hostbyte=DID_OK driverbyte=DRIVER_SENSE</span><br>[20833.271882] sd 0:0:18:0: [sdb] tag<span class="hljs-comment">#0 Sense Key : Medium Error [current]</span><br>[20833.271890] sd 0:0:18:0: [sdb] tag<span class="hljs-comment">#0 Add. Sense: Unrecovered read error</span><br>[20833.271898] sd 0:0:18:0: [sdb] tag<span class="hljs-comment">#0 CDB: Read(16) 88 00 00 00 00 00 00 00 15 b0 00 00 00 08 00 00</span><br>[20833.271905] blk_update_request: critical medium error, dev sdb, sector 5552<br>[20833.274074] Buffer I/O error on dev sdb, logical block 694, async page <span class="hljs-built_in">read</span><br>[20849.631237]  sdb: sdb1<br></code></pre></td></tr></table></figure><p>显示的是 dev sdb, sector 5552</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab102 ~]<span class="hljs-comment"># cat zp</span><br>2776<br>2777<br>2778<br>2779<br></code></pre></td></tr></table></figure><p>5552 - 5558 应该都损坏了，我们先正常修复提示的</p><h3 id="修复坏快"><a href="#修复坏快" class="headerlink" title="修复坏快"></a>修复坏快</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab102 ~]<span class="hljs-comment"># hdparm --yes-i-know-what-i-am-doing --repair-sector 5552 /dev/sdb</span><br><br>/dev/sdb:<br>re-writing sector 5552: succeeded<br></code></pre></td></tr></table></figure><p>提示成功了，再次检测，提示损坏了 sector 5553</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab102 ~]<span class="hljs-comment"># hdparm --yes-i-know-what-i-am-doing --read-sector 5553 /dev/sdb</span><br><br>/dev/sdb:<br>reading sector 5553: FAILED: Input/output error<br></code></pre></td></tr></table></figure><p>验证读取确实出错了 </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab102 ~]<span class="hljs-comment"># hdparm --yes-i-know-what-i-am-doing --repair-sector 5553 /dev/sdb</span><br><br>/dev/sdb:<br>re-writing sector 5553: succeeded<br>[root@lab102 ~]<span class="hljs-comment"># hdparm --yes-i-know-what-i-am-doing --read-sector 5553 /dev/sdb</span><br><br>/dev/sdb:<br>reading sector 5553: succeeded<br>0000 0000 0000 0000 0000 0000 0000 0000<br></code></pre></td></tr></table></figure><h3 id="确认修复成功了"><a href="#确认修复成功了" class="headerlink" title="确认修复成功了"></a>确认修复成功了</h3><p>修复后就可以读取了 </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab102 ~]<span class="hljs-comment"># badblocks -v -s /dev/sdb 3333 -o zp</span><br>Checking blocks 0 to 3333<br>Checking <span class="hljs-keyword">for</span> bad blocks (read-only <span class="hljs-built_in">test</span>): <span class="hljs-keyword">done</span><br>Pass completed, 0 bad blocks found. (0/0/0 errors)<br></code></pre></td></tr></table></figure><p>修复完毕后，就没有提示了</p><p>这里的修复是把sector标记为0了，避免read error完全读取不了的情况，软件会因为文件在，但是读取不到，直接崩溃，并且删除也无法删除，这个标记为0后，删除以后，也就是丢失了对应的文件，而不是完全无法用，这个可以权衡再操作</p><h2 id="其它知识"><a href="#其它知识" class="headerlink" title="其它知识"></a>其它知识</h2><p>检查指定区间的方法</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab102 ~]<span class="hljs-comment"># badblocks -s -v /dev/sdb  2000 1000</span><br>Checking blocks 1000 to 2000<br>Checking <span class="hljs-keyword">for</span> bad blocks (read-only <span class="hljs-built_in">test</span>): <span class="hljs-keyword">done</span><br>Pass completed, 0 bad blocks found. (0/0/0 errors)<br></code></pre></td></tr></table></figure><p>查看sector总数的方法</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab102 ~]<span class="hljs-comment"># fdisk -l /dev/sdb</span><br>WARNING: fdisk GPT support is currently new, and therefore <span class="hljs-keyword">in</span> an experimental phase. Use at your own discretion.<br><br>Disk /dev/sdb: 4000.8 GB, 4000787030016 bytes, 7814037168 sectors<br></code></pre></td></tr></table></figure><p>查看blocks的方法</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab102 ~]<span class="hljs-comment"># badblocks -s -v /dev/sdb</span><br>Checking blocks 0 to 3907018583<br></code></pre></td></tr></table></figure><p>可以通过计算，算出block和sector的关系后，如果定位到sector的错误，可以通过算出block的位置来检查磁盘的block的错误，我们这里没有使用badblock的修复方法，上面的那个修复方法是sector的级别的，更好一点，这里可以用于检查使用</p>]]></content>
    
    
    <categories>
      
      <category>故障处理</category>
      
    </categories>
    
    
    <tags>
      
      <tag>磁盘</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>开启s3的https客户端无法使用的问题</title>
    <link href="/2024/04/30/%E5%BC%80%E5%90%AFs3%E7%9A%84https%E5%AE%A2%E6%88%B7%E7%AB%AF%E6%97%A0%E6%B3%95%E4%BD%BF%E7%94%A8%E7%9A%84%E9%97%AE%E9%A2%98/"/>
    <url>/2024/04/30/%E5%BC%80%E5%90%AFs3%E7%9A%84https%E5%AE%A2%E6%88%B7%E7%AB%AF%E6%97%A0%E6%B3%95%E4%BD%BF%E7%94%A8%E7%9A%84%E9%97%AE%E9%A2%98/</url>
    
    <content type="html"><![CDATA[<h2 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h2><p>在内网部署的https的时候，我们采用的是自签名的ssl证书，内网的这个证书是无法被客户端认证通过的，这个地方主要体现在下面几个现象可以看到</p><h3 id="chrome下的表现"><a href="#chrome下的表现" class="headerlink" title="chrome下的表现"></a>chrome下的表现</h3><p><img src="/images/blog/chromeerror.png" alt="chromeerror"><br><img src="/images/blog/chromeerror2.png" alt="chromeerror2"></p><p>s3cmd下面的表现<br><img src="/images/blog/2024-04-30-12-16-32.png"></p><h3 id="s3browser下的表现"><a href="#s3browser下的表现" class="headerlink" title="s3browser下的表现"></a>s3browser下的表现</h3><p><img src="/images/blog/2024-04-30-12-08-18.png"></p><p>这些共同的表现就是无法建立信任的关系</p><p>这里我们实际要解决的问题就是，内网是可以绕过这个证书的检测的</p><h2 id="不同客户端的处理"><a href="#不同客户端的处理" class="headerlink" title="不同客户端的处理"></a>不同客户端的处理</h2><h3 id="s3cmd的处理"><a href="#s3cmd的处理" class="headerlink" title="s3cmd的处理"></a>s3cmd的处理</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab103 ~]<span class="hljs-comment"># s3cmd ls  --no-check-certificate   s3://mybucket/</span><br></code></pre></td></tr></table></figure><h3 id="s3browser的处理"><a href="#s3browser的处理" class="headerlink" title="s3browser的处理"></a>s3browser的处理</h3><p><img src="/images/blog/2024-04-30-12-09-30.png"></p><p>可以看到官方在11.5.7版本开始考虑到这个问题了，可以忽略证书的认证，我们只是用到https传输即可</p><p><img src="/images/blog/2024-04-30-12-09-47.png"></p><p>把这个选项勾选，也就是信任所有的证书，重启客户端</p><p><img src="/images/blog/2024-04-30-12-10-02.png"></p><p>可以看到启用https，并且使用了最新的客户端，也是可以正常运行了，也就是证书的问题</p><h2 id="通过自颁发证书来解决证书验证问题"><a href="#通过自颁发证书来解决证书验证问题" class="headerlink" title="通过自颁发证书来解决证书验证问题"></a>通过自颁发证书来解决证书验证问题</h2><h2 id="自颁发证书"><a href="#自颁发证书" class="headerlink" title="自颁发证书"></a>自颁发证书</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">yum install openssl openssl-devel -y<br></code></pre></td></tr></table></figure><h3 id="step1-生成证书请求文件"><a href="#step1-生成证书请求文件" class="headerlink" title="step1: 生成证书请求文件"></a>step1: 生成证书请求文件</h3><p>新建openssl.cnf，内容如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><code class="hljs bash">[req]<br>distinguished_name = req_distinguished_name<br>req_extensions = v3_req<br><br>[req_distinguished_name]<br>countryName = Country Name (2 letter code)<br>countryName_default = CH<br>stateOrProvinceName = State or Province Name (full name)<br>stateOrProvinceName_default = GD<br>localityName = Locality Name (eg, city)<br>localityName_default = ShenZhen<br>organizationalUnitName  = Organizational Unit Name (eg, section)<br>organizationalUnitName_default  = organizationalUnitName<br>commonName = Internet Widgits Ltd<br>commonName_max  = 64<br><br>[ v3_req ]<br><span class="hljs-comment"># Extensions to add to a certificate request</span><br>basicConstraints = CA:FALSE<br>keyUsage = nonRepudiation, digitalSignature, keyEncipherment<br>subjectAltName = @alt_names<br><br>[alt_names]<br><br><span class="hljs-comment"># 改成自己的域名</span><br><span class="hljs-comment">#DNS.1 = kb.example.com</span><br><span class="hljs-comment">#DNS.2 = helpdesk.example.org</span><br><span class="hljs-comment">#DNS.3 = systems.example.net</span><br><br><span class="hljs-comment"># 改成自己的ip</span><br>IP.1 = 192.168.19.103<br>IP.2 = 192.168.19.102<br></code></pre></td></tr></table></figure><h3 id="step2-生成私钥"><a href="#step2-生成私钥" class="headerlink" title="step2: 生成私钥"></a>step2: 生成私钥</h3><p>san_domain_com 为最终生成的文件名，一般以服务器命名，可改。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">openssl genrsa -out san_domain_com.key 2048<br></code></pre></td></tr></table></figure><h3 id="step3-创建CSR文件"><a href="#step3-创建CSR文件" class="headerlink" title="step3: 创建CSR文件"></a>step3: 创建CSR文件</h3><p>创建CSR文件命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">openssl req -new -out san_domain_com.csr -key san_domain_com.key -config openssl.cnf<br></code></pre></td></tr></table></figure><p>执行后，系统会提示输入组织等信息，按提示输入如即可。<br>测试CSR文件是否生成成功，可以使用下面的命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">openssl req -text -noout -<span class="hljs-keyword">in</span> san_domain_com.csr<br></code></pre></td></tr></table></figure><h3 id="step4-自签名并创建证书"><a href="#step4-自签名并创建证书" class="headerlink" title="step4: 自签名并创建证书"></a>step4: 自签名并创建证书</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">openssl x509 -req -days 3650 -<span class="hljs-keyword">in</span> san_domain_com.csr -signkey san_domain_com.key -out san_domain_com.crt -extensions v3_req -extfile openssl.cnf<br></code></pre></td></tr></table></figure><p>执行后，可看到本目录下多了以下三个文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">san_domain_com.crt<br>san_domain_com.csr<br>san_domain_com.key<br></code></pre></td></tr></table></figure><p>如果是给nginx用就是一个key 一个crt的</p><p>我们准备给ceph的证书(用上面生成的)</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">cp</span> san_domain_com.crt  ceph.crt<br><span class="hljs-built_in">cat</span> san_domain_com.key &gt;&gt; ceph.crt<br></code></pre></td></tr></table></figure><p>ceph的配置</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">[client.radosgw1]<br>host = lab103<br>rgw frontends = civetweb port=192.168.19.103:7481+192.168.19.103:443s ssl_certificate=/ceph.crt<br>rgw_content_length_compat = <span class="hljs-literal">true</span><br></code></pre></td></tr></table></figure><p>客户端导入证书san_domain_com.crt<br>注意，导入的时候要添加到选择受信任的颁发机构里面<br><img src="/images/blog/2024-04-30-12-12-41.png"></p><p><img src="/images/blog/2024-04-30-12-12-56.png"></p><p><img src="/images/blog/2024-04-30-12-13-07.png"><br><img src="/images/blog/2024-04-30-12-13-17.png"><br><img src="/images/blog/2024-04-30-12-13-27.png"><br>安装以后，这个就是再访问https就是正常的了</p><p><img src="/images/blog/2024-04-30-12-13-42.png"><br>改成https的之后进行访问，这样可以不用关闭证书验证了，证书是可以认证的了</p>]]></content>
    
    
    <categories>
      
      <category>存储相关</category>
      
    </categories>
    
    
    <tags>
      
      <tag>ceph</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>vmware平台下IPv6丢包</title>
    <link href="/2024/04/17/vmware%E5%B9%B3%E5%8F%B0%E4%B8%8BIPv6%E4%B8%A2%E5%8C%85/"/>
    <url>/2024/04/17/vmware%E5%B9%B3%E5%8F%B0%E4%B8%8BIPv6%E4%B8%A2%E5%8C%85/</url>
    
    <content type="html"><![CDATA[<h2 id="问题现象"><a href="#问题现象" class="headerlink" title="问题现象"></a>问题现象</h2><p>在vmware的平台下的虚拟机里面配置了IPv6，然后测试的时候发现丢包严重，引起带宽非常低</p><h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>Esxi虚拟机采用的是vmxnet3 driver，底层在处理IPv6的包的时候，额外加上了一些信息<br>所以默认的1500的MTU无法处理完整的包，也就需要丢弃一些，然后造成了需要重传，带宽就低了</p><p>MTU调整为1515之后，问题消失，带宽恢复正常</p><p>这个只在IPv6使用的时候有这个问题，应该是跟这个驱动有关系，物理机下面没这个问题</p><h2 id="调整MTU的方法"><a href="#调整MTU的方法" class="headerlink" title="调整MTU的方法"></a>调整MTU的方法</h2><h3 id="在线调整"><a href="#在线调整" class="headerlink" title="在线调整"></a>在线调整</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">ifconfig ens160 mtu 1515<br></code></pre></td></tr></table></figure><h3 id="配置文件调整"><a href="#配置文件调整" class="headerlink" title="配置文件调整"></a>配置文件调整</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">MTU=<span class="hljs-string">&#x27;1515&#x27;</span><br></code></pre></td></tr></table></figure><p>重启网络服务</p>]]></content>
    
    
    <categories>
      
      <category>系统管理</category>
      
    </categories>
    
    
    <tags>
      
      <tag>网络配置</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>一个编译参数引起的ceph性能大幅下降</title>
    <link href="/2024/03/21/%E4%B8%80%E4%B8%AA%E7%BC%96%E8%AF%91%E5%8F%82%E6%95%B0%E5%BC%95%E8%B5%B7%E7%9A%84ceph%E6%80%A7%E8%83%BD%E5%A4%A7%E5%B9%85%E4%B8%8B%E9%99%8D/"/>
    <url>/2024/03/21/%E4%B8%80%E4%B8%AA%E7%BC%96%E8%AF%91%E5%8F%82%E6%95%B0%E5%BC%95%E8%B5%B7%E7%9A%84ceph%E6%80%A7%E8%83%BD%E5%A4%A7%E5%B9%85%E4%B8%8B%E9%99%8D/</url>
    
    <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>最近翻ceph的官方的博客，发现了一篇博客提到了一个ubuntu下面的编译参数引起的rocksdb的性能下降，这个地方应该是ceph官方代码的参数没生效</p><p>受影响的系统</p><ul><li>P版本之前的ceph版本</li><li>操作系统是ubuntu的</li><li>某些ceph版本</li></ul><p>这个要素比较多，所以运行的环境并不一定受到影响，那么我们看下，收到影响的版本是哪些,非ubuntu的系统可以忽略这个问题</p><p>我对15的版本比较熟，就以这个版本举例子</p><h2 id="受到影响的版本"><a href="#受到影响的版本" class="headerlink" title="受到影响的版本"></a>受到影响的版本</h2><p>这个版本是从ceph官方同步过来的版本</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">https://mirrors.tuna.tsinghua.edu.cn/ceph/debian-15.2.17/pool/main/c/ceph/<br></code></pre></td></tr></table></figure><h2 id="不受影响的版本"><a href="#不受影响的版本" class="headerlink" title="不受影响的版本"></a>不受影响的版本</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">https://launchpad.net/ubuntu/focal/+<span class="hljs-built_in">source</span>/ceph<br></code></pre></td></tr></table></figure><p>这个是ubuntu官方打包的版本</p><h2 id="下载不同的版本"><a href="#下载不同的版本" class="headerlink" title="下载不同的版本"></a>下载不同的版本</h2><p>源文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs bash">root@lab103:~<span class="hljs-comment"># cat /etc/apt/sources.list</span><br>deb http://archive.ubuntu.com/ubuntu/ focal main restricted<br>deb http://archive.ubuntu.com/ubuntu/ focal-updates main restricted<br>deb http://archive.ubuntu.com/ubuntu/ focal universe<br>deb http://archive.ubuntu.com/ubuntu/ focal-updates universe<br>deb http://archive.ubuntu.com/ubuntu/ focal multiverse<br>deb http://archive.ubuntu.com/ubuntu/ focal-updates multiverse<br>deb http://archive.ubuntu.com/ubuntu/ focal-backports main restricted universe multiverse<br>deb http://security.ubuntu.com/ubuntu/ focal-security main restricted<br>deb http://security.ubuntu.com/ubuntu/ focal-security universe<br>deb http://security.ubuntu.com/ubuntu/ focal-security multiverse<br>deb [trusted=<span class="hljs-built_in">yes</span>]  https://mirrors.tuna.tsinghua.edu.cn/ceph/debian-15.2.17 focal  main<br></code></pre></td></tr></table></figure><p>上面的源是安装ceph的软件包的，如果屏蔽掉最后一行，安装的就是ubuntu官方的版本，如果留着最后一行，安装的就是ceph官方的版本，包的名称不一样，这个很好区分</p><h3 id="两个版本的区别"><a href="#两个版本的区别" class="headerlink" title="两个版本的区别"></a>两个版本的区别</h3><p>我们下载ubuntu官方的debian的打包文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">wget https://launchpad.net/ubuntu/+archive/primary/+sourcefiles/ceph/15.2.17-0ubuntu0.20.04.6/ceph_15.2.17-0ubuntu0.20.04.6.debian.tar.xz<br></code></pre></td></tr></table></figure><p>ubuntu的官方的包是把debian的目录剥离出来的</p><p>我们再下载ceph官方的源码包，这个跟git里面是同步的我们看下内容</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">wget https://mirrors.tuna.tsinghua.edu.cn/ceph/debian-15.2.17/pool/main/c/ceph/ceph_15.2.17.orig.tar.gz<br></code></pre></td></tr></table></figure><p>ceph的官方包里面是有debian目录的，我们直接查看内容</p><p>我们需要比对的是debian&#x2F;rules里面的内容</p><h4 id="ubuntu官方的"><a href="#ubuntu官方的" class="headerlink" title="ubuntu官方的"></a>ubuntu官方的</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment">## Set JAVAC to prevent FTBFS due to incorrect use of &#x27;gcj&#x27; if found (see &quot;m4/ac_prog_javac.m4&quot;).</span><br><span class="hljs-built_in">export</span> JAVAC=javac<br><br>extraopts += -DWITH_OCF=ON -DWITH_NSS=ON -DWITH_PYTHON3=ON -DWITH_DEBUG=ON<br>extraopts += -DWITH_PYTHON2=OFF -DMGR_PYTHON_VERSION=3<br>extraopts += -DWITH_CEPHFS_JAVA=ON<br>extraopts += -DWITH_CEPHFS_SHELL=ON<br>extraopts += -DWITH_TESTS=OFF<br>extraopts += -DWITH_SYSTEM_BOOST=ON<br>extraopts += -DWITH_LTTNG=OFF -DWITH_EMBEDDED=OFF<br>extraopts += -DCMAKE_INSTALL_LIBEXECDIR=/usr/lib<br>extraopts += -DWITH_MGR_DASHBOARD_FRONTEND=OFF<br>extraopts += -DWITH_SYSTEMD=ON -DCEPH_SYSTEMD_ENV_DIR=/etc/default<br>extraopts += -DCMAKE_INSTALL_SYSCONFDIR=/etc<br>extraopts += -DCMAKE_INSTALL_SYSTEMD_SERVICEDIR=/lib/systemd/system<br>extraopts += -DWITH_RADOSGW_KAFKA_ENDPOINT=OFF<br>extraopts += -DCMAKE_BUILD_TYPE=RelWithDebInfo<br><br>ifneq (,$(filter parallel=%,$(DEB_BUILD_OPTIONS)))<br>  NUMJOBS = $(patsubst parallel=%,%,$(filter parallel=%,$(DEB_BUILD_OPTIONS)))<br>  extraopts += -DBOOST_J=$(NUMJOBS)<br>endif<br></code></pre></td></tr></table></figure><h4 id="ceph官方的"><a href="#ceph官方的" class="headerlink" title="ceph官方的"></a>ceph官方的</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs bash">extraopts += -DWITH_OCF=ON -DWITH_LTTNG=ON<br>extraopts += -DWITH_MGR_DASHBOARD_FRONTEND=OFF<br>extraopts += -DWITH_PYTHON3=3<br>extraopts += -DWITH_CEPHFS_JAVA=ON<br>extraopts += -DWITH_CEPHFS_SHELL=ON<br>extraopts += -DWITH_SYSTEMD=ON -DCEPH_SYSTEMD_ENV_DIR=/etc/default<br>extraopts += -DWITH_GRAFANA=ON<br><span class="hljs-comment"># assumes that ceph is exmpt from multiarch support, so we override the libdir.</span><br>extraopts += -DCMAKE_INSTALL_LIBDIR=/usr/lib<br>extraopts += -DCMAKE_INSTALL_LIBEXECDIR=/usr/lib<br>extraopts += -DCMAKE_INSTALL_SYSCONFDIR=/etc<br>extraopts += -DCMAKE_INSTALL_SYSTEMD_SERVICEDIR=/lib/systemd/system<br>ifneq (,$(filter parallel=%,$(DEB_BUILD_OPTIONS)))<br>  NUMJOBS = $(patsubst parallel=%,%,$(filter parallel=%,$(DEB_BUILD_OPTIONS)))<br>  extraopts += -DBOOST_J=$(NUMJOBS)<br>endif<br></code></pre></td></tr></table></figure><p>区别就是这个</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">extraopts += -DCMAKE_BUILD_TYPE=RelWithDebInfo<br></code></pre></td></tr></table></figure><p>RelWithDebInfo: 既优化又能调试的版本</p><p>这个参数带来的效果是</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">-DCMAKE_CXX_FLAGS=<span class="hljs-string">&#x27;-Wno-deprecated-copy -Wno-pessimizing-move&#x27;</span><span class="hljs-string">&quot;</span><br></code></pre></td></tr></table></figure><p>会带来这两个参数，以及一些其它的关闭，属于生产包需要加这个参数好一点</p><p>ceph官方是放在自己的cmake里面控制，但是deb打包的时候，有自己的这个参数，变量就被冲掉了，也就是没生效</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">https://github.com/ceph/ceph/pull/55500<br></code></pre></td></tr></table></figure><p>官方现在改了，应该是解决了，但是ubuntu官方里面是直接在最上层就用参数去控制了，也就没有这个问题<br>修改cmake&#x2F;modules&#x2F;BuildRocksDB.cmake</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs bash">endif()<br>  include(CheckCXXCompilerFlag)<br>  check_cxx_compiler_flag(<span class="hljs-string">&quot;-Wno-deprecated-copy&quot;</span> HAS_WARNING_DEPRECATED_COPY)<br>  <span class="hljs-built_in">set</span>(rocksdb_CXX_FLAGS <span class="hljs-string">&quot;<span class="hljs-variable">$&#123;CMAKE_CXX_FLAGS&#125;</span>&quot;</span>)<br>  <span class="hljs-keyword">if</span>(HAS_WARNING_DEPRECATED_COPY)<br>-    <span class="hljs-built_in">set</span>(rocksdb_CXX_FLAGS -Wno-deprecated-copy)<br>+    string(APPEND rocksdb_CXX_FLAGS <span class="hljs-string">&quot; -Wno-deprecated-copy&quot;</span>)<br>  endif()<br>  check_cxx_compiler_flag(<span class="hljs-string">&quot;-Wno-pessimizing-move&quot;</span> HAS_WARNING_PESSIMIZING_MOVE)<br>  <span class="hljs-keyword">if</span>(HAS_WARNING_PESSIMIZING_MOVE)<br>-    <span class="hljs-built_in">set</span>(rocksdb_CXX_FLAGS <span class="hljs-string">&quot;<span class="hljs-variable">$&#123;rocksdb_CXX_FLAGS&#125;</span> -Wno-pessimizing-move&quot;</span>)<br>+    string(APPEND rocksdb_CXX_FLAGS <span class="hljs-string">&quot; -Wno-pessimizing-move&quot;</span>)<br>  endif()<br>  <span class="hljs-keyword">if</span>(rocksdb_CXX_FLAGS)<br>    list(APPEND rocksdb_CMAKE_ARGS -DCMAKE_CXX_FLAGS=<span class="hljs-string">&#x27;$&#123;rocksdb_CXX_FLAGS&#125;&#x27;</span>)<br><br></code></pre></td></tr></table></figure><p>打包过程可以看到这两个参数加进去了没</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">[  8%] Performing configure step <span class="hljs-keyword">for</span> <span class="hljs-string">&#x27;rocksdb_ext&#x27;</span><br><span class="hljs-built_in">cd</span> /ceph/ceph-15.2.17/obj-x86_64-linux-gnu/src/rocksdb &amp;&amp; /usr/bin/cmake -DCMAKE_POSITION_INDEPENDENT_CODE=ON -DWITH_GFLAGS=OFF -DCMAKE_PREFIX_PATH= -DCMAKE_CXX_COMPILER=/usr/bin/c++ -DWITH_SNAPPY=TRUE -DWITH_LZ4=TRUE -DLZ4_INCLUDE_DIR=/usr/include -DLZ4_LIBRARIES=/usr/lib/x86_64-linux-gnu/liblz4.so -DWITH_ZLIB=TRUE -DPORTABLE=ON -DCMAKE_AR=/usr/bin/ar -DCMAKE_BUILD_TYPE=RelWithDebInfo -DFAIL_ON_WARNINGS=OFF -DUSE_RTTI=1 <span class="hljs-string">&quot;-GUnix Makefiles&quot;</span> -DCMAKE_C_FLAGS=-Wno-stringop-truncation <span class="hljs-string">&quot;-DCMAKE_CXX_FLAGS=&#x27; -Wno-deprecated-copy -Wno-pessimizing-move&#x27;&quot;</span> <span class="hljs-string">&quot;-GUnix Makefiles&quot;</span> /ceph/ceph-15.2.17/src/rocksdb<br>-- Build files have been written to: /ceph/ceph-15.2.17/obj-x86_64-linux-gnu/src/rocksdb<br></code></pre></td></tr></table></figure><p>那么我们整体捋一捋</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">- ceph在代码里面加了参数，参数被打包过程冲掉了，引起了性能下降<br>- ubuntu在打包ceph里面加另外一个参数，让这两个参数生效了，所以打出来的包没有问题<br></code></pre></td></tr></table></figure><p>大概就是这么回事，差不多就是发行版本出的包，没有按优化的版本打包，问题很小，影响还比较大，如果正好使用的就是这个ubuntu的官方包的话</p><h3 id="性能测试"><a href="#性能测试" class="headerlink" title="性能测试"></a>性能测试</h3><p>上面是说了这个问题的来源，我们来体验一下这个性能的区别</p><p>为了测试的准确性，构建了一个单机，单副本的环境，单个nvme的osd，在宿主机创建好osd之后，停止osd，然后把osd映射到docker里面进行手动启动</p><p>这样做的目的是，osd不变，减少变量，容器系统一致，只替换了ceph的包</p><p>容器启动方式</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">docker run -it --privileged=<span class="hljs-literal">true</span> -v /dev/:/dev/ -v /var/lib/ceph/:/var/lib/ceph -v /etc/ceph:/etc/ceph --network host ubuntu:focal /bin/bash --name ceph_deb<br></code></pre></td></tr></table></figure><h3 id="ceph官方包"><a href="#ceph官方包" class="headerlink" title="ceph官方包"></a>ceph官方包</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">root@lab103:/<span class="hljs-comment"># dpkg --list|grep ceph</span><br>ii  ceph                                 15.2.17-1focal                    amd64        distributed storage and file system<br></code></pre></td></tr></table></figure><p>手动启动osd</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">/usr/bin/ceph-osd -f --cluster ceph --<span class="hljs-built_in">id</span> 0 --setuser ceph --setgroup ceph<br></code></pre></td></tr></table></figure><p>测试</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">rados -p data bench 30 write -b 4096<br></code></pre></td></tr></table></figure><p>也可以用其它命令，小io4k的比较明显,时间不久，就都贴上来</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab103 zp]<span class="hljs-comment"># rados -p data bench 30 write -b 4096</span><br>hints = 1<br>Maintaining 16 concurrent writes of 4096 bytes to objects of size 4096 <span class="hljs-keyword">for</span> up to 30 seconds or 0 objects<br>Object prefix: benchmark_data_lab103_235759<br>  sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)<br>    0       0         0         0         0         0           -           0<br>    1      16      6811      6795   26.5416    26.543  0.00314961  0.00234824<br>    2      15     13061     13046   25.4781    24.418  0.00280007  0.00244734<br>    3      16     18804     18788   24.4609   22.4297  0.00179994  0.00255073<br>    4      16     24430     24414    23.839   21.9766   0.0027436  0.00261775<br>    5      16     29900     29884   23.3441   21.3672  0.00184661  0.00267347<br>    6      16     35338     35322   22.9932   21.2422  0.00376749  0.00271445<br>    7      16     40864     40848   22.7917   21.5859   0.0023982  0.00273855<br>    8      16     46654     46638   22.7695   22.6172  0.00265023   0.0027412<br>    9      15     52491     52476   22.7731   22.8047  0.00237885  0.00274086<br>   10      16     58334     58318   22.7775   22.8203   0.0032622  0.00274042<br>   11      16     64011     63995   22.7224   22.1758  0.00224359  0.00274685<br>   12      16     69693     69677   22.6782   22.1953  0.00282742  0.00275244<br>   13      16     75483     75467   22.6733   22.6172   0.0024294  0.00275293<br>   14      16     81331     81315   22.6852   22.8438  0.00243682  0.00275157<br>   15      16     87110     87094   22.6776   22.5742  0.00249853  0.00275236<br>   16      16     92892     92876   22.6717   22.5859  0.00234551  0.00275325<br>   17      16     98682     98666   22.6683   22.6172  0.00279748  0.00275354<br>   18      16    104350    104334   22.6388   22.1406  0.00247821  0.00275716<br>   19      16    110108    110092   22.6309   22.4922  0.00265833  0.00275813<br>2024-03-21T18:09:10.318468+0800 min lat: 0.000901059 max lat: 0.0108369 avg lat: 0.00275815<br>  sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)<br>   20      16    115907    115891   22.6318   22.6523  0.00308136  0.00275815<br>   21      16    121632    121616   22.6188   22.3633  0.00107606  0.00275984<br>   22      16    127334    127318   22.6029   22.2734  0.00307552  0.00276152<br>   23      16    133012    132996   22.5844   22.1797  0.00268477  0.00276388<br>   24      16    138652    138636   22.5612   22.0312  0.00284827  0.00276685<br>   25      16    144273    144257   22.5369    21.957   0.0022021  0.00276986<br>   26      16    149771    149755    22.496   21.4766  0.00333811  0.00277466<br>   27      16    155250    155234   22.4553   21.4023  0.00249963  0.00277992<br>   28      16    160745    160729   22.4199   21.4648  0.00245874  0.00278427<br>   29      16    166247    166231   22.3878   21.4922  0.00295188  0.00278836<br>Total time run:         30.0023<br>Total writes made:      171762<br>Write size:             4096<br>Object size:            4096<br>Bandwidth (MB/sec):     22.3631<br>Stddev Bandwidth:       1.02477<br>Max bandwidth (MB/sec): 26.543<br>Min bandwidth (MB/sec): 21.2422<br>Average IOPS:           5724<br>Stddev IOPS:            262.341<br>Max IOPS:               6795<br>Min IOPS:               5438<br>Average Latency(s):     0.00279145<br>Stddev Latency(s):      0.000623496<br>Max latency(s):         0.0108369<br>Min latency(s):         0.000901059<br>Cleaning up (deleting benchmark objects)<br>Removed 171762 objects<br>Clean up completed and total clean up time :30.6685<br></code></pre></td></tr></table></figure><h3 id="ubuntu官方包"><a href="#ubuntu官方包" class="headerlink" title="ubuntu官方包"></a>ubuntu官方包</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">root@lab103:/<span class="hljs-comment"># dpkg --list|grep ceph</span><br>root@lab103:/<span class="hljs-comment"># dpkg --list|grep ceph</span><br>ii  ceph                                 15.2.17-0ubuntu0.20.04.6          amd64        distributed storage and file system<br>ii  ceph-base                            15.2.17-0ubuntu0.20.04.6          amd64        common ceph daemon libraries and management tools<br></code></pre></td></tr></table></figure><p>手动启动osd</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">/usr/bin/ceph-osd -f --cluster ceph --<span class="hljs-built_in">id</span> 0 --setuser ceph --setgroup ceph<br></code></pre></td></tr></table></figure><p>测试</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">rados -p data bench 30 write -b 4096<br></code></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab103 zp]<span class="hljs-comment"># rados -p data bench 30 write -b 4096</span><br>hints = 1<br>Maintaining 16 concurrent writes of 4096 bytes to objects of size 4096 <span class="hljs-keyword">for</span> up to 30 seconds or 0 objects<br>Object prefix: benchmark_data_lab103_236362<br>  sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)<br>    0       0         0         0         0         0           -           0<br>    1      16     15303     15287   59.7107   59.7148  0.00106137  0.00104294<br>    2      16     30338     30322   59.2169   58.7305 0.000799582  0.00105216<br>    3      16     46691     46675   60.7683   63.8789  0.00116935  0.00102532<br>    4      16     62548     62532   61.0597   61.9414 0.000970205  0.00102051<br>    5      16     78056     78040   60.9618   60.5781 0.000754982  0.00102213<br>    6      16     93210     93194   60.6659   59.1953 0.000796767  0.00102717<br>    7      16    108491    108475   60.5256   59.6914 0.000820318   0.0010296<br>    8      15    123545    123530     60.31   58.8086  0.00117671  0.00103331<br>    9      15    139028    139013   60.3281   60.4805   0.0010041  0.00103303<br>   10      16    154628    154612   60.3878   60.9336  0.00106916  0.00103203<br>   11      16    169869    169853   60.3095   59.5352  0.00100121  0.00103337<br>   12      16    185143    185127    60.255   59.6641 0.000874392  0.00103434<br>   13      16    200261    200245    60.162   59.0547 0.000937596  0.00103596<br>   14      16    212138    212122   59.1782   46.3945 0.000740705  0.00105322<br>   15      16    227092    227076   59.1267   58.4141 0.000774189  0.00105417<br>   16      16    241651    241635   58.9853   56.8711  0.00140325  0.00105664<br>   17      16    257762    257746    59.217   62.9336 0.000777715  0.00105251<br>   18      15    273338    273323   59.3072   60.8477 0.000842773  0.00105092<br>   19      16    288882    288866   59.3808   60.7148 0.000944547  0.00104963<br>2024-03-21T18:13:37.381819+0800 min lat: 0.000545632 max lat: 0.037882 avg lat: 0.00104838<br>  sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)<br>   20      16    304446    304430   59.4512   60.7969 0.000774379  0.00104838<br>   21      16    320053    320037   59.5228   60.9648 0.000891544  0.00104713<br>   22      16    335395    335379   59.5409   59.9297  0.00105081  0.00104681<br>   23      16    351159    351143   59.6291   61.5781 0.000691308  0.00104527<br>   24      15    365701    365686   59.5112   56.8086 0.000910524  0.00104736<br>   25      16    381282    381266   59.5649   60.8594 0.000934152  0.00104642<br>   26      16    396656    396640   59.5834   60.0547 0.000904033  0.00104609<br>   27      15    411989    411974   59.5947   59.8984  0.00273626  0.00104583<br>   28      16    424394    424378   59.1965   48.4531 0.000757671  0.00105296<br>   29      15    435855    435840   58.6989   44.7734  0.00237303  0.00106191<br>Total time run:         30.0008<br>Total writes made:      449321<br>Write size:             4096<br>Object size:            4096<br>Bandwidth (MB/sec):     58.5038<br>Stddev Bandwidth:       4.48913<br>Max bandwidth (MB/sec): 63.8789<br>Min bandwidth (MB/sec): 44.7734<br>Average IOPS:           14976<br>Stddev IOPS:            1149.22<br>Max IOPS:               16353<br>Min IOPS:               11462<br>Average Latency(s):     0.00106545<br>Stddev Latency(s):      0.000482816<br>Max latency(s):         0.0431733<br>Min latency(s):         0.00047586<br>Cleaning up (deleting benchmark objects)<br>Removed 449321 objects<br>Clean up completed and total clean up time :22.541<br></code></pre></td></tr></table></figure><p>一个iops 14976(58MB&#x2F;s)  - ubuntu的包<br>一个iops是5724(22MB&#x2F;s)  - ceph的包</p><p>可以看到差距还是很明显的</p><p>这个如果正好碰到了，可以按ubuntu这个方式改下debian的rule，或者把ceph的pr弄进去重新打包即可，具体验证有没有问题，可以通过检查下打包的参数或者直接单机验证下性能也可以</p>]]></content>
    
    
    <categories>
      
      <category>存储相关</category>
      
    </categories>
    
    
    <tags>
      
      <tag>ceph</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>storecli管理raid</title>
    <link href="/2024/03/20/storecli%E7%AE%A1%E7%90%86raid/"/>
    <url>/2024/03/20/storecli%E7%AE%A1%E7%90%86raid/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>本篇记录一些常用的配置raid的命令</p><h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><p>下载storcli管理软件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">wget https://docs.broadcom.com/docs-and-downloads/raid-controllers/raid-controllers-common-files/1.21.16_StorCLI.zip<br></code></pre></td></tr></table></figure><h2 id="常用操作"><a href="#常用操作" class="headerlink" title="常用操作"></a>常用操作</h2><h3 id="查询控制器信息"><a href="#查询控制器信息" class="headerlink" title="查询控制器信息"></a>查询控制器信息</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs bash">root@ubuntu:~<span class="hljs-comment"># storcli show</span><br>---------------------------------------------------------------------------------------------------<br>Ctl Model                                  Ports PDs DGs DNOpt VDs VNOpt BBU sPR DS  EHS ASOs Hlth<br>---------------------------------------------------------------------------------------------------<br>  0 Intel(R)IntegratedRAIDModuleRMS25CB080     8  12  12     0  12     0 Opt On  1&amp;2 Y      4 Opt<br>---------------------------------------------------------------------------------------------------<br></code></pre></td></tr></table></figure><h3 id="查询整个控制器的信息"><a href="#查询整个控制器的信息" class="headerlink" title="查询整个控制器的信息"></a>查询整个控制器的信息</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">root@ubuntu:~<span class="hljs-comment"># storcli /c0 show</span><br></code></pre></td></tr></table></figure><h3 id="删除foregin的配置"><a href="#删除foregin的配置" class="headerlink" title="删除foregin的配置"></a>删除foregin的配置</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">root@ubuntu:~<span class="hljs-comment"># storcli /c0/fall del</span><br></code></pre></td></tr></table></figure><h3 id="创建一个raid-0"><a href="#创建一个raid-0" class="headerlink" title="创建一个raid 0"></a>创建一个raid 0</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">storcli /c0 add vd <span class="hljs-built_in">type</span>=raid0  size=1.817TB  drives=8:0<br></code></pre></td></tr></table></figure><p>后面为磁盘的编号，通过控制器信息里面可以查到<br>不加容量参数就是所有，后面drivers后面的编号可以用数字增加更多</p><h3 id="删除raid-后面是VD编号"><a href="#删除raid-后面是VD编号" class="headerlink" title="删除raid(后面是VD编号)"></a>删除raid(后面是VD编号)</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">storcli /c0/v8 del<br></code></pre></td></tr></table></figure><p>后面的编号为vd的编号</p><h3 id="导入外部所有配置"><a href="#导入外部所有配置" class="headerlink" title="导入外部所有配置"></a>导入外部所有配置</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">storcli /c0/fall import<br></code></pre></td></tr></table></figure><h3 id="设置unconfig-bad的为good"><a href="#设置unconfig-bad的为good" class="headerlink" title="设置unconfig bad的为good"></a>设置unconfig bad的为good</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">storcli /c0/e0/s1 <span class="hljs-built_in">set</span> good force<br></code></pre></td></tr></table></figure><h3 id="设置卷的缓存模式"><a href="#设置卷的缓存模式" class="headerlink" title="设置卷的缓存模式"></a>设置卷的缓存模式</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">storcli /c0/v0 <span class="hljs-built_in">set</span> wrcache=awb<br></code></pre></td></tr></table></figure><h3 id="处理ubad状态的磁盘"><a href="#处理ubad状态的磁盘" class="headerlink" title="处理ubad状态的磁盘"></a>处理ubad状态的磁盘</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">storcli /c0 /e10 /s23 <span class="hljs-built_in">set</span> good<br></code></pre></td></tr></table></figure><h3 id="jbod模式与raid模式的切换"><a href="#jbod模式与raid模式的切换" class="headerlink" title="jbod模式与raid模式的切换"></a>jbod模式与raid模式的切换</h3><p>开启RAID卡直通功能</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">storcli /c0 <span class="hljs-built_in">set</span> jbod=on  <span class="hljs-comment"># storcli /cx set personality=JBOD</span><br></code></pre></td></tr></table></figure><p>查看配置是否设置成功</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">storcli /c0 show jbod<br></code></pre></td></tr></table></figure><p>切换raid模式为jbod</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@node174 Linux]<span class="hljs-comment"># storcli /c0/e45/s24 set jbod</span><br></code></pre></td></tr></table></figure><p>切换jbod模式为raid</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@node174 Linux]<span class="hljs-comment"># storcli /c0/e45/s24 set good force</span><br></code></pre></td></tr></table></figure><h3 id="点亮磁盘"><a href="#点亮磁盘" class="headerlink" title="点亮磁盘"></a>点亮磁盘</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@node174 Linux]<span class="hljs-comment"># storcli  /c0/e8/s1 start locate  </span><br></code></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@node174 Linux]<span class="hljs-comment"># storcli  /c0/e8/s1 stop locate  </span><br></code></pre></td></tr></table></figure><p>设置bootdrive</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">The Storage Command Line Tool supports the following OPROM BIOS commands:<br>storcli /cx/ex/sx <span class="hljs-built_in">set</span> bootdrive=on|off<br>storcli /cx/vx <span class="hljs-built_in">set</span> bootdrive=on|off<br>storcli /cx show bootdrive<br></code></pre></td></tr></table></figure><h3 id="清理缓存"><a href="#清理缓存" class="headerlink" title="清理缓存"></a>清理缓存</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab103 Linux]<span class="hljs-comment"># storcli /c0/vall delete preservedcache</span><br></code></pre></td></tr></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本篇会根据实际用到的命令进行增加</p>]]></content>
    
    
    <categories>
      
      <category>系统管理</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>查看ceph的osd的bluestore_min_alloc_size</title>
    <link href="/2024/03/07/%E6%9F%A5%E7%9C%8Bceph%E7%9A%84osd%E7%9A%84bluestore-min-alloc-size/"/>
    <url>/2024/03/07/%E6%9F%A5%E7%9C%8Bceph%E7%9A%84osd%E7%9A%84bluestore-min-alloc-size/</url>
    
    <content type="html"><![CDATA[<h2 id="需求"><a href="#需求" class="headerlink" title="需求"></a>需求</h2><p>bluestore_min_alloc_size可以修改ceph的osd的底层的最小分配单元，初始化的时候能够调整，这个修改后我们如何确认呢</p><h2 id="查询"><a href="#查询" class="headerlink" title="查询"></a>查询</h2><p>记录大小的日志在bluestore日志里面，需要调整osd的bluestore日志级别</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">debug_bluestore=10<br></code></pre></td></tr></table></figure><p>调整好了以后重启osd，查看日志</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab103 ceph]<span class="hljs-comment"># cat /var/log/ceph/ceph-osd.2.log |grep min_alloc_size|grep order</span><br>2024-03-07T11:00:42.419+0800 7ff28b4a0bc0 10 bluestore(/var/lib/ceph/osd/ceph-2) _set_alloc_sizes min_alloc_size 0x1000 order 12 max_alloc_size 0x0 prefer_deferred_size 0x0 deferred_batch_ops 64<br></code></pre></td></tr></table></figure><p>上面的order 12就是获取的值</p><p>这个值需要计算一下</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab103 <span class="hljs-built_in">local</span>]<span class="hljs-comment"># echo &quot;2^12&quot;|bc</span><br>4096<br></code></pre></td></tr></table></figure><p>可以看到这个值4096就是4k的大小</p>]]></content>
    
    
    <categories>
      
      <category>存储相关</category>
      
    </categories>
    
    
    <tags>
      
      <tag>ceph</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>drbd的快速初始化</title>
    <link href="/2024/03/06/drbd%E7%9A%84%E5%BF%AB%E9%80%9F%E5%88%9D%E5%A7%8B%E5%8C%96/"/>
    <url>/2024/03/06/drbd%E7%9A%84%E5%BF%AB%E9%80%9F%E5%88%9D%E5%A7%8B%E5%8C%96/</url>
    
    <content type="html"><![CDATA[<h2 id="需求"><a href="#需求" class="headerlink" title="需求"></a>需求</h2><p>默认启动drbd后，两个设备需要进行同步，而某些测试情况下，设备是空的，并不需要同步，都是空的就不需要同步了,但是系统自身是不会判断是否为空的，需要按下面的步骤处理一下</p><h2 id="操作方法"><a href="#操作方法" class="headerlink" title="操作方法"></a>操作方法</h2><p>1、两台机器上面都执行初始化元数据</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">drbdadm create-md &lt;resource&gt;<br></code></pre></td></tr></table></figure><p>2、两台设备上面都执行启动</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">drbdadm up &lt;resource&gt;<br></code></pre></td></tr></table></figure><p>3、在备用节点上面执行清理操作</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">drbdadm -- --clear-bitmap new-current-uuid &lt;resource&gt;<br></code></pre></td></tr></table></figure><p>4、在主节点上面执行设置主的操作</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">drbdadm primary &lt;resource&gt;<br></code></pre></td></tr></table></figure><p>5、检查设备同步情况</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">drbdadm status &lt;resource&gt;<br></code></pre></td></tr></table></figure><p>上面的操作后，主备的设备就是同步状态了，在测试场景下，能够节约不少时间</p><p>查看连接的情况</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@node152 drbd.d]<span class="hljs-comment"># cat /sys/kernel/debug/drbd/resources/r0/connections/node153/0/proc_drbd</span><br> 1: cs:Established ro:Primary/Secondary ds:UpToDate/UpToDate A r-----<br>    ns:0 nr:0 dw:0 dr:260304 al:0 bm:0 lo:0 pe:[0;0] ua:0 ap:[0;0] ep:1 wo:1 oos:0<br>    act_log: used:0/1237 hits:0 misses:0 starving:0 locked:0 changed:0<br>    blocked on activity <span class="hljs-built_in">log</span>: 0/0/0<br></code></pre></td></tr></table></figure><p>查看连接的协议<br>比如下面的我们需要使用rdma的</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@node152 drbd.d]<span class="hljs-comment"># cat /sys/kernel/debug/drbd/resources/r0/connections/node153/transport</span><br>v: 0<br><br>data stream<br>  corked: 0<br>  unsent: 0 bytes<br>  allocated: 0 bytes<br>control stream<br>  corked: 0<br>  unsent: 0 bytes<br>  allocated: 0 bytes<br><br>transport_type: rdma<br>v: 0<br><br>192.168.19.152 - 192.168.19.153: CONNECTED<br> data    field:  posted  alloc  desired   max<br>      tx_descs:     0             258<br> peer_rx_descs:   132 (receive window at peer)<br>      rx_descs:   133     133     129     258<br> rx_peer_knows:   132 (what the peer knows about my receive window)<br><br> control field:  posted  alloc  desired   max<br>      tx_descs:     0               8<br> peer_rx_descs:     3 (receive window at peer)<br>      rx_descs:     4       4       4       8<br> rx_peer_knows:     3 (what the peer knows about my receive window)<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>存储设备</category>
      
    </categories>
    
    
    <tags>
      
      <tag>系统配置</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Intel E810-XXV开启rdma功能</title>
    <link href="/2024/03/06/Intel-E810-XXV%E5%BC%80%E5%90%AFrdma%E5%8A%9F%E8%83%BD/"/>
    <url>/2024/03/06/Intel-E810-XXV%E5%BC%80%E5%90%AFrdma%E5%8A%9F%E8%83%BD/</url>
    
    <content type="html"><![CDATA[<h2 id="操作系统环境"><a href="#操作系统环境" class="headerlink" title="操作系统环境"></a>操作系统环境</h2><h3 id="操作系统"><a href="#操作系统" class="headerlink" title="操作系统"></a>操作系统</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab102 ~]<span class="hljs-comment"># cat /etc/redhat-release</span><br>CentOS Linux release 7.7.1908 (Core)<br>[root@lab102 ~]<span class="hljs-comment"># uname  -a</span><br>Linux lab102 3.10.0-1062.el7.x86_64 <span class="hljs-comment">#1 SMP Wed Aug 7 18:08:02 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux</span><br></code></pre></td></tr></table></figure><h3 id="查看pci设备"><a href="#查看pci设备" class="headerlink" title="查看pci设备"></a>查看pci设备</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab102 ~]<span class="hljs-comment"># lspci |grep Eth|grep 81</span><br>81:00.0 Ethernet controller: Intel Corporation Device 159b (rev 02)<br>81:00.1 Ethernet controller: Intel Corporation Device 159b (rev 02)<br></code></pre></td></tr></table></figure><p>更新pciid</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">update-pciids<br></code></pre></td></tr></table></figure><p>再次查看设备</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab102 ~]<span class="hljs-comment"># lspci |grep Eth|grep 81</span><br>81:00.0 Ethernet controller: Intel Corporation Ethernet Controller E810-XXV <span class="hljs-keyword">for</span> SFP (rev 02)<br>81:00.1 Ethernet controller: Intel Corporation Ethernet Controller E810-XXV <span class="hljs-keyword">for</span> SFP (rev 02)<br></code></pre></td></tr></table></figure><h2 id="相关驱动包"><a href="#相关驱动包" class="headerlink" title="相关驱动包"></a>相关驱动包</h2><p>这个是7.7的内核，7.9的应该也没问题，这里以这个举例子<br>主要涉及到三个驱动的下载</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">ice<br>irdma<br>rdma-core<br></code></pre></td></tr></table></figure><p>三个驱动要按顺序进行安装，并且rdma-core代码需要使用irdma里面的一个文件进行补丁的操作</p><h3 id="下载相关驱动包"><a href="#下载相关驱动包" class="headerlink" title="下载相关驱动包"></a>下载相关驱动包</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">wget https://downloadmirror.intel.com/812404/ice-1.13.7.tar.gz<br>wget https://downloadmirror.intel.com/812530/irdma-1.13.43.tgz<br>wget https://github.com/linux-rdma/rdma-core/releases/download/v46.0/rdma-core-46.0.tar.gz<br></code></pre></td></tr></table></figure><p>下载好了后放在一个目录下面</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab102 rdma]<span class="hljs-comment"># pwd</span><br>/root/rdma<br>[root@lab102 rdma]<span class="hljs-comment"># ll</span><br>total 3500<br>-rw-r--r--. 1 root root 1298302 Dec 28 11:42 ice-1.13.7.tar.gz<br>-rw-r--r--. 1 root root  342440 Dec 30 07:36 irdma-1.13.43.tgz<br>-rw-r--r--. 1 root root 1940926 Mar  5 18:32 rdma-core-46.0.tar.gz<br></code></pre></td></tr></table></figure><h3 id="安装内核devel包和依赖包"><a href="#安装内核devel包和依赖包" class="headerlink" title="安装内核devel包和依赖包"></a>安装内核devel包和依赖包</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab102 rdma]<span class="hljs-comment"># rpm -ivh /mnt/Packages/kernel-devel-3.10.0-1062.el7.x86_64.rpm</span><br>[root@lab102 rdma]<span class="hljs-comment"># yum install rpm-build  -y</span><br>[root@lab102 rdma]<span class="hljs-comment"># yum groupinstall &quot;Development Tools&quot; -y</span><br>[root@lab102 rdma]<span class="hljs-comment"># yum install python-docutils systemd-devel valgrind-devel libudev-devel cmake  libnl3-devel perftest -y</span><br></code></pre></td></tr></table></figure><p>这个注意跟内核版本要匹配上</p><h3 id="安装ice驱动"><a href="#安装ice驱动" class="headerlink" title="安装ice驱动"></a>安装ice驱动</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab102 rdma]<span class="hljs-comment"># tar -xvf ice-1.13.7.tar.gz</span><br>[root@lab102 rdma]<span class="hljs-comment"># cd ice-1.13.7/</span><br>[root@lab102 ice-1.13.7]<span class="hljs-comment"># cp -ra ../ice-1.13.7.tar.gz /root/rpmbuild/SOURCES/</span><br>[root@lab102 ice-1.13.7]<span class="hljs-comment"># rpmbuild -bb ice.spec</span><br>[root@lab102 ice-1.13.7]<span class="hljs-comment"># mv /root/rpmbuild/RPMS/x86_64/ice-1.13.7-1.x86_64.rpm ./</span><br>[root@lab102 ice-1.13.7]<span class="hljs-comment"># mv /root/rpmbuild/RPMS/x86_64/intel_auxiliary-1.0.2-1.x86_64.rpm ./</span><br>[root@lab102 ice-1.13.7]<span class="hljs-comment"># rpm -ivh ice-1.13.7-1.x86_64.rpm intel_auxiliary-1.0.2-1.x86_64.rpm</span><br>[root@lab102 ice-1.13.7]<span class="hljs-comment"># modprobe ice</span><br></code></pre></td></tr></table></figure><p>查看设备</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab102 ice-1.13.7]<span class="hljs-comment"># ethtool -i  enp129s0f0</span><br>driver: ice<br>version: 1.13.7<br>firmware-version: 4.30 0x8001af27 1.3429.0<br>expansion-rom-version:<br>bus-info: 0000:81:00.0<br>supports-statistics: <span class="hljs-built_in">yes</span><br>supports-test: <span class="hljs-built_in">yes</span><br>supports-eeprom-access: <span class="hljs-built_in">yes</span><br>supports-register-dump: <span class="hljs-built_in">yes</span><br>supports-priv-flags: <span class="hljs-built_in">yes</span><br></code></pre></td></tr></table></figure><p>到这里这个网卡就加载成功了，如果只是网卡使用这里就完成了，我们需要用的是rdma，就还需要继续</p><h3 id="安装irdma驱动"><a href="#安装irdma驱动" class="headerlink" title="安装irdma驱动"></a>安装irdma驱动</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab102 rdma]<span class="hljs-comment"># tar -xvf irdma-1.13.43.tgz</span><br>[root@lab102 irdma-1.13.43]<span class="hljs-comment"># ./build.sh</span><br></code></pre></td></tr></table></figure><p>执行完就安装好了</p><h3 id="安装rdma-core驱动"><a href="#安装rdma-core驱动" class="headerlink" title="安装rdma-core驱动"></a>安装rdma-core驱动</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab102 rdma]<span class="hljs-comment"># tar -xvf rdma-core-46.0.tar.gz</span><br>[root@lab102 rdma]<span class="hljs-comment"># cd rdma-core-46.0/</span><br>[root@lab102 rdma-core-46.0]<span class="hljs-comment"># patch -p2 &lt; /root/rdma/irdma-1.13.43/libirdma-46.0.patch</span><br>[root@lab102 rdma-core-46.0]<span class="hljs-comment"># cd ../</span><br>[root@lab102 rdma]<span class="hljs-comment"># rm -rf rdma-core-46.0.tar.gz</span><br>[root@lab102 rdma]<span class="hljs-comment"># tar -czvf rdma-core-46.0.tgz rdma-core-46.0</span><br>[root@lab102 rdma]<span class="hljs-comment"># cp -ra rdma-core-46.0.tgz /root/rpmbuild/SOURCES/</span><br>[root@lab102 rdma]<span class="hljs-comment"># cd rdma-core-46.0/</span><br>[root@lab102 rdma-core-46.0]<span class="hljs-comment"># rpmbuild -bb redhat/rdma-core.spec</span><br>[root@lab102 rdma-core-46.0]<span class="hljs-comment"># cd /root/rpmbuild/RPMS/x86_64/</span><br>[root@lab102 x86_64]<span class="hljs-comment"># rpm -ivh rdma-core-46.0-1.el7.x86_64.rpm rdma-core-devel-46.0-1.el7.x86_64.rpm libibverbs-46.0-1.el7.x86_64.rpm libibverbs-utils-46.0-1.el7.x86_64.rpm libibumad-46.0-1.el7.x86_64.rpm librdmacm-46.0-1.el7.x86_64.rpm librdmacm-utils-46.0-1.el7.x86_64.rpm infiniband-diags-46.0-1.el7.x86_64.rpm</span><br></code></pre></td></tr></table></figure><h3 id="加载rdma驱动"><a href="#加载rdma驱动" class="headerlink" title="加载rdma驱动"></a>加载rdma驱动</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab102 ~]<span class="hljs-comment"># rmmod irdma</span><br>[root@lab102 ~]<span class="hljs-comment"># modprobe irdma roce_ena=1</span><br></code></pre></td></tr></table></figure><p>默认加载的不是roce的驱动，需要加上后面的参数</p><h3 id="查看状态"><a href="#查看状态" class="headerlink" title="查看状态"></a>查看状态</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab102 ~]<span class="hljs-comment"># ibv_devices</span><br>    device                 node GUID<br>    ------              ----------------<br>    rdmap129s0f0        6eb311fffe21e748<br>    rdmap129s0f1        6eb311fffe21e749<br>[root@lab102 ~]<span class="hljs-comment"># ibstatus</span><br>Infiniband device <span class="hljs-string">&#x27;rdmap129s0f0&#x27;</span> port 1 status:<br>    default gid:     fe80:0000:0000:0000:6eb3:11ff:fe21:e748<br>    base lid:    0x1<br>    sm lid:      0x0<br>    state:       1: DOWN<br>    phys state:  3: Disabled<br>    rate:        100 Gb/sec (4X EDR)<br>    link_layer:  Ethernet<br><br>Infiniband device <span class="hljs-string">&#x27;rdmap129s0f1&#x27;</span> port 1 status:<br>    default gid:     fe80:0000:0000:0000:6eb3:11ff:fe21:e749<br>    base lid:    0x1<br>    sm lid:      0x0<br>    state:       4: ACTIVE<br>    phys state:  5: LinkUp<br>    rate:        10 Gb/sec (1X FDR10)<br>    link_layer:  Ethernet<br></code></pre></td></tr></table></figure><p>有这个就是驱动加载正常了，我们按常规的方法给rdmap129s0f1对应的网卡配置一个ip</p><h3 id="配置IP"><a href="#配置IP" class="headerlink" title="配置IP"></a>配置IP</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs bash">enp129s0f1: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  mtu 1500<br>        inet 192.167.19.102  netmask 255.255.0.0  broadcast 192.167.255.255<br>        inet6 fe80::85f4:6e55:d58a:fcbd  prefixlen 64  scopeid 0x20&lt;<span class="hljs-built_in">link</span>&gt;<br>        ether 6c:b3:11:21:e7:49  txqueuelen 1000  (Ethernet)<br>        RX packets 20562  bytes 1280394 (1.2 MiB)<br>        RX errors 0  dropped 6  overruns 0  frame 0<br>        TX packets 32  bytes 2908 (2.8 KiB)<br>        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0<br></code></pre></td></tr></table></figure><p>注意不要跟本地的其它网卡设置到一个网段了，不然走默认路由，会加载不了rdma的协议</p><h3 id="检查rdma的通信"><a href="#检查rdma的通信" class="headerlink" title="检查rdma的通信"></a>检查rdma的通信</h3><p>准备了两台机器，都配置好了后，检查联通情况</p><h3 id="服务端执行"><a href="#服务端执行" class="headerlink" title="服务端执行"></a>服务端执行</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab102 ~]<span class="hljs-comment"># ib_write_bw -d  rdmap129s0f1</span><br><br>************************************<br>* Waiting <span class="hljs-keyword">for</span> client to connect... *<br>************************************<br>---------------------------------------------------------------------------------------<br>                    RDMA_Write BW Test<br> Dual-port       : OFF      Device         : rdmap129s0f1<br> Number of qps   : 1        Transport <span class="hljs-built_in">type</span> : IB<br> Connection <span class="hljs-built_in">type</span> : RC       Using SRQ      : OFF<br> CQ Moderation   : 100<br> Mtu             : 1024[B]<br> Link <span class="hljs-built_in">type</span>       : Ethernet<br> GID index       : 2<br> Max inline data : 0[B]<br> rdma_cm QPs     : OFF<br> Data ex. method : Ethernet<br>---------------------------------------------------------------------------------------<br> <span class="hljs-built_in">local</span> address: LID 0x01 QPN 0x0004 PSN 0xfaaaae RKey 0xe9a9f75 VAddr 0x007f0ec3619000<br> GID: 00:00:00:00:00:00:00:00:00:00:255:255:192:167:19:102<br> remote address: LID 0x01 QPN 0x000a PSN 0x5a11ee RKey 0xde636e65 VAddr 0x007fa3cfaca000<br> GID: 00:00:00:00:00:00:00:00:00:00:255:255:192:167:19:103<br>---------------------------------------------------------------------------------------<br> <span class="hljs-comment">#bytes     #iterations    BW peak[MB/sec]    BW average[MB/sec]   MsgRate[Mpps]</span><br> 65536      5000             1103.34            1103.34        0.017653<br>---------------------------------------------------------------------------------------<br></code></pre></td></tr></table></figure><h3 id="客户端执行"><a href="#客户端执行" class="headerlink" title="客户端执行"></a>客户端执行</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab103 ~]<span class="hljs-comment"># ib_write_bw 192.167.19.102</span><br>---------------------------------------------------------------------------------------<br>                    RDMA_Write BW Test<br> Dual-port       : OFF      Device         : rocep129s0f0<br> Number of qps   : 1        Transport <span class="hljs-built_in">type</span> : IB<br> Connection <span class="hljs-built_in">type</span> : RC       Using SRQ      : OFF<br> TX depth        : 128<br> CQ Moderation   : 100<br> Mtu             : 1024[B]<br> Link <span class="hljs-built_in">type</span>       : Ethernet<br> GID index       : 1<br> Max inline data : 0[B]<br> rdma_cm QPs     : OFF<br> Data ex. method : Ethernet<br>---------------------------------------------------------------------------------------<br> <span class="hljs-built_in">local</span> address: LID 0x01 QPN 0x000a PSN 0x5a11ee RKey 0xde636e65 VAddr 0x007fa3cfaca000<br> GID: 00:00:00:00:00:00:00:00:00:00:255:255:192:167:19:103<br> remote address: LID 0x01 QPN 0x0004 PSN 0xfaaaae RKey 0xe9a9f75 VAddr 0x007f0ec3619000<br> GID: 00:00:00:00:00:00:00:00:00:00:255:255:192:167:19:102<br>---------------------------------------------------------------------------------------<br> <span class="hljs-comment">#bytes     #iterations    BW peak[MB/sec]    BW average[MB/sec]   MsgRate[Mpps]</span><br> 65536      5000             1103.34            1103.34        0.017653<br>---------------------------------------------------------------------------------------<br></code></pre></td></tr></table></figure><p>到这里就roce的驱动和功能已经开起来了，并且可以通信了，后面根据需要进行相关的软件配置即可</p>]]></content>
    
    
    <categories>
      
      <category>网络配置</category>
      
    </categories>
    
    
    <tags>
      
      <tag>系统配置</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>使用iptable配置代理网关</title>
    <link href="/2024/03/01/%E4%BD%BF%E7%94%A8iptable%E9%85%8D%E7%BD%AE%E4%BB%A3%E7%90%86%E7%BD%91%E5%85%B3/"/>
    <url>/2024/03/01/%E4%BD%BF%E7%94%A8iptable%E9%85%8D%E7%BD%AE%E4%BB%A3%E7%90%86%E7%BD%91%E5%85%B3/</url>
    
    <content type="html"><![CDATA[<h2 id="需求"><a href="#需求" class="headerlink" title="需求"></a>需求</h2><p>内网需要通过一台服务器作为网关，服务器只有一个网卡</p><h2 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-meta">#! /bin/sh</span><br><br>iptables -X<br>iptables -F<br>iptables -t nat -F<br><span class="hljs-comment"># 允许网段 192.168.0.0/24 的终端通过拨号主机转发</span><br>iptables -I FORWARD -t filter -s 192.168.0.0/24 -j ACCEPT<br>iptables -A POSTROUTING -t nat -s 192.168.0.0/24 -o ens33 -j MASQUERADE<br></code></pre></td></tr></table></figure><p>上面的配置以后，其它机器就可以通过这台服务器进行上网了</p>]]></content>
    
    
    <categories>
      
      <category>系统配置</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>SCST配置ALUA的支持</title>
    <link href="/2024/02/28/SCST%E9%85%8D%E7%BD%AEALUA%E7%9A%84%E6%94%AF%E6%8C%81/"/>
    <url>/2024/02/28/SCST%E9%85%8D%E7%BD%AEALUA%E7%9A%84%E6%94%AF%E6%8C%81/</url>
    
    <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>在多路径配置下面，如果是主备的情况下，那么哪个盘是主盘，这个默认来说是没有办法进行配置的，这个时候需要服务端进行一个控制，来实现这个，这个是通过SCSI的协议把相关的信息保存在磁盘属性里面的，然后客户端程序再根据这个通用的标准进行主备的选择，这里不详细讲ALUA</p><p>像那种双控的盘阵，比如有两个控制器，一个主，一个备的，然后主上面有4个链路，备的上面有4个链路</p><p>多路径连上去以后，多路径软件能够把主控和备控制器进行区分，存在不同的优先级，进行分组，然后主的里面也可以根据配置去选择是单主还是多主</p><h2 id="实现的软件"><a href="#实现的软件" class="headerlink" title="实现的软件"></a>实现的软件</h2><p>tgt这个默认是不支持这个功能的，scst是支持的，本次配置就是通过scst进行配置</p><h2 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h2><h3 id="主target的配置"><a href="#主target的配置" class="headerlink" title="主target的配置"></a>主target的配置</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab102 ~]<span class="hljs-comment"># cat /etc/scst.conf</span><br>HANDLER vdisk_fileio &#123;<br>    DEVICE disk1 &#123;<br>        filename /dev/rbd/rbd/testiscsi<br>        nv_cache 1<br>    &#125;<br>&#125;<br><br>TARGET_DRIVER iscsi &#123;<br>    enabled 1<br>    TARGET iscsi_target1 &#123;<br>        LUN 0 disk1<br>        enabled 1<br>    &#125;<br>&#125;<br><br>DEVICE_GROUP dgroup1 &#123;<br>   DEVICE disk1<br><br>   TARGET_GROUP tgroup1 &#123;<br>        group_id 1<br>        state active<br>        preferred 1<br><br>        TARGET iscsi_target1 &#123;<br>             rel_tgt_id 1<br>        &#125;<br>   &#125;<br></code></pre></td></tr></table></figure><h3 id="备target的配置"><a href="#备target的配置" class="headerlink" title="备target的配置"></a>备target的配置</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ~]<span class="hljs-comment"># cat /etc/scst.conf</span><br>HANDLER vdisk_fileio &#123;<br>    DEVICE disk1 &#123;<br>        filename /dev/rbd/rbd/testiscsi<br>        nv_cache 1<br>    &#125;<br>&#125;<br><br>TARGET_DRIVER iscsi &#123;<br>    enabled 1<br>    TARGET iscsi_target1 &#123;<br>        LUN 0 disk1<br>        enabled 1<br>    &#125;<br>&#125;<br>DEVICE_GROUP dgroup1 &#123;<br>       DEVICE disk1<br><br>       TARGET_GROUP tgroup1 &#123;<br>            group_id 1<br>            state nonoptimized<br>            preferred 0<br><br>            TARGET iscsi_target1 &#123;<br>                 rel_tgt_id 1<br>            &#125;<br>       &#125;<br><br>  &#125;<br></code></pre></td></tr></table></figure><p>关键配置<br>主设置</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">state active<br>preferred 1<br></code></pre></td></tr></table></figure><p>备设置</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">state nonoptimized<br>preferred 0<br></code></pre></td></tr></table></figure><h3 id="iscsi客户端连接"><a href="#iscsi客户端连接" class="headerlink" title="iscsi客户端连接"></a>iscsi客户端连接</h3><p>查看连接的信息</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab105 ~]<span class="hljs-comment"># dmesg</span><br>[29885.203661] scsi host46: iSCSI Initiator over TCP/IP<br>[29885.220742] scsi 46:0:0:0: Direct-Access     SCST_FIO disk1            360  PQ: 0 ANSI: 6<br>[29885.238029] scsi 46:0:0:0: alua: supports implicit TPGS<br>[29885.238040] scsi 46:0:0:0: alua: device eui.6238666462643332 port group 1 rel port 1<br>[29885.238047] scsi 46:0:0:0: alua: Attached<br>[29885.239421] sd 46:0:0:0: Attached scsi generic sg2 <span class="hljs-built_in">type</span> 0<br>[29885.239834] sd 46:0:0:0: Power-on or device reset occurred<br>[29885.244954] sd 46:0:0:0: alua: transition <span class="hljs-built_in">timeout</span> <span class="hljs-built_in">set</span> to 60 seconds<br>[29885.244965] sd 46:0:0:0: alua: port group 01 state A preferred supports TOlUSNA<br></code></pre></td></tr></table></figure><p>上面的是连接的信息，可以看到几个关键信息</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">port group 01 state A preferred<br></code></pre></td></tr></table></figure><p>这个就是状态是A，并且preffered的</p><p>查看磁盘信息</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab105 ~]<span class="hljs-comment"># sg_rtpg -v /dev/sdd</span><br>    report target port <span class="hljs-built_in">groups</span> cdb: a3 0a 00 00 00 00 00 00 04 00 00 00<br>    report target port group: pass-through requested 1024 bytes but got 16 bytes<br>Report list length = 16<br>Report target port <span class="hljs-built_in">groups</span>:<br>  target port group <span class="hljs-built_in">id</span> : 0x1 , Pref=1<br>    target port group asymmetric access state : 0x00<br>    T_SUP : 1, O_SUP : 1, LBD_SUP : 0, U_SUP : 1, S_SUP : 1, AN_SUP : 1, AO_SUP : 1<br>    status code : 0x02<br>    vendor unique status : 0x00<br>    target port count : 01<br>    Relative target port ids:<br>      0x01<br></code></pre></td></tr></table></figure><p>可以看到磁盘的状态</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">Pref=1<br></code></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab105 ~]<span class="hljs-comment"># sg_inq /dev/sdd</span><br>standard INQUIRY:<br>  PQual=0  Device_type=0  RMB=0  version=0x06  [SPC-4]<br>  [AERC=0]  [TrmTsk=0]  NormACA=1  HiSUP=0  Resp_data_format=2<br>  SCCS=0  ACC=0  TPGS=1  3PC=1  Protect=0  [BQue=0]<br>  EncServ=0  MultiP=1 (VS=0)  [MChngr=0]  [ACKREQQ=0]  Addr16=0<br>  [RelAdr=0]  WBus16=0  Sync=0  Linked=0  [TranDis=0]  CmdQue=1<br>  [SPI: Clocking=0x0  QAS=0  IUS=0]<br>    length=66 (0x42)   Peripheral device <span class="hljs-built_in">type</span>: disk<br> Vendor identification: SCST_FIO<br> Product identification: disk1<br> Product revision level: 360<br> Unit serial number: b8fdbd32<br></code></pre></td></tr></table></figure><p>这个命令可以看到支持了TPGS&#x3D;1，需要这个显示是1就是支持了</p><p>备用节点的iscsi信息</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab105 ~]<span class="hljs-comment"># dmesg</span><br>[30103.927420] scsi host47: iSCSI Initiator over TCP/IP<br>[30103.944329] scsi 47:0:0:0: Direct-Access     SCST_FIO disk1            360  PQ: 0 ANSI: 6<br>[30103.965061] scsi 47:0:0:0: alua: supports implicit TPGS<br>[30103.965072] scsi 47:0:0:0: alua: device eui.6238666462643332 port group 1 rel port 1<br>[30103.965079] scsi 47:0:0:0: alua: Attached<br>[30103.965366] sd 47:0:0:0: Attached scsi generic sg3 <span class="hljs-built_in">type</span> 0<br>[30103.967790] sd 47:0:0:0: Power-on or device reset occurred<br>[30103.973556] sd 47:0:0:0: [sde] 41943040 512-byte logical blocks: (21.4 GB/20.0 GiB)<br>[30103.973562] sd 47:0:0:0: [sde] 4096-byte physical blocks<br>[30103.973621] sd 47:0:0:0: alua: port group 01 state N non-preferred supports TOlUSNA<br></code></pre></td></tr></table></figure><p>可以看到状态是state N non-preferred，跟我们设置的一致</p><h3 id="多路径配置"><a href="#多路径配置" class="headerlink" title="多路径配置"></a>多路径配置</h3><p>做最简单的配置</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab105 ~]<span class="hljs-comment"># cat /etc/multipath.conf</span><br>blacklist &#123;<br>       devnode <span class="hljs-string">&quot;^sd[a]&quot;</span><br>&#125;<br>defaults &#123;<br>        user_friendly_names <span class="hljs-built_in">yes</span><br>        prio <span class="hljs-string">&quot;alua&quot;</span><br>&#125;<br></code></pre></td></tr></table></figure><p>跟默认配置只增加了prio alua，这个就是告诉系统启用alua的</p><p>查看信息</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab105 ~]<span class="hljs-comment"># multipath -ll</span><br>mpathc (26238666462643332) dm-3 SCST_FIO,disk1<br>size=20G features=<span class="hljs-string">&#x27;0&#x27;</span> hwhandler=<span class="hljs-string">&#x27;0&#x27;</span> wp=rw<br>|-+- policy=<span class="hljs-string">&#x27;service-time 0&#x27;</span> prio=50 status=active<br>| `- 46:0:0:0 sdd 8:48 active ready running<br>`-+- policy=<span class="hljs-string">&#x27;service-time 0&#x27;</span> prio=10 status=enabled<br>  `- 47:0:0:0 sde 8:64 active ready running<br></code></pre></td></tr></table></figure><p>查询磁盘的状态，可以看到主的prio&#x3D;50，备的prio&#x3D;10，这个跟我们设置的一致，通过上面的设置以后，就能够在客户端这边进行多路径的主备的优先级的选择了</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>multipath多路径主主和主备配置</title>
    <link href="/2024/02/28/multipath%E5%A4%9A%E8%B7%AF%E5%BE%84%E4%B8%BB%E4%B8%BB%E5%92%8C%E4%B8%BB%E5%A4%87%E9%85%8D%E7%BD%AE/"/>
    <url>/2024/02/28/multipath%E5%A4%9A%E8%B7%AF%E5%BE%84%E4%B8%BB%E4%B8%BB%E5%92%8C%E4%B8%BB%E5%A4%87%E9%85%8D%E7%BD%AE/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>多路径的配置方法，尽量最小操作的配置</p><h2 id="配置方法"><a href="#配置方法" class="headerlink" title="配置方法"></a>配置方法</h2><h3 id="主备的配置"><a href="#主备的配置" class="headerlink" title="主备的配置"></a>主备的配置</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs bash"> defaults &#123;<br>    user_friendly_names <span class="hljs-built_in">yes</span><br>&#125;<br>blacklist &#123;<br>devnode <span class="hljs-string">&quot;^sd[a]&quot;</span><br>&#125;<br><br></code></pre></td></tr></table></figure><p>核心控制主备的是<br>多活</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">path_grouping_policy    multibus<br>path_selector       <span class="hljs-string">&quot;round-robin 0&quot;</span><br></code></pre></td></tr></table></figure><p> 主备（默认选项）<br> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">path_grouping_policy <span class="hljs-string">&quot;failover&quot;</span><br>path_selector <span class="hljs-string">&quot;service-time 0&quot;</span><br></code></pre></td></tr></table></figure><br>注意在device设备的配置优先级更高，注意下device写的时候一定要匹配正确，如果有多种设备使用的时候并且配置不一样的时候才启动device的配置文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@node231 etc]<span class="hljs-comment"># multipath -ll|tail -n 6</span><br>dg03_v0001 (3600c0ff00053a815593a676201000000) dm-5 SEAGATE ,5565<br>size=116T features=<span class="hljs-string">&#x27;0&#x27;</span> hwhandler=<span class="hljs-string">&#x27;0&#x27;</span> wp=rw<br>|-+- policy=<span class="hljs-string">&#x27;service-time 0&#x27;</span> prio=1 status=active<br>| `- 16:0:0:2 sdd 8:48  active ready running<br>`-+- policy=<span class="hljs-string">&#x27;service-time 0&#x27;</span> prio=1 status=enabled<br>  `- 16:0:1:2 sdl 8:176 active ready running<br></code></pre></td></tr></table></figure><h3 id="双活的配置"><a href="#双活的配置" class="headerlink" title="双活的配置"></a>双活的配置</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@node231 etc]<span class="hljs-comment"># cat /etc/multipath.conf</span><br>defaults &#123;<br>    path_selector       <span class="hljs-string">&quot;round-robin 0&quot;</span><br>    path_grouping_policy    <span class="hljs-string">&quot;multibus&quot;</span><br>    user_friendly_names <span class="hljs-built_in">yes</span><br>    find_multipaths <span class="hljs-built_in">yes</span><br>&#125;<br>blacklist &#123;<br>devnode <span class="hljs-string">&quot;^sd[a]&quot;</span><br>&#125;<br></code></pre></td></tr></table></figure><h3 id="独立device配置"><a href="#独立device配置" class="headerlink" title="独立device配置"></a>独立device配置</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs bash"><br>blacklist &#123;<br>       devnode <span class="hljs-string">&quot;^sd[a]&quot;</span><br>&#125;<br>defaults &#123;<br>        user_friendly_names <span class="hljs-built_in">yes</span><br>&#125;<br><br>devices &#123;<br>    device &#123;<br>        vendor <span class="hljs-string">&quot;SCST_FIO&quot;</span><br>        product <span class="hljs-string">&quot;disk1&quot;</span><br>        path_grouping_policy <span class="hljs-string">&quot;multibus&quot;</span><br>        path_selector      <span class="hljs-string">&quot;round-robin 0&quot;</span><br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure><p>上面就是默认的主备，指定设备配置成主主的情况，独立配置覆盖默认配置</p><p>vendor和product的获取</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab105 ~]<span class="hljs-comment"># lsscsi --scsi_id</span><br>[0:0:0:0]    disk    VMware,  VMware Virtual S 1.0   /dev/sda   -<br>[2:0:0:0]    <span class="hljs-built_in">cd</span>/dvd  NECVMWar VMware IDE CDR10 1.00  /dev/sr0   -<br>[44:0:0:0]   disk    SCST_FIO disk1            360   /dev/sdb   26238666462643332<br>[45:0:0:0]   disk    SCST_FIO disk1            360   /dev/sdc   26238666462643332<br></code></pre></td></tr></table></figure><p>上面的 SCST_FIO disk1  就是分别对应的vendor product</p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>上面的就完成了两种模式的配置，更多的配置，后续有需要用到再补充说明</p>]]></content>
    
    
    <categories>
      
      <category>系统配置</category>
      
    </categories>
    
    
    <tags>
      
      <tag>系统配置</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>fio的联机模式</title>
    <link href="/2024/02/23/fio%E7%9A%84%E8%81%94%E6%9C%BA%E6%A8%A1%E5%BC%8F/"/>
    <url>/2024/02/23/fio%E7%9A%84%E8%81%94%E6%9C%BA%E6%A8%A1%E5%BC%8F/</url>
    
    <content type="html"><![CDATA[<h2 id="需求"><a href="#需求" class="headerlink" title="需求"></a>需求</h2><p>通过fio控制多台同时测试，让io并发</p><h2 id="使用方法"><a href="#使用方法" class="headerlink" title="使用方法"></a>使用方法</h2><p>在受控客户端每台机器上面执行</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">fio --server<br></code></pre></td></tr></table></figure><p>运行测试执行命令</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@node232 testnvme]<span class="hljs-comment"># fio --client=host.list  fio1.job</span><br></code></pre></td></tr></table></figure><p>测试主机配置文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@node232 testnvme]<span class="hljs-comment"># cat host.list</span><br>192.168.122.2<br>192.168.122.163<br></code></pre></td></tr></table></figure><p>fio配置文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@node232 testnvme]<span class="hljs-comment"># cat fio1.job</span><br>[global]<br>ioengine=libaio<br>direct=1<br>thread=1<br>time_based<br>filename=/nvmedisk/test.temp<br>runtime=120<br>size=1G<br><br>[SEQ1MQ8T1-write]<br>bs=1M<br>iodepth=8<br>rw=write<br></code></pre></td></tr></table></figure><p>使用上面的方法就可以并发测试多台机器了，选项可以根据自己的需求进行修改</p>]]></content>
    
    
    <categories>
      
      <category>测试工具</category>
      
    </categories>
    
    
    <tags>
      
      <tag>测试</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>通过命令行创建KVM虚拟机</title>
    <link href="/2024/02/23/%E9%80%9A%E8%BF%87%E5%91%BD%E4%BB%A4%E8%A1%8C%E5%88%9B%E5%BB%BAKVM%E8%99%9A%E6%8B%9F%E6%9C%BA/"/>
    <url>/2024/02/23/%E9%80%9A%E8%BF%87%E5%91%BD%E4%BB%A4%E8%A1%8C%E5%88%9B%E5%BB%BAKVM%E8%99%9A%E6%8B%9F%E6%9C%BA/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>本篇是记录通过命令行创建一个虚拟机的测试环境</p><h2 id="创建过程"><a href="#创建过程" class="headerlink" title="创建过程"></a>创建过程</h2><h3 id="下载ISO镜像"><a href="#下载ISO镜像" class="headerlink" title="下载ISO镜像"></a>下载ISO镜像</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">wget https://mirrors.tuna.tsinghua.edu.cn/centos/7.7.1908/isos/x86_64/CentOS-7-x86_64-DVD-1908.iso<br></code></pre></td></tr></table></figure><h3 id="安装虚拟化相关的软件"><a href="#安装虚拟化相关的软件" class="headerlink" title="安装虚拟化相关的软件"></a>安装虚拟化相关的软件</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">yum install virt-install libvirt qemu-kvm<br></code></pre></td></tr></table></figure><h3 id="配置桥接网络"><a href="#配置桥接网络" class="headerlink" title="配置桥接网络"></a>配置桥接网络</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">yum install bridge-utils<br></code></pre></td></tr></table></figure><p>修改配置文件，把onboot改错no，也就是开机不启动</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs raw">/etc/sysconfig/network-scripts/ifcfg-ens33 <br>ONBOOT=&quot;no&quot;<br></code></pre></td></tr></table></figure><p>修改&#x2F;etc&#x2F;rc.local</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs bash">ifconfig ens33 down<br>ifconfig ens33 0.0.0.0<br>brctl addbr br0<br>brctl addif br0 ens33<br>ifconfig br0 192.168.0.101/24 up<br>brctl stp br0 off<br>route add default gw 192.168.0.1 br0<br></code></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">chmod</span> +x /etc/rc.d/rc.local<br></code></pre></td></tr></table></figure><p>还原</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs bash">ifconfig ens33 down<br>ifconfig br0 down<br>brctl delif br0 ens33<br>brctl delbr br0<br>ifconfig ens33 192.168.0.101/24 up<br>route add default gw 192.168.0.1 ens33<br></code></pre></td></tr></table></figure><p>创建磁盘</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">qemu-img create -f qcow2 ./node1.qcow2 100G<br></code></pre></td></tr></table></figure><h3 id="创建虚拟机"><a href="#创建虚拟机" class="headerlink" title="创建虚拟机"></a>创建虚拟机</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">virt-install --name=ubuntu16 --vcpus=16 --ram=8192 --disk path=node1.qcow2,format=qcow2,bus=virtio --network bridge=br0 --cdrom /home/CentOS-7-x86_64-DVD-1908.iso --graphics vnc,listen=0.0.0.0,port=5900 --force --autostart<br></code></pre></td></tr></table></figure><p>上面为做的网桥模式，如果使用默认nat模式</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">virt-install --name=ubuntu16 --vcpus=16 --ram=8192 --disk path=node1.qcow2,format=qcow2,bus=virtio --network network=default --cdrom /home/CentOS-7-x86_64-DVD-1908.iso --graphics vnc,listen=0.0.0.0,port=5900 --force --autostart<br></code></pre></td></tr></table></figure><h3 id="克隆虚拟机"><a href="#克隆虚拟机" class="headerlink" title="克隆虚拟机"></a>克隆虚拟机</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">virt-clone -o centos01 -n centos02 -f ./centos02.qcow2<br></code></pre></td></tr></table></figure><p>-o指定源虚拟机名称<br>-n指定新虚拟机名称<br>-f指定存储新虚拟机的文件路径</p><p>启动虚拟机</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 kvm1]<span class="hljs-comment"># virsh start centos02</span><br>Domain centos02 started<br></code></pre></td></tr></table></figure><p>进入虚拟机</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 kvm1]<span class="hljs-comment"># virsh console centos02</span><br></code></pre></td></tr></table></figure><p>默认会卡住的<br>修改内核相关的配置文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab201 ~]<span class="hljs-comment"># cat /etc/default/grub </span><br>GRUB_CMDLINE_LINUX=<span class="hljs-string">&quot;rhgb quiet  console=ttyS0,115200&quot;</span><br></code></pre></td></tr></table></figure><p>然后重新生成grub，然后重启后就可以console控制了</p><p>console退出是按</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">ctrl+]<br></code></pre></td></tr></table></figure><p>也可以导入</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">virt-install   --name=longsong-base --vcpus=4 --ram=8192 --disk path=loosong-base.qcow2,format=qcow2,bus=virtio --network network=default  --graphics vnc,listen=0.0.0.0,port=5908 --force --import  --autostart<br></code></pre></td></tr></table></figure><h3 id="网卡替换"><a href="#网卡替换" class="headerlink" title="网卡替换"></a>网卡替换</h3><p>删除网卡</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ~]<span class="hljs-comment"># virsh detach-interface  centos01 bridge  52:54:00:bf:52:5b   --config</span><br>Interface detached successfully<br></code></pre></td></tr></table></figure><p>重新添加网卡</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ~]<span class="hljs-comment"># virsh attach-interface --domain centos01 --type bridge --source br1 --config</span><br>Interface attached successfully<br></code></pre></td></tr></table></figure><h3 id="虚拟机IP查询"><a href="#虚拟机IP查询" class="headerlink" title="虚拟机IP查询"></a>虚拟机IP查询</h3><p>通过virsh命令查询虚拟机的mac地址</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@node83 ~]<span class="hljs-comment"># virsh dumpxml longsong-zp|grep mac</span><br>    &lt;partition&gt;/machine&lt;/partition&gt;<br>    &lt;<span class="hljs-built_in">type</span> <span class="hljs-built_in">arch</span>=<span class="hljs-string">&#x27;x86&#x27;</span> machine=<span class="hljs-string">&#x27;x86&#x27;</span>&gt;hvm&lt;/type&gt;<br>      &lt;mac address=<span class="hljs-string">&#x27;52:54:00:d0:77:53&#x27;</span>/&gt;<br></code></pre></td></tr></table></figure><p>通过arp查询mac对应的ip地址</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@node83 ~]<span class="hljs-comment"># arp -a |grep 52:54:00:d0:77:53</span><br>? (192.168.122.100) at 52:54:00:d0:77:53 [ether] on virbr0<br></code></pre></td></tr></table></figure><h3 id="添加盘"><a href="#添加盘" class="headerlink" title="添加盘"></a>添加盘</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab233 bigdisk]<span class="hljs-comment"># qemu-img create -f qcow2 ./zp1-disk1.qcow2 100G</span><br>或者<br>[root@lab233 bigdisk]<span class="hljs-comment">#dd if=/dev/zero of=zp1-disk1.qcow2 bs=1M seek=102400 count=0</span><br>[root@lab233 bigdisk]<span class="hljs-comment"># virsh attach-disk  --config zp1 /bigdisk/zp1-disk1.qcow2 vdb</span><br>Disk attached successfully<br></code></pre></td></tr></table></figure><p>完成了磁盘的添加</p><h3 id="ISO相关操作"><a href="#ISO相关操作" class="headerlink" title="ISO相关操作"></a>ISO相关操作</h3><p>添加</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 kvm]<span class="hljs-comment"># virsh attach-disk centos7.6-1 /home/CentOS-7-x86_64-DVD-1810.iso vdm</span><br>Disk attached successfully<br></code></pre></td></tr></table></figure><p>查询</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 kvm]<span class="hljs-comment"># virsh domblklist centos7.6-1</span><br>Target     Source<br>------------------------------------------------<br>vda        /kvm/centos7.6-1.qcow2<br>vdm        /home/CentOS-7-x86_64-DVD-1810.iso<br>hda        -<br></code></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 kvm]<span class="hljs-comment"># virsh detach-disk  centos7.6-1 vdm</span><br>Disk detached successfully<br></code></pre></td></tr></table></figure><h3 id="win10加载iso"><a href="#win10加载iso" class="headerlink" title="win10加载iso"></a>win10加载iso</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 kvmhost]<span class="hljs-comment"># virt-install --name=win10 --vcpus=10 --ram=24576  --disk path=./node1.qcow2,format=qcow2,bus=virtio --network bridge=brq847bb79a-3e --disk path=/kvmhost/cn_windows_10_business_editions_version_1903_x64_dvd_e001dd2c.iso,device=cdrom --accelerate -c /kvmhost/virtio-win-0.1.240.iso --graphics vnc,listen=0.0.0.0,port=5900 --force --autostart</span><br>或者<br>[root@lab101 kvmhost]<span class="hljs-comment"># virt-install --name=win10 --vcpus=16 --ram=16384 --disk path=/dev/cas1-1,format=raw,bus=virtio --accelerate --network bridge=br0 --disk device=cdrom,path=/opt/cn_windows_10_multi-edition_version_1709_updated_sept_2017_x64_dvd_100090804.iso  --disk device=cdrom,path=/opt/virtio-win-0.1.240.iso  --graphics vnc,listen=0.0.0.0,port=5900 --force --autostart</span><br></code></pre></td></tr></table></figure><h3 id="快照相关"><a href="#快照相关" class="headerlink" title="快照相关"></a>快照相关</h3><p>创建快照</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 kvm]<span class="hljs-comment"># virsh snapshot-create-as  centos7.6-small  --name origin --description &quot;yuanshianzhuang&quot;</span><br>Domain snapshot origin created<br></code></pre></td></tr></table></figure><p>查询快照</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 kvm]<span class="hljs-comment"># virsh snapshot-list centos7.6-small</span><br> Name                 Creation Time             State<br>------------------------------------------------------------<br> origin               2021-05-14 12:10:14 +0800 shutoff<br></code></pre></td></tr></table></figure><p>还原快照</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ~]<span class="hljs-comment">#  virsh snapshot-list centos7.6-small</span><br> Name                 Creation Time             State<br>------------------------------------------------------------<br> origin               2021-05-14 12:10:14 +0800 shutoff<br><br>[root@lab101 ~]<span class="hljs-comment">#  virsh snapshot-revert  centos7.6-small origin</span><br></code></pre></td></tr></table></figure><p>删除快照</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ~]<span class="hljs-comment"># virsh  snapshot-delete centos7.6-small  --snapshotname origin </span><br>Domain snapshot origin  deleted<br></code></pre></td></tr></table></figure><h2 id="查看虚拟机的ip"><a href="#查看虚拟机的ip" class="headerlink" title="查看虚拟机的ip"></a>查看虚拟机的ip</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab102 kvm]<span class="hljs-comment"># virsh domifaddr centos7</span><br> Name       MAC address          Protocol     Address<br>-------------------------------------------------------------------------------<br> vnet0      52:54:00:4a:2e:6a    ipv4         192.168.122.48/24<br></code></pre></td></tr></table></figure><h3 id="存储替换"><a href="#存储替换" class="headerlink" title="存储替换"></a>存储替换</h3><p>如果安装系统的时候想换成ceph</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs bash">&lt;disk <span class="hljs-built_in">type</span>=<span class="hljs-string">&#x27;file&#x27;</span> device=<span class="hljs-string">&#x27;disk&#x27;</span>&gt;<br>      &lt;driver name=<span class="hljs-string">&#x27;qemu&#x27;</span> <span class="hljs-built_in">type</span>=<span class="hljs-string">&#x27;qcow2&#x27;</span>/&gt;<br>      &lt;<span class="hljs-built_in">source</span> file=<span class="hljs-string">&#x27;/kvm/node1.qcow2&#x27;</span>/&gt;<br>      &lt;target dev=<span class="hljs-string">&#x27;vda&#x27;</span> bus=<span class="hljs-string">&#x27;virtio&#x27;</span>/&gt;<br>      &lt;address <span class="hljs-built_in">type</span>=<span class="hljs-string">&#x27;pci&#x27;</span> domain=<span class="hljs-string">&#x27;0x0000&#x27;</span> bus=<span class="hljs-string">&#x27;0x00&#x27;</span> slot=<span class="hljs-string">&#x27;0x05&#x27;</span> <span class="hljs-keyword">function</span>=<span class="hljs-string">&#x27;0x0&#x27;</span>/&gt;<br>    &lt;/disk&gt;<br><br><br>&lt;disk <span class="hljs-built_in">type</span>=<span class="hljs-string">&#x27;network&#x27;</span> device=<span class="hljs-string">&#x27;disk&#x27;</span>&gt;<br>        &lt;<span class="hljs-built_in">source</span> protocol=<span class="hljs-string">&#x27;rbd&#x27;</span> name=<span class="hljs-string">&#x27;rbd/kvmrbd&#x27;</span>&gt;<br>                &lt;host name=<span class="hljs-string">&#x27;192.168.19.104&#x27;</span> port=<span class="hljs-string">&#x27;6789&#x27;</span>/&gt;<br>        &lt;/source&gt;<br>        &lt;target dev=<span class="hljs-string">&#x27;vda&#x27;</span> bus=<span class="hljs-string">&#x27;virtio&#x27;</span>/&gt;<br>&lt;/disk&gt;<br></code></pre></td></tr></table></figure><p>手动添加iso</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab102 kvm]<span class="hljs-comment"># virsh domblklist centos7</span><br>[root@lab102 kvm]<span class="hljs-comment"># virt-xml centos7 --remove-device --disk target=hda</span><br>Domain <span class="hljs-string">&#x27;centos7&#x27;</span> defined successfully.<br>[root@lab102 kvm]<span class="hljs-comment"># virt-xml centos7  --add-device --disk /kvm/CentOS-7-x86_64-DVD-2009.iso,device=cdrom,target=hda</span><br>Domain <span class="hljs-string">&#x27;centos7&#x27;</span> defined successfully.<br></code></pre></td></tr></table></figure><p>修改启动项</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">&lt;os&gt;<br>  &lt;<span class="hljs-built_in">type</span> <span class="hljs-built_in">arch</span>=<span class="hljs-string">&#x27;x86_64&#x27;</span> machine=<span class="hljs-string">&#x27;pc-i440fx-rhel7.0.0&#x27;</span>&gt;hvm&lt;/type&gt;<br>  &lt;boot dev=<span class="hljs-string">&#x27;cdrom&#x27;</span>/&gt;<br>&lt;/os&gt;<br></code></pre></td></tr></table></figure><p>hd修改成上面的cdrom<br>改动后就可以通过iso来安装系统到ceph的rbd里面了，上面的过程全部是手动的过程</p>]]></content>
    
    
    <categories>
      
      <category>虚拟化</category>
      
    </categories>
    
    
    <tags>
      
      <tag>虚拟化</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>DNS在第四个为什么解析失败了</title>
    <link href="/2024/02/21/DNS%E5%9C%A8%E7%AC%AC%E5%9B%9B%E4%B8%AA%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A7%A3%E6%9E%90%E5%A4%B1%E8%B4%A5%E4%BA%86/"/>
    <url>/2024/02/21/DNS%E5%9C%A8%E7%AC%AC%E5%9B%9B%E4%B8%AA%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A7%A3%E6%9E%90%E5%A4%B1%E8%B4%A5%E4%BA%86/</url>
    
    <content type="html"><![CDATA[<h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>测试环境发现在linux的域名解析配置里面添加四个nameserver的时候，第四个dns没有生效</p><h2 id="模拟"><a href="#模拟" class="headerlink" title="模拟"></a>模拟</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@node102 ~]<span class="hljs-comment"># cat /etc/resolv.conf</span><br>options edns0 trust-ad<br><span class="hljs-comment"># Generated by NetworkManager</span><br>nameserver 192.168.5.28<br>nameserver 192.168.5.29<br>nameserver 192.168.5.30<br>nameserver 223.5.5.5<br></code></pre></td></tr></table></figure><p>指定三个无效的DNS，然后最后一个使用正常的nameserver<br>然后ping <a href="http://www.baidu.com这个是无法解析的/">www.baidu.com这个是无法解析的</a><br>然后改成</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">nameserver 192.168.5.28<br>nameserver 192.168.5.29<br><span class="hljs-comment">#nameserver 192.168.5.30</span><br>nameserver 223.5.5.5<br></code></pre></td></tr></table></figure><p>这样就能正常解析了，这个是为什么？<br>是因为linux只支持最多3个DNS的配置，那么放在第四个就不生效了</p><p><img src="/images/blog/dnsonly3.png"><br><img src="/images/blog/resolvehead.png"></p><p>代码写死了就是3个</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">RHEL 6/7 - Change MAXNS <span class="hljs-keyword">in</span> /usr/include/resolv.h to allow more than 3 nameservers.<br>RHEL 8/9 - Change MAXNS <span class="hljs-keyword">in</span> /usr/include/bits/types/res_state.h to allow more than 3 nameservers.<br>Recompile glibc (not supported; not recommended)<br></code></pre></td></tr></table></figure><p>如果有需求必须超过3个dns的话，那么就需要修改相关的值，然后编译基础库glibc（不推荐动基础库），<br>但是通常不需要这么做，因为dns 3个应该是能够满足的，如果不满足的话，也可以通过上层的dns进行指定上级指定处理即可</p>]]></content>
    
    
    <categories>
      
      <category>Linux知识</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>推送监控消息到钉钉群</title>
    <link href="/2024/01/08/%E6%8E%A8%E9%80%81%E7%9B%91%E6%8E%A7%E6%B6%88%E6%81%AF%E5%88%B0%E9%92%89%E9%92%89%E7%BE%A4/"/>
    <url>/2024/01/08/%E6%8E%A8%E9%80%81%E7%9B%91%E6%8E%A7%E6%B6%88%E6%81%AF%E5%88%B0%E9%92%89%E9%92%89%E7%BE%A4/</url>
    
    <content type="html"><![CDATA[<h2 id="需求"><a href="#需求" class="headerlink" title="需求"></a>需求</h2><p>实现的需求很简单，就是需要定期发送一个监控信息到钉钉群里面，避免需要人工登录机器查询信息，需要监控的机器本身无法上网，通过一台能上网的windows机器做中转</p><h2 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h2><p>1、被监控机器定期获取数据，并开启http服务<br>2、windows机器通过http请求定期获取被监控机器的数据<br>3、windows机器把消息发送出去</p><h2 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h2><h3 id="机器提供数据"><a href="#机器提供数据" class="headerlink" title="机器提供数据"></a>机器提供数据</h3><h4 id="开启http服务"><a href="#开启http服务" class="headerlink" title="开启http服务"></a>开启http服务</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">python -m http.server 4444<br></code></pre></td></tr></table></figure><h4 id="获取监控数据"><a href="#获取监控数据" class="headerlink" title="获取监控数据"></a>获取监控数据</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-meta">#! /bin/sh</span><br><span class="hljs-comment">#一分钟取一次 前端自定义获取时间</span><br><br><span class="hljs-keyword">for</span> a <span class="hljs-keyword">in</span> `<span class="hljs-built_in">seq</span> 2000`<br><span class="hljs-keyword">do</span><br><span class="hljs-built_in">date</span> &gt; ceph.info<br>ceph -s &gt;&gt; ceph.info<br><span class="hljs-built_in">sleep</span> 180<br><span class="hljs-keyword">done</span><br></code></pre></td></tr></table></figure><p>通过<code>http://ip:4444/ceph.info</code>获取数据</p><h3 id="推送的python脚本"><a href="#推送的python脚本" class="headerlink" title="推送的python脚本"></a>推送的python脚本</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># /srr/bin/python</span><br><span class="hljs-comment"># -*- coding:utf-8 -*-</span><br><br>import requests<br>import json<br>import time<br><br><span class="hljs-keyword">while</span> True:<br><br>    url=<span class="hljs-string">&quot;http://myip:4444/ceph.info&quot;</span><br>    r = requests.get(url)<br>    reslist=r.text<br>    <span class="hljs-built_in">print</span>(reslist)<br>    r.close()<br>    url = <span class="hljs-string">&#x27;https://oapi.dingtalk.com/robot/send?access_token=xxxxxxxxxxxxxxxxxxxxxxxxxx</span><br><span class="hljs-string">    headers = &#123;&#x27;</span>Content-Type<span class="hljs-string">&#x27;: &#x27;</span>application/json<span class="hljs-string">&#x27;&#125;</span><br><span class="hljs-string"></span><br><span class="hljs-string"></span><br><span class="hljs-string">    data = &#123; &#x27;</span>msgtype<span class="hljs-string">&#x27;: &#x27;</span>text<span class="hljs-string">&#x27;,   &#x27;</span>text<span class="hljs-string">&#x27;: &#123;   &#x27;</span>content<span class="hljs-string">&#x27;: &#x27;</span>状态(30分钟发送一次)：%s<span class="hljs-string">&#x27; %(reslist)  &#125; &#125;</span><br><span class="hljs-string">    response = requests.post(url, headers=headers, json=data)</span><br><span class="hljs-string"></span><br><span class="hljs-string">    print(response.json())</span><br><span class="hljs-string">    response.close()</span><br><span class="hljs-string">    time.sleep(1800)</span><br><span class="hljs-string">    ## 获取并推送数据</span><br></code></pre></td></tr></table></figure><p>上面的注意关闭下链接，否则在超过1小时的监控周期的时候，连接会中断,也可以在里面加入重试的操作</p><p>上面的操作之后，就可以实现消息到钉钉群的功能了</p>]]></content>
    
    
    <categories>
      
      <category>经验总结</category>
      
    </categories>
    
    
    <tags>
      
      <tag>监控</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>如何导出cnblog里面的md文件</title>
    <link href="/2024/01/06/%E5%A6%82%E4%BD%95%E5%AF%BC%E5%87%BAcnblog%E9%87%8C%E9%9D%A2%E7%9A%84md%E6%96%87%E4%BB%B6/"/>
    <url>/2024/01/06/%E5%A6%82%E4%BD%95%E5%AF%BC%E5%87%BAcnblog%E9%87%8C%E9%9D%A2%E7%9A%84md%E6%96%87%E4%BB%B6/</url>
    
    <content type="html"><![CDATA[<h2 id="下载文章"><a href="#下载文章" class="headerlink" title="下载文章"></a>下载文章</h2><p>下面的脚本是从网站下载md的文件，100篇一页，下载几次即可，修改下脚本</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#! /bin/python</span><br><span class="hljs-comment"># -*- coding:utf-8 -*-</span><br><br><span class="hljs-keyword">import</span> requests<br><span class="hljs-keyword">import</span> json<br><br><span class="hljs-comment"># 获取当前登录用户信息</span><br><span class="hljs-comment">#url = &#x27;https://api.cnblogs.com/api/users&#x27;</span><br><span class="hljs-comment"># 获取个人信息</span><br><span class="hljs-comment">#url = &#x27;https://api.cnblogs.com/api/blogs/zphj1987&#x27;</span><br><br><span class="hljs-comment"># 获取个人随笔列表</span><br>url=<span class="hljs-string">&quot;https://api.cnblogs.com/api/blogs/zphj1987/posts?pageSize=100&amp;pageIndex=1&quot;</span><br>headers = &#123;<span class="hljs-string">&quot;Authorization&quot;</span>:<span class="hljs-string">&quot;Bearer &quot;</span>+<span class="hljs-string">&quot;------token cnblog获取&quot;</span>&#125;<br>r = requests.get(url, headers=headers)<br>reslist=r.text<br>reslist=json.loads(reslist)<br><span class="hljs-built_in">print</span>(reslist)<br><span class="hljs-built_in">print</span>(json.dumps(reslist,ensure_ascii=<span class="hljs-literal">False</span>))<br><span class="hljs-keyword">for</span> val <span class="hljs-keyword">in</span> reslist:<br>    newval=json.dumps(val,ensure_ascii=<span class="hljs-literal">False</span>)<br>    <span class="hljs-built_in">print</span>(newval)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;获取的url&quot;</span>)<br>    <span class="hljs-built_in">print</span>(val[<span class="hljs-string">&quot;Url&quot;</span>])<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;文章标题&quot;</span>)<br>    <span class="hljs-built_in">print</span>(val[<span class="hljs-string">&quot;Title&quot;</span>])<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;获取发布的时间&quot;</span>)<br>    <span class="hljs-built_in">print</span>(val[<span class="hljs-string">&quot;PostDate&quot;</span>].replace(<span class="hljs-string">&quot;T&quot;</span>,<span class="hljs-string">&quot; &quot;</span>))<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;获取markdown的地址&quot;</span>)<br>    mdurl=val[<span class="hljs-string">&quot;Url&quot;</span>].replace(<span class="hljs-string">&quot;.html&quot;</span>,<span class="hljs-string">&quot;.md&quot;</span> )<br>    <span class="hljs-built_in">print</span>(mdurl)<br>    headers = &#123;<br>       <span class="hljs-string">&quot;User-Agent&quot;</span>: <span class="hljs-string">&quot;Apifox/1.0.0 (https://apifox.com)&quot;</span>,<br>        <span class="hljs-string">&quot;Authorization&quot;</span>:<span class="hljs-string">&quot;Bearer ------------------token cnblog网站获取----------------&quot;</span>,<br>        <span class="hljs-string">&quot;Accept&quot;</span>: <span class="hljs-string">&quot;*/*&quot;</span>,<br>        <span class="hljs-string">&quot;Host&quot;</span>: <span class="hljs-string">&quot;www.cnblogs.com&quot;</span>,<br>        <span class="hljs-string">&quot;Connection&quot;</span>: <span class="hljs-string">&quot;keep-alive&quot;</span>,<br>    &#125;<br>    content = requests.get(mdurl, headers=headers).content<br>    <span class="hljs-built_in">print</span>(content)<br>    <span class="hljs-built_in">print</span>(val[<span class="hljs-string">&quot;Title&quot;</span>])<br>    <span class="hljs-built_in">print</span>(val[<span class="hljs-string">&quot;PostDate&quot;</span>].replace(<span class="hljs-string">&quot;T&quot;</span>,<span class="hljs-string">&quot; &quot;</span>))<br>    head_content= <span class="hljs-string">&quot;&quot;&quot;---</span><br><span class="hljs-string">title: %s</span><br><span class="hljs-string">date: %s</span><br><span class="hljs-string">tags: &quot;暂未分类&quot;</span><br><span class="hljs-string">categories: &quot;暂未分类&quot;</span><br><span class="hljs-string">---</span><br><span class="hljs-string">&quot;&quot;&quot;</span>%(val[<span class="hljs-string">&quot;Title&quot;</span>].encode(<span class="hljs-string">&#x27;utf-8&#x27;</span>),val[<span class="hljs-string">&quot;PostDate&quot;</span>].replace(<span class="hljs-string">&quot;T&quot;</span>,<span class="hljs-string">&quot; &quot;</span>).encode(<span class="hljs-string">&#x27;utf-8&#x27;</span>))<br>    <span class="hljs-built_in">print</span>(head_content)<br>    all_content=head_content + content<br>    <span class="hljs-built_in">print</span>(all_content)<br>    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;output/%s.md&#x27;</span> %(val[<span class="hljs-string">&quot;Title&quot;</span>]),<span class="hljs-string">&#x27;wb&#x27;</span>) <span class="hljs-keyword">as</span> file:<br>        file.write(all_content+ <span class="hljs-string">&#x27;\n&#x27;</span>)<br></code></pre></td></tr></table></figure><p>下载完成后就得到了全部的md文件，这个里面的img引用的还是cnblog的资源地址，我们需要下载相关的资源，然后替换blog内的引用地址</p><h2 id="获取资源的文件列表"><a href="#获取资源的文件列表" class="headerlink" title="获取资源的文件列表"></a>获取资源的文件列表</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">grep <span class="hljs-string">&quot;cnblogs&quot;</span> -R *.md |grep -v html &gt; getlist.txt<br></code></pre></td></tr></table></figure><h2 id="下载资源并修改资源引用"><a href="#下载资源并修改资源引用" class="headerlink" title="下载资源并修改资源引用"></a>下载资源并修改资源引用</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-meta">#! /bin/bash</span><br><br><span class="hljs-comment"># 请将文件路径替换为实际的文件路径</span><br><span class="hljs-comment"># grep &quot;cnblogs&quot; -R *.md  &gt; getlist.txt</span><br>file_path=<span class="hljs-string">&quot;getlist.txt&quot;</span><br><br><span class="hljs-comment"># 使用while循环和read命令逐行读取文件</span><br><span class="hljs-keyword">while</span> IFS= <span class="hljs-built_in">read</span> -r line; <span class="hljs-keyword">do</span><br>  <span class="hljs-comment"># 在这里可以对每一行进行处理，例如打印或进行其他操作</span><br>  filename=`<span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;<span class="hljs-variable">$line</span>&quot;</span>|awk -F<span class="hljs-string">&#x27;:&#x27;</span>  <span class="hljs-string">&#x27;&#123;print $1&#125;&#x27;</span>`<br>  <span class="hljs-comment">#echo $filename</span><br>  httpad=`<span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;<span class="hljs-variable">$line</span>&quot;</span>|awk -F<span class="hljs-string">&#x27;:&#x27;</span> <span class="hljs-string">&#x27;&#123; for (i=2; i&lt;=NF; i++) &#123; printf &quot;%s%s&quot;, $i, (i&lt;NF ? &quot;:&quot; : &quot;\n&quot;) &#125; &#125;&#x27;</span> `<br>  <span class="hljs-comment">#echo $httpad</span><br>  url=$(<span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;<span class="hljs-variable">$httpad</span>&quot;</span> | grep -oE <span class="hljs-string">&#x27;https://[^ )]+&#x27;</span>)<br>  <span class="hljs-comment">#echo $url</span><br>  <span class="hljs-comment">#wget -N -P ../images/blog/ $url</span><br>  imgname=`<span class="hljs-built_in">echo</span> <span class="hljs-variable">$url</span>|awk -F<span class="hljs-string">&#x27;/&#x27;</span> <span class="hljs-string">&#x27;&#123;print $NF&#125;&#x27;</span>`<br>  newpath=/images/blog/<span class="hljs-variable">$imgname</span><br>  <span class="hljs-comment">#echo $newpath</span><br>  <span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;sed -i &#x27;&#x27; &#x27;s|<span class="hljs-variable">$url</span>|<span class="hljs-variable">$newpath</span>|g&#x27;  \&quot;<span class="hljs-variable">$filename</span>\&quot;&quot;</span><br>  sed -i <span class="hljs-string">&#x27;&#x27;</span>  <span class="hljs-string">&#x27;s|$url|$newpath|g&#x27;</span>  <span class="hljs-string">&quot;<span class="hljs-variable">$filename</span>&quot;</span><br><br><span class="hljs-keyword">done</span> &lt; <span class="hljs-string">&quot;<span class="hljs-variable">$file_path</span>&quot;</span><br></code></pre></td></tr></table></figure><p>上面的注释掉了，有的时候可能文章名称特殊，无法完全执行，可以把打印的结果自己再手动执行下即可</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>经过上面的操作以后，整个博客就迁移出来了，比自己一篇篇处理要快很多,剩余的分类的就自己再处理下</p>]]></content>
    
    
    <categories>
      
      <category>经验总结</category>
      
    </categories>
    
    
    <tags>
      
      <tag>博客相关</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>nmon文件过大的处理方法</title>
    <link href="/2024/01/05/nmon%E6%96%87%E4%BB%B6%E8%BF%87%E5%A4%A7%E7%9A%84%E5%A4%84%E7%90%86%E6%96%B9%E6%B3%95/"/>
    <url>/2024/01/05/nmon%E6%96%87%E4%BB%B6%E8%BF%87%E5%A4%A7%E7%9A%84%E5%A4%84%E7%90%86%E6%96%B9%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<p>nmon的文件存在过大的问题</p><p>获取第一个ZZZZ之前的内容，大概在2000行之前</p><p>root@ruichi-Default-string:&#x2F;chia# head -n 2000 ruichi-Default-string_210428_2224.nmon |grep -n “ZZZZ”<br>1288:ZZZZ,T0001,22:24:43,28-APR-2021<br>1310:ZZZZ,T0002,22:24:44,28-APR-2021<br>1333:ZZZZ,T0003,22:24:45,28-APR-2021<br>1357:ZZZZ,T0004,22:24:46,28-APR-2021<br>1379:ZZZZ,T0005,22:24:47,28-APR-2021<br>1404:ZZZZ,T0006,22:24:48,28-APR-2021<br>1428:ZZZZ,T0007,22:24:49,28-APR-2021</p><p>可以看到应该在1288行开始，那么相当于头部文件为1287行<br>对原始文件分割</p><p>我采集的是43200次的数据，1秒一次<br>产生的文件是<br>root@ruichi-Default-string:&#x2F;chia# cat ruichi-Default-string_210428_2224.nmon |wc -l<br>1047124<br>104万行</p><p>先处理到希望监控的区间<br>root@ruichi-Default-string:&#x2F;chia# cat ruichi-Default-string_210428_2224.nmon |grep -n “04:32:00”<br>547259:ZZZZ,T21891,04:32:00,29-APR-2021<br>实际只用取这个值之上的数据<br>547259<br>取547258<br>root@ruichi-Default-string:&#x2F;chia# ll ruichi-qujianzhi.nmon  -hl<br>-rw-r–r– 1 root root 31M 4月  29 14:06 ruichi-qujianzhi.nmon</p><p>文件大小为31M<br>我们先分为2个文件，看是否满足<br>先获取head相关的</p><p>head -n 1287 ruichi-qujianzhi.nmon &gt; ruichi-head.nmon<br>547258-1287   &#x2F;2&#x3D;272985<br> 272985+1287  274272 第一段<br>274253:ZZZZ,T10969,01:28:48,29-APR-2021<br>root@ruichi-Default-string:&#x2F;chia# cat ruichi-qujianzhi.nmon |head -n 274272|grep -n “ZZZZ”<br>我们取<br>1-274252<br>274252-547258<br>第二段是取好后，需要加上head</p><p>第一段取的命令<br>cat ruichi-qujianzhi.nmon |head -n 274252<br>第二段取的命令<br>cat ruichi-qujianzhi.nmon |tail -n +274253</p><p>root@ruichi-Default-string:&#x2F;chia# cp ruichi-head.nmon ruichi-2.nmon<br>root@ruichi-Default-string:&#x2F;chia# cat ruichi-qujianzhi.nmon |tail -n +274253 &gt;&gt; ruichi-2.nmon</p><p>可以看到大概27万行数据是没问题的</p><p>但是，从监控来看，数据并不太好看，还是转换下，我们之前的分析系统查看下</p>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>openeuler不识别cmake_build</title>
    <link href="/2023/06/06/openeuler%E4%B8%8D%E8%AF%86%E5%88%ABcmake-build/"/>
    <url>/2023/06/06/openeuler%E4%B8%8D%E8%AF%86%E5%88%ABcmake-build/</url>
    
    <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>在openeuler上面打包rpm的时候，有的src.rpm内部调用了%cmake_build的宏变量，openeuler的默认cmake3.22版本不支持，可以通过修改宏变量来实现可用</p><h2 id="修改"><a href="#修改" class="headerlink" title="修改"></a>修改</h2><p>修改文件路径为: &#x2F;usr&#x2F;lib&#x2F;rpm&#x2F;macros.d&#x2F;macros.cmake</p><span id="more"></span><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@openeuler ~]<span class="hljs-comment"># cat /usr/lib/rpm/macros.d/macros.cmake</span><br><span class="hljs-comment">#</span><br><span class="hljs-comment"># Macros for cmake</span><br><span class="hljs-comment">#</span><br>%_cmake_lib_suffix64 -DLIB_SUFFIX=64<br>%_cmake_skip_rpath -DCMAKE_SKIP_RPATH:BOOL=ON<br>%_cmake_version 3.22.0<br>%__cmake /usr/bin/cmake<br>%__ctest /usr/bin/ctest<br>%__cmake_builddir %&#123;!?__cmake_in_source_build:%&#123;_vpath_builddir&#125;&#125;%&#123;?__cmake_in_source_build:.&#125;<br><br><br><span class="hljs-comment"># - Set default compile flags</span><br><span class="hljs-comment"># - CMAKE_*_FLAGS_RELEASE are added *after* the *FLAGS environment variables</span><br><span class="hljs-comment"># and default to -O3 -DNDEBUG.  Strip the -O3 so we can override with *FLAGS</span><br><span class="hljs-comment"># - Turn on verbose makefiles so we can see and verify compile flags</span><br><span class="hljs-comment"># - Set default install prefixes and library install directories</span><br><span class="hljs-comment"># - Turn on shared libraries by default</span><br>%cmake \<br>  CFLAGS=<span class="hljs-string">&quot;<span class="hljs-variable">$&#123;CFLAGS:-%optflags&#125;</span>&quot;</span> ; <span class="hljs-built_in">export</span> CFLAGS ; \<br>  CXXFLAGS=<span class="hljs-string">&quot;<span class="hljs-variable">$&#123;CXXFLAGS:-%optflags&#125;</span>&quot;</span> ; <span class="hljs-built_in">export</span> CXXFLAGS ; \<br>  FFLAGS=<span class="hljs-string">&quot;<span class="hljs-variable">$&#123;FFLAGS:-%optflags%&#123;?_fmoddir: -I%_fmoddir&#125;</span>&#125;&quot;</span> ; <span class="hljs-built_in">export</span> FFLAGS ; \<br>  FCFLAGS=<span class="hljs-string">&quot;<span class="hljs-variable">$&#123;FCFLAGS:-%optflags%&#123;?_fmoddir: -I%_fmoddir&#125;</span>&#125;&quot;</span> ; <span class="hljs-built_in">export</span> FCFLAGS ; \<br>  %&#123;?__global_ldflags:LDFLAGS=<span class="hljs-string">&quot;<span class="hljs-variable">$&#123;LDFLAGS:-%__global_ldflags&#125;</span>&quot;</span> ; <span class="hljs-built_in">export</span> LDFLAGS ;&#125; \<br>  %__cmake \\\<br>        %&#123;!?__cmake_in_source_build:-S <span class="hljs-string">&quot;%&#123;_vpath_srcdir&#125;&quot;</span>&#125; \\\<br>        %&#123;!?__cmake_in_source_build:-B <span class="hljs-string">&quot;%&#123;__cmake_builddir&#125;&quot;</span>&#125; \\\<br>        -DCMAKE_C_FLAGS_RELEASE:STRING=<span class="hljs-string">&quot;-DNDEBUG&quot;</span> \\\<br>        -DCMAKE_CXX_FLAGS_RELEASE:STRING=<span class="hljs-string">&quot;-DNDEBUG&quot;</span> \\\<br>        -DCMAKE_Fortran_FLAGS_RELEASE:STRING=<span class="hljs-string">&quot;-DNDEBUG&quot;</span> \\\<br>        -DCMAKE_VERBOSE_MAKEFILE:BOOL=ON \\\<br>        -DCMAKE_INSTALL_PREFIX:PATH=%&#123;_prefix&#125; \\\<br>        -DINCLUDE_INSTALL_DIR:PATH=%&#123;_includedir&#125; \\\<br>        -DLIB_INSTALL_DIR:PATH=%&#123;_libdir&#125; \\\<br>        -DSYSCONF_INSTALL_DIR:PATH=%&#123;_sysconfdir&#125; \\\<br>        -DSHARE_INSTALL_PREFIX:PATH=%&#123;_datadir&#125; \\\<br>%<span class="hljs-keyword">if</span> <span class="hljs-string">&quot;%&#123;?_lib&#125;&quot;</span> == <span class="hljs-string">&quot;lib64&quot;</span> \<br>        %&#123;?_cmake_lib_suffix64&#125; \\\<br>%endif \<br>        -DBUILD_SHARED_LIBS:BOOL=ON<br><br>%cmake_build \<br>  %__cmake --build <span class="hljs-string">&quot;%&#123;__cmake_builddir&#125;&quot;</span> %&#123;?_smp_mflags&#125; --verbose<br><br>%cmake_install \<br>  DESTDIR=<span class="hljs-string">&quot;%&#123;buildroot&#125;&quot;</span> %__cmake --install <span class="hljs-string">&quot;%&#123;__cmake_builddir&#125;&quot;</span><br><br>%ctest(:-:) \<br><span class="hljs-built_in">cd</span> <span class="hljs-string">&quot;%&#123;__cmake_builddir&#125;&quot;</span> \<br>%__ctest --output-on-failure --force-new-ctest-process %&#123;?_smp_mflags&#125; %&#123;**&#125; \<br><span class="hljs-built_in">cd</span> -<br><br>%cmake3 %cmake<br>%cmake3_build %cmake_build<br>%cmake3_install %cmake_install<br>%ctest3(:-:) %ctest %&#123;**&#125;<br></code></pre></td></tr></table></figure><p>通过上面的修改后，即可适配高版本的编译</p>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>比较两个系统安装的rpm</title>
    <link href="/2023/06/05/%E6%AF%94%E8%BE%83%E4%B8%A4%E4%B8%AA%E7%B3%BB%E7%BB%9F%E5%AE%89%E8%A3%85%E7%9A%84rpm/"/>
    <url>/2023/06/05/%E6%AF%94%E8%BE%83%E4%B8%A4%E4%B8%AA%E7%B3%BB%E7%BB%9F%E5%AE%89%E8%A3%85%E7%9A%84rpm/</url>
    
    <content type="html"><![CDATA[<h2 id="需求"><a href="#需求" class="headerlink" title="需求"></a>需求</h2><p>有的时候需要做系统适配的时候，远端的系统是经过定制的系统，本地的安装是按正常流程进行的安装，需要在远端进行安装包的离线安装，那么本地就需要提前做好一模一样的系统，本篇的脚本就是用于比较这个版本的区别，然后在本地做好一样的系统，然后再做适配</p><span id="more"></span><h2 id="脚本"><a href="#脚本" class="headerlink" title="脚本"></a>脚本</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><code class="hljs python">[root@build syncos]<span class="hljs-comment"># cat check.py</span><br><span class="hljs-comment">#! /bin/python3</span><br><span class="hljs-comment"># -*- coding:utf-8 -*-</span><br><br><span class="hljs-keyword">import</span> sys<br><span class="hljs-keyword">import</span> re<br><span class="hljs-comment">#print(sys.argv[1])</span><br><span class="hljs-comment">#print(sys.argv[2])</span><br>qian=&#123;&#125;<br>hou=&#123;&#125;<br><br>f = <span class="hljs-built_in">open</span>(sys.argv[<span class="hljs-number">1</span>])<br>line = f.readline()<br><span class="hljs-keyword">while</span> line:<br>    line=line.strip(<span class="hljs-string">&#x27;\n&#x27;</span>)<br>    new_rpm = re.sub(<span class="hljs-string">r&#x27;-\d+\.&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>, line)<br>    new_rpm = new_rpm.split(<span class="hljs-string">&quot;.&quot;</span>)[<span class="hljs-number">0</span>]<br>    qian[new_rpm]=line<br>    line = f.readline()<br>f.close()<br><br>f = <span class="hljs-built_in">open</span>(sys.argv[<span class="hljs-number">2</span>])<br>line = f.readline()<br><span class="hljs-keyword">while</span> line:<br>    line=line.strip(<span class="hljs-string">&#x27;\n&#x27;</span>)<br>    new_rpm = re.sub(<span class="hljs-string">r&#x27;-\d+\.&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>, line)<br>    new_rpm = new_rpm.split(<span class="hljs-string">&quot;.&quot;</span>)[<span class="hljs-number">0</span>]<br>    hou[new_rpm]=line<br>    line = f.readline()<br>f.close()<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;===检查本地有远程没有的情况==需要本地卸载==&quot;</span>)<br><span class="hljs-keyword">for</span> key,val <span class="hljs-keyword">in</span> qian.items():<br>    <span class="hljs-keyword">if</span> key  <span class="hljs-keyword">in</span> hou:<br>        <span class="hljs-keyword">if</span> qian[key].strip(<span class="hljs-string">&#x27; &#x27;</span>) == hou[key].strip(<span class="hljs-string">&#x27; &#x27;</span>):<br>            <span class="hljs-keyword">pass</span><br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-built_in">print</span>(qian[key]+<span class="hljs-string">&quot;  &quot;</span>+hou[key])<br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;yum remove -y %s&quot;</span> %(key))<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;===检查远程有本地没有的情况=需要本地安装===&quot;</span>)<br><br><span class="hljs-keyword">for</span> key,val <span class="hljs-keyword">in</span> hou.items():<br>    <span class="hljs-keyword">if</span> key  <span class="hljs-keyword">in</span> qian:<br>        <span class="hljs-keyword">if</span> hou[key].strip(<span class="hljs-string">&#x27; &#x27;</span>) == qian[key].strip(<span class="hljs-string">&#x27; &#x27;</span>):<br>            <span class="hljs-keyword">pass</span><br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-built_in">print</span>(qian[key]+<span class="hljs-string">&quot;  &quot;</span>+hou[key])<br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;rpm -ivh %s&quot;</span> %(key))<br></code></pre></td></tr></table></figure><p>按上面的执行后，就可以比较远程和本地的包的区别，保持一致性，后面的为目的的系统 </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">rpm -qa &gt; rpm.list<br></code></pre></td></tr></table></figure><h2 id="运行方法"><a href="#运行方法" class="headerlink" title="运行方法"></a>运行方法</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">python3 check.py local.list remote.list<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>生活日常</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>openai的反向代理设置</title>
    <link href="/2023/04/12/chatgpt%E7%9A%84%E5%8F%8D%E5%90%91%E4%BB%A3%E7%90%86%E8%AE%BE%E7%BD%AE/"/>
    <url>/2023/04/12/chatgpt%E7%9A%84%E5%8F%8D%E5%90%91%E4%BB%A3%E7%90%86%E8%AE%BE%E7%BD%AE/</url>
    
    <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>chatgpt屏蔽了很多地方的访问，使用起来很麻烦，现在的需求就是想使用本地网络就可以使用这个，那么可以通过nginx作代理进行访问</p><h2 id="网络架构"><a href="#网络架构" class="headerlink" title="网络架构"></a>网络架构</h2><p>本身的请求是直接发给openai的，这里我们使用一台服务器运行nginx，作为反向代理发送请求到api.openai.com</p><span id="more"></span><h3 id="nginx配置文件"><a href="#nginx配置文件" class="headerlink" title="nginx配置文件"></a>nginx配置文件</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs bash">server &#123;<br>    listen       7480 default_server;<br>    server_name  _;<br>    root         /usr/share/nginx/html;<br><br>    <span class="hljs-comment"># Load configuration files for the default server block.</span><br>    include /etc/nginx/default.d/*.conf;<br><br>    location / &#123;<br>    proxy_pass  https://api.openai.com/;<br>    proxy_set_header Host api.openai.com;<br>    proxy_set_header Connection <span class="hljs-string">&#x27;&#x27;</span>;<br>    proxy_http_version 1.1;<br>    chunked_transfer_encoding off;<br>    proxy_buffering off;<br>    proxy_cache off;<br>    proxy_set_header X-Forwarded-For <span class="hljs-variable">$remote_addr</span>;<br>    proxy_set_header X-Forwarded-Proto <span class="hljs-variable">$scheme</span>;<br>  proxy_ssl_server_name on;<br>  proxy_ssl_session_reuse off;<br>  proxy_ssl_protocols TLSv1.2;<br>    &#125;<br><br>    error_page 404 /404.html;<br>        location = /40x.html &#123;<br>    &#125;<br><br>    error_page 500 502 503 504 /50x.html;<br>        location = /50x.html &#123;<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure><p>这个开的是80端口，可以自己设置</p><h3 id="设置api-base"><a href="#设置api-base" class="headerlink" title="设置api_base"></a>设置api_base</h3><p>在python脚本里面设置下</p><blockquote><p>openai.api_base&#x3D;”<a href="http://myserver.com/v1">http://myserver.com/v1</a>“</p></blockquote><p>设置好这个就可以使用了</p><p>请求是通过这个api地址转发的，如果有更多的地址进行设置即可</p><h2 id="部署内网版本"><a href="#部署内网版本" class="headerlink" title="部署内网版本"></a>部署内网版本</h2><p>搭建环境</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">bash &lt;(curl -s https://raw.githubusercontent.com/Yidadaa/ChatGPT-Next-Web/main/scripts/setup.sh)<br></code></pre></td></tr></table></figure><p>启动参数</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">OPENAI_API_KEY=sk-xxxx  CODE=passwd PROTOCOL=http BASE_URL=myid:7480  yarn start<br></code></pre></td></tr></table></figure><p>然后就可以内网使用了</p>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>chatgpt</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>基于openai做的语音识别</title>
    <link href="/2023/04/12/%E5%9F%BA%E4%BA%8Eopenai%E5%81%9A%E7%9A%84%E8%AF%AD%E9%9F%B3%E8%AF%86%E5%88%AB/"/>
    <url>/2023/04/12/%E5%9F%BA%E4%BA%8Eopenai%E5%81%9A%E7%9A%84%E8%AF%AD%E9%9F%B3%E8%AF%86%E5%88%AB/</url>
    
    <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>开会通过录屏的方式把会议的视频和语音录下来了，会后想对会议内容进行一个提取，或者是通过录音的方式记录了一段对话，需要对对话过程做个提取<br>钉钉会议提供了会议记录的一些功能，但是也有可能忘了开启，或者是其它场景下的录音</p><h2 id="需求"><a href="#需求" class="headerlink" title="需求"></a>需求</h2><p>就是对语音进行文字的转换的操作，这个在国内的一些云厂商都提供了这个，但是费用和开通过程都有点贵，最近正好使用了openai,通过这个处理的费用还好，本篇就是记录这个处理过程的</p><h2 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h2><h3 id="mp4提取mp3"><a href="#mp4提取mp3" class="headerlink" title="mp4提取mp3"></a>mp4提取mp3</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">ffmpeg -i video.mp4 -f mp3 -vn myoutput.mp3<br></code></pre></td></tr></table></figure><p>这一步就是通过使用ffmpeg对视频内的音频进行提取的操作</p><span id="more"></span><h3 id="对mp3进行分段"><a href="#对mp3进行分段" class="headerlink" title="对mp3进行分段"></a>对mp3进行分段</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">ffmpeg -i myoutput.mp3 -f segment -segment_time 600 -c copy newoutput%3d.mp3<br></code></pre></td></tr></table></figure><p>这一步是把语音文件拆分成多个文件，因为做语音转换的时候，单个文件过大的话，需要特殊处理，单个文件官方是推荐25MB以下</p><h3 id="对文件进行处理"><a href="#对文件进行处理" class="headerlink" title="对文件进行处理"></a>对文件进行处理</h3><p>安装python的gpt</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">yum install python3.9 -y<br>pip3.9 install openai<br></code></pre></td></tr></table></figure><p>处理的脚本</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-meta">#! /bint/python</span><br><span class="hljs-comment"># -*- coding: UTF-8 -*-</span><br>import os<br>import openai<br>import sys<br><br>openai.organization = <span class="hljs-string">&quot;org-xxxxxxxxxxxxxxxxE&quot;</span><br>openai.api_key = <span class="hljs-string">&quot;sk-xxxxxxxxxxxxxxxxxxxxxxxxxx&quot;</span><br><br>mp3file=sys.argv[1]<br><br>audio_file= open(<span class="hljs-string">&quot;%s&quot;</span> %(mp3file), <span class="hljs-string">&quot;rb&quot;</span>)<br>transcript = openai.Audio.transcribe(<span class="hljs-string">&quot;whisper-1&quot;</span>, audio_file)<br><br><span class="hljs-comment">#print(transcript)</span><br><br>mystr=str(transcript[<span class="hljs-string">&quot;text&quot;</span>])<br><br><span class="hljs-built_in">print</span>(mystr)<br></code></pre></td></tr></table></figure><p>执行</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">python3.9 translate.py jixiao000.mp3 &gt; jixiao000.txt<br></code></pre></td></tr></table></figure><h2 id="更多控制的方法"><a href="#更多控制的方法" class="headerlink" title="更多控制的方法"></a>更多控制的方法</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">transcript = openai.Audio.transcribe(<span class="hljs-string">&quot;whisper-1&quot;</span>, audio_file,prompt=<span class="hljs-string">&quot;简体中文&quot;</span>,response_format=<span class="hljs-string">&quot;text&quot;</span>)<br></code></pre></td></tr></table></figure><p>这个地方通过可变参数的方式接受请求的</p><blockquote><p>&#x2F;usr&#x2F;local&#x2F;lib&#x2F;python3.9&#x2F;site-packages&#x2F;openai&#x2F;api_resources&#x2F;audio.py</p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs bash">def transcribe(<br>    cls,<br>    model,<br>    file,<br>    api_key=None,<br>    api_base=None,<br>    api_type=None,<br>    api_version=None,<br>    organization=None,<br>    **params,<br>):<br></code></pre></td></tr></table></figure><p>上面可以控制返回的是json还是text的，也可以其它格式，以及告诉openai，我们需要的是简体中文的，这个地方不指定的话，有时候会返回的繁体字</p><h2 id="curl方式"><a href="#curl方式" class="headerlink" title="curl方式"></a>curl方式</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">curl --request POST --url https://api.openai.com/v1/audio/transcriptions --header <span class="hljs-string">&#x27;Authorization: Bearer sk-xxxxxxxxxxxx&#x27;</span> --header <span class="hljs-string">&#x27;Content-Type: multipart/form-data&#x27;</span> --<br>form file=@jixiaoxxxx.mp3 --form model=whisper-1 --form prompt=<span class="hljs-string">&quot;简体中文&quot;</span> --form response_format=<span class="hljs-string">&quot;text&quot;</span><br></code></pre></td></tr></table></figure><p>通过上面的命令行可以直接返回文本</p><h2 id="后续"><a href="#后续" class="headerlink" title="后续"></a>后续</h2><p>python方式的已经弄清楚控制参数，通过curl方式的目前语法也都清楚，比如希望返回的都是简体中文，这个需要给一个prompt过去，命令行的可以给</p>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>openai</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>希捷硬盘升级固件方法</title>
    <link href="/2023/02/24/%E5%B8%8C%E6%8D%B7%E7%A1%AC%E7%9B%98%E5%8D%87%E7%BA%A7%E5%9B%BA%E4%BB%B6%E6%96%B9%E6%B3%95/"/>
    <url>/2023/02/24/%E5%B8%8C%E6%8D%B7%E7%A1%AC%E7%9B%98%E5%8D%87%E7%BA%A7%E5%9B%BA%E4%BB%B6%E6%96%B9%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<h2 id="固件升级"><a href="#固件升级" class="headerlink" title="固件升级"></a>固件升级</h2><p>一般情况下不需要升级，但是在测试磁盘能源管理的时候，发现部分硬盘无法休眠，通过固件升级后得到解决</p><h2 id="升级步骤"><a href="#升级步骤" class="headerlink" title="升级步骤"></a>升级步骤</h2><h3 id="查询硬盘型号"><a href="#查询硬盘型号" class="headerlink" title="查询硬盘型号"></a>查询硬盘型号</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@node17 ~]<span class="hljs-comment"># smartctl -i /dev/sdb</span><br>smartctl 6.5 2016-05-07 r4318 [x86_64-linux-4.14.113-1.el7.x86_64] (<span class="hljs-built_in">local</span> build)<br>Copyright (C) 2002-16, Bruce Allen, Christian Franke, www.smartmontools.org<br><br>=== START OF INFORMATION SECTION ===<br>Device Model:     ST4000NM0035-1V4107<br>Serial Number:    ZC13ZGZP<br>LU WWN Device Id: 5 000c50 0a53981ae<br>Firmware Version: TN04<br>User Capacity:    4,000,787,030,016 bytes [4.00 TB]<br>Sector Size:      512 bytes logical/physical<br>Rotation Rate:    7200 rpm<br>Form Factor:      3.5 inches<br>Device is:        Not <span class="hljs-keyword">in</span> smartctl database [<span class="hljs-keyword">for</span> details use: -P showall]<br>ATA Version is:   ACS-3 T13/2161-D revision 5<br>SATA Version is:  SATA 3.1, 6.0 Gb/s (current: 6.0 Gb/s)<br>Local Time is:    Thu Feb 23 18:15:04 2023 CST<br>SMART support is: Available - device has SMART capability.<br>SMART support is: Enabled<br></code></pre></td></tr></table></figure><p>通过smartctl查询硬盘的序列号，也就是上面的ZC13ZGZP</p><span id="more"></span><h3 id="下载固件"><a href="#下载固件" class="headerlink" title="下载固件"></a>下载固件</h3><p>进入官方的固件查找网址</p><blockquote><p><a href="https://apps1.seagate.com/downloads/request.html">https://apps1.seagate.com/downloads/request.html</a></p></blockquote><p><img src="/images/blog/Pasted%20image%2020230224100202.png"><br>输入序列号<br><img src="/images/Pasted%20image%2020230224100245.png"></p><p>点击下载固件即可，刷固件的工具和固件都在这个zip包里面</p><h3 id="刷写固件"><a href="#刷写固件" class="headerlink" title="刷写固件"></a>刷写固件</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@node102 sn07]<span class="hljs-comment"># ./command\ line\ tools/SeaChest/SeaChest_Firmware_232_1132_64 -d /dev/sdc  --downloadFW firmware/ConstellationES3-Megalodon-SATA-StdOEM-SN07.lod </span><br>===============================================================================<br> SeaChest_Firmware - Seagate drive utilities<br> Copyright (c) 2017 Seagate Technology LLC, All Rights Reserved<br> SeaChest_Firmware Version: 2.3.2-1_13_2 X86_64<br> Build Date: Mar  9 2017<br> Today: Fri Feb 24 10:35:58 2023<br>===============================================================================<br><br>sg2 - ST4000NM0033-9ZM170 - Z1Z7VEPH - ATA<br>....<br>Firmware Download successful<br>Firmware Download time (s): 21.87<br>Average time/segment  (ms): 397.69<br>Activate Time          (s): 10.30<br>New firmware version is SN07<br><br></code></pre></td></tr></table></figure><h3 id="检查固件版本"><a href="#检查固件版本" class="headerlink" title="检查固件版本"></a>检查固件版本</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@node102 sn07]<span class="hljs-comment"># smartctl -i /dev/sdc</span><br>smartctl 7.0 2018-12-30 r4883 [x86_64-linux-4.14.113-1.el7.x86_64] (<span class="hljs-built_in">local</span> build)<br>Copyright (C) 2002-18, Bruce Allen, Christian Franke, www.smartmontools.org<br><br>=== START OF INFORMATION SECTION ===<br>Model Family:     Seagate Constellation ES.3<br>Device Model:     ST4000NM0033-9ZM170<br>Serial Number:    Z1Z7VEPH<br>LU WWN Device Id: 5 000c50 07a810918<br>Firmware Version: SN07<br>User Capacity:    4,000,787,030,016 bytes [4.00 TB]<br>Sector Size:      512 bytes logical/physical<br>Rotation Rate:    7200 rpm<br>Form Factor:      3.5 inches<br>Device is:        In smartctl database [<span class="hljs-keyword">for</span> details use: -P show]<br>ATA Version is:   ACS-2 (minor revision not indicated)<br>SATA Version is:  SATA 3.0, 6.0 Gb/s (current: 6.0 Gb/s)<br>Local Time is:    Fri Feb 24 10:36:57 2023 CST<br>SMART support is: Available - device has SMART capability.<br>SMART support is: Enabled<br></code></pre></td></tr></table></figure><p>刷完固件磁盘可以顺利进入休眠了</p><h2 id="备注"><a href="#备注" class="headerlink" title="备注"></a>备注</h2><p>官网下载比较慢，可以通过这个hddguru网站下载会比较快一点<br><img src="/images/Pasted%20image%2020230224111719.png"></p>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>influxdb常用命令</title>
    <link href="/2023/02/09/influxdb%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"/>
    <url>/2023/02/09/influxdb%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>之前在研究lustreperfmon，发现软件采用的是collectd加上influxdb的软件架构，使用过程中涉及到一些命令记录下来，方便后续使用</p><h2 id="相关命令"><a href="#相关命令" class="headerlink" title="相关命令"></a>相关命令</h2><p>查看数据库</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@test201 influxdb]<span class="hljs-comment"># influx  --execute &quot;show databases;&quot;</span><br>name: databases<br>name<br>----<br>_internal<br></code></pre></td></tr></table></figure><span id="more"></span><p>创建数据库</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@test201 influxdb]<span class="hljs-comment"># influx  --execute &#x27;CREATE DATABASE esmon_database&#x27;</span><br></code></pre></td></tr></table></figure><p>删除数据库</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@test201 influxdb]<span class="hljs-comment"># influx  --execute &#x27;DROP DATABASE esmon_database&#x27;</span><br></code></pre></td></tr></table></figure><p>创建cq查询</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@test201 influxdb]<span class="hljs-comment"># influx -database &#x27;esmon_database&#x27;  --execute &#x27;CREATE CONTINUOUS QUERY cq_ost_stats_bytes_fs_name_optype ON esmon_database BEGIN SELECT sum(value) / 3 INTO esmon_database.autogen.&quot;cqm_ost_stats_bytes-fs_name-optype&quot; FROM esmon_database.autogen.ost_stats_bytes GROUP BY time(6s), fs_name, optype END&#x27;</span><br></code></pre></td></tr></table></figure><p>删除cq查询</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@test201 influxdb]<span class="hljs-comment"># influx -database &#x27;esmon_database&#x27;  --execute &quot;DROP CONTINUOUS QUERY cq_ost_stats_bytes_fs_name_optype ON &quot;esmon_database&quot;;&quot;</span><br></code></pre></td></tr></table></figure><p>查询创建的cq查询</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@test201 influxdb]<span class="hljs-comment"># influx -database &#x27;esmon_database&#x27;  --execute &quot;SHOW CONTINUOUS QUERIES;&quot;</span><br></code></pre></td></tr></table></figure><p>查看创建的measurement</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@test201 /]<span class="hljs-comment"># influx -database &#x27;esmon_database&#x27;  --execute &quot;show measurements&quot;</span><br></code></pre></td></tr></table></figure><p>删除measurement</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@test201 /]<span class="hljs-comment"># influx -database &#x27;esmon_database&#x27;  --execute &#x27;DROP measurement  &quot;cqm_ost_stats_bytes-fs_name-optype&quot;&#x27;;</span><br></code></pre></td></tr></table></figure><p>查询数据</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@test201 /]<span class="hljs-comment"># influx -database &#x27;esmon_database&#x27;  --execute &#x27;SELECT * FROM &quot;md_stats&quot;&#x27;</span><br>[root@test201 pyesmon]<span class="hljs-comment"># influx -database &#x27;esmon_database&#x27;  --execute &quot;SELECT &quot;sum&quot; FROM \&quot;cqm_md_stats-fs_name\&quot; WHERE fs_name = &#x27;lustrefs&#x27; AND  time &gt; now() - 1m&quot;</span><br>[root@test201 pyesmon]<span class="hljs-comment"># influx -database &#x27;esmon_database&#x27;  --execute &quot;SELECT &quot;sum&quot; FROM \&quot;cqm_ost_stats_bytes-fs_name-optype\&quot; WHERE fs_name=&#x27;lustrefs&#x27; AND  time &gt; now() - 1m GROUP BY &quot;optype&quot;&quot;</span><br></code></pre></td></tr></table></figure><p>如果查询内容有单引号也有双引号，那么外部包上双引号，内部保持单引号不变，然后对measurement进行双引号转义，命令行才需要这样处理</p><ul><li>group by是对查询可以分组</li><li>time  &gt; now() - 1m 是可以在时间上选择一个范围</li></ul>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>海康威视AnalyzeDatasdk的python调用方法</title>
    <link href="/2023/02/02/%E6%B5%B7%E5%BA%B7%E5%A8%81%E8%A7%86AnalyzeDatasdk%E7%9A%84python%E8%B0%83%E7%94%A8%E6%96%B9%E6%B3%95/"/>
    <url>/2023/02/02/%E6%B5%B7%E5%BA%B7%E5%A8%81%E8%A7%86AnalyzeDatasdk%E7%9A%84python%E8%B0%83%E7%94%A8%E6%96%B9%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>sdk是通过海康开放平台下载下来的，但是这个只提供了C的库，测试下用python调用的方法</p><h2 id="实践"><a href="#实践" class="headerlink" title="实践"></a>实践</h2><h3 id="获取sdk的版本号"><a href="#获取sdk的版本号" class="headerlink" title="获取sdk的版本号"></a>获取sdk的版本号</h3><p>获取版本号是最基本的调用方法，可以验证下python最小运行环境<br>使用的sdk版本</p><blockquote><p>AnalyzeData_4.3.2.9_CentOs_release_5.4_64bit_base_20180710.zip</p></blockquote><p>文件存放目录结构</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 AnalyzeData_4.3.2.9_CentOs_release_5.4_64bit_base_20180710]<span class="hljs-comment"># tree</span><br>.<br>├── hikanalyze.py<br>├── include<br>│   ├── AnalyzeDataDefine.h<br>│   └── AnalyzeDataNewInterface.h<br>└── libs<br>    └── libanalyzedata.so<br></code></pre></td></tr></table></figure><span id="more"></span><p>代码如下</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python">[root@lab101 AnalyzeData_4<span class="hljs-number">.3</span><span class="hljs-number">.2</span><span class="hljs-number">.9</span>_CentOs_release_5<span class="hljs-number">.4_64</span>bit_base_20180710]<span class="hljs-comment"># cat hikanalyze.py</span><br><span class="hljs-comment">#! /usr/bin/python</span><br><span class="hljs-comment"># -*- coding:utf-8 -*-</span><br><br><span class="hljs-keyword">from</span> ctypes <span class="hljs-keyword">import</span> *<br>dynamicLibString = <span class="hljs-string">&#x27;./libs/libanalyzedata.so&#x27;</span><br>mylib = cdll.LoadLibrary(dynamicLibString)<br><br>version=mylib.HIKANA_GetVersion()<br>baseline=(version &amp; <span class="hljs-number">0xc0000000</span>)&gt;&gt;<span class="hljs-number">30</span><br>year=(version &amp; <span class="hljs-number">0x3e000000</span>) &gt;&gt;<span class="hljs-number">25</span><br>month=(version &amp; <span class="hljs-number">0x01e00000</span>) &gt;&gt;<span class="hljs-number">21</span><br>day=(version &amp; <span class="hljs-number">0x1f0000</span>) &gt;&gt;<span class="hljs-number">16</span><br>major=(version &amp; <span class="hljs-number">0xf000</span> ) &gt;&gt;<span class="hljs-number">12</span><br>minor=(version &amp; <span class="hljs-number">0xf00</span>) &gt;&gt;<span class="hljs-number">8</span><br>modify=(version &amp; <span class="hljs-number">0xf0</span>) &gt;&gt;<span class="hljs-number">4</span><br>test=(version &amp; <span class="hljs-number">0xf</span>)<br><span class="hljs-built_in">print</span> <span class="hljs-string">&quot;sdk发布日期:&quot;</span>,<span class="hljs-built_in">str</span>(year+<span class="hljs-number">2000</span>)+<span class="hljs-string">&quot;-&quot;</span>+<span class="hljs-built_in">str</span>(month)+<span class="hljs-string">&quot;-&quot;</span>+<span class="hljs-built_in">str</span>(day)<br><span class="hljs-built_in">print</span> <span class="hljs-string">&quot;sdk发布版本:&quot;</span>,<span class="hljs-built_in">str</span>(major)+<span class="hljs-string">&quot;.&quot;</span>+<span class="hljs-built_in">str</span>(minor)+<span class="hljs-string">&quot;.&quot;</span>+<span class="hljs-built_in">str</span>(modify)+<span class="hljs-string">&quot;.&quot;</span>+<span class="hljs-built_in">str</span>(test)<br></code></pre></td></tr></table></figure><p>运行效果</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 AnalyzeData_4.3.2.9_CentOs_release_5.4_64bit_base_20180710]<span class="hljs-comment"># python hikanalyze.py</span><br>sdk发布日期: 2018-7-9<br>sdk发布版本: 4.3.2.9<br></code></pre></td></tr></table></figure><p>可以看到最小的运行环境运行成功了，so可以正常的调用，获取的方法官方提供的文档有说明字段<br><img src="/images/blog/media/202302/16753092763465.jpg"></p><p>发布日期距离现在有点时间了，不过这个基本的功能改动应该不是很大</p>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>s3的v2认证构造-lua</title>
    <link href="/2023/01/31/s3%E7%9A%84v2%E8%AE%A4%E8%AF%81%E6%9E%84%E9%80%A0-lua/"/>
    <url>/2023/01/31/s3%E7%9A%84v2%E8%AE%A4%E8%AF%81%E6%9E%84%E9%80%A0-lua/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>本篇内容实现了相关功能的硬编码的部分，还有细节未处理，验证了可行性</p><h2 id="相关代码记录"><a href="#相关代码记录" class="headerlink" title="相关代码记录"></a>相关代码记录</h2><figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br></pre></td><td class="code"><pre><code class="hljs lua">ngx.<span class="hljs-built_in">log</span>(ngx.INFO, <span class="hljs-string">&quot;文件路径为:&quot;</span>, ngx.var.uri)<br>ngx.<span class="hljs-built_in">log</span>(ngx.INFO, <span class="hljs-string">&quot;请求方法为:&quot;</span>, ngx.var.request_method)<br><span class="hljs-comment">--这个打印的是收到的header的内容</span><br>ngx.<span class="hljs-built_in">log</span>(ngx.INFO, <span class="hljs-string">&quot;请求header2为:&quot;</span>, ngx.req.raw_header(<span class="hljs-literal">true</span>))<br><br><span class="hljs-comment">--获取编码需要的信息</span><br>ngx.<span class="hljs-built_in">log</span>(ngx.INFO, <span class="hljs-string">&quot;请求的method&quot;</span>, ngx.var.request_method)<br>ngx.<span class="hljs-built_in">log</span>(ngx.INFO, <span class="hljs-string">&quot;请求的content-md5:&quot;</span>, ngx.req.get_headers()[<span class="hljs-string">&#x27;content-md5&#x27;</span>])<br>ngx.<span class="hljs-built_in">log</span>(ngx.INFO, <span class="hljs-string">&quot;请求的content-type:&quot;</span>, ngx.req.get_headers()[<span class="hljs-string">&#x27;content-type&#x27;</span>])<br>ngx.<span class="hljs-built_in">log</span>(ngx.INFO, <span class="hljs-string">&quot;请求的date:&quot;</span>, ngx.req.get_headers()[<span class="hljs-string">&#x27;date&#x27;</span>])<br><span class="hljs-comment">--从这里开始的获取的应该进行一个key的排序，动态的变量，然后拼接</span><br>ngx.<span class="hljs-built_in">log</span>(ngx.INFO, <span class="hljs-string">&quot;请求的x-amz-date:&quot;</span>, ngx.req.get_headers()[<span class="hljs-string">&#x27;x-amz-date&#x27;</span>])<br>ngx.<span class="hljs-built_in">log</span>(ngx.INFO, <span class="hljs-string">&quot;请求的x-amz-meta-s3cmd-attrs:&quot;</span>, ngx.req.get_headers()[<span class="hljs-string">&#x27;x-amz-meta-s3cmd-attrs&#x27;</span>])<br>ngx.<span class="hljs-built_in">log</span>(ngx.INFO, <span class="hljs-string">&quot;请求的x-amz-storage-class:&quot;</span>, ngx.req.get_headers()[<span class="hljs-string">&#x27;x-amz-storage-class&#x27;</span>])<br><br><span class="hljs-keyword">local</span> string_to_sign = ngx.var.request_method .. <span class="hljs-string">&quot;\n&quot;</span> <br><span class="hljs-keyword">if</span> ngx.req.get_headers()[<span class="hljs-string">&#x27;content-md5&#x27;</span>] == <span class="hljs-literal">nil</span> <span class="hljs-keyword">then</span><br>    string_to_sign =  string_to_sign .. <span class="hljs-string">&quot;\n&quot;</span><br><span class="hljs-keyword">else</span><br>    string_to_sign =  string_to_sign .. ngx.req.get_headers()[<span class="hljs-string">&#x27;content-md5&#x27;</span>]<br><span class="hljs-keyword">end</span><br><br><span class="hljs-keyword">if</span> ngx.req.get_headers()[<span class="hljs-string">&#x27;content-type&#x27;</span>] == <span class="hljs-literal">nil</span> <span class="hljs-keyword">then</span><br>    string_to_sign =  string_to_sign .. <span class="hljs-string">&quot;\n&quot;</span><br><span class="hljs-keyword">else</span><br>string_to_sign = string_to_sign .. ngx.req.get_headers()[<span class="hljs-string">&#x27;content-type&#x27;</span>] .. <span class="hljs-string">&quot;\n&quot;</span><br><span class="hljs-keyword">end</span> <br><br><span class="hljs-keyword">if</span> ngx.req.get_headers()[<span class="hljs-string">&#x27;date&#x27;</span>] == <span class="hljs-literal">nil</span> <span class="hljs-keyword">then</span><br>    string_to_sign =  string_to_sign .. <span class="hljs-string">&quot;\n&quot;</span><br><span class="hljs-keyword">else</span> <br>    string_to_sign = string_to_sign .. ngx.req.get_headers()[<span class="hljs-string">&#x27;date&#x27;</span>] .. <span class="hljs-string">&quot;\n&quot;</span><br><span class="hljs-keyword">end</span><br><br><span class="hljs-keyword">local</span> headers = ngx.req.get_headers()  <br><span class="hljs-keyword">local</span> hkeys = &#123;&#125;<br><span class="hljs-keyword">for</span> k,_ <span class="hljs-keyword">in</span> <span class="hljs-built_in">pairs</span>(headers) <span class="hljs-keyword">do</span> <br>    <span class="hljs-built_in">table</span>.<span class="hljs-built_in">insert</span>(hkeys, k) <br><span class="hljs-keyword">end</span><br><span class="hljs-built_in">table</span>.<span class="hljs-built_in">sort</span>(hkeys)<br><br><span class="hljs-keyword">for</span> _,key <span class="hljs-keyword">in</span> <span class="hljs-built_in">pairs</span>(hkeys) <span class="hljs-keyword">do</span>  <br>      <span class="hljs-keyword">if</span> <span class="hljs-built_in">string</span>.<span class="hljs-built_in">find</span>(key,<span class="hljs-string">&quot;x-amz-&quot;</span>) ~= <span class="hljs-literal">nil</span> <span class="hljs-keyword">then</span><br>        <span class="hljs-keyword">if</span> key == <span class="hljs-string">&quot;x-amz-storage-class&quot;</span> <span class="hljs-keyword">then</span><br>            ngx.<span class="hljs-built_in">log</span>(ngx.INFO,<span class="hljs-string">&quot;需要替换的class:&quot;</span>,key,headers[key])<br>            string_to_sign = string_to_sign .. key .. <span class="hljs-string">&quot;:&quot;</span> ..  <span class="hljs-string">&quot;STANDARD&quot;</span> .. <span class="hljs-string">&quot;\n&quot;</span><br>        <span class="hljs-keyword">else</span><br><br>        ngx.<span class="hljs-built_in">log</span>(ngx.INFO,<span class="hljs-string">&quot;zp_key_panduan: &quot;</span>, key)<br>        string_to_sign = string_to_sign .. key .. <span class="hljs-string">&quot;:&quot;</span> ..  headers[key] .. <span class="hljs-string">&quot;\n&quot;</span><br>        <span class="hljs-keyword">end</span><br>      <span class="hljs-keyword">end</span><br>      <span class="hljs-keyword">if</span> <span class="hljs-built_in">string</span>.<span class="hljs-built_in">find</span>(key,<span class="hljs-string">&quot;x-emc-&quot;</span>) ~= <span class="hljs-literal">nil</span> <span class="hljs-keyword">then</span><br>        string_to_sign = string_to_sign .. key .. <span class="hljs-string">&quot;:&quot;</span> ..  headers[key] .. <span class="hljs-string">&quot;\n&quot;</span><br>      <span class="hljs-keyword">end</span><br><span class="hljs-keyword">end</span><br>string_to_sign = string_to_sign .. ngx.var.uri<br><br>ngx.<span class="hljs-built_in">log</span>(ngx.INFO,<span class="hljs-string">&quot;自定义的string_to_sign:&quot;</span>,string_to_sign)<br><br><span class="hljs-comment">-- print(string_to_sign)</span><br><span class="hljs-comment">--截取函数</span><br><span class="hljs-function"><span class="hljs-keyword">function</span> <span class="hljs-title">Str_Cut</span><span class="hljs-params">(str,s_begin,s_end)</span></span><br> <br>    <span class="hljs-keyword">local</span> StrLen = <span class="hljs-built_in">string</span>.<span class="hljs-built_in">len</span>(str)<br>    <span class="hljs-keyword">local</span> s_begin_Len = <span class="hljs-built_in">string</span>.<span class="hljs-built_in">len</span>(s_begin)<br>    <span class="hljs-keyword">local</span> s_end_Len = <span class="hljs-built_in">string</span>.<span class="hljs-built_in">len</span>(s_end)<br>    <span class="hljs-keyword">local</span> s_begin_x = <span class="hljs-built_in">string</span>.<span class="hljs-built_in">find</span>(str, s_begin, <span class="hljs-number">1</span>)<br>    <span class="hljs-comment">--print(s_begin_x)</span><br>    <span class="hljs-keyword">local</span> s_end_x = <span class="hljs-built_in">string</span>.<span class="hljs-built_in">find</span>(str, s_end, s_begin_x+<span class="hljs-number">1</span>)<br>    <span class="hljs-comment">--print(s_end_x)</span><br>    <span class="hljs-keyword">local</span> rs=(<span class="hljs-built_in">string</span>.<span class="hljs-built_in">sub</span>(str, s_begin_x+s_begin_Len, s_end_x<span class="hljs-number">-1</span>))<br>    <span class="hljs-keyword">return</span> rs<br> <br><span class="hljs-keyword">end</span><br><span class="hljs-comment">--</span><br><br><span class="hljs-keyword">local</span> secret_key = <span class="hljs-string">&quot;test1&quot;</span><br><span class="hljs-keyword">local</span> digest = ngx.hmac_sha1(secret_key, string_to_sign)<br>ngx.<span class="hljs-built_in">log</span>(ngx.INFO,<span class="hljs-string">&quot;加密后的Authorization:&quot;</span>,ngx.encode_base64(digest))<br><span class="hljs-keyword">local</span> header_authorization = ngx.req.get_headers()[<span class="hljs-string">&quot;authorization&quot;</span>]<br><span class="hljs-comment">--local accesskey = string.match(header_authorization,&quot;%s+.+:&quot;)</span><br><span class="hljs-keyword">local</span> auth_start,auth_end,auth_name,auth_access,auth_authorization  = <span class="hljs-built_in">string</span>.<span class="hljs-built_in">find</span>(header_authorization,<span class="hljs-string">&#x27;(.+)%s(.+):(.+)&#x27;</span>)<br>ngx.<span class="hljs-built_in">log</span>(ngx.INFO,<span class="hljs-string">&quot;accesskey:&quot;</span>,auth_access)<br>ngx.<span class="hljs-built_in">log</span>(ngx.INFO,<span class="hljs-string">&quot;authorization:&quot;</span>,auth_authorization)<br><br><span class="hljs-comment">--这个是从header里面提取的原始的</span><br><span class="hljs-comment">--new_auth= &quot;AWS &quot; .. auth_access ..&quot;:&quot; .. auth_authorization</span><br><span class="hljs-comment">-- 下面是自己在openresty里面重新构造的</span><br>new_auth= <span class="hljs-string">&quot;AWS &quot;</span> .. auth_access ..<span class="hljs-string">&quot;:&quot;</span> .. ngx.encode_base64(digest)<br>ngx.<span class="hljs-built_in">log</span>(ngx.INFO,<span class="hljs-string">&quot;new_auth:&quot;</span>,new_auth)<br><span class="hljs-comment">--到这里完成了相关的构造，现在要尝试修改headers的内容以及重新生产的auth</span><br>ngx.req.set_header(<span class="hljs-string">&quot;Authorization&quot;</span>, new_auth )<br>ngx.req.set_header(<span class="hljs-string">&quot;x-amz-storage-class&quot;</span>, <span class="hljs-string">&quot;STANDARD&quot;</span> )<br>ngx.<span class="hljs-built_in">log</span>(ngx.INFO,<span class="hljs-string">&quot;获取的header的authorization: &quot;</span>,ngx.req.get_headers()[<span class="hljs-string">&quot;authorization&quot;</span>])<br><span class="hljs-comment">-- </span><br><span class="hljs-keyword">local</span> attrs = ngx.req.get_headers()[<span class="hljs-string">&quot;x-amz-meta-s3cmd-attrs&quot;</span>]<br>ngx.<span class="hljs-built_in">log</span>(ngx.INFO,<span class="hljs-string">&quot;获取的x-amz-meta-s3cmd-attrs&quot;</span>,attrs)<br><br><br><span class="hljs-keyword">local</span> <span class="hljs-built_in">date</span>=<span class="hljs-built_in">os</span>.<span class="hljs-built_in">date</span>(<span class="hljs-string">&quot;%Y%m%d%H%M%S&quot;</span>);<br>ngx.<span class="hljs-built_in">log</span>(ngx.INFO, <span class="hljs-string">&quot;当前时间:&quot;</span>,<span class="hljs-built_in">date</span>)<br></code></pre></td></tr></table></figure><p>在s3传输的过程中，可以通过构造改变请求的方式，在中间截取认证，并且控制数据流向</p>]]></content>
    
    
    
    <tags>
      
      <tag>ceph</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>AX210蓝牙识别可用rock5b内核-armbian</title>
    <link href="/2023/01/31/AX210%E8%93%9D%E7%89%99%E8%AF%86%E5%88%AB%E5%8F%AF%E7%94%A8rock5b%E5%86%85%E6%A0%B8-armbian/"/>
    <url>/2023/01/31/AX210%E8%93%9D%E7%89%99%E8%AF%86%E5%88%AB%E5%8F%AF%E7%94%A8rock5b%E5%86%85%E6%A0%B8-armbian/</url>
    
    <content type="html"><![CDATA[<h1 id="AX210蓝牙识别可用rock5b内核-armbian"><a href="#AX210蓝牙识别可用rock5b内核-armbian" class="headerlink" title="AX210蓝牙识别可用rock5b内核-armbian"></a>AX210蓝牙识别可用rock5b内核-armbian</h1><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>之前已经给ubuntu做了蓝牙的适配，ubuntu打的内核包并不适用于armbian的，armbian的内核是自己通过一个脚本打出来的，并且armbian下面的编译参数跟ubuntu不同</p><p><img src="/media/rk3588/%E7%A1%AC%E4%BB%B6%E7%AF%87/assets/16696953784773.jpg"></p><p>比如我们需要用到的这个btusb，就是armbian做的buildin的，也就是放内核里面，不是以内核模块处理的，这个地方正常也没有问题，但是这个模块需要加载firmware的，在内核里面是无法加载外面的，需要单独处理</p><p>默认的出错显示如下:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">root@rock-5b:/lib/firmware/intel<span class="hljs-comment"># dmesg |grep blue</span><br>[    5.090354] [BT_RFKILL]: bluetooth_platdata_parse_dt: get property: uart_rts_gpios = 34.<br>[    5.090374] [BT_RFKILL]: bluetooth_platdata_parse_dt: get property: BT,reset_gpio = 102.<br>[    5.090390] [BT_RFKILL]: bluetooth_platdata_parse_dt: get property: BT,wake_host_irq = 21.<br>[    5.845000] bluetooth hci0: Direct firmware load <span class="hljs-keyword">for</span> intel/ibt-0041-0041.sfi failed with error -2<br></code></pre></td></tr></table></figure><p>这个加载的启动5秒就加载了，而在ubuntu下面是14秒左右加载的，通过这里判断应该是firmware的加载问题</p><h2 id="相关处理"><a href="#相关处理" class="headerlink" title="相关处理"></a>相关处理</h2><h3 id="下载armbian工具"><a href="#下载armbian工具" class="headerlink" title="下载armbian工具"></a>下载armbian工具</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">git <span class="hljs-built_in">clone</span> https://github.com/armbian/build.git<br></code></pre></td></tr></table></figure><h3 id="把编译内核指定到自己的内核"><a href="#把编译内核指定到自己的内核" class="headerlink" title="把编译内核指定到自己的内核"></a>把编译内核指定到自己的内核</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs bash">root@ubuntu:~/armbian/build<span class="hljs-comment"># cat config/sources/families/rockchip-rk3588.conf</span><br><span class="hljs-built_in">source</span> <span class="hljs-string">&quot;<span class="hljs-variable">$&#123;BASH_SOURCE%/*&#125;</span>/include/rockchip64_common.inc&quot;</span><br><br>BOOTSOURCE=<span class="hljs-string">&#x27;https://github.com/radxa/u-boot.git&#x27;</span><br>BOOTBRANCH=<span class="hljs-string">&#x27;branch:stable-5.10-rock5&#x27;</span><br>BOOTPATCHDIR=<span class="hljs-string">&quot;legacy&quot;</span><br><br>OVERLAY_PREFIX=<span class="hljs-string">&#x27;rockchip-rk3588&#x27;</span><br><br><span class="hljs-keyword">case</span> <span class="hljs-variable">$BRANCH</span> <span class="hljs-keyword">in</span><br><br>legacy)<br>UBOOT_COMPILER=<span class="hljs-string">&quot;aarch64-linux-gnu-&quot;</span><br>UBOOT_USE_GCC=<span class="hljs-string">&#x27;&lt; 8.0&#x27;</span><br>BOOTDIR=<span class="hljs-string">&#x27;u-boot-rockchip64&#x27;</span><br>KERNELDIR=<span class="hljs-string">&#x27;linux-rockchip64&#x27;</span><br>KERNELSOURCE=<span class="hljs-string">&#x27;https://github.com/zphj1987/kernel&#x27;</span><br>KERNELBRANCH=<span class="hljs-string">&#x27;branch:stable-5.10-rock5&#x27;</span><br>KERNELPATCHDIR=<span class="hljs-string">&#x27;rockchip-rk3588-legacy&#x27;</span><br><br>;;<br><br><span class="hljs-keyword">esac</span><br><br>prepare_boot_configuration<br><br><span class="hljs-function"><span class="hljs-title">family_tweaks_bsp</span></span>() &#123;<br>:<br>&#125;<br></code></pre></td></tr></table></figure><p>上面的KERNELSOURCE修改为我已经打好补丁的分支的，脚本会自己去下载内核</p><h3 id="把固件打到内核里面去"><a href="#把固件打到内核里面去" class="headerlink" title="把固件打到内核里面去"></a>把固件打到内核里面去</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs bash">root@ubuntu:~/armbian/build<span class="hljs-comment"># cat config/kernel/linux-rockchip-rk3588-legacy.config|grep FIRMWARE</span><br><span class="hljs-comment"># CONFIG_FIRMWARE_MEMMAP is not set</span><br><span class="hljs-comment"># CONFIG_GOOGLE_FIRMWARE is not set</span><br>CONFIG_PREVENT_FIRMWARE_BUILD=y<br>CONFIG_EXTRA_FIRMWARE=<span class="hljs-string">&quot;intel/ibt-0041-0041.sfi intel/ibt-0041-0041.ddc&quot;</span><br>CONFIG_EXTRA_FIRMWARE_DIR=<span class="hljs-string">&quot;/lib/firmware&quot;</span><br>CONFIG_HOSTAP_FIRMWARE=y<br>CONFIG_HOSTAP_FIRMWARE_NVRAM=y<br>CONFIG_DRM_LOAD_EDID_FIRMWARE=y<br><span class="hljs-comment"># CONFIG_FIRMWARE_EDID is not set</span><br><span class="hljs-comment"># CONFIG_TEST_FIRMWARE is not set</span><br></code></pre></td></tr></table></figure><p>注意上面的FIRMWARE目录就是&#x2F;lib&#x2F;firmware，固件的需要把目录带上，否则打不进去<br>打进去的内核是会在内核编译目录里面看到的，注意下，当前的编译机器的 &#x2F;lib&#x2F;firmware&#x2F;intel&#x2F;ibt-0041-0041.sfi这个文件路径需要存在，这里是调用编译机器的，弄到内核里面去的</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs bash">root@ubuntu:~/armbian/build<span class="hljs-comment"># ll cache/sources/linux-rockchip64/stable-5.10-rock5/drivers/base/firmware_loader/builtin/intel/</span><br>total 696<br>drwxrwxr-x 2 root root   4096 Nov 29 12:02 ./<br>drwxrwxr-x 3 root root   4096 Nov 29 12:01 ../<br>-rw-rw-r-- 1 root root   1360 Nov 29 12:01 ibt-0041-0041.ddc.gen.o<br>-rw-rw-r-- 1 root root   1509 Nov 29 12:01 .ibt-0041-0041.ddc.gen.o.cmd<br>-rw-rw-r-- 1 root root    516 Nov 29 12:01 ibt-0041-0041.ddc.gen.S<br>-rw-rw-r-- 1 root root 683224 Nov 29 12:01 ibt-0041-0041.sfi.gen.o<br>-rw-rw-r-- 1 root root   1509 Nov 29 12:01 .ibt-0041-0041.sfi.gen.o.cmd<br>-rw-rw-r-- 1 root root    516 Nov 29 12:01 ibt-0041-0041.sfi.gen.S<br></code></pre></td></tr></table></figure><p>可以看到我们buildin的intel的 firmware</p><h3 id="开始编译"><a href="#开始编译" class="headerlink" title="开始编译"></a>开始编译</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">root@ubuntu:~/armbian/build<span class="hljs-comment"># ./compile.sh</span><br></code></pre></td></tr></table></figure><p><img src="/media/rk3588/%E7%A1%AC%E4%BB%B6%E7%AF%87/assets/16696960563471.jpg"></p><p>可以看到我改过的文件，脚本提示了，继续就行<br><img src="/media/rk3588/%E7%A1%AC%E4%BB%B6%E7%AF%87/assets/16696960827173.jpg"></p><p>选第一个就行，我们上面改的是linux-rockchip-rk3588-legacy.config这个配置文件，选这个就是指的用这个配置文件</p><p><img src="/media/rk3588/%E7%A1%AC%E4%BB%B6%E7%AF%87/assets/16696961404183.jpg"><br><img src="/media/rk3588/%E7%A1%AC%E4%BB%B6%E7%AF%87/assets/16696961507427.jpg"></p><p>默认是没有的，需要选板子里面找<br><img src="/media/rk3588/%E7%A1%AC%E4%BB%B6%E7%AF%87/assets/16696961846355.jpg"></p><p>可以找到rock-5b的选项<br><img src="/media/rk3588/%E7%A1%AC%E4%BB%B6%E7%AF%87/assets/16696962066484.jpg"><br>根据os选，我的版本这个，选的这个<br><img src="/media/rk3588/%E7%A1%AC%E4%BB%B6%E7%AF%87/assets/16696962304502.jpg"><br>这个我选的这个<br><img src="/media/rk3588/%E7%A1%AC%E4%BB%B6%E7%AF%87/assets/16696962431667.jpg"><br>继续<br>然后就开始了，这个会下载很多东西，网络搞通畅</p><p>不用等完全编译完，等内核输出了deb包就可以</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs bash">root@ubuntu:~/armbian/build<span class="hljs-comment"># ll output/debs</span><br>total 245980<br>drwxrwxr-x 3 root root      4096 Nov 29 12:06 ./<br>drwxrwxr-x 8 root root      4096 Nov 29 00:33 ../<br>-rw-r--r-- 1 root root    128460 Nov 29 12:05 armbian-config_22.11.0-trunk_all.deb<br>-rw-r--r-- 1 root root   9441404 Nov 29 12:06 armbian-firmware_22.11.0-trunk_all.deb<br>-rw-r--r-- 1 root root    120776 Nov 29 12:06 armbian-plymouth-theme_22.11.0-trunk_all.deb<br>-rw-r--r-- 1 root root   2424128 Nov 29 12:06 armbian-zsh_22.11.0-trunk_all.deb<br>drwxrwxr-x 2 root root      4096 Nov 29 11:07 extra/<br>-rw-r--r-- 1 root root    733160 Nov 29 12:05 linux-dtb-legacy-rockchip-rk3588_22.11.0-trunk_arm64.deb<br>-rw-r--r-- 1 root root  12046524 Nov 29 12:05 linux-headers-legacy-rockchip-rk3588_22.11.0-trunk_arm64.deb<br>-rw-r--r-- 1 root root  18750932 Nov 29 12:05 linux-image-legacy-rockchip-rk3588_22.11.0-trunk_arm64.deb<br>-rw-r--r-- 1 root root   1190208 Nov 29 12:05 linux-libc-dev_22.11.0-trunk_arm64.deb<br>-rw-r--r-- 1 root root 206047844 Nov 29 12:00 linux-source-legacy-rockchip-rk3588_22.11.0-trunk_all.deb<br>-rw-r--r-- 1 root root    965872 Nov 29 11:58 linux-u-boot-legacy-rock-5b_22.11.0-trunk_arm64.deb<br></code></pre></td></tr></table></figure><p>差不多就是上面的包，主要安装的就是这个包</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">linux-image-legacy-rockchip-rk3588_22.11.0-trunk_arm64.deb<br></code></pre></td></tr></table></figure><p>然后把这个包放到机器上面安装即可，然后重启就行了</p><h2 id="截图"><a href="#截图" class="headerlink" title="截图"></a>截图</h2><p><img src="/media/rk3588/%E7%A1%AC%E4%BB%B6%E7%AF%87/assets/16696964714781.jpg"></p><p>没有问题，扫描也可以看到相关的蓝牙设备</p><h2 id="安装内核包"><a href="#安装内核包" class="headerlink" title="安装内核包"></a>安装内核包</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs bash">root@rock-5b:~<span class="hljs-comment"># wget https://zphj1987.com/media/rk3588/硬件篇/assets/files/linux-image-armbian-ax210-bluetooth.tar.gz  (待修改)</span><br>root@rock-5b:~<span class="hljs-comment"># tar -xvf linux-image-armbian-ax210-bluetooth.tar.gz</span><br>root@rock-5b:~<span class="hljs-comment"># cd debs/</span><br>root@rock-5b:~/debs<span class="hljs-comment"># dpkg -i linux-image-legacy-rockchip-rk3588_22.11.0-trunk_arm64.deb </span><br>(Reading database ... 116838 files and directories currently installed.)<br>Preparing to unpack linux-image-legacy-rockchip-rk3588_22.11.0-trunk_arm64.deb ...<br>Unpacking linux-image-legacy-rockchip-rk3588 (22.11.0-trunk) over (22.11.0-trunk) ...<br>Setting up linux-image-legacy-rockchip-rk3588 (22.11.0-trunk) ...<br>update-initramfs: Generating /boot/initrd.img-5.10.72-rockchip-rk3588<br>W: Possible missing firmware /lib/firmware/rtl_bt/rtl8821a_config.bin <span class="hljs-keyword">for</span> built-in driver btrtl<br>W: Possible missing firmware /lib/firmware/rtl_bt/rtl8761a_config.bin <span class="hljs-keyword">for</span> built-in driver btrtl<br>update-initramfs: Converting to u-boot format<br>Free space after deleting the package linux-image-legacy-rockchip-rk3588 <span class="hljs-keyword">in</span> /boot: 120M<br>root@rock-5b:~/debs<span class="hljs-comment"># reboot</span><br></code></pre></td></tr></table></figure><p>查看信息</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><code class="hljs bash">root@rock-5b:~<span class="hljs-comment"># lspci </span><br>0002:20:00.0 PCI bridge: Rockchip Electronics Co., Ltd Device 3588 (rev 01)<br>0002:21:00.0 Network controller: Intel Corporation Wi-Fi 6 AX210/AX211/AX411 160MHz (rev 1a)<br>0004:40:00.0 PCI bridge: Rockchip Electronics Co., Ltd Device 3588 (rev 01)<br>0004:41:00.0 Ethernet controller: Realtek Semiconductor Co., Ltd. RTL8125 2.5GbE Controller (rev 04)<br>root@rock-5b:~<span class="hljs-comment"># lsusb </span><br>Bus 006 Device 001: ID 1d6b:0003 Linux Foundation 3.0 root hub<br>Bus 005 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub<br>Bus 008 Device 001: ID 1d6b:0003 Linux Foundation 3.0 root hub<br>Bus 007 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub<br>Bus 004 Device 001: ID 1d6b:0001 Linux Foundation 1.1 root hub<br>Bus 002 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub<br>Bus 003 Device 001: ID 1d6b:0001 Linux Foundation 1.1 root hub<br>Bus 001 Device 003: ID 8087:0032 Intel Corp. AX210 Bluetooth<br>Bus 001 Device 002: ID 1a40:0101 Terminus Technology Inc. Hub<br>Bus 001 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub<br><br>root@rock-5b:~<span class="hljs-comment"># hciconfig -a</span><br>hci0:Type: Primary  Bus: USB<br>BD Address: C4:03:A8:86:DD:AE  ACL MTU: 1021:4  SCO MTU: 96:6<br>UP RUNNING <br>RX bytes:18498 acl:0 sco:0 events:2863 errors:0<br>TX bytes:695221 acl:0 sco:0 commands:2861 errors:0<br>Features: 0xbf 0xfe 0x0f 0xfe 0xdb 0xff 0x7b 0x87<br>Packet <span class="hljs-built_in">type</span>: DM1 DM3 DM5 DH1 DH3 DH5 HV1 HV2 HV3 <br>Link policy: RSWITCH SNIFF <br>Link mode: PERIPHERAL ACCEPT <br>Name: <span class="hljs-string">&#x27;rock-5b&#x27;</span><br>Class: 0x000000<br>Service Classes: Unspecified<br>Device Class: Miscellaneous, <br>HCI Version:  (0xc)  Revision: 0x3484<br>LMP Version:  (0xc)  Subversion: 0x3484<br>Manufacturer: Intel Corp. (2)<br><br></code></pre></td></tr></table></figure><p>命令行测试扫描蓝牙</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs bash">root@rock-5b:~<span class="hljs-comment"># bluetoothctl </span><br>Agent registered<br>[CHG] Controller C4:03:A8:86:DD:AE Pairable: <span class="hljs-built_in">yes</span><br>[bluetooth]<span class="hljs-comment"># power on</span><br>Changing power on succeeded<br>[bluetooth]<span class="hljs-comment"># scan on</span><br>Discovery started<br>[CHG] Controller C4:03:A8:86:DD:AE Discovering: <span class="hljs-built_in">yes</span><br>[NEW] Device 49:87:30:7B:75:24 49-87-30-7B-75-24<br>[NEW] Device 5B:DE:97:D4:1F:A5 5B-DE-97-D4-1F-A5<br>[NEW] Device EA:B4:45:8C:0A:72 HE3490021305<br>[NEW] Device 67:95:6A:BA:87:8C 67-95-6A-BA-87-8C<br>[NEW] Device 6E:72:EE:46:DE:93 6E-72-EE-46-DE-93<br>[NEW] Device 1C:BE:24:61:79:49 1C-BE-24-61-79-49<br>[bluetooth]<span class="hljs-comment"># </span><br></code></pre></td></tr></table></figure><p>可以看到没有问题</p><h2 id="资源"><a href="#资源" class="headerlink" title="资源"></a>资源</h2><p>打好的包</p><p><a href="linux-image-armbian-ax210-bluetooth.tar.gz">linux-image-armbian-ax210-bluetooth.tar</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>AX210蓝牙识别可用rock5b内核</title>
    <link href="/2023/01/31/AX210%E8%93%9D%E7%89%99%E8%AF%86%E5%88%AB%E5%8F%AF%E7%94%A8rock5b%E5%86%85%E6%A0%B8/"/>
    <url>/2023/01/31/AX210%E8%93%9D%E7%89%99%E8%AF%86%E5%88%AB%E5%8F%AF%E7%94%A8rock5b%E5%86%85%E6%A0%B8/</url>
    
    <content type="html"><![CDATA[<h1 id="AX210蓝牙识别可用rock5b内核"><a href="#AX210蓝牙识别可用rock5b内核" class="headerlink" title="AX210蓝牙识别可用rock5b内核"></a>AX210蓝牙识别可用rock5b内核</h1><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>AX210是intel的一个wifi和蓝牙一起的模块，这个小的模块一般是在笔记本里面的，rock5b上面的接口是支持这个模块的<br>AX210因为比较新，所以wifi的支持在5.10内核是没有问题的，但蓝牙在这个上面就无法启动了，报下面的错误，这个错误在网上看到很多包括X86下面的内核上面也出现了，基本上就是驱动相关的问题，然后kernel官方主线内核的5.10.155也没有合入相关的支持，没有做低版本内核的backport（其实改动量也不大，就是新增的），rock5b的5.10内核也自然就不支持这个蓝牙了</p><p>开始的时候打算整个bluetooth目录替换，发现编译不过去，内核后来的版本可能还改了其它东西，最后还是只把相关的改动弄到5.10上面来，改动还好不是很大，这个地方设计的比较好的就是只是新增了相关的函数，所以也基本上对其它的蓝牙功能没影响的</p><h2 id="错误信息"><a href="#错误信息" class="headerlink" title="错误信息"></a>错误信息</h2><p>可以看到这个错误，网上关键字很多，就是wifi可用，蓝牙不可用</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[   46.734253] Bluetooth: hci0: Reading Intel version information failed (-22)<br>[   46.734305] Bluetooth: hci0: Intel Read version failed (-22)<br></code></pre></td></tr></table></figure><h2 id="设备相关信息"><a href="#设备相关信息" class="headerlink" title="设备相关信息"></a>设备相关信息</h2><p><img src="/media/rk3588/%E7%A1%AC%E4%BB%B6%E7%AF%87/assets/3EA36E43838FFF77E315DE65AD2BD8FD.jpg" alt="3EA36E43838FFF77E315DE65AD2BD8FD"></p><p><img src="/media/rk3588/%E7%A1%AC%E4%BB%B6%E7%AF%87/assets/QQ20221125-0.jpg" alt="QQ20221125-0"></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs bash">root@rock-5b:/home/rock/Pictures<span class="hljs-comment"># lsusb</span><br>Bus 006 Device 001: ID 1d6b:0003 Linux Foundation 3.0 root hub<br>Bus 005 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub<br>Bus 008 Device 001: ID 1d6b:0003 Linux Foundation 3.0 root hub<br>Bus 007 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub<br>Bus 004 Device 001: ID 1d6b:0001 Linux Foundation 1.1 root hub<br>Bus 002 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub<br>Bus 003 Device 001: ID 1d6b:0001 Linux Foundation 1.1 root hub<br>Bus 001 Device 004: ID 8087:0032 Intel Corp.<br>Bus 001 Device 002: ID 1a40:0101 Terminus Technology Inc. Hub<br>Bus 001 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub<br></code></pre></td></tr></table></figure><p>上面的Bus 001 Device 004: ID 8087:0032 Intel Corp.这个就是AX210的设备，这个8087:0032是固定的设备id</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">root@rock-5b:/home/rock/Pictures<span class="hljs-comment"># lspci</span><br>0002:20:00.0 PCI bridge: Fuzhou Rockchip Electronics Co., Ltd Device 3588 (rev 01)<br>0002:21:00.0 Network controller: Intel Corporation Device 2725 (rev 1a)<br>0004:40:00.0 PCI bridge: Fuzhou Rockchip Electronics Co., Ltd Device 3588 (rev 01)<br>0004:41:00.0 Ethernet controller: Realtek Semiconductor Co., Ltd. RTL8125 2.5GbE Controller (rev 04)<br></code></pre></td></tr></table></figure><p>上面的Network controller: Intel Corporation Device 2725这个也是AX210的设备id</p><h2 id="AX210蓝牙使用情况"><a href="#AX210蓝牙使用情况" class="headerlink" title="AX210蓝牙使用情况"></a>AX210蓝牙使用情况</h2><p>windows上面的蓝牙用过，感觉功能比较简单，linux下没用过，下面就记录下一些功能，我的系统安装的是官方提供的ubuntu</p><blockquote><p><a href="https://github.com/radxa/debos-radxa/releases/download/20221031-1045/rock-5b-ubuntu-focal-server-arm64-20221031-1328-gpt.img.xz">https://github.com/radxa/debos-radxa/releases/download/20221031-1045/rock-5b-ubuntu-focal-server-arm64-20221031-1328-gpt.img.xz</a></p></blockquote><p>系统安装完成后就安装下桌面</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">apt install ubuntu-desktop<br></code></pre></td></tr></table></figure><p>然后重启，进入图形界面</p><h3 id="内核替换"><a href="#内核替换" class="headerlink" title="内核替换"></a>内核替换</h3><p>替换上已经打好蓝牙补丁的内核</p><p>内核下载地址:<br><a href="media/rk3588/%E7%A1%AC%E4%BB%B6%E7%AF%87/file/linux-kernel-rock5b-ax210-bluetooth.tar.gz">linux-kernel-rock5b-ax210-bluetooth.tar.gz</a></p><h4 id="安装内核"><a href="#安装内核" class="headerlink" title="安装内核"></a>安装内核</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">root@rock-5b:~/kernel/packages<span class="hljs-comment"># dpkg -i linux-image-5.10.66-blue-10-rockchip-g311cf1c0d_5.10.66-blue-10-rockchip_arm64.deb</span><br></code></pre></td></tr></table></figure><h4 id="增加sfi文件和ddc文件"><a href="#增加sfi文件和ddc文件" class="headerlink" title="增加sfi文件和ddc文件"></a>增加sfi文件和ddc文件</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">wget https://anduin.linuxfromscratch.org/sources/linux-firmware/intel/ibt-0041-0041.sfi  -O /lib/firmware/intel/ibt-0041-0041.sfi<br>wget https://anduin.linuxfromscratch.org/sources/linux-firmware/intel/ibt-0041-0041.ddc -O /lib/firmware/intel/ibt-0041-0041.ddc<br></code></pre></td></tr></table></figure><p>注意上面是大O(是哦不是零不是小哦)，不确定就检查一下文件内容</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">root@rock-5b:~<span class="hljs-comment"># ll /lib/firmware/intel/ibt-0041-0041.sfi</span><br>-rw-r--r-- 1 root root 690128 Nov 19 04:46 /lib/firmware/intel/ibt-0041-0041.sfi<br></code></pre></td></tr></table></figure><p>检查启动项</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs bash">root@rock-5b:~/kernel/packages<span class="hljs-comment"># cat /boot/extlinux/extlinux.conf</span><br><span class="hljs-comment">#timeout 10</span><br><span class="hljs-comment">#menu title select kernel</span><br><br>label kernel-5.10.66-blue-10-rockchip-g311cf1c0d<br>    kernel /vmlinuz-5.10.66-blue-10-rockchip-g311cf1c0d<br>    initrd /initrd.img-5.10.66-blue-10-rockchip-g311cf1c0d<br>    devicetreedir /dtbs/5.10.66-blue-10-rockchip-g311cf1c0d<br>    fdtoverlays  /dtbs/5.10.66-blue-10-rockchip-g311cf1c0d/rockchip/overlay/rk3588-uart7-m2.dtbo<br>    append   root=UUID=67ad0e7b-3914-48d6-97c2-c48e5e0e405b earlycon=uart8250,mmio32,0xfeb50000 console=ttyFIQ0 console=tty1 consoleblank=0 loglevel=0 panic=10 rootwait rw init=/sbin/init rootfstype=ext4 cgroup_enable=cpuset cgroup_memory=1 cgroup_enable=memory swapaccount=1 irqchip.gicv3_pseudo_nmi=0 switolb=1 coherent_pool=2M<br><br>label kernel-5.10.66-27-rockchip-gea60d388902d<br>    kernel /vmlinuz-5.10.66-27-rockchip-gea60d388902d<br>    initrd /initrd.img-5.10.66-27-rockchip-gea60d388902d<br>    devicetreedir /dtbs/5.10.66-27-rockchip-gea60d388902d<br>    fdtoverlays  /dtbs/5.10.66-27-rockchip-gea60d388902d/rockchip/overlay/rk3588-uart7-m2.dtbo<br>    append   root=UUID=67ad0e7b-3914-48d6-97c2-c48e5e0e405b earlycon=uart8250,mmio32,0xfeb50000 console=ttyFIQ0 console=tty1 consoleblank=0 loglevel=0 panic=10 rootwait rw init=/sbin/init rootfstype=ext4 cgroup_enable=cpuset cgroup_memory=1 cgroup_enable=memory swapaccount=1 irqchip.gicv3_pseudo_nmi=0 switolb=1 coherent_pool=2M<br></code></pre></td></tr></table></figure><p>注意上面修改的blue的标签的应该排在上面的，重启板卡</p><h3 id="正常启动日志"><a href="#正常启动日志" class="headerlink" title="正常启动日志"></a>正常启动日志</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs bash">[   11.990430] usb 1-1.3: new full-speed USB device number 4 using ehci-platform<br>[   12.135296] squashfs: version 4.0 (2009/01/31) Phillip Lougher<br>[   12.209438] usb 1-1.3: New USB device found, idVendor=8087, idProduct=0032, bcdDevice= 0.00<br>[   12.209450] usb 1-1.3: New USB device strings: Mfr=0, Product=0, SerialNumber=0<br>[   12.213051] Bluetooth: hci0: Device revision is 0<br>[   12.213057] Bluetooth: hci0: Secure boot is enabled<br>[   12.213059] Bluetooth: hci0: OTP lock is enabled<br>[   12.213061] Bluetooth: hci0: API lock is enabled<br>[   12.213063] Bluetooth: hci0: Debug lock is disabled<br>[   12.213065] Bluetooth: hci0: Minimum firmware build 1 week 10 2014<br>[   12.213068] Bluetooth: hci0: Bootloader timestamp 2019.40 buildtype 1 build 38<br>[   12.213804] Bluetooth: hci0: Found device firmware: intel/ibt-0041-0041.sfi<br>[   12.787396] ttyFIQ ttyFIQ0: tty_port_close_start: <span class="hljs-built_in">tty</span>-&gt;count = 1 port count = 2<br>[   12.881301] fuse: init (API version 7.32)<br>[   13.094139] FAT-fs (mmcblk0p1): utf8 is not a recommended IO charset <span class="hljs-keyword">for</span> FAT filesystems, filesystem will be <span class="hljs-keyword">case</span> sensitive!<br>[   13.830192] Bluetooth: hci0: Waiting <span class="hljs-keyword">for</span> firmware download to complete<br>[   13.831152] Bluetooth: hci0: Firmware loaded <span class="hljs-keyword">in</span> 1582218 usecs<br>[   13.831226] Bluetooth: hci0: Waiting <span class="hljs-keyword">for</span> device to boot<br>[   13.857113] Bluetooth: hci0: Device booted <span class="hljs-keyword">in</span> 25334 usecs<br>[   13.858975] Bluetooth: hci0: Found Intel DDC parameters: intel/ibt-0041-0041.ddc<br>[   13.861222] Bluetooth: hci0: Applying Intel DDC parameters completed<br>[   13.865219] Bluetooth: hci0: Firmware timestamp 2022.39 buildtype 1 build 52159<br>[   17.172415] r8125: enP4p65s0: <span class="hljs-built_in">link</span> up<br>[   17.172554] IPv6: ADDRCONF(NETDEV_CHANGE): enP4p65s0: <span class="hljs-built_in">link</span> becomes ready<br>[   20.887172] dwhdmi-rockchip fdea0000.hdmi: use tmds mode<br>[   21.100098] dwhdmi-rockchip fdea0000.hdmi: use tmds mode<br>[   37.005172] vcc3v3_pcie30: disabling<br></code></pre></td></tr></table></figure><p>可以看到正常的加载了，上面的信息就是蓝牙加载这部分的内容</p><h2 id="蓝牙使用体验"><a href="#蓝牙使用体验" class="headerlink" title="蓝牙使用体验"></a>蓝牙使用体验</h2><p>AX210如果没有装天线，蓝牙需要在10cm左右的距离才能识别的比较好，远了有问题，这个后面装了天线再验证信号情况</p><p><img src="/media/rk3588/%E7%A1%AC%E4%BB%B6%E7%AF%87/assets/desktop-bluetoothicon.png" alt="desktop-bluetoothicon.png"><br>可以看到右上角的蓝牙图标可以点了，不是完全灰掉的，没打驱动无法使用</p><p><img src="/media/rk3588/%E7%A1%AC%E4%BB%B6%E7%AF%87/assets/%E8%93%9D%E7%89%99%E5%8A%A9%E6%89%8B.png"><br>通过蓝牙助手进行蓝牙的配对，这个地方配对需要两边都确认，可以在系统设置里面配对</p><p><img src="/media/rk3588/%E7%A1%AC%E4%BB%B6%E7%AF%87/assets/%E6%89%AB%E6%8F%8F%E8%93%9D%E7%89%99%E8%AE%BE%E5%A4%87.png"><br>识别到的设备</p><p><img src="/media/rk3588/%E7%A1%AC%E4%BB%B6%E7%AF%87/assets/%E9%85%8D%E5%AF%B9%E6%88%90%E5%8A%9F.png"><br>可以看到配对成功，显示的是已经连接</p><p><img src="/media/rk3588/%E7%A1%AC%E4%BB%B6%E7%AF%87/assets/%E8%AE%BE%E5%A4%87%E5%88%97%E8%A1%A8%E6%9F%A5%E7%9C%8B.png"><br>设备列表里面还有其它的蓝牙设备</p><p><img src="/media/rk3588/%E7%A1%AC%E4%BB%B6%E7%AF%87/assets/%E8%BF%9B%E8%A1%8C%E9%85%8D%E5%AF%B9.png"><br>进行配对的操作</p><p><img src="/media/rk3588/%E7%A1%AC%E4%BB%B6%E7%AF%87/assets/%E8%AE%BE%E7%BD%AE%E5%8F%AF%E4%BB%A5%E8%A2%AB%E6%89%AB%E6%8F%8F.png"><br>设置蓝牙设备可以被扫描</p><p><img src="/media/rk3588/%E7%A1%AC%E4%BB%B6%E7%AF%87/assets/%E7%B3%BB%E7%BB%9F%E8%AE%BE%E7%BD%AE%E9%87%8C%E9%9D%A2%E8%BF%9B%E8%A1%8C%E8%BF%9E%E6%8E%A5.png"><br>在系统设置里面进行连接，双击设备名称可以传文件给设备</p><p><img src="/media/rk3588/%E7%A1%AC%E4%BB%B6%E7%AF%87/assets/%E4%BF%A1%E5%8F%B7%E6%9F%A5%E7%9C%8B.png"><br>可以看到信号<br><img src="/media/rk3588/%E7%A1%AC%E4%BB%B6%E7%AF%87/assets/%E4%BB%8Erock5b%E5%BE%80%E6%89%8B%E6%9C%BA%E4%BC%A0%E6%96%87%E4%BB%B6.png"><br>文件正在传输，没有问题</p><p><img src="/media/rk3588/%E7%A1%AC%E4%BB%B6%E7%AF%87/assets/%E8%93%9D%E7%89%99%E8%AE%BE%E5%A4%87%E4%BF%A1%E6%81%AF.png"><br>hci查询蓝牙信息<br><img src="/media/rk3588/%E7%A1%AC%E4%BB%B6%E7%AF%87/assets/%E6%89%8B%E6%9C%BA%E9%80%9A%E8%BF%87%E8%93%9D%E7%89%99%E6%8A%95%E6%94%BE%E9%9F%B3%E9%A2%91%E7%BB%99rock5b.png"><br>手机连rock5b以后，在手机那边播放音乐，在rock5b上面可以识别到，并且能够听到声音，作为一个音频设备处理的</p><p><img src="/media/rk3588/%E7%A1%AC%E4%BB%B6%E7%AF%87/assets/wifi%E7%9A%84%E8%AF%86%E5%88%AB.png"><br>wifi识别也是没有问题的</p><p>鼠标识别没有问题</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">root@rock-5b:~<span class="hljs-comment"># dmesg</span><br>[  514.872857] input: RAPOO BT4.0 Mouse as /devices/virtual/misc/uhid/0005:000E:3412.0005/input/input16<br>[  514.873282] input: RAPOO BT4.0 Mouse Consumer Control as /devices/virtual/misc/uhid/0005:000E:3412.0005/input/input17<br>[  514.874248] hid-generic 0005:000E:3412.0005: input,hidraw4: BLUETOOTH HID v67.01 Mouse [RAPOO BT4.0 Mouse] on c4:03:a8:86:<span class="hljs-built_in">dd</span>:ae<br></code></pre></td></tr></table></figure><p><img src="/media/rk3588/%E7%A1%AC%E4%BB%B6%E7%AF%87/assets/mouseinput.png" alt="mouseinput"></p><p>耳机测试没有问题<br><img src="/media/rk3588/%E7%A1%AC%E4%BB%B6%E7%AF%87/assets/erji.png" alt="erji"></p><h2 id="蓝牙已测功能"><a href="#蓝牙已测功能" class="headerlink" title="蓝牙已测功能"></a>蓝牙已测功能</h2><ul><li>1、蓝牙连接（正常）</li><li>2、rock5b给手机发送文件(正常)</li><li>3、手机通过蓝牙发送音频给rock5b,rock5b播放声音（正常）</li><li>4、鼠标连接蓝牙到rock5b(正常)</li><li>5、蓝牙连接蓝牙耳机（正常）</li></ul><p>上面可以看到，rock5b到设备的可以发送，设备到rock5b也是能发送的，两边都是通路的<br>测试过程中发现，一定要加天线，否则功能可能因为信号不足，不正常</p><p><img src="/media/rk3588/%E7%A1%AC%E4%BB%B6%E7%AF%87/assets/%E5%A4%A9%E7%BA%BF.jpg" alt="天线"></p><p>上面就是买的天线ipe4接口，上面有个塑料壳子（1块钱）可以固定接口更稳定一点</p><h2 id="相关资源"><a href="#相关资源" class="headerlink" title="相关资源"></a>相关资源</h2><h3 id="patch文件"><a href="#patch文件" class="headerlink" title="patch文件"></a>patch文件</h3><p><a href="file/AX210-bluetooth.patch">AX210-bluetooth.patch</a></p><p>patch文件是以rock5b官方的内核做的patch文件,就是下面这个分支的</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">git <span class="hljs-built_in">clone</span> -b stable-5.10-rock5 https://github.com/radxa/kernel.git<br></code></pre></td></tr></table></figure><p>在内核代码的上一层目录里面下载上面的patch文件，然后打上patch</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs bash">root@ubuntu:~/rk3588-sdk/<span class="hljs-comment"># wget 【上面的链接】</span><br>root@ubuntu:~/rk3588-sdk/<span class="hljs-comment"># cd kernel</span><br>root@ubuntu:~/rk3588-sdk/kernel<span class="hljs-comment"># patch -p1 &lt; ../AX210-bluetooth.patch</span><br>patching file drivers/bluetooth/btintel.c<br>patching file drivers/bluetooth/btintel.h<br>patching file drivers/bluetooth/btusb.c<br></code></pre></td></tr></table></figure><p>然后编译打包即可</p><h3 id="或者直接下载我的分支"><a href="#或者直接下载我的分支" class="headerlink" title="或者直接下载我的分支"></a>或者直接下载我的分支</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">root@ubuntu:~/rk3588-sdk/<span class="hljs-comment"># git clone -b stable-5.10-rock5   --single-branch --depth 1   https://github.com/zphj1987/kernel.git</span><br></code></pre></td></tr></table></figure><p>这个分支已经打好了蓝牙的补丁的</p><p>如果考虑跟官方的版本的话，就用上面的打patch的文件的方式</p><h3 id="或者用我打好的内核deb文件如下"><a href="#或者用我打好的内核deb文件如下" class="headerlink" title="或者用我打好的内核deb文件如下"></a>或者用我打好的内核deb文件如下</h3><p><a href="file/linux-kernel-rock5b-ax210-bluetooth.tar.gz">linux-kernel-rock5b-ax210-bluetooth.tar.gz</a></p><h2 id="排错指南"><a href="#排错指南" class="headerlink" title="排错指南"></a>排错指南</h2><h3 id="错误：Reading-Intel-version-information-failed"><a href="#错误：Reading-Intel-version-information-failed" class="headerlink" title="错误：Reading Intel version information failed"></a>错误：Reading Intel version information failed</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs bash">[   45.961353] Bluetooth: hci0: Intel reset sent to retry FW download<br>[   46.183435] usb 1-1.3: USB disconnect, device number 50<br>[   46.512737] usb 1-1.3: new full-speed USB device number 51 using ehci-platform<br>[   46.729278] usb 1-1.3: New USB device found, idVendor=8087, idProduct=0032, bcdDevice= 0.00<br>[   46.729313] usb 1-1.3: New USB device strings: Mfr=0, Product=0, SerialNumber=0<br>[   46.734253] Bluetooth: hci0: Reading Intel version information failed (-22)<br>[   46.734305] Bluetooth: hci0: Intel Read version failed (-22)<br>[   46.734415] Bluetooth: hci0: Intel reset sent to retry FW download<br>[   46.950300] usb 1-1.3: USB disconnect, device number 51<br></code></pre></td></tr></table></figure><p>未安装打补丁的内核，安装即可</p><h3 id="错误：-Direct-firmware-load-for-intel-ibt-0041-0041-sfi-failed-with-error-2"><a href="#错误：-Direct-firmware-load-for-intel-ibt-0041-0041-sfi-failed-with-error-2" class="headerlink" title="错误： Direct firmware load for intel&#x2F;ibt-0041-0041.sfi failed with error -2"></a>错误： Direct firmware load for intel&#x2F;ibt-0041-0041.sfi failed with error -2</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs bash">[    9.790666] Bluetooth: hci0: Debug lock is disabled<br>[    9.790669] Bluetooth: hci0: Minimum firmware build 1 week 10 2014<br>[    9.790672] Bluetooth: hci0: Bootloader timestamp 2019.40 buildtype 1 build 38<br>[    9.814394] bluetooth hci0: Direct firmware load <span class="hljs-keyword">for</span> intel/ibt-0041-0041.sfi failed with error -2<br>[    9.814410] Bluetooth: hci0: Failed to load Intel firmware file (-2)<br>[    9.819751] Intel(R) Wireless WiFi driver <span class="hljs-keyword">for</span> Linux<br>[    9.829338] iwlwifi 0002:21:00.0: enabling device (0000 -&gt; 0002)<br>[    9.872571] iwlwifi 0002:21:00.0: api flags index 2 larger than supported by driver<br>[    9.872638] iwlwifi 0002:21:00.0: TLV_FW_FSEQ_VERSION: FSEQ Version: 93.8.63.28<br></code></pre></td></tr></table></figure><p>未下载sfi文件，下载即可</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">wget https://anduin.linuxfromscratch.org/sources/linux-firmware/intel/ibt-0041-0041.sfi  -o /lib/firmware/intel/ibt-0041-0041.sfi<br></code></pre></td></tr></table></figure><h3 id="错误：Bluetooth-hci0-Failed-to-load-Intel-DDC-file-intel-ibt-0041-0041-ddc-2"><a href="#错误：Bluetooth-hci0-Failed-to-load-Intel-DDC-file-intel-ibt-0041-0041-ddc-2" class="headerlink" title="错误：Bluetooth: hci0: Failed to load Intel DDC file intel&#x2F;ibt-0041-0041.ddc (-2)"></a>错误：Bluetooth: hci0: Failed to load Intel DDC file intel&#x2F;ibt-0041-0041.ddc (-2)</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs bash">[   14.499042] Bluetooth: hci0: Waiting <span class="hljs-keyword">for</span> firmware download to complete<br>[   14.499117] Bluetooth: hci0: Firmware loaded <span class="hljs-keyword">in</span> 1556348 usecs<br>[   14.499215] Bluetooth: hci0: Waiting <span class="hljs-keyword">for</span> device to boot<br>[   14.525032] Bluetooth: hci0: Device booted <span class="hljs-keyword">in</span> 25277 usecs<br>[   14.525162] Bluetooth: hci0: Failed to load Intel DDC file intel/ibt-0041-0041.ddc (-2)<br>[   14.529103] Bluetooth: hci0: Firmware timestamp 2022.39 buildtype 1 build 52159<br>[   17.810272] r8125: enP4p65s0: <span class="hljs-built_in">link</span> up<br>[   17.810336] IPv6: ADDRCONF(NETDEV_CHANGE): enP4p65s0:<br></code></pre></td></tr></table></figure><p>解决办法：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">wget https://anduin.linuxfromscratch.org/sources/linux-firmware/intel/ibt-0041-0041.ddc -O /lib/firmware/intel/ibt-0041-0041.ddc<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>3D打印</category>
      
    </categories>
    
    
    <tags>
      
      <tag>问题处理</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Linux下三种配置iscsi的方法</title>
    <link href="/2023/01/31/Linux%E4%B8%8B%E4%B8%89%E7%A7%8D%E9%85%8D%E7%BD%AEiscsi%E7%9A%84%E6%96%B9%E6%B3%95/"/>
    <url>/2023/01/31/Linux%E4%B8%8B%E4%B8%89%E7%A7%8D%E9%85%8D%E7%BD%AEiscsi%E7%9A%84%E6%96%B9%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<h1 id="Linux下三种配置iscsi的方法"><a href="#Linux下三种配置iscsi的方法" class="headerlink" title="Linux下三种配置iscsi的方法"></a>Linux下三种配置iscsi的方法</h1><p>linux下面配置iscsi有多种方法，用户态的有tgt，内核态的有scst和lio，如果只是普通的使用来说，没有太大的区别，对于某些特殊的场景就存在区别了</p><p>比如tgt不支持pacemaker里面的 Persistent (SCSI-3) Reservations，而scst支持<br>tgt不支持vmware的vaai的某些属性，而lio支持<br>比如scst还支持srpt的功能，而tgt也不支持</p><p>所以在某些场景下，我们需要根据自己的需要来选择服务，总体上来说iscsi算是一个中间的组件，并不影响数据本身，只是一个对外的服务，满足需求即可</p><h2 id="tgt的配置"><a href="#tgt的配置" class="headerlink" title="tgt的配置"></a>tgt的配置</h2><span id="more"></span><p>因为tgt是支持librbd的，也是支持krbd的，所以这个地方存在两种配置，默认的tgt是不支持rbd的，需要自己重新打包支持开启</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab103 ~]<span class="hljs-comment"># tgtadm --lld iscsi --mode system --op show | grep rbd</span><br></code></pre></td></tr></table></figure><p>打包的文档见这里：<a href="mweblib://16613240145984">打包一个支持rbd的tgt</a></p><h3 id="安装打好的包"><a href="#安装打好的包" class="headerlink" title="安装打好的包"></a>安装打好的包</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab103 ~]<span class="hljs-comment"># rpm -ivh scsi-target-utils-*</span><br>Preparing...                          <span class="hljs-comment">################################# [100%]</span><br>Updating / installing...<br>   1:scsi-target-utils-1.0.84-4.el7   <span class="hljs-comment">################################# [ 50%]</span><br>   2:scsi-target-utils-rbd-1.0.84-4.el<span class="hljs-comment">################################# [100%]</span><br></code></pre></td></tr></table></figure><p>检查librdb的支持情况</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab103 ~]<span class="hljs-comment"># tgtadm --lld iscsi --mode system --op show | grep rbd</span><br>    rbd (bsoflags <span class="hljs-built_in">sync</span>:direct)<br></code></pre></td></tr></table></figure><h3 id="支持librbd的配置方式"><a href="#支持librbd的配置方式" class="headerlink" title="支持librbd的配置方式"></a>支持librbd的配置方式</h3><p>修改配置文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab103 ~]<span class="hljs-comment"># cat /etc/tgt/conf.d/sample.conf</span><br>&lt;target iqn.2022-01.com.example:server.target1&gt;<br>    bs-type rbd<br>    backing-store rbd/testrbd<br>&lt;/target&gt;<br></code></pre></td></tr></table></figure><p>启动服务</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab103 ~]<span class="hljs-comment"># systemctl start tgtd</span><br></code></pre></td></tr></table></figure><p>查看生成的服务</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab103 ~]<span class="hljs-comment"># tgt-admin -s</span><br>Target 1: iqn.2022-01.com.example:server.target1<br>    System information:<br>        Driver: iscsi<br>        State: ready<br>    I_T nexus information:<br>    LUN information:<br>        LUN: 0<br>            Type: controller<br>            SCSI ID: IET     00010000<br>            SCSI SN: beaf10<br>            Size: 0 MB, Block size: 1<br>            Online: Yes<br>            Removable media: No<br>            Prevent removal: No<br>            Readonly: No<br>            SWP: No<br>            Thin-provisioning: No<br>            Backing store <span class="hljs-built_in">type</span>: null<br>            Backing store path: None<br>            Backing store flags:<br>        LUN: 1<br>            Type: disk<br>            SCSI ID: IET     00010001<br>            SCSI SN: beaf11<br>            Size: 32212 MB, Block size: 512<br>            Online: Yes<br>            Removable media: No<br>            Prevent removal: No<br>            Readonly: No<br>            SWP: No<br>            Thin-provisioning: No<br>            Backing store <span class="hljs-built_in">type</span>: rbd<br>            Backing store path: rbd/testrbd<br>            Backing store flags:<br>    Account information:<br>    ACL information:<br>        ALL<br></code></pre></td></tr></table></figure><p>可以看到这里多了个</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">Backing store <span class="hljs-built_in">type</span>: rbd<br>Backing store path: rbd/testrbd<br></code></pre></td></tr></table></figure><p>客户端登录后查看磁盘</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ~]<span class="hljs-comment"># parted -s /dev/sdc print</span><br>Error: /dev/sdc: unrecognised disk label<br>Model: IET VIRTUAL-DISK (scsi)<br>Disk /dev/sdc: 32.2GB<br>Sector size (logical/physical): 512B/4194304B<br>Partition Table: unknown<br>Disk Flags:<br></code></pre></td></tr></table></figure><p>可以看到物理扇区的大小为4194304B，这个在很多环境，rac，vmware都可能不识别，需要修改扇区的大小，这个是在tgt的配置文件里面控制的</p><p>修改配置文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab103 ~]<span class="hljs-comment"># cat /etc/tgt/conf.d/sample.conf</span><br>&lt;target iqn.2022-01.com.example:server.target1&gt;<br>    bs-type rbd<br>    backing-store rbd/testrbd<br>    lbppbe 0<br>&lt;/target&gt;<br></code></pre></td></tr></table></figure><p>这个lbppbe 0就是关键配置，设置以后客户端的连接后看到的磁盘扇区就是512b的了</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ~]<span class="hljs-comment"># parted -s /dev/sdc print</span><br>Error: /dev/sdc: unrecognised disk label<br>Model: IET VIRTUAL-DISK (scsi)<br>Disk /dev/sdc: 32.2GB<br>Sector size (logical/physical): 512B/512B<br>Partition Table: unknown<br>Disk Flags:<br></code></pre></td></tr></table></figure><h3 id="支持krbd的配置方式"><a href="#支持krbd的配置方式" class="headerlink" title="支持krbd的配置方式"></a>支持krbd的配置方式</h3><p>修改配置文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">&lt;target iqn.2022-01.com.example:server.target1&gt;<br>    backing-store /dev/rbd/rbd/testrbd<br>&lt;/target&gt;<br></code></pre></td></tr></table></figure><p>启动服务</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab103 ~]<span class="hljs-comment"># systemctl start tgtd</span><br></code></pre></td></tr></table></figure><p>查看生成的服务</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab103 ~]<span class="hljs-comment"># tgt-admin -s</span><br>Target 1: iqn.2022-01.com.example:server.target1<br>    System information:<br>        Driver: iscsi<br>        State: ready<br>    I_T nexus information:<br>    LUN information:<br>        LUN: 0<br>            Type: controller<br>            SCSI ID: IET     00010000<br>            SCSI SN: beaf10<br>            Size: 0 MB, Block size: 1<br>            Online: Yes<br>            Removable media: No<br>            Prevent removal: No<br>            Readonly: No<br>            SWP: No<br>            Thin-provisioning: No<br>            Backing store <span class="hljs-built_in">type</span>: null<br>            Backing store path: None<br>            Backing store flags:<br>        LUN: 1<br>            Type: disk<br>            SCSI ID: IET     00010001<br>            SCSI SN: beaf11<br>            Size: 32212 MB, Block size: 512<br>            Online: Yes<br>            Removable media: No<br>            Prevent removal: No<br>            Readonly: No<br>            SWP: No<br>            Thin-provisioning: No<br>            Backing store <span class="hljs-built_in">type</span>: rdwr<br>            Backing store path: /dev/rbd/rbd/testrbd<br>            Backing store flags:<br>    Account information:<br>    ACL information:<br>        ALL<br></code></pre></td></tr></table></figure><p>这个地方注意下看下客户端的映射的盘的扇区大小</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ~]<span class="hljs-comment"># parted -s /dev/sdc print</span><br>Error: /dev/sdc: unrecognised disk label<br>Model: IET VIRTUAL-DISK (scsi)<br>Disk /dev/sdc: 32.2GB<br>Sector size (logical/physical): 512B/512B<br>Partition Table: unknown<br>Disk Flags:<br></code></pre></td></tr></table></figure><h2 id="LIO配置"><a href="#LIO配置" class="headerlink" title="LIO配置"></a>LIO配置</h2><h3 id="使用命令行配置"><a href="#使用命令行配置" class="headerlink" title="使用命令行配置"></a>使用命令行配置</h3><p>创建一个block设备</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab103 ~]<span class="hljs-comment"># targetcli /backstores/block/ create disk01 /dev/rbd/rbd/testrbd</span><br>Created block storage object disk01 using /dev/rbd/rbd/testrbd.<br></code></pre></td></tr></table></figure><p>查看当前配置</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab103 ~]<span class="hljs-comment"># targetcli ls /</span><br>o- / ............................................................ [...]<br>  o- backstores ................................................. [...]<br>  | o- block ..................................... [Storage Objects: 1]<br>  | | o- disk01  [/dev/rbd/rbd/testrbd (30.0GiB) write-thru deactivated]<br>  | |   o- alua ...................................... [ALUA Groups: 1]<br>  | |     o- default_tg_pt_gp .......... [ALUA state: Active/optimized]<br>  | o- fileio .................................... [Storage Objects: 0]<br>  | o- pscsi ..................................... [Storage Objects: 0]<br>  | o- ramdisk ................................... [Storage Objects: 0]<br>  o- iscsi ............................................... [Targets: 0]<br>  o- loopback ............................................ [Targets: 0]<br></code></pre></td></tr></table></figure><p>创建一个target</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab103 ~]<span class="hljs-comment"># targetcli /iscsi/ create iqn.2022-01.org.linux-iscsi.lab103.x8664:sn.192f7d06da4a</span><br>Created target iqn.2022-01.org.linux-iscsi.lab103.x8664:sn.192f7d06da4a.<br>Created TPG 1.<br>Global pref auto_add_default_portal=<span class="hljs-literal">true</span><br>Created default portal listening on all IPs (0.0.0.0), port 3260.<br></code></pre></td></tr></table></figure><p>创建一个lun</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab103 ~]<span class="hljs-comment"># targetcli  /iscsi/iqn.2022-01.org.linux-iscsi.lab103.x8664:sn.192f7d06da4a/tpg1/luns create /backstores/block/disk01</span><br>Created LUN 0.<br></code></pre></td></tr></table></figure><p>设置target的属性</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab103 ~]<span class="hljs-comment"># targetcli  /iscsi/iqn.2022-01.org.linux-iscsi.lab103.x8664:sn.192f7d06da4a/tpg1 set attribute authentication=0 demo_mode_write_protect=0 generate_node_acls=1 cache_dynamic_acls=1</span><br></code></pre></td></tr></table></figure><p>保存配置文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab103 ~]<span class="hljs-comment"># targetcli /  saveconfig</span><br>Last 10 configs saved <span class="hljs-keyword">in</span> /etc/target/backup/.<br>Configuration saved to /etc/target/saveconfig.json<br></code></pre></td></tr></table></figure><p>启动target</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab103 ~]<span class="hljs-comment"># systemctl start target</span><br></code></pre></td></tr></table></figure><h2 id="scst的配置"><a href="#scst的配置" class="headerlink" title="scst的配置"></a>scst的配置</h2><p>配置fileio模式</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab103 ~]<span class="hljs-comment"># cat /etc/scst.conf</span><br>HANDLER vdisk_fileio &#123;<br>    DEVICE disk01 &#123;<br>        filename /dev/rbd/rbd/testrbd<br>        nv_cache 1<br>    &#125;<br>&#125;<br><br>TARGET_DRIVER iscsi &#123;<br>    enabled 1<br>    TARGET iqn.2022-05.net.vlnb:tgt &#123;<br>        LUN 0 disk01<br>        enabled 1<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure><p>配置blockio模式</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab103 ~]<span class="hljs-comment"># cat /etc/scst.conf</span><br>HANDLER vdisk_blockio &#123;<br>    DEVICE disk01 &#123;<br>        filename /dev/rbd/rbd/testrbd<br>        nv_cache 1<br>    &#125;<br>&#125;<br><br>TARGET_DRIVER iscsi &#123;<br>    enabled 1<br>    TARGET iqn.2022-05.net.vlnb:tgt &#123;<br>        LUN 0 disk01<br>        enabled 1<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure><p>启动服务</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab103 ~]<span class="hljs-comment"># systemctl start scst</span><br></code></pre></td></tr></table></figure>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>MAC-M1可以运行的opencloudos-8</title>
    <link href="/2023/01/31/MAC-M1%E5%8F%AF%E4%BB%A5%E8%BF%90%E8%A1%8C%E7%9A%84opencloudos-8.6/"/>
    <url>/2023/01/31/MAC-M1%E5%8F%AF%E4%BB%A5%E8%BF%90%E8%A1%8C%E7%9A%84opencloudos-8.6/</url>
    
    <content type="html"><![CDATA[<h1 id="MAC-M1可以运行的opencloudos-8-6"><a href="#MAC-M1可以运行的opencloudos-8-6" class="headerlink" title="MAC-M1可以运行的opencloudos-8.6"></a>MAC-M1可以运行的opencloudos-8.6</h1><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>在centos7比较晚的版本，以及centos8的aarch64的版本里面，内核的pagesize已经默认改成了64K，而mac M1的操作系统底层最大支持的pagesize是16K的，所以启动系统的时候，安装就直接闪退</p><p><img src="/media/16633226856220/16633228021078.jpg"></p><p>显示是上面的效果，这个在centos系统里面都有这个问题，fedora里面就还是维持4k，没有问题</p><span id="more"></span><h2 id="问题分析"><a href="#问题分析" class="headerlink" title="问题分析"></a>问题分析</h2><p>操作系统安装的时候会加载一个内核，这个地方需要修改的地方</p><ul><li>initrd.gz</li><li>vmlinuz<br>这两个需要修改为4K的</li></ul><p>然后加载安装的时候，会加载install.img，这个里面带的是系统安装过程需要的内核模块，所以这个地方也需要替换</p><p>进入安装系统以后，安装的内核rpm包也是64k的，那么也需要把这个地方替换掉</p><p>上面的步骤完成以后，打的新的iso 就是可以正常在M1下运行的ISO了</p><p><img src="/media/16633226856220/16633230068670.jpg"><br><img src="/media/16633226856220/16633239872905.jpg"></p><h2 id="说明"><a href="#说明" class="headerlink" title="说明"></a>说明</h2><p>M1是aarch64架构的，可以使用vmware fusion 做虚拟机，这个是免费的，不用购买pd来做，那么这个M1的架构有什么好处，就是如果在做国产化或者arm的适配的时候，有一个高性能的虚拟机会方便的很多，比如需要对opencloudos进行arm的其它包的开发的时候，如果物理机器资源不是那么充足的情况下，就可以独立在虚拟机里面完成适配，目前看就是这个pagesize影响了安装，其它的ubuntu，centos，也都做了定制ISO进行了发布</p><p>安装部分内核调用了fedora的一部分东西，所以分区如果不喜欢看到fedora字样，可以不用lvm分区即可，不过这个本身是虚拟机，问题不大，ISO其它部分没有做任何改动</p><h2 id="资源地址"><a href="#资源地址" class="headerlink" title="资源地址"></a>资源地址</h2><p>链接: <a href="https://pan.baidu.com/s/1-UBAAco0GQTDKLrQtl8IMA?pwd=x9bp">https://pan.baidu.com/s/1-UBAAco0GQTDKLrQtl8IMA?pwd=x9bp</a> 提取码: x9bp</p><h2 id="开源摘星计划"><a href="#开源摘星计划" class="headerlink" title="开源摘星计划"></a>开源摘星计划</h2><p>本文已参与「开源摘星计划」，欢迎正在阅读的你加入。活动链接：<a href="https://github.com/weopenprojects/WeOpen-Star">https://github.com/weopenprojects/WeOpen-Star</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>centos7yum源配置</title>
    <link href="/2023/01/31/centos7yum%E6%BA%90%E9%85%8D%E7%BD%AE/"/>
    <url>/2023/01/31/centos7yum%E6%BA%90%E9%85%8D%E7%BD%AE/</url>
    
    <content type="html"><![CDATA[<h1 id="centos7yum源配置"><a href="#centos7yum源配置" class="headerlink" title="centos7yum源配置"></a>centos7yum源配置</h1><h2 id="备注"><a href="#备注" class="headerlink" title="备注"></a>备注</h2><p>默认的源可能是国外源，下载比较慢，所以替换下地址为国内源比较好</p><h3 id="清华源"><a href="#清华源" class="headerlink" title="清华源"></a>清华源</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">mv</span> /etc/yum.repos.d/* /opt/ <br>vim /etc/yum.repos.d/my.repo<br></code></pre></td></tr></table></figure><p>内容如下:<br>有四个源，默认开启base和epel，其它两个自行开启</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs bash">[centos7-base]<br>name=centos7-base<br>baseurl=https://mirrors.tuna.tsinghua.edu.cn/centos/7/os/x86_64/<br>gpgcheck=0<br><span class="hljs-built_in">enable</span>=1<br><br>[centos7-updates]<br>name=centos7-updates<br>baseurl=https://mirrors.tuna.tsinghua.edu.cn/centos/7/updates/x86_64/<br>gpgcheck=0<br><span class="hljs-built_in">enable</span>=0<br><br>[centos7-extras]<br>name=centos7-extras<br>baseurl=https://mirrors.tuna.tsinghua.edu.cn/centos/7/extras/x86_64/<br>gpgcheck=0<br><span class="hljs-built_in">enable</span>=0<br><br>[centos7-epel]<br>name=centos7-epel<br>baseurl=https://mirrors.tuna.tsinghua.edu.cn/epel/7/x86_64/<br>gpgcheck=0<br><span class="hljs-built_in">enable</span>=1<br>[centos7-plus]<br>name=centos7-plus<br>baseurl=https://mirrors.tuna.tsinghua.edu.cn/centos/7/centosplus/x86_64/<br>gpgcheck=0<br>enabled=0<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>操作系统</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>ceph的设备关系获取</title>
    <link href="/2023/01/31/ceph%E7%9A%84%E8%AE%BE%E5%A4%87%E5%85%B3%E7%B3%BB%E8%8E%B7%E5%8F%96/"/>
    <url>/2023/01/31/ceph%E7%9A%84%E8%AE%BE%E5%A4%87%E5%85%B3%E7%B3%BB%E8%8E%B7%E5%8F%96/</url>
    
    <content type="html"><![CDATA[<h1 id="ceph的设备关系获取"><a href="#ceph的设备关系获取" class="headerlink" title="ceph的设备关系获取"></a>ceph的设备关系获取</h1><h2 id="需求"><a href="#需求" class="headerlink" title="需求"></a>需求</h2><p>因为ceph采用的是lvm管理，磁盘对应关系找起来没有以前方便，所以用一个脚本把当前机器的对应关系列出来</p><span id="more"></span><h2 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@node235 ceph]<span class="hljs-comment"># cat get.sh </span><br><span class="hljs-comment">#! /bin/bash</span><br>IFS=$<span class="hljs-string">&#x27;\n\n&#x27;</span> <br><span class="hljs-keyword">for</span> osd <span class="hljs-keyword">in</span> `ceph osd dump|grep osd|grep -v req|grep -v max|awk <span class="hljs-string">&#x27;&#123;print $1,$19&#125;&#x27;</span>`<br><span class="hljs-keyword">do</span> <br><br>osdnum=`<span class="hljs-built_in">echo</span>  <span class="hljs-variable">$osd</span>|awk <span class="hljs-string">&#x27;&#123;print $1&#125;&#x27;</span>`<br>osdid=`<span class="hljs-built_in">echo</span>  <span class="hljs-variable">$osd</span>|awk <span class="hljs-string">&#x27;&#123;print $2&#125;&#x27;</span>`<br>vgname=`lvdisplay |grep <span class="hljs-variable">$osdid</span> -A 1|<span class="hljs-built_in">tail</span>  -n 1|awk <span class="hljs-string">&#x27;&#123;print $3&#125;&#x27;</span>`<br>pvname=`pvdisplay |grep <span class="hljs-variable">$vgname</span> -B1|<span class="hljs-built_in">head</span> -n 1|awk <span class="hljs-string">&#x27;&#123;print $3&#125;&#x27;</span>`<br><br><span class="hljs-built_in">echo</span> <span class="hljs-variable">$osdnum</span>  <span class="hljs-variable">$osdid</span>  <span class="hljs-variable">$vgname</span>  <span class="hljs-variable">$pvname</span><br><br><span class="hljs-keyword">done</span><br><br></code></pre></td></tr></table></figure><h2 id="效果"><a href="#效果" class="headerlink" title="效果"></a>效果</h2><p>需要用bash命令运行</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@node235 ceph]<span class="hljs-comment"># bash get.sh </span><br>osd.0 c40d9963-4894-42bd-a7a8-c9d8c3280e9f ceph-f0f4f8ce-8d02-4111-97bd-fae7ff20a961 /dev/sdd1<br>osd.1 c4b7f141-51a0-4b97-80b5-e9681025f7db ceph-f786b5a2-82f4-4086-b0d1-7f5e6787c421 /dev/sdc1<br>osd.2 24f68dd2-5a4f-4b41-8775-ae0211a06e5d ceph-07e22954-93d0-4bb1-9ce3-59485eab1673 /dev/sdi1<br>osd.3 deed6a2e-456f-40be-bade-07a4b358e2c1 ceph-f1f76e61-d0d6-447b-ac26-a9a2d7fd2d5d /dev/sds1<br>osd.4 f325a546-a6c7-4610-9916-31062ec6decc ceph-5c1357a5-036a-4f2e-83a8-4463e4349264 /dev/sdf1<br>osd.5 3790748f-f212-4d46-8b72-b6c9ba007019 ceph-9874a1ac-9163-4936-a2f9-976260f19d6d /dev/sdr1<br>osd.6 50140786-a01d-47c4-bca4-68ad7730863c ceph-0e4af92c-9766-467c-a7d5-db4435aadd6b /dev/sdk1<br>osd.7 5e56f9a0-7637-477a-95cf-b995987e6270 ceph-c0de6d4b-1907-49d7-aa9b-194ccbc72465 /dev/sdb1<br>osd.8 261ea9d8-581b-42ec-9493-1df7b74394b7 ceph-54946fde-edc4-4cf1-b59a-375d69bfff7a /dev/sdm1<br>osd.9 a4212438-e463-48f3-be58-7668df524c35 ceph-805428c1-077c-4092-80f5-95b0ac9f5ea5 /dev/sdh1<br>osd.10 ebfb8caf-a3e6-4715-b992-9ca560104c4e ceph-7e9809bf-c4c0-4686-84f6-4e8639f02dac /dev/sde1<br>osd.11 7c310379-994d-4faa-9724-1ec1bf6d85bd ceph-3b602d70-99d0-496e-8b96-c34b36d973bc /dev/sdo1<br>osd.12 cd608ef9-d491-4d09-8f7f-ac82e026b321 ceph-2a8d2275-d307-41ab-9125-17dbe610559a /dev/sdg1<br>osd.14 782cb3ed-9fcd-40aa-9db3-1cd7babda3fb ceph-9117f02b-c6a0-43a0-ac3f-c0af75381c5f /dev/sdq1<br>osd.15 77275ae5-d953-44fc-8196-d6b31e005fb3 ceph-c30f789a-3213-49ce-8831-2296e5697119 /dev/sdp1<br>osd.16 1a325633-209d-4c3a-a89a-c90d6c26688f ceph-397d3bce-9f1a-4e33-9bd0-48f52804a7f6 /dev/sdj1<br></code></pre></td></tr></table></figure>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>ceph编译打包中断继续</title>
    <link href="/2023/01/31/ceph%E7%BC%96%E8%AF%91%E6%89%93%E5%8C%85%E4%B8%AD%E6%96%AD%E7%BB%A7%E7%BB%AD/"/>
    <url>/2023/01/31/ceph%E7%BC%96%E8%AF%91%E6%89%93%E5%8C%85%E4%B8%AD%E6%96%AD%E7%BB%A7%E7%BB%AD/</url>
    
    <content type="html"><![CDATA[<h1 id="ceph编译打包中断继续"><a href="#ceph编译打包中断继续" class="headerlink" title="ceph编译打包中断继续"></a>ceph编译打包中断继续</h1><h2 id="需求"><a href="#需求" class="headerlink" title="需求"></a>需求</h2><p>使用rpmbuild打包的时候，如果编译了很久，中间突然出问题，需要从头来处理，这个地方如果有多个错误需要处理的话，每次从头开始就比较浪费时间，我们可以让rpmbuild打包不去删除编译好的包，直接继续</p><span id="more"></span><h2 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h2><p>修改ceph.spec脚本</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@vcluster ceph]<span class="hljs-comment"># rpmbuild -bb ceph.spec</span>修改下ceph.spec文件，可以实现中断继续的功能%prep%autosetup -D -p1 -n ceph-12.2.13<span class="hljs-comment">#mkdir build</span><br></code></pre></td></tr></table></figure><p>上面的修改了以后，就可以中断继续编译了</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>gluster副本三配置down两个可写</title>
    <link href="/2023/01/31/gluster%E5%89%AF%E6%9C%AC%E4%B8%89%E9%85%8D%E7%BD%AEdown%E4%B8%A4%E4%B8%AA%E5%8F%AF%E5%86%99/"/>
    <url>/2023/01/31/gluster%E5%89%AF%E6%9C%AC%E4%B8%89%E9%85%8D%E7%BD%AEdown%E4%B8%A4%E4%B8%AA%E5%8F%AF%E5%86%99/</url>
    
    <content type="html"><![CDATA[<h1 id="gluster副本三配置down两个可写"><a href="#gluster副本三配置down两个可写" class="headerlink" title="gluster副本三配置down两个可写"></a>gluster副本三配置down两个可写</h1><h2 id="需求"><a href="#需求" class="headerlink" title="需求"></a>需求</h2><p>默认做了控制，副本三down两个就阻止写了，这个可以通过参数控制</p><h2 id="参数调整"><a href="#参数调整" class="headerlink" title="参数调整"></a>参数调整</h2><p>副本三，关闭两个可写</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">gluster volume <span class="hljs-built_in">set</span> testvolume cluster.quorum-type fixed<br>gluster volume <span class="hljs-built_in">set</span> testvolume cluster.quorum-count 1<br>gluster volume <span class="hljs-built_in">set</span> testvolume cluster.quorum-reads <span class="hljs-built_in">yes</span><br>gluster volume <span class="hljs-built_in">set</span>  testvolume network.ping-timeout 10<br></code></pre></td></tr></table></figure><span id="more"></span><p>副本二，关闭一个可写</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">gluster volume <span class="hljs-built_in">set</span> testvolume cluster.quorum-count 1<br>gluster volume <span class="hljs-built_in">set</span> testvolume cluster.quorum-reads <span class="hljs-built_in">yes</span><br>gluster volume <span class="hljs-built_in">set</span>  testvolume network.ping-timeout 10<br></code></pre></td></tr></table></figure><p>上面的区别是，副本3默认把cluster.quorum-type设置为auto了</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://docs.gluster.org/en/latest/Administrator-Guide/arbiter-volumes-and-quorum/">Arbiter volumes and quorum options in gluster</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>opencloudos-aarch64可以使用的vdbench</title>
    <link href="/2023/01/31/opencloudos-aarch64%E5%8F%AF%E4%BB%A5%E4%BD%BF%E7%94%A8%E7%9A%84vdbench/"/>
    <url>/2023/01/31/opencloudos-aarch64%E5%8F%AF%E4%BB%A5%E4%BD%BF%E7%94%A8%E7%9A%84vdbench/</url>
    
    <content type="html"><![CDATA[<h1 id="opencloudos-aarch64可以使用的vdbench"><a href="#opencloudos-aarch64可以使用的vdbench" class="headerlink" title="opencloudos-aarch64可以使用的vdbench"></a>opencloudos-aarch64可以使用的vdbench</h1><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>vdbench是一个I&#x2F;O工作负载生成器，通常用于验证数据完整性和测试存储性能。它可以运行在windows、linux环境，可用于测试文件系统或块设备基准性能。很多大型的测试都是采用这个工具进行测试的，能过模拟很多不同的负载</p><p>vdbench采用java编写的，能够比较好的支持跨平台，但是内部引用的库是会挑系统平台的，默认的zip包里面就没有带aarch64的库，需要自己获取源码，然后在aarch64上编译库文件，然后再放到zip包里面，本篇结尾的资源就是提供的这个包</p><h2 id="问题现象"><a href="#问题现象" class="headerlink" title="问题现象"></a>问题现象</h2><h3 id="安装基本软件"><a href="#安装基本软件" class="headerlink" title="安装基本软件"></a>安装基本软件</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">yum install java-1.8.0-openjdk<br></code></pre></td></tr></table></figure><h3 id="解压"><a href="#解压" class="headerlink" title="解压"></a>解压</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">unzip vdbench50407.zip -d vdbench50407<br></code></pre></td></tr></table></figure><span id="more"></span><h3 id="执行测试"><a href="#执行测试" class="headerlink" title="执行测试"></a>执行测试</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@172 ~]<span class="hljs-comment"># ./vdbench50407/vdbench -t</span><br><br><br>Copyright (c) 2000, 2018, Oracle and/or its affiliates. All rights reserved.<br>Vdbench distribution: vdbench50407 Tue June 05  9:49:29 MDT 2018<br>For documentation, see <span class="hljs-string">&#x27;vdbench.pdf&#x27;</span>.<br><br>08:17:47.598 Created output directory <span class="hljs-string">&#x27;/root/output&#x27;</span><br>08:17:47.653 input argument scanned: <span class="hljs-string">&#x27;-f/tmp/parmfile&#x27;</span><br>08:17:47.690 Starting slave: /root/vdbench50407/vdbench SlaveJvm -m localhost -n localhost-10-220930-08.17.47.589 -l localhost-0 -p 5570<br>08:17:47.716<br>08:17:47.716 File /root/vdbench50407/linux/aarch64.so does not exist.<br>08:17:47.716 This may be an OS that a shared library currently<br>08:17:47.716 is not available <span class="hljs-keyword">for</span>. You may have to <span class="hljs-keyword">do</span> your own compile.<br>08:17:47.716 t: java.lang.UnsatisfiedLinkError: Can<span class="hljs-string">&#x27;t load library: /root/vdbench50407/linux/aarch64.so</span><br><span class="hljs-string">08:17:47.716</span><br><span class="hljs-string">08:17:47.716 Loading of shared library /root/vdbench50407/linux/aarch64.so failed.</span><br><span class="hljs-string">08:17:47.716 There may be issues related to a cpu type not being</span><br><span class="hljs-string">08:17:47.716 acceptable to Vdbench, e.g. MAC PPC vs. X86</span><br><span class="hljs-string">08:17:47.716 Contact me at the Oracle Vdbench Forum for support.</span><br><span class="hljs-string">08:17:47.716</span><br><span class="hljs-string">08:17:48.219</span><br><span class="hljs-string">08:17:48.219 Failure loading shared library</span><br></code></pre></td></tr></table></figure><p>可以看到关键的信息</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">Can<span class="hljs-string">&#x27;t load library: /root/vdbench50407/linux/aarch64.so</span><br></code></pre></td></tr></table></figure><p>这个就是缺少的库文件</p><h2 id="使用修改好的包"><a href="#使用修改好的包" class="headerlink" title="使用修改好的包"></a>使用修改好的包</h2><h3 id="解压-1"><a href="#解压-1" class="headerlink" title="解压"></a>解压</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@172 ~]<span class="hljs-comment"># unzip vdbench50407-opencloudos-aarch64.zip</span><br></code></pre></td></tr></table></figure><h3 id="执行测试-1"><a href="#执行测试-1" class="headerlink" title="执行测试"></a>执行测试</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@172 ~]<span class="hljs-comment"># ./vdbench50407/vdbench -t</span><br><br><br>Copyright (c) 2000, 2018, Oracle and/or its affiliates. All rights reserved.<br>Vdbench distribution: vdbench50407 Tue June 05  9:49:29 MDT 2018<br>For documentation, see <span class="hljs-string">&#x27;vdbench.pdf&#x27;</span>.<br><br>08:19:32.437 input argument scanned: <span class="hljs-string">&#x27;-f/tmp/parmfile&#x27;</span><br>08:19:32.475 Starting slave: /root/vdbench50407/vdbench SlaveJvm -m localhost -n localhost-10-220930-08.19.32.370 -l localhost-0 -p 5570<br>08:19:32.676 All slaves are now connected<br>08:19:34.003 Starting RD=rd1; I/O rate: 100; elapsed=5; For loops: None<br><br>Sep 30, 2022    interval        i/o   MB/sec   bytes   <span class="hljs-built_in">read</span>     resp     <span class="hljs-built_in">read</span>    write     <span class="hljs-built_in">read</span>    write     resp  queue  cpu%  cpu%<br>                               rate  1024**2     i/o    pct     time     resp     resp      max      max   stddev  depth sys+u   sys<br>08:19:35.102           1       67.0     0.07    1024  47.76    0.074    0.058    0.088     0.12     0.29    0.048    0.0  17.1   4.7<br>08:19:36.013           2      110.0     0.11    1024  55.45    0.046    0.037    0.056     0.09     0.15    0.026    0.0  11.5   1.0<br>08:19:37.011           3       91.0     0.09    1024  47.25    0.069    0.054    0.083     0.11     0.15    0.033    0.0   9.0   1.0<br>08:19:38.010           4      108.0     0.11    1024  51.85    0.061    0.046    0.077     0.15     0.74    0.072    0.0   7.1   2.0<br>08:19:39.026           5       90.0     0.09    1024  57.78    0.062    0.051    0.076     0.22     0.30    0.042    0.0   7.1   1.0<br>08:19:39.058     avg_2-5       99.8     0.10    1024  53.13    0.059    0.046    0.073     0.22     0.74    0.048    0.0   8.6   1.3<br>08:19:39.849 Vdbench execution completed successfully. Output directory: /root/output<br></code></pre></td></tr></table></figure><p>可以看到可以正常运行了</p><h2 id="资源地址"><a href="#资源地址" class="headerlink" title="资源地址"></a>资源地址</h2><p>vdbench50407-opencloudos-aarch64.zip下载地址：</p><blockquote><p>链接: <a href="https://pan.baidu.com/s/1YdPWJnhdl3XTZZIhAX6NIA?pwd=gwm3">https://pan.baidu.com/s/1YdPWJnhdl3XTZZIhAX6NIA?pwd=gwm3</a> 提取码: gwm3</p></blockquote>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>opencloudos容器镜像优化</title>
    <link href="/2023/01/31/opencloudos%E5%AE%B9%E5%99%A8%E9%95%9C%E5%83%8F%E4%BC%98%E5%8C%96/"/>
    <url>/2023/01/31/opencloudos%E5%AE%B9%E5%99%A8%E9%95%9C%E5%83%8F%E4%BC%98%E5%8C%96/</url>
    
    <content type="html"><![CDATA[<h1 id="opencloudos容器镜像优化"><a href="#opencloudos容器镜像优化" class="headerlink" title="opencloudos容器镜像优化"></a>opencloudos容器镜像优化</h1><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>目前因为官方的容器也在移植阶段，并且系统是刚出来的，各方面的应该都会慢慢完善，因为之前也做了相关的移植，所以移植的作为备选的即可</p><p>测试发现官方的容器目前还有一些需要处理一些东西，本篇就是简单的介绍怎么用官方的容器，以及存在的问题，最后提供下我自己移植好的容器</p><h2 id="官方容器使用方法"><a href="#官方容器使用方法" class="headerlink" title="官方容器使用方法"></a>官方容器使用方法</h2><h3 id="拉取镜像"><a href="#拉取镜像" class="headerlink" title="拉取镜像"></a>拉取镜像</h3><p>这个操作是使用容器的常规操作了，官方也都发布到了docker hub里面，直接拉取即可</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 /]<span class="hljs-comment"># docker pull opencloudos/opencloudos:8.6</span><br>[root@lab101 /]<span class="hljs-comment"># docker image ls</span><br>docker.io/opencloudos/opencloudos   8.6                 85381566b97e        7 days ago               280 MB<br></code></pre></td></tr></table></figure><p>可以看到官方的为280MB</p><span id="more"></span><h3 id="启动容器"><a href="#启动容器" class="headerlink" title="启动容器"></a>启动容器</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 /]<span class="hljs-comment"># docker run --name opencloudos  --net=host  --hostname=opencloudos  -dit   --privileged=true  opencloudos/opencloudos:8.6   /sbin/init</span><br>bbe06493e7b2305b430c635c23b06b6eeca7d0df38ebfa6a9ab8428a39f1b11c<br></code></pre></td></tr></table></figure><h3 id="进入容器"><a href="#进入容器" class="headerlink" title="进入容器"></a>进入容器</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 /]<span class="hljs-comment"># docker exec -it opencloudos /bin/bash</span><br>bash-4.4<span class="hljs-comment">#</span><br></code></pre></td></tr></table></figure><p>可以看到进入容器是显示的bash-4.4不是主机名<br>可以看到</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">bash-4.4<span class="hljs-comment"># ls /root/</span><br></code></pre></td></tr></table></figure><p>root下面是空的<br>应该是要有下面的</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@centos8 /]<span class="hljs-comment"># ls /root/.</span><br>./             ../            .bash_history  .bash_logout   .bash_profile  .bashrc        .cshrc         .tcshrc<br></code></pre></td></tr></table></figure><p>检查常用包</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">bash-4.4<span class="hljs-comment"># rpm -qa|grep hostname</span><br>bash-4.4<span class="hljs-comment"># rpm -qa|grep vi</span><br>device-mapper-1.02.181-3.oc8.x86_64<br>device-mapper-libs-1.02.181-3.oc8.x86_64<br></code></pre></td></tr></table></figure><p>发现vi和hostname命令都没加进去，这里基本的包还是保持比较好，这个在centos8的官方容器里面都做了保留</p><p>上面的主机名的处理也可以自己修改&#x2F;root&#x2F;.下面做处理做恢复，相关的包也可以自己安装下即可</p><h2 id="优化"><a href="#优化" class="headerlink" title="优化"></a>优化</h2><p>基于官方容器的优化，这个实际上我是基于操作系统，然后按centos8的容器包的标准，以及根据centos8的裁剪力度，来做的容器包</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 /]<span class="hljs-comment"># docker pull zphj1987/opencloudos:8.6</span><br></code></pre></td></tr></table></figure><p>使用上面的就可以拿到opencloudos的8.6的容器版本，容器大小做到了跟官方的centos8基本一致的大小</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 /]<span class="hljs-comment"># docker pull zphj1987/opencloudos:8.6</span><br>Trying to pull repository docker.io/zphj1987/opencloudos ...<br>8.6: Pulling from docker.io/zphj1987/opencloudos<br>e6ad0a38566d: Downloading [&gt;                                                  ] 528.9 kB/87.57 MB<br></code></pre></td></tr></table></figure><p>容器iamge大小</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@opencloud ~]<span class="hljs-comment"># docker image ls</span><br>zphj1987/opencloudos   8.6              8779b4d2cd5c   5 hours ago     227MB<br></code></pre></td></tr></table></figure><p>基本跟centos官方的 231 MB大小一致</p><h3 id="做容器过程的一个经验"><a href="#做容器过程的一个经验" class="headerlink" title="做容器过程的一个经验"></a>做容器过程的一个经验</h3><p>如果改了容器，不要在原来的基础上面commit,直接export出来，然后创建成新的容器，这样是一个比较简单的控制容器大小的方法，一直commit会叠加容器的大小</p><h2 id="对待问题的方法"><a href="#对待问题的方法" class="headerlink" title="对待问题的方法"></a>对待问题的方法</h2><ul><li>发现问题</li><li>提出方案</li><li>解决问题</li></ul><h2 id="开源摘星计划"><a href="#开源摘星计划" class="headerlink" title="开源摘星计划"></a>开源摘星计划</h2><p>本文已参与「开源摘星计划」，欢迎正在阅读的你加入。活动链接：<a href="https://github.com/weopenprojects/WeOpen-Star">https://github.com/weopenprojects/WeOpen-Star</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>rock5b可以运行的UOS操作系统</title>
    <link href="/2023/01/31/rock5b%E5%8F%AF%E4%BB%A5%E8%BF%90%E8%A1%8C%E7%9A%84UOS%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"/>
    <url>/2023/01/31/rock5b%E5%8F%AF%E4%BB%A5%E8%BF%90%E8%A1%8C%E7%9A%84UOS%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/</url>
    
    <content type="html"><![CDATA[<h1 id="rock5b可以运行的UOS操作系统"><a href="#rock5b可以运行的UOS操作系统" class="headerlink" title="rock5b可以运行的UOS操作系统"></a>rock5b可以运行的UOS操作系统</h1><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>uos是国产操作系统里面桌面做的顶尖的操作系统了，前身是deepin，基于的是debian，ubuntu系的做的桌面，国产操作系统的优势是会做一些办公工具相关的适配</p><p>查了下资料，pro版本的试用期是90天，没有授权的话，只是不能使用应用商店，其它的使用没有区别</p><h2 id="相关资源"><a href="#相关资源" class="headerlink" title="相关资源"></a>相关资源</h2><p>这个img为初始的版本，比较大，有12G的，看了下内部装完系统占用有7G左右，这个大小方面和使用后续会持续发布新的版本，内核相关的后面看下是否也更新替换下，目前采用的是官方提供的</p><h2 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h2><p>由于网盘的原因，采用的分卷压缩的，全部分卷下载完成后，使用360zip进行解压即可</p><span id="more"></span><h3 id="登陆"><a href="#登陆" class="headerlink" title="登陆"></a>登陆</h3><p>用户名密码：<br>root<br>123456</p><p>默认为命令行界面，执行下startx即可进入桌面</p><p>这个地方有个问题，设置为图形界面启动后，tty1会成为日志输出，tty2才可以登陆，设置文本登陆，然后再执行startx即可<br><img src="/images/blog/media/16659908084571/16659915417239.jpg"></p><p>登陆以后</p><p><img src="/images/blog/media/16659908084571/16659941281094.jpg"></p><h2 id="版本发布说明"><a href="#版本发布说明" class="headerlink" title="版本发布说明"></a>版本发布说明</h2><p>rock-5b-uos-202210171400<br>发布说明：<br>    初始版本，可以使用<br>资源地址：<br>    链接: <a href="https://pan.baidu.com/s/1W-FwrCBzPAh3Ty4HrcX1Fw?pwd=3vmj">https://pan.baidu.com/s/1W-FwrCBzPAh3Ty4HrcX1Fw?pwd=3vmj</a> 提取码: 3vmj</p>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>rockpi5b安装系统-microsd</title>
    <link href="/2023/01/31/rockpi5b%E5%AE%89%E8%A3%85%E7%B3%BB%E7%BB%9F-microsd/"/>
    <url>/2023/01/31/rockpi5b%E5%AE%89%E8%A3%85%E7%B3%BB%E7%BB%9F-microsd/</url>
    
    <content type="html"><![CDATA[<h1 id="rock5b安装系统-microsd"><a href="#rock5b安装系统-microsd" class="headerlink" title="rock5b安装系统-microsd"></a>rock5b安装系统-microsd</h1><h2 id="系统安装到哪里"><a href="#系统安装到哪里" class="headerlink" title="系统安装到哪里"></a>系统安装到哪里</h2><p>从之前的接口介绍里面我们介绍了，存储接口有:</p><ul><li>micro sd</li><li>emmc 存储</li><li>nvme m.2 ssd</li></ul><p>我们看下这三种设备<br><img src="/media/rk3588/%E5%9F%BA%E7%A1%80%E7%AF%87/assets/IMG_1234.JPG" alt="IMG_1234"></p><p>从安装的系统来看，目前主要有两大系统类型:</p><ul><li>安卓操作系统</li><li>linux操作系统</li></ul><p>不同的设备有不同的安装方式,同一种设备也有不同的安装方式，这里都做下相关的介绍</p><h2 id="microsd卡"><a href="#microsd卡" class="headerlink" title="microsd卡"></a>microsd卡</h2><p>这个直接买卡就行，性能有差别，性能有需求的话，可以买贵一点的，都有提供指标参数</p><p><img src="/media/rk3588/%E5%9F%BA%E7%A1%80%E7%AF%87/assets/16687655213008.jpg"></p><blockquote><p><a href="https://wiki.radxa.com/Rock5/downloads">https://wiki.radxa.com/Rock5/downloads</a><br>官方目前提供了个系统，一个安卓的，一个debian的一个ubuntu的，应该也可以自己构建armbian，这里我们以ubuntu和安卓举例子</p></blockquote><h2 id="刷ubuntu系统"><a href="#刷ubuntu系统" class="headerlink" title="刷ubuntu系统"></a>刷ubuntu系统</h2><p>下载地址：</p><blockquote><p><a href="https://github.com/radxa/debos-radxa/releases/download/20221031-1045/rock-5b-ubuntu-focal-server-arm64-20221031-1328-gpt.img.xz">https://github.com/radxa/debos-radxa/releases/download/20221031-1045/rock-5b-ubuntu-focal-server-arm64-20221031-1328-gpt.img.xz</a></p></blockquote><p>从change log里面点进去可以看到，github上面更新了，但是网站上面可能没更新，这个主意看下版本，目前最新ubuntu适配是11月18日的</p><blockquote><p><a href="https://github.com/radxa/debos-radxa/releases/download/20221118-1212/rock-5b-ubuntu-focal-server-arm64-20221118-1434-gpt.img.xz">https://github.com/radxa/debos-radxa/releases/download/20221118-1212/rock-5b-ubuntu-focal-server-arm64-20221118-1434-gpt.img.xz</a></p></blockquote><p>从日期来看是2022年10月31日构建的，比较新了，官方也会根据一些实际情况进行更新推送，这里就用这个版本看看</p><p>官方推荐使用的是：<br>Etcher - A user friendly Image Writer</p><p><img src="/media/rk3588/%E5%9F%BA%E7%A1%80%E7%AF%87/assets/2022-11-21-16-08-59.png"></p><p>刷系统实际上就是把官方做好的img文件，刷到我们提供的磁盘上面，内部的分区和软件都是装好的，这个img我们可以自己制作或者修改，这个后面会讲</p><p><img src="/media/rk3588/%E5%9F%BA%E7%A1%80%E7%AF%87/assets/2022-11-21-16-14-25.png"></p><p>刷系统完成后就是这样的</p><p>默认的用户名密码</p><ul><li>用户名: rock</li><li>密码:  rock</li></ul><p>登陆上去后自己设置下root用户名密码,ssh的默认不支持ssh密码登陆，修改下能ssh登陆</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs bash">root@rock-5b:~<span class="hljs-comment"># cat /etc/os-release</span><br>NAME=<span class="hljs-string">&quot;Ubuntu&quot;</span><br>VERSION=<span class="hljs-string">&quot;20.04 LTS (Focal Fossa)&quot;</span><br>ID=ubuntu<br>ID_LIKE=debian<br>PRETTY_NAME=<span class="hljs-string">&quot;Ubuntu 20.04 LTS&quot;</span><br>VERSION_ID=<span class="hljs-string">&quot;20.04&quot;</span><br>HOME_URL=<span class="hljs-string">&quot;https://www.ubuntu.com/&quot;</span><br>SUPPORT_URL=<span class="hljs-string">&quot;https://help.ubuntu.com/&quot;</span><br>BUG_REPORT_URL=<span class="hljs-string">&quot;https://bugs.launchpad.net/ubuntu/&quot;</span><br>PRIVACY_POLICY_URL=<span class="hljs-string">&quot;https://www.ubuntu.com/legal/terms-and-policies/privacy-policy&quot;</span><br>VERSION_CODENAME=focal<br>UBUNTU_CODENAME=focal<br>root@rock-5b:~<span class="hljs-comment"># uname -a</span><br>Linux rock-5b 5.10.66-28-rockchip-gc428536281d6 <span class="hljs-comment">#rockchip SMP Fri Nov 18 08:10:28 UTC 2022 aarch64 aarch64 aarch64 GNU/Linux</span><br>root@rock-5b:~<span class="hljs-comment"># lspci</span><br>0004:40:00.0 PCI bridge: Fuzhou Rockchip Electronics Co., Ltd Device 3588 (rev 01)<br>0004:41:00.0 Ethernet controller: Realtek Semiconductor Co., Ltd. RTL8125 2.5GbE Controller (rev 04)<br></code></pre></td></tr></table></figure><p>可以看下版本和内核，到这里系统就安装完毕了</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>rockpi5b硬件组成</title>
    <link href="/2023/01/31/rockpi5b%E7%A1%AC%E4%BB%B6%E7%BB%84%E6%88%90/"/>
    <url>/2023/01/31/rockpi5b%E7%A1%AC%E4%BB%B6%E7%BB%84%E6%88%90/</url>
    
    <content type="html"><![CDATA[<h1 id="rk3588硬件构成"><a href="#rk3588硬件构成" class="headerlink" title="rk3588硬件构成"></a>rk3588硬件构成</h1><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>rk3588是瑞芯微的一套新的arm64的板子，上一代用的比较多的是rk3399，新的硬件设备比之前更强大，接口更多，本系列就是介绍相关的硬件软件的一些资料，后面会根据不同的使用进行分篇的介绍</p><p>很多资料官网有提供，细节上面会做一些补充，更方便操作和理解</p><h2 id="用的板子"><a href="#用的板子" class="headerlink" title="用的板子"></a>用的板子</h2><p>手上用的是rockpi5b，之前在搞rk的板子的时候，发现rockpi的内核是做的最好的，瑞芯微提供的是4.4的内核，而在rockpi4b的时候，这个板子可以运行比较高的版本的内核，对于对内核有一定的需求的，这个还是比较好的</p><p>本系列是基于rockpi5b进行记录的</p><h2 id="rockpi5b的图片"><a href="#rockpi5b的图片" class="headerlink" title="rockpi5b的图片"></a>rockpi5b的图片</h2><h3 id="开发版"><a href="#开发版" class="headerlink" title="开发版"></a>开发版</h3><p>高清照片<br><img src="/media/rk3588/%E5%9F%BA%E7%A1%80%E7%AF%87/assets/IMG_1227-1.JPG" alt="IMG_1227"><br><img src="/media/rk3588/%E5%9F%BA%E7%A1%80%E7%AF%87/assets/IMG_1228.JPG" alt="IMG_1228"></p><p>外包装有一些参数的介绍</p><p><img src="/media/rk3588/%E5%9F%BA%E7%A1%80%E7%AF%87/assets/IMG_1219.JPG" alt="IMG_1219"><br>左下角有个rtc的时钟的插口，插个2pin的小电池就行，内存可以看到是sk hynix的，下面有个编号2022.08.29,不清楚是版本日期，还是生产日期，板子默认没带蓝牙和wifi模块，上面的那个m2接口可以插蓝牙wifi模块的</p><p><img src="/media/rk3588/%E5%9F%BA%E7%A1%80%E7%AF%87/assets/IMG_1220.JPG" alt="IMG_1220"></p><p>这个是背面的照片，可以emmc存储，也可以插m.2的nvme的ssd，ssd之前有就没买，emmc的买了一个，左上方有个插micro sd卡的，这个手上之前也有</p><p>可以看到，存储接口这里有三种方式的物理接口</p><ul><li>nvme SSD m.2</li><li>micro sd</li><li>emmc卡</li></ul><p><img src="/media/rk3588/%E5%9F%BA%E7%A1%80%E7%AF%87/assets/IMG_1222.JPG" alt="IMG_1222"><br>对外的一些接口，声音设备，typec口（充电或者自己加扩展卡增加口），两个hdmi的输出口，usb口几个，网口一个</p><p><img src="/media/rk3588/%E5%9F%BA%E7%A1%80%E7%AF%87/assets/IMG_1223.JPG" alt="IMG_1223"><br>上面有个hdmi in的口，这个之前的rockpi4b不带这个的，这个可以拿来当采集卡使用，还没有实验效果</p><p><img src="/media/rk3588/%E5%9F%BA%E7%A1%80%E7%AF%87/assets/IMG_1224.JPG" alt="IMG_1224"></p><h3 id="壳子"><a href="#壳子" class="headerlink" title="壳子"></a>壳子</h3><p>官方有提供壳子<br>黑色的地方是金属的<br>绿色的地方是塑料的</p><p><img src="/media/rk3588/%E5%9F%BA%E7%A1%80%E7%AF%87/assets/IMG_1232.JPG" alt="IMG_1232"><br><img src="/media/rk3588/%E5%9F%BA%E7%A1%80%E7%AF%87/assets/IMG_1231.JPG" alt="IMG_1231"><br><img src="/media/rk3588/%E5%9F%BA%E7%A1%80%E7%AF%87/assets/IMG_1230.JPG" alt="IMG_1230"><br><img src="/media/rk3588/%E5%9F%BA%E7%A1%80%E7%AF%87/assets/IMG_1229.JPG" alt="IMG_1229"></p><p>目前还没组装起来，后面看下散热的效果，现在摸这个外壳是冰的，非常冷的感觉</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>rockpi5b配件</title>
    <link href="/2023/01/31/rockpi5b%E9%85%8D%E4%BB%B6/"/>
    <url>/2023/01/31/rockpi5b%E9%85%8D%E4%BB%B6/</url>
    
    <content type="html"><![CDATA[<h1 id="rockpi5b配件"><a href="#rockpi5b配件" class="headerlink" title="rockpi5b配件"></a>rockpi5b配件</h1><h2 id="配件"><a href="#配件" class="headerlink" title="配件"></a>配件</h2><p>因为开发版默认购买的时候是一个裸板子，后续的其它工作，需要配件来支撑完成，所以本篇就是记录这些配件的，以及可能的用途，有的配件并不是必要，可以通过其它方式实现，这里会把所有用到的配件都记录下来</p><h3 id="电源"><a href="#电源" class="headerlink" title="电源"></a>电源</h3><p>这个电源是pd电源，也就是一个typec接口的电源，通过协商电压进行供电的，而协商电压是有内核里面控制的，最新版的内核应该把协商电压处理好了，这个协商就是低电压高电压，都可能能够运行，但是跟那个电源关系比较大，目前我手上确定可以用的电源如下，这个官方的群里面有提供一个收集的兼容列表，这里就记录我手上确定可以用的和确定不可以用的：</p><ul><li>oneplus 8T的pd充电器(不行)</li><li>联想（Lenovo）CC100W充电器 氮化镓充电器 便携适配器（可以）</li></ul><h3 id="存储设备"><a href="#存储设备" class="headerlink" title="存储设备"></a>存储设备</h3><ul><li>micro sd</li><li>emmc 存储</li><li>nvme m.2 ssd</li></ul><p><img src="/media/rk3588/%E5%9F%BA%E7%A1%80%E7%AF%87/assets/IMG_1234.JPG" alt="IMG_1234"></p><p>上面三种设备在rockpi5b上面是有接口的，但是需要往里面写系统的话，就需要相关的连接卡了，<img src="/media/rk3588/%E5%9F%BA%E7%A1%80%E7%AF%87/assets/IMG_1235.JPG" alt="IMG_1235"><br>上面的那个是nvme的盒子，下面的microsd卡可以用两种方式转换，如果你的电脑带sd卡槽的话，可以用上面的，如果没有的话，就买个小的usb就行，emmc的转接卡<img src="/media/rk3588/%E5%9F%BA%E7%A1%80%E7%AF%87/assets/IMG_1237.JPG" alt="IMG_1237"></p><p>可以看到官方的这个emmc的转接卡是把emmc转接为microsd卡的接口，然后再通过转大sd卡，再通过卡槽来识别的，可以买一个usb转接卡支持sd和micro sd卡的，这样方便一点，如果电脑没有sd卡槽的话</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>ubuntu-server安装桌面</title>
    <link href="/2023/01/31/ubuntu-server%E5%AE%89%E8%A3%85%E6%A1%8C%E9%9D%A2/"/>
    <url>/2023/01/31/ubuntu-server%E5%AE%89%E8%A3%85%E6%A1%8C%E9%9D%A2/</url>
    
    <content type="html"><![CDATA[<h1 id="ubuntu-server安装桌面"><a href="#ubuntu-server安装桌面" class="headerlink" title="ubuntu-server安装桌面"></a>ubuntu-server安装桌面</h1><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>默认安装的是不带图形界面的，我们自己安装下桌面</p><h2 id="修改源文件"><a href="#修改源文件" class="headerlink" title="修改源文件"></a>修改源文件</h2><p>默认也是用的官方的ubuntu的源，我们修改为国内的源要快一些</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">root@rock-5b:~<span class="hljs-comment"># cp /etc/apt/sources.list /etc/apt/sources.list.bk</span><br>root@rock-5b:~<span class="hljs-comment"># sed -i &#x27;s/ports.ubuntu.com/mirrors.tuna.tsinghua.edu.cn/g&#x27; /etc/apt/sources.list</span><br>root@rock-5b:~<span class="hljs-comment"># apt-get update</span><br></code></pre></td></tr></table></figure><p>或者华为源（最近清华源有点慢）</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">root@rock-5b:~<span class="hljs-comment"># sed -i &#x27;s/ports.ubuntu.com/repo.huaweicloud.com/g&#x27; /etc/apt/sources.list</span><br></code></pre></td></tr></table></figure><h2 id="安装图形界面"><a href="#安装图形界面" class="headerlink" title="安装图形界面"></a>安装图形界面</h2><p>这里我们安装gnome</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">root@rock-5b:~<span class="hljs-comment"># apt install ubuntu-desktop</span><br></code></pre></td></tr></table></figure><p>安装完成以后进行重启，就会自动进入图形界面</p><p>图形界面如下：</p><p><img src="/media/rk3588/%E5%9F%BA%E7%A1%80%E7%AF%87/assets/ubuntu-print.png" alt="ubuntu"></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>vdbench测试生成器</title>
    <link href="/2023/01/31/vdbench%E6%B5%8B%E8%AF%95%E7%94%9F%E6%88%90%E5%99%A8/"/>
    <url>/2023/01/31/vdbench%E6%B5%8B%E8%AF%95%E7%94%9F%E6%88%90%E5%99%A8/</url>
    
    <content type="html"><![CDATA[<h1 id="vdbench测试生成器"><a href="#vdbench测试生成器" class="headerlink" title="vdbench测试生成器"></a>vdbench测试生成器</h1><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>vdbench测试是根据配置文件进行测试的，有的时候我们需要循环的进行不同的测试用例进行测试，并且进行数据的大量填充，大小文件进行交错的填充来检测环境的稳定性</p><h2 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h2><p>我们定义几组不同的配置文件，然后根据配置文件定义测试的次数，然后定义总次数，以及测试目录，然后生成配置文件，再使用脚本按顺序进行测试并记录结果</p><h2 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h2><h3 id="写几个模版配置文件"><a href="#写几个模版配置文件" class="headerlink" title="写几个模版配置文件"></a>写几个模版配置文件</h3><p>这个没有什么特殊的地方，根据自己的需要编写即可<br>测试的目录留空</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">anchor=<br></code></pre></td></tr></table></figure><h3 id="编写run-ini配置文件"><a href="#编写run-ini配置文件" class="headerlink" title="编写run.ini配置文件"></a>编写run.ini配置文件</h3><p>安装解析ini的软件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">yum install crudini<br></code></pre></td></tr></table></figure><span id="more"></span><p>这个是用于定义上面配置文件的运行组合的</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs bash">[runcase]<br>run1=base-big-1M<br>run2=base-mid-500K<br>run3=base-small-100K<br>[runtime]<br>run1=1<br>run2=2<br>run3=1<br>total=8<br>[rundir]<br>rundir=/mnt<br></code></pre></td></tr></table></figure><h3 id="生成测试配置文件"><a href="#生成测试配置文件" class="headerlink" title="生成测试配置文件"></a>生成测试配置文件</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-meta">#! /bin/sh</span><br><span class="hljs-built_in">unalias</span> <span class="hljs-built_in">cp</span><br>i=0<br>rundir=`crudini --get run.ini rundir rundir`<br>total=`crudini --get run.ini runtime total`<br><span class="hljs-keyword">while</span> (( total &gt; <span class="hljs-number">0</span> ))<br><span class="hljs-keyword">do</span><br>    <span class="hljs-comment">#echo $num</span><br>    <span class="hljs-keyword">for</span> runcase <span class="hljs-keyword">in</span> `crudini --get run.ini runcase`<br>    <span class="hljs-keyword">do</span><br>        runtime=`crudini --get run.ini runtime <span class="hljs-variable">$runcase</span>`<br>        <span class="hljs-keyword">for</span> run <span class="hljs-keyword">in</span>  `<span class="hljs-built_in">seq</span> <span class="hljs-variable">$runtime</span>`<br>        <span class="hljs-keyword">do</span>   <br>            i=$(( <span class="hljs-variable">$i</span> + <span class="hljs-number">1</span> ))<br>            <span class="hljs-comment">#echo $run</span><br>            myrun=`crudini --get run.ini runcase <span class="hljs-variable">$runcase</span>`<br>            mytest=<span class="hljs-variable">$myrun</span>-<span class="hljs-variable">$i</span><br>            <span class="hljs-built_in">echo</span> <span class="hljs-variable">$mytest</span><br>            <span class="hljs-built_in">mkdir</span> runcase/<span class="hljs-variable">$mytest</span> -p<br>            total=$(( <span class="hljs-variable">$total</span> - <span class="hljs-number">1</span> ))<br>            <span class="hljs-built_in">cp</span> -ra <span class="hljs-variable">$myrun</span> runcase/<span class="hljs-variable">$mytest</span>/case.conf<br>            sed -i <span class="hljs-string">&quot;s#anchor=*,#anchor=<span class="hljs-variable">$&#123;rundir&#125;</span>/<span class="hljs-variable">$&#123;mytest&#125;</span>,#&quot;</span> runcase/<span class="hljs-variable">$mytest</span>/case.conf<br>        <span class="hljs-keyword">done</span><br>    <span class="hljs-keyword">done</span><br><br><span class="hljs-keyword">done</span><br></code></pre></td></tr></table></figure><h3 id="运行测试"><a href="#运行测试" class="headerlink" title="运行测试"></a>运行测试</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-meta">#! /bin/sh</span><br>vdbench=/root/vdbench50407/vdbench<br><span class="hljs-built_in">unalias</span> <span class="hljs-built_in">cp</span><br>i=0<br>rundir=`crudini --get run.ini rundir rundir`<br>total=`crudini --get run.ini runtime total`<br><span class="hljs-keyword">while</span> (( total &gt; <span class="hljs-number">0</span> ))<br><span class="hljs-keyword">do</span><br>    <span class="hljs-comment">#echo $num</span><br>    <span class="hljs-keyword">for</span> runcase <span class="hljs-keyword">in</span> `crudini --get run.ini runcase`<br>    <span class="hljs-keyword">do</span><br>        runtime=`crudini --get run.ini runtime <span class="hljs-variable">$runcase</span>`<br>        <span class="hljs-keyword">for</span> run <span class="hljs-keyword">in</span>  `<span class="hljs-built_in">seq</span> <span class="hljs-variable">$runtime</span>`<br>        <span class="hljs-keyword">do</span>   <br>            i=$(( <span class="hljs-variable">$i</span> + <span class="hljs-number">1</span> ))<br>            <span class="hljs-comment">#echo $run</span><br>            myrun=`crudini --get run.ini runcase <span class="hljs-variable">$runcase</span>`<br>            mytest=<span class="hljs-variable">$myrun</span>-<span class="hljs-variable">$i</span><br>            <span class="hljs-built_in">echo</span> <span class="hljs-variable">$mytest</span><br>            total=$(( <span class="hljs-variable">$total</span> - <span class="hljs-number">1</span> ))<br>            dofile=runcase/<span class="hljs-variable">$mytest</span>/dodo<br>            <span class="hljs-keyword">if</span> [ ! -f <span class="hljs-string">&quot;<span class="hljs-variable">$dofile</span>&quot;</span> ]; <span class="hljs-keyword">then</span><br>                <span class="hljs-variable">$vdbench</span> -f runcase/<span class="hljs-variable">$mytest</span>/case.conf -o runcase/<span class="hljs-variable">$mytest</span>/output<br>                <span class="hljs-built_in">touch</span> runcase/<span class="hljs-variable">$mytest</span>/dodo<br>            <span class="hljs-keyword">fi</span><br>        <span class="hljs-keyword">done</span><br>    <span class="hljs-keyword">done</span><br><br><span class="hljs-keyword">done</span><br></code></pre></td></tr></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>使用脚本就控制几个循环的执行，可以自定义不同的vdbench怎么去跑的，整体上操作难度较低，测试结果也分开保存了，数据也写到不同的目录</p><h2 id="归档项目地址"><a href="#归档项目地址" class="headerlink" title="归档项目地址"></a>归档项目地址</h2><blockquote><p><a href="https://e.coding.net/zphj1987/vdbench-generater/vdbench-generater.git">https://e.coding.net/zphj1987/vdbench-generater/vdbench-generater.git</a></p></blockquote>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>vsphere硬件加速VAAI的实现</title>
    <link href="/2023/01/31/vsphere%E7%A1%AC%E4%BB%B6%E5%8A%A0%E9%80%9FVAAI%E7%9A%84%E5%AE%9E%E7%8E%B0/"/>
    <url>/2023/01/31/vsphere%E7%A1%AC%E4%BB%B6%E5%8A%A0%E9%80%9FVAAI%E7%9A%84%E5%AE%9E%E7%8E%B0/</url>
    
    <content type="html"><![CDATA[<h1 id="vsphere硬件加速VAAI的实现"><a href="#vsphere硬件加速VAAI的实现" class="headerlink" title="vsphere硬件加速VAAI的实现"></a>vsphere硬件加速VAAI的实现</h1><h2 id="tgtd的支持情况"><a href="#tgtd的支持情况" class="headerlink" title="tgtd的支持情况"></a>tgtd的支持情况</h2><h3 id="librbd支持的情况"><a href="#librbd支持的情况" class="headerlink" title="librbd支持的情况"></a>librbd支持的情况</h3><p><img src="/media/16613328956330/16613278210932.jpg"></p><span id="more"></span><p>从平台上面查询可以看到显示的受支持的</p><p>查询</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@192:~] esxcli storage core device vaai status get<br>naa.60000000000000000e00000000010000<br>   VAAI Plugin Name:<br>   ATS Status: unsupported<br>   Clone Status: unsupported<br>   Zero Status: unsupported<br>   Delete Status: unsupported<br><br>naa.60000000000000000e00000000010001<br>   VAAI Plugin Name:<br>   ATS Status: supported<br>   Clone Status: unsupported<br>   Zero Status: supported<br>   Delete Status: unsupported<br>[root@192:~] esxcli storage core device list -d naa.60000000000000000e00000000010001|grep VAAI<br>   VAAI Status: supported   <br></code></pre></td></tr></table></figure><p>查询支持情况</p><p>可以看到四项里面支持两项</p><h3 id="krbd的支持情况"><a href="#krbd的支持情况" class="headerlink" title="krbd的支持情况"></a>krbd的支持情况</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@192:~] esxcli storage core device vaai status get<br>naa.60000000000000000e00000000010000<br>   VAAI Plugin Name:<br>   ATS Status: unsupported<br>   Clone Status: unsupported<br>   Zero Status: unsupported<br>   Delete Status: unsupported<br><br>naa.60000000000000000e00000000010001<br>   VAAI Plugin Name:<br>   ATS Status: supported<br>   Clone Status: unsupported<br>   Zero Status: supported<br>   Delete Status: unsupported<br>[root@192:~] esxcli storage core device list -d naa.60000000000000000e00000000010001|grep VAAI<br>   VAAI Status: supported<br></code></pre></td></tr></table></figure><p>可以看到四项里面支持两项</p><p>krbd和librbd支持的情况一样</p><h2 id="LIO的支持情况"><a href="#LIO的支持情况" class="headerlink" title="LIO的支持情况"></a>LIO的支持情况</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@192:~] esxcli storage core device vaai status get<br>mpx.vmhba64:C0:T0:L0<br>   VAAI Plugin Name:<br>   ATS Status: unsupported<br>   Clone Status: unsupported<br>   Zero Status: unsupported<br>   Delete Status: unsupported<br><br>naa.6001405c6758535437b4da58d6f20420<br>   VAAI Plugin Name:<br>   ATS Status: supported<br>   Clone Status: supported<br>   Zero Status: supported<br>   Delete Status: unsupported<br>[root@192:~] esxcli storage core device list -d naa.6001405c6758535437b4da58d6f20420|grep VAAI<br>   VAAI Status: supported<br></code></pre></td></tr></table></figure><p>默认支持三项，有一项不支持</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab103 ~]<span class="hljs-comment"># targetcli /backstores/block/disk01/ get attribute emulate_3pc</span><br>emulate_3pc=1<br>[root@lab103 ~]<span class="hljs-comment"># targetcli /backstores/block/disk01/ get attribute emulate_tpu</span><br>emulate_tpu=0<br>[root@lab103 ~]<span class="hljs-comment"># targetcli /backstores/block/disk01/ get attribute emulate_caw</span><br>emulate_caw=1<br></code></pre></td></tr></table></figure><p>VAAI相关的支持对应到上面的三个参数，默认开启了两个，有一个discard相关的默认关闭了</p><p>开启Delete Status支持</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab103 ~]<span class="hljs-comment"># targetcli /backstores/block/disk01/ set attribute emulate_tpu=1</span><br>Parameter emulate_tpu is now <span class="hljs-string">&#x27;1&#x27;</span>.<br>[root@lab103 ~]<span class="hljs-comment"># targetcli / saveconfig</span><br>Configuration saved to /etc/target/saveconfig.json<br></code></pre></td></tr></table></figure><p>要断开iscsi再重连一下</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@192:~] esxcli storage core device vaai status get<br>mpx.vmhba64:C0:T0:L0<br>   VAAI Plugin Name:<br>   ATS Status: unsupported<br>   Clone Status: unsupported<br>   Zero Status: unsupported<br>   Delete Status: unsupported<br><br>naa.6001405c6758535437b4da58d6f20420<br>   VAAI Plugin Name:<br>   ATS Status: supported<br>   Clone Status: supported<br>   Zero Status: supported<br>   Delete Status: supported<br>[root@192:~] esxcli storage core device list -d naa.6001405c6758535437b4da58d6f20420|grep VAAI<br>   VAAI Status: supported<br></code></pre></td></tr></table></figure><p>从上面的查询可以看到LIO能够做到所有的VAAI功能的全支持</p><h2 id="SCST支持情况"><a href="#SCST支持情况" class="headerlink" title="SCST支持情况"></a>SCST支持情况</h2><h3 id="blockio模式全支持"><a href="#blockio模式全支持" class="headerlink" title="blockio模式全支持"></a>blockio模式全支持</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@192:~] esxcli storage core device vaai status get<br>eui.3237636464633731<br>   VAAI Plugin Name:<br>   ATS Status: supported<br>   Clone Status: supported<br>   Zero Status: supported<br>   Delete Status: supported<br><br>mpx.vmhba64:C0:T0:L0<br>   VAAI Plugin Name:<br>   ATS Status: unsupported<br>   Clone Status: unsupported<br>   Zero Status: unsupported<br>   Delete Status: unsupported<br><br>mpx.vmhba0:C0:T0:L0<br>   VAAI Plugin Name:<br>   ATS Status: unsupported<br>   Clone Status: unsupported<br>   Zero Status: unsupported<br>   Delete Status: unsupported<br>[root@192:~] esxcli storage core device list -d eui.3237636464633731|grep VAAI<br>   VAAI Status: supported<br></code></pre></td></tr></table></figure><h3 id="fileio模式不支持Delete-Status"><a href="#fileio模式不支持Delete-Status" class="headerlink" title="fileio模式不支持Delete Status"></a>fileio模式不支持Delete Status</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@192:~] esxcli storage core device vaai status get<br>eui.3237636464633731<br>   VAAI Plugin Name:<br>   ATS Status: supported<br>   Clone Status: supported<br>   Zero Status: supported<br>   Delete Status: unsupported<br></code></pre></td></tr></table></figure><h2 id="简单的判断方法"><a href="#简单的判断方法" class="headerlink" title="简单的判断方法"></a>简单的判断方法</h2><p>linux连接iscsi磁盘，看盘是否支持discard<br>配置的fileio模式如下</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ~]<span class="hljs-comment"># lsblk /dev/sdc  --discard</span><br>NAME              DISC-ALN DISC-GRAN DISC-MAX DISC-ZERO<br>sdc                      0        0B       0B         0<br>└─testcipanpiaoyi        0        0B       0B         0<br></code></pre></td></tr></table></figure><p>配置的block模式如下</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ~]<span class="hljs-comment"># lsblk /dev/sdc  --discard</span><br>NAME              DISC-ALN DISC-GRAN DISC-MAX DISC-ZERO<br>sdc                      0      512B     256M         1<br>└─testcipanpiaoyi        0      512B     256M         1<br></code></pre></td></tr></table></figure><h2 id="补充"><a href="#补充" class="headerlink" title="补充"></a>补充</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@centos7-nfstest ~]<span class="hljs-comment"># tgtadm --lld iscsi --mode logicalunit --op update --tid 1 --lun 0 --params thin_provisioning=1</span><br></code></pre></td></tr></table></figure><p>tgt开启精简配置支持后，能够支持VAAI的一些属性了，这个通过配置文件的设置有点问题，暂时先这样设置</p><p>参考下面的文章</p><blockquote><p><a href="https://blog.csdn.net/bobpen/article/details/79445104">https://blog.csdn.net/bobpen/article/details/79445104</a></p></blockquote><h2 id="支持情况总结"><a href="#支持情况总结" class="headerlink" title="支持情况总结"></a>支持情况总结</h2><table><thead><tr><th>VAAI功能</th><th>tgt</th><th>SCST</th><th>LIO</th></tr></thead><tbody><tr><td>ATS</td><td>Y</td><td>Y</td><td>Y</td></tr><tr><td>Clone</td><td>N</td><td>Y</td><td>Y</td></tr><tr><td>Zero</td><td>Y</td><td>Y</td><td>Y</td></tr><tr><td>Delete</td><td>N</td><td>Y(default N)</td><td>Y</td></tr></tbody></table>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>使用串口连接查看启动信息</title>
    <link href="/2023/01/31/%E4%BD%BF%E7%94%A8%E4%B8%B2%E5%8F%A3%E8%BF%9E%E6%8E%A5%E6%9F%A5%E7%9C%8B%E5%90%AF%E5%8A%A8%E4%BF%A1%E6%81%AF/"/>
    <url>/2023/01/31/%E4%BD%BF%E7%94%A8%E4%B8%B2%E5%8F%A3%E8%BF%9E%E6%8E%A5%E6%9F%A5%E7%9C%8B%E5%90%AF%E5%8A%A8%E4%BF%A1%E6%81%AF/</url>
    
    <content type="html"><![CDATA[<h1 id="使用串口连接查看启动信息"><a href="#使用串口连接查看启动信息" class="headerlink" title="使用串口连接查看启动信息"></a>使用串口连接查看启动信息</h1><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>一般情况下通过屏幕连接就可以查看信息，但是有的时候，需要看下系统启动过程中的一些问题，所以就需要串口方式了</p><h2 id="使用方法"><a href="#使用方法" class="headerlink" title="使用方法"></a>使用方法</h2><h3 id="连接设备"><a href="#连接设备" class="headerlink" title="连接设备"></a>连接设备</h3><p>把三根线插到板子的针脚下面，这个按规定的插上就行</p><h3 id="windows电脑连接"><a href="#windows电脑连接" class="headerlink" title="windows电脑连接"></a>windows电脑连接</h3><p>使用xshell里面连接设置即可</p><p><img src="/media/rk3588/%E7%A1%AC%E4%BB%B6%E7%AF%87/assets/2022-11-25-16-37-37.png"></p><p>在设备管理器里面找到编号，这个的属性里面可以不调整，里面也没有我们需要设置的速率，不动就行</p><p><img src="/media/rk3588/%E7%A1%AC%E4%BB%B6%E7%AF%87/assets/16693656453631.jpg"></p><p>xshell设置配置</p><p><img src="/media/rk3588/%E7%A1%AC%E4%BB%B6%E7%AF%87/assets/16693656121943.jpg"></p><p>设置好了以后重新上电即可</p><h2 id="线材问题"><a href="#线材问题" class="headerlink" title="线材问题"></a>线材问题</h2><p>线材我使用的是：</p><blockquote><p>FT232RL USB转TTL模块串口线下载线刷机线FT232升级小板带壳</p></blockquote><p><img src="/media/rk3588/%E7%A1%AC%E4%BB%B6%E7%AF%87/assets/16693658366133.jpg"></p><p>这个线就可以支持的，其它的可以自己试试，速度支持1500000的就可以</p><p>连接方式就是按这个顺序连<br><img src="/media/rk3588/%E7%A1%AC%E4%BB%B6%E7%AF%87/assets/16693661267490.jpg"></p><p>靠外面一排，从倒数第五个针脚开始，绿白黑连接，红色线不管</p><p>连上以后就可以操作终端了</p><p><img src="/media/rk3588/%E7%A1%AC%E4%BB%B6%E7%AF%87/assets/16693662544154.jpg"></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>如何获取cubefs的master的信息</title>
    <link href="/2023/01/31/%E5%A6%82%E4%BD%95%E8%8E%B7%E5%8F%96cubefs%E7%9A%84master%E7%9A%84%E4%BF%A1%E6%81%AF/"/>
    <url>/2023/01/31/%E5%A6%82%E4%BD%95%E8%8E%B7%E5%8F%96cubefs%E7%9A%84master%E7%9A%84%E4%BF%A1%E6%81%AF/</url>
    
    <content type="html"><![CDATA[<h1 id="如何获取cubefs的master的信息"><a href="#如何获取cubefs的master的信息" class="headerlink" title="如何获取cubefs的master的信息"></a>如何获取cubefs的master的信息</h1><p>背景<br>最近关注到一套全新的分布式文件系统cubefs，这个之前的名称是chubaofs，看github上面应该有做切换了一段时间，但是不清楚什么原因,文档和包切换的还不是很彻底</p><p>所以在使用过程中需要注意下，注意下版本选择，这个在后面的文档里面再单独的说下这个问题</p><p>这个分布式文件系统跟ceph类似，也是采用了管理节点的角色，本篇就是讲如何获取这个管理节点的信息的，从官方的指导文档看，这块讲的还是不够详细</p><span id="more"></span><p>在github的issue里面已经提了一些咨询的，看下官方后面是否会补充，如果没有处理的，这边应该会基于我自己的经验做一些相关的说明</p><p>本篇就是基于这个背景写下的第一篇相关的文章，从开源存储来看，我们自己真正完全开源并且持续维护的软件并不是很多，网易数帆有Curve分布式存储开源，还有个juicefs是可以面向公网把对象转文件的，这个cubefs最开始应该是jd开源出来的，现在看是oppo在维护，大厂下台来开源，品质上可能有一定的保证，这个说可能而不是一定的原因是，本身的开发人员的流动性的可能比国外的要大,再一个这一套是不是对企业本身能够存在收益</p><p>这些开源系统的背景大多是目前的开源的存储已经满足不了当前的业务场景的需求，所以需要重新造一套，而开源，可以提升影响力，并且还能让更多人参与进来，用的人多，自然发现的问题越快，越多，而参与的人多，软件的健壮性也会越来越高，当然个人觉得开源软件一个重要的点是可持续性，之前的大厂也开源了很多东西，后面就慢慢不维护了，可能KPI的影响比较大，或者动力不足，希望这个系统能够更长久一些</p><h2 id="问题来源"><a href="#问题来源" class="headerlink" title="问题来源"></a>问题来源</h2><p>在系统里面做操作，应该是有个默认的习惯的，我们做的任何一个操作，应该是需要有反馈的，不管是失败还是成功，常规的软件基本离不开增删改查这几个操作的</p><p>那么这个软件的第一步就是部署的管理节点，问题就是来源于这里，我部署了三个节点，但是我想去查询我部署了哪三个节点，这块不清楚是不是我没找到方法，在文档里面是并没有看到相关的信息，所以在摸索了一番后，终于找到了查询的方法,这里同样提一点，好的软件设计应该是，同时两个人在同一个环境下面操作，A用户在B用户不看的情况下操作一个事情，A用户不告诉B用户具体的信息，B用户能够查到A用户的操作，这个说起来很拗口，我们就以这个举例子</p><ul><li>A用户增加了三个master节点，101，102，103</li><li>A告诉B，我在101上面操作加了三个master节点</li><li>B能够在A不提供更多信息的情况下，以最快的速度查询到加的哪些节点</li></ul><p>这个在gluster里面就是gluster peer status</p><p>在ceph里面就是ceph mon dump或者ceph -s</p><p>在弄清楚这个问题的过程中，还新学习到了一些东西</p><h2 id="查询的方法"><a href="#查询的方法" class="headerlink" title="查询的方法"></a>查询的方法</h2><h3 id="搭建console平台"><a href="#搭建console平台" class="headerlink" title="搭建console平台"></a>搭建console平台</h3><p>这个搭建过程就不说了，官方文档就有，这个使用的是老的2.2.2版本，新的改名字后的2.2.2版本的包没放对，也是提交了bug，等官方修复</p><p>这个登录的地方的默认密码硬编码不是很习惯，至少我需要找地方去看这个密码，并且可能会变的</p><p>默认用户名密码是</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">root<br>ChubaoFSRoot<br></code></pre></td></tr></table></figure><p>后面版本可能改成CubeFSRoot，这个就是这种可能变的情况，可以有更好的办法处理</p><p>比较通用的做法</p><ul><li>沿用操作系统的用户-这个是红帽官方web管理平台的做法（cockpit）</li><li>类似minio去采用环境变量，或者配置文件，在启动的时候进行一个设置</li><li>类似mysql那种通过命令行去设置，生成一个加密后的密码记录，然后也方便忘记的时候修改,重设密码</li><li>或者类似linux的密码逻辑，第一次启动页面设置一个密码，在本地生成加密的密码文件，需要修改密码删掉加密的文件</li></ul><p>至少有这四种比较灵活的处理登录密码的方法,这块看后期是否会优化下，或者开发者本身觉得这个没多大问题</p><p>这里还是自我提问几个问题</p><ul><li>从什么途径获取(设置)密码</li><li>怎么修改密码</li><li>忘记了密码无法登录怎么修改密码</li></ul><h3 id="获取token"><a href="#获取token" class="headerlink" title="获取token"></a>获取token</h3><p>这里我们需要登录，登录的时候会跳转，但是我们想获取到这个请求的发送方式，那么就需要设置记录日志<br><img src="/media/16608989078869/16608992689095.jpg"></p><p>按F12进入调试模式，有个保留日志的选项，勾选上</p><p><img src="/media/16608989078869/16608993444981.jpg"></p><p>在网络tab页里面找到第一个post的请求login，这个从右边可以看到请求类型是post的</p><p><img src="/media/16608989078869/16608993725538.jpg"></p><p>tab页里面有个载荷的，这个地方应该是请求的内容</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">&#123;<span class="hljs-string">&quot;operationName&quot;</span>:<span class="hljs-string">&quot;Login&quot;</span>,<span class="hljs-string">&quot;variables&quot;</span>:&#123;<span class="hljs-string">&quot;userID&quot;</span>:<span class="hljs-string">&quot;root&quot;</span>,<span class="hljs-string">&quot;password&quot;</span>:<span class="hljs-string">&quot;082c2c44e2bfae761275e7e2f71d8771b276b32a&quot;</span>&#125;,<span class="hljs-string">&quot;query&quot;</span>:<span class="hljs-string">&quot;query Login(<span class="hljs-variable">$userID</span>: String, <span class="hljs-variable">$password</span>: String) &#123;\n  login(userID: <span class="hljs-variable">$userID</span>, password: <span class="hljs-variable">$password</span>) &#123;\n    token\n    userID\n    __typename\n  &#125;\n&#125;\n&quot;</span>&#125;<br></code></pre></td></tr></table></figure><p>这个是个json的，后面我们会用到,上面是带了用户名密码信息的，这个后面的password应该是处理过的不明文发送的，所以这个记住就行，根据你的密码的固定值的</p><p><img src="/media/16608989078869/16608993944875.jpg"></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">&#123;<span class="hljs-string">&quot;data&quot;</span>:&#123;<span class="hljs-string">&quot;login&quot;</span>:&#123;<span class="hljs-string">&quot;__typename&quot;</span>:<span class="hljs-string">&quot;UserToken&quot;</span>,<span class="hljs-string">&quot;token&quot;</span>:<span class="hljs-string">&quot;2990b09c-21be-40b5-bc86-802663cf0751&quot;</span>,<span class="hljs-string">&quot;userID&quot;</span>:<span class="hljs-string">&quot;root&quot;</span>&#125;&#125;&#125;<br></code></pre></td></tr></table></figure><p>这样就拿到了token</p><h2 id="获取master的信息"><a href="#获取master的信息" class="headerlink" title="获取master的信息"></a>获取master的信息</h2><p><img src="/media/16608989078869/16608994294713.jpg"></p><p>可以看到是对着<a href="http://192.168.0.101/api/cluster%E5%9C%B0%E5%9D%80%E5%8F%91%E9%80%81%E7%9A%84POST%E8%AF%B7%E6%B1%82">http://192.168.0.101/api/cluster地址发送的POST请求</a></p><p><img src="/media/16608989078869/16608994379386.jpg"></p><p>请求的内容如下</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">&#123;<span class="hljs-string">&quot;operationName&quot;</span>:null,<span class="hljs-string">&quot;variables&quot;</span>:&#123;<span class="hljs-string">&quot;num&quot;</span>:10000&#125;,<span class="hljs-string">&quot;query&quot;</span>:<span class="hljs-string">&quot;&#123;\n  clusterView &#123;\n    name\n    volumeCount\n    __typename\n  &#125;\n  dataNodeList &#123;\n    addr\n    __typename\n  &#125;\n  metaNodeList &#123;\n    addr\n    __typename\n  &#125;\n  masterList &#123;\n    addr\n    __typename\n  &#125;\n&#125;\n&quot;</span>&#125;<br></code></pre></td></tr></table></figure><p><img src="/media/16608989078869/16608994511876.jpg"></p><p>响应的内容如下:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">&#123;<span class="hljs-string">&quot;code&quot;</span>:200,<span class="hljs-string">&quot;data&quot;</span>:&#123;<span class="hljs-string">&quot;clusterView&quot;</span>:&#123;<span class="hljs-string">&quot;__typename&quot;</span>:<span class="hljs-string">&quot;ClusterView&quot;</span>,<span class="hljs-string">&quot;name&quot;</span>:<span class="hljs-string">&quot;chubaofs01&quot;</span>,<span class="hljs-string">&quot;volumeCount&quot;</span>:0&#125;,<span class="hljs-string">&quot;dataNodeList&quot;</span>:[],<span class="hljs-string">&quot;masterList&quot;</span>:[&#123;<span class="hljs-string">&quot;__typename&quot;</span>:<span class="hljs-string">&quot;MasterInfo&quot;</span>,<span class="hljs-string">&quot;addr&quot;</span>:<span class="hljs-string">&quot;192.168.0.101&quot;</span>&#125;,&#123;<span class="hljs-string">&quot;__typename&quot;</span>:<span class="hljs-string">&quot;MasterInfo&quot;</span>,<span class="hljs-string">&quot;addr&quot;</span>:<span class="hljs-string">&quot;192.168.0.102&quot;</span>&#125;,&#123;<span class="hljs-string">&quot;__typename&quot;</span>:<span class="hljs-string">&quot;MasterInfo&quot;</span>,<span class="hljs-string">&quot;addr&quot;</span>:<span class="hljs-string">&quot;192.168.0.103&quot;</span>&#125;],<span class="hljs-string">&quot;metaNodeList&quot;</span>:[]&#125;,<span class="hljs-string">&quot;errors&quot;</span>:null&#125;<br></code></pre></td></tr></table></figure><p>到这里我们就通过管理平台拿到master节点的信息，这里是通过graphql master api获取的信息</p><blockquote><p><a href="https://github.com/cubefs/cubefs/blob/master/proto/admin_proto.go">https://github.com/cubefs/cubefs/blob/master/proto/admin_proto.go</a></p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">//graphql master api<br>    AdminClusterAPI = <span class="hljs-string">&quot;/api/cluster&quot;</span><br>    AdminUserAPI    = <span class="hljs-string">&quot;/api/user&quot;</span><br>    AdminVolumeAPI  = <span class="hljs-string">&quot;/api/volume&quot;</span><br></code></pre></td></tr></table></figure><p>这个可以自己了解下graphql是什么，这个跟之前的restful的api的最大区别是地址固定，可以通过一个固定的地址获取到不同的信息，把更多的解析工作放到后台了，这里知道是这个就行</p><p>我们梳理下流程：</p><ul><li>获取token</li><li>拿着token去对着api发送信息</li><li>返回信息，拿到需要的信息</li></ul><h2 id="使用POSTMAN跑下上面的流程"><a href="#使用POSTMAN跑下上面的流程" class="headerlink" title="使用POSTMAN跑下上面的流程"></a>使用POSTMAN跑下上面的流程</h2><h3 id="POSTMAN获取token"><a href="#POSTMAN获取token" class="headerlink" title="POSTMAN获取token"></a>POSTMAN获取token</h3><p><img src="/media/16608989078869/16608995047551.jpg"></p><p>对着<a href="http://192.168.0.101/login%E5%8F%91%E9%80%81post%E8%AF%B7%E6%B1%82%EF%BC%8C%E5%86%85%E5%AE%B9%E4%B8%BA(%E4%B8%8A%E9%9D%A2chrome%E9%87%8C%E9%9D%A2%E6%8A%93%E7%9A%84)">http://192.168.0.101/login发送post请求，内容为(上面chrome里面抓的)</a>:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">&#123;<span class="hljs-string">&quot;operationName&quot;</span>:<span class="hljs-string">&quot;Login&quot;</span>,<span class="hljs-string">&quot;variables&quot;</span>:&#123;<span class="hljs-string">&quot;userID&quot;</span>:<span class="hljs-string">&quot;root&quot;</span>,<span class="hljs-string">&quot;password&quot;</span>:<span class="hljs-string">&quot;082c2c44e2bfae761275e7e2f71d8771b276b32a&quot;</span>&#125;,<span class="hljs-string">&quot;query&quot;</span>:<span class="hljs-string">&quot;query Login(<span class="hljs-variable">$userID</span>: String, <span class="hljs-variable">$password</span>: String) &#123;\n  login(userID: <span class="hljs-variable">$userID</span>, password: <span class="hljs-variable">$password</span>) &#123;\n    token\n    userID\n    __typename\n  &#125;\n&#125;\n&quot;</span>&#125;<br></code></pre></td></tr></table></figure><p>得到了token:379637f5-0540-4695-b167-75abf7c81074</p><p>这个记住就行</p><h3 id="POSTMAN获取信息"><a href="#POSTMAN获取信息" class="headerlink" title="POSTMAN获取信息"></a>POSTMAN获取信息</h3><p><img src="/media/16608989078869/16608995323650.jpg"></p><p>可以看到提示的是没有认证的相关的信息</p><p>我们加入认证的信息，在oauth2.0里面添加，注意保持prefix为空，填写上获取到的token的信息<br><img src="/media/16608989078869/16608995419651.jpg"></p><p><img src="/media/16608989078869/16608995489147.jpg"></p><p>直接发送的话会提示上面的信息</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">&#123;<br>    <span class="hljs-string">&quot;errors&quot;</span>: [<br>        <span class="hljs-string">&quot;&#123;\&quot;message\&quot;:\&quot;EOF\&quot;,\&quot;code\&quot;:500&#125;&quot;</span><br>    ]<br>&#125;<br></code></pre></td></tr></table></figure><p>这个是因为没有把请求放进去，我们放入请求</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">&#123;<span class="hljs-string">&quot;operationName&quot;</span>:null,<span class="hljs-string">&quot;variables&quot;</span>:&#123;<span class="hljs-string">&quot;num&quot;</span>:10000&#125;,<span class="hljs-string">&quot;query&quot;</span>:<span class="hljs-string">&quot;&#123;\n  clusterView &#123;\n    name\n    volumeCount\n    __typename\n  &#125;\n  dataNodeList &#123;\n    addr\n    __typename\n  &#125;\n  metaNodeList &#123;\n    addr\n    __typename\n  &#125;\n  masterList &#123;\n    addr\n    __typename\n  &#125;\n&#125;\n&quot;</span>&#125;<br></code></pre></td></tr></table></figure><p>再次发送请求<br><img src="/media/16608989078869/16608995843954.jpg"></p><p>可以看到得到了我们想要的信息</p><h2 id="使用curl实现我们的请求"><a href="#使用curl实现我们的请求" class="headerlink" title="使用curl实现我们的请求"></a>使用curl实现我们的请求</h2><p>上面使用的是postman客户端，而我们可能更多的在命令行下面执行的，所以我们使用curl把上面完整实现一下</p><h2 id="curl获取token"><a href="#curl获取token" class="headerlink" title="curl获取token"></a>curl获取token</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 cli]<span class="hljs-comment"># cat login.json</span><br>&#123;<span class="hljs-string">&quot;operationName&quot;</span>:<span class="hljs-string">&quot;Login&quot;</span>,<span class="hljs-string">&quot;variables&quot;</span>:&#123;<span class="hljs-string">&quot;userID&quot;</span>:<span class="hljs-string">&quot;root&quot;</span>,<span class="hljs-string">&quot;password&quot;</span>:<span class="hljs-string">&quot;082c2c44e2bfae761275e7e2f71d8771b276b32a&quot;</span>&#125;,<span class="hljs-string">&quot;query&quot;</span>:<span class="hljs-string">&quot;query Login(<span class="hljs-variable">$userID</span>: String, <span class="hljs-variable">$password</span>: String) &#123;\n  login(userID: <span class="hljs-variable">$userID</span>, password: <span class="hljs-variable">$password</span>) &#123;\n    token\n    userID\n    __typename\n  &#125;\n&#125;\n&quot;</span>&#125;<br><br>[root@lab101 cli]<span class="hljs-comment"># curl -X POST   http://192.168.0.101/login -d &quot;@login.json&quot;</span><br>&#123;<span class="hljs-string">&quot;data&quot;</span>:&#123;<span class="hljs-string">&quot;login&quot;</span>:&#123;<span class="hljs-string">&quot;__typename&quot;</span>:<span class="hljs-string">&quot;UserToken&quot;</span>,<span class="hljs-string">&quot;token&quot;</span>:<span class="hljs-string">&quot;7537d007-6598-484e-a08d-95720593b3ec&quot;</span>,<span class="hljs-string">&quot;userID&quot;</span>:<span class="hljs-string">&quot;root&quot;</span>&#125;&#125;&#125;<br></code></pre></td></tr></table></figure><p>上面的信息还是之前在chrome拿到的那个登录的信息,可以看到我们正确的获取到了token</p><p>我们发送请求</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 cli]<span class="hljs-comment"># cat post.json</span><br>&#123;<span class="hljs-string">&quot;operationName&quot;</span>:null,<span class="hljs-string">&quot;variables&quot;</span>:&#123;<span class="hljs-string">&quot;num&quot;</span>:10000&#125;,<span class="hljs-string">&quot;query&quot;</span>:<span class="hljs-string">&quot;&#123;\n  clusterView &#123;\n    name\n    volumeCount\n    __typename\n  &#125;\n  dataNodeList &#123;\n    addr\n    __typename\n  &#125;\n  metaNodeList &#123;\n    addr\n    __typename\n  &#125;\n  masterList &#123;\n    addr\n    __typename\n  &#125;\n&#125;\n&quot;</span>&#125;<br>[root@lab101 cli]<span class="hljs-comment"># curl -X POST -H &quot;Authorization: 7537d007-6598-484e-a08d-95720593b3ec&quot;  &quot;http://192.168.0.101:17010/api/cluster&quot; -d &quot;@post.json&quot;</span><br>&#123;<span class="hljs-string">&quot;errors&quot;</span>:[<span class="hljs-string">&quot;not found [_user_key] in header&quot;</span>]&#125;<br></code></pre></td></tr></table></figure><p>这个不清楚为什么跟postman相比多了个这个_user_key的提示，我们加上</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 cli]<span class="hljs-comment"># curl -X POST -H &quot;Authorization: 7537d007-6598-484e-a08d-95720593b3ec&quot; -H &quot;_user_key:root&quot;  &quot;http://192.168.0.101:17010/api/cluster&quot; -d &quot;@post.json&quot;</span><br>&#123;<span class="hljs-string">&quot;data&quot;</span>:&#123;<span class="hljs-string">&quot;clusterView&quot;</span>:&#123;<span class="hljs-string">&quot;__typename&quot;</span>:<span class="hljs-string">&quot;ClusterView&quot;</span>,<span class="hljs-string">&quot;name&quot;</span>:<span class="hljs-string">&quot;chubaofs01&quot;</span>,<span class="hljs-string">&quot;volumeCount&quot;</span>:0&#125;,<span class="hljs-string">&quot;dataNodeList&quot;</span>:[],<span class="hljs-string">&quot;masterList&quot;</span>:[&#123;<span class="hljs-string">&quot;__typename&quot;</span>:<span class="hljs-string">&quot;MasterInfo&quot;</span>,<span class="hljs-string">&quot;addr&quot;</span>:<span class="hljs-string">&quot;192.168.0.101&quot;</span>&#125;,&#123;<span class="hljs-string">&quot;__typename&quot;</span>:<span class="hljs-string">&quot;MasterInfo&quot;</span>,<span class="hljs-string">&quot;addr&quot;</span>:<span class="hljs-string">&quot;192.168.0.102&quot;</span>&#125;,&#123;<span class="hljs-string">&quot;__typename&quot;</span>:<span class="hljs-string">&quot;MasterInfo&quot;</span>,<span class="hljs-string">&quot;addr&quot;</span>:<span class="hljs-string">&quot;192.168.0.103&quot;</span>&#125;],<span class="hljs-string">&quot;metaNodeList&quot;</span>:[]&#125;,<span class="hljs-string">&quot;errors&quot;</span>:null&#125;<br></code></pre></td></tr></table></figure><p>可以看到，成功获取到这个信息，在这个时候发现了另外一个有趣的事情，我们看下</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 cli]<span class="hljs-comment"># curl -X POST  -H &quot;_user_key:root&quot;  &quot;http://192.168.0.101:17010/api/cluster&quot; -d &quot;@post.json&quot;</span><br>&#123;<span class="hljs-string">&quot;data&quot;</span>:&#123;<span class="hljs-string">&quot;clusterView&quot;</span>:&#123;<span class="hljs-string">&quot;__typename&quot;</span>:<span class="hljs-string">&quot;ClusterView&quot;</span>,<span class="hljs-string">&quot;name&quot;</span>:<span class="hljs-string">&quot;chubaofs01&quot;</span>,<span class="hljs-string">&quot;volumeCount&quot;</span>:0&#125;,<span class="hljs-string">&quot;dataNodeList&quot;</span>:[],<span class="hljs-string">&quot;masterList&quot;</span>:[&#123;<span class="hljs-string">&quot;__typename&quot;</span>:<span class="hljs-string">&quot;MasterInfo&quot;</span>,<span class="hljs-string">&quot;addr&quot;</span>:<span class="hljs-string">&quot;192.168.0.101&quot;</span>&#125;,&#123;<span class="hljs-string">&quot;__typename&quot;</span>:<span class="hljs-string">&quot;MasterInfo&quot;</span>,<span class="hljs-string">&quot;addr&quot;</span>:<span class="hljs-string">&quot;192.168.0.102&quot;</span>&#125;,&#123;<span class="hljs-string">&quot;__typename&quot;</span>:<span class="hljs-string">&quot;MasterInfo&quot;</span>,<span class="hljs-string">&quot;addr&quot;</span>:<span class="hljs-string">&quot;192.168.0.103&quot;</span>&#125;],<span class="hljs-string">&quot;metaNodeList&quot;</span>:[]&#125;,<span class="hljs-string">&quot;errors&quot;</span>:null&#125;<br></code></pre></td></tr></table></figure><p>我没有用token居然在curl下也能够获取到这个信息的，但是在postman下面是必须加上的，所以这个是不是存在鉴权漏洞的，因为这个使用的是2.2.2版本的，后面更新下版本再看下,如果确认有问题再向官方提交了,目前确认2.5.2版本还是有这个问题的,已经提交相关问题，等待回复</p><p>鉴权问题</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>在尝试获取master信息过程中，学习了一些请求相关的知识，同时也发现了一些可以优化的点，这个先自己记下来就行</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>局域网内的文件传输</title>
    <link href="/2023/01/31/%E5%B1%80%E5%9F%9F%E7%BD%91%E5%86%85%E7%9A%84%E6%96%87%E4%BB%B6%E4%BC%A0%E8%BE%93/"/>
    <url>/2023/01/31/%E5%B1%80%E5%9F%9F%E7%BD%91%E5%86%85%E7%9A%84%E6%96%87%E4%BB%B6%E4%BC%A0%E8%BE%93/</url>
    
    <content type="html"><![CDATA[<h1 id="局域网内的文件传输"><a href="#局域网内的文件传输" class="headerlink" title="局域网内的文件传输"></a>局域网内的文件传输</h1><h2 id="需求"><a href="#需求" class="headerlink" title="需求"></a>需求</h2><p>局域网内的文件传输，使用QQ或者其它网盘什么的，超大文件很容易传输失败，所以可以通过架设http传输服务器的方式进行处理</p><p>同样的也可以在有公网IP的服务器上面搭建服务</p><h2 id="搭建服务器"><a href="#搭建服务器" class="headerlink" title="搭建服务器"></a>搭建服务器</h2><p>这个服务器可以在windows或者linux中转服务器都行</p><span id="more"></span><h3 id="下载软件服务"><a href="#下载软件服务" class="headerlink" title="下载软件服务"></a>下载软件服务</h3><blockquote><p><a href="https://github.com/dutchcoders/transfer.sh">https://github.com/dutchcoders/transfer.sh</a></p></blockquote><h3 id="启动http服务"><a href="#启动http服务" class="headerlink" title="启动http服务"></a>启动http服务</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">.\transfersh-v1.4.0-windows-amd64.exe --provider=<span class="hljs-built_in">local</span> --listener :8080 --temp-path=.\<span class="hljs-built_in">local</span> --basedir=.\<span class="hljs-built_in">local</span><br></code></pre></td></tr></table></figure><p>这个在windows或者linux上面都行</p><h2 id="访问服务"><a href="#访问服务" class="headerlink" title="访问服务"></a>访问服务</h2><p>通过手机或者另外的客户端直接访问上面的地址端口，然后上传文件即可</p><p><img src="/media/16612402845294/16612405489519.jpg"></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>打包一个支持rbd的tgt</title>
    <link href="/2023/01/31/%E6%89%93%E5%8C%85%E4%B8%80%E4%B8%AA%E6%94%AF%E6%8C%81rbd%E7%9A%84tgt/"/>
    <url>/2023/01/31/%E6%89%93%E5%8C%85%E4%B8%80%E4%B8%AA%E6%94%AF%E6%8C%81rbd%E7%9A%84tgt/</url>
    
    <content type="html"><![CDATA[<h1 id="打包一个支持rbd的tgt"><a href="#打包一个支持rbd的tgt" class="headerlink" title="打包一个支持rbd的tgt"></a>打包一个支持rbd的tgt</h1><h2 id="下载源码包"><a href="#下载源码包" class="headerlink" title="下载源码包"></a>下载源码包</h2><p>当前的最新版本到了1.0.84,本篇就以这个举例子</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab103 tgtd]<span class="hljs-comment"># wget https://github.com/fujita/tgt/archive/refs/tags/v1.0.84.tar.gz</span><br>[root@lab103 tgtd]<span class="hljs-comment"># tar -xvf v1.0.84.tar.gz</span><br>[root@lab103 tgtd]<span class="hljs-comment"># cd tgt-1.0.84/</span><br>[root@lab103 tgt-1.0.84]<span class="hljs-comment"># mv README.md README</span><br>[root@lab103 scsi]<span class="hljs-comment"># cd ../</span><br>[root@lab103 scsi]<span class="hljs-comment"># tar -czvf tgt-1.0.84.tar.gz tgt-1.0.84</span><br></code></pre></td></tr></table></figure><p>上面有个打包的脚本没处理好，需要改下上面的文件名称,然后压缩成新的包</p><span id="more"></span><p>安装依赖包</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">yum install libibverbs-devel librdmacm-devel libaio-devel docbook-style-xsl  systemd-devel glusterfs-api-devel -y<br></code></pre></td></tr></table></figure><p>官方的包路径</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">https://download-ib01.fedoraproject.org/pub/epel/7/SRPMS/Packages/s/scsi-target-utils-1.0.55-4.el7.src.rpm<br></code></pre></td></tr></table></figure><p>官方的包只做到了1.0.55版本的，我们基于这个版本合入我们的最新版本的</p><p>解压源码包</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab103 scsi]<span class="hljs-comment"># rpm2cpio scsi-target-utils-1.0.55-4.el7.src.rpm |cpio -div</span><br></code></pre></td></tr></table></figure><p>修改打包的脚本</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs bash">···<br>%global with_rbd 1<br>%global with_glfs 0<br>···<br>Name:           scsi-target-utils<br>Version:        1.0.84<br>···<br><span class="hljs-comment">#%patch0 -p1</span><br></code></pre></td></tr></table></figure><p>开启rbd，关闭gluster，去掉patch0</p><p>把包放打包路径</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab103 scsi]<span class="hljs-comment"># cp -ra tgt-1.0.84.tar.gz /root/rpmbuild/SOURCES/</span><br>[root@lab103 scsi]<span class="hljs-comment"># cp -ra * /root/rpmbuild/SOURCES/</span><br></code></pre></td></tr></table></figure><p>打包</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab103 scsi]<span class="hljs-comment"># rpmbuild -bb scsi-target-utils.spec</span><br>···<br>Wrote: /root/rpmbuild/RPMS/x86_64/scsi-target-utils-1.0.84-4.el7.x86_64.rpm<br>Wrote: /root/rpmbuild/RPMS/x86_64/scsi-target-utils-rbd-1.0.84-4.el7.x86_64.rpm<br>···<br></code></pre></td></tr></table></figure><p>这两个包就是我们需要的包</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>端口转发配置</title>
    <link href="/2023/01/31/%E7%AB%AF%E5%8F%A3%E8%BD%AC%E5%8F%91%E9%85%8D%E7%BD%AE/"/>
    <url>/2023/01/31/%E7%AB%AF%E5%8F%A3%E8%BD%AC%E5%8F%91%E9%85%8D%E7%BD%AE/</url>
    
    <content type="html"><![CDATA[<h1 id="端口转发配置"><a href="#端口转发配置" class="headerlink" title="端口转发配置"></a>端口转发配置</h1><h2 id="需求"><a href="#需求" class="headerlink" title="需求"></a>需求</h2><p>有的时候机器有一台网关机器，需要做端口转发的工作，那么可以使用iptables或者使用firwalld进行配置，firewalld底层也是调用的跟iptables一样的内核模块的，只是命令的不同</p><span id="more"></span><h2 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h2><h3 id="环境说明"><a href="#环境说明" class="headerlink" title="环境说明"></a>环境说明</h3><p><img src="/media/16633019023390/16633021891118.jpg"></p><ul><li><p>机器A</p><ul><li>外网IP：192.168.0.85</li><li>内网IP：192.168.1.103</li></ul></li><li><p>机器B</p><ul><li>内网IP：192.168.1.102</li></ul></li><li><p>机器C</p><ul><li>内网IP：192.168.1.101</li></ul></li></ul><p>配置访问192.168.0.85端口44561映射到192.168.1.102的4456端口<br>配置访问192.168.0.85端口44562映射到192.168.1.101的4456端口</p><h3 id="通用配置"><a href="#通用配置" class="headerlink" title="通用配置"></a>通用配置</h3><p>开启内核转发功能</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab103 ~]<span class="hljs-comment"># cat /etc/sysctl.conf |grep forwa</span><br>net.ipv4.ip_forward = 1<br>[root@lab103 ~]<span class="hljs-comment"># sysctl -p</span><br></code></pre></td></tr></table></figure><h3 id="使用iptable配置"><a href="#使用iptable配置" class="headerlink" title="使用iptable配置"></a>使用iptable配置</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-meta">#!/bin/sh</span><br>IPT=<span class="hljs-string">&quot;/sbin/iptables&quot;</span><br>/bin/echo <span class="hljs-string">&quot;1&quot;</span> &gt; /proc/sys/net/ipv4/ip_forward<br>/sbin/modprobe ip_tables<br>/sbin/modprobe iptable_filter<br>/sbin/modprobe iptable_nat<br>/sbin/modprobe ip_conntrack<br>/sbin/modprobe ip_conntrack_ftp<br>/sbin/modprobe ip_nat_ftp<br><span class="hljs-variable">$IPT</span> -F<br><span class="hljs-variable">$IPT</span> -t nat -F<br><span class="hljs-variable">$IPT</span> -X<br><span class="hljs-variable">$IPT</span> -t nat -X<br><span class="hljs-variable">$IPT</span> -Z<br><span class="hljs-variable">$IPT</span> -t nat -Z<br>iptables -t filter -P FORWARD  ACCEPT<br><span class="hljs-comment">#DNAT 做端口转发 lab102</span><br><span class="hljs-variable">$IPT</span> -t nat -A PREROUTING -d 192.168.0.85 -p tcp --dport 44561 -j DNAT --to-destination 192.168.1.102:4456<br><span class="hljs-comment">#SNAT 做端口转发</span><br><span class="hljs-variable">$IPT</span> -t nat -A POSTROUTING -p tcp -d 192.168.1.102 --dport 4456 -j SNAT --to-source 192.168.1.103<br><br><br><span class="hljs-comment">#DNAT 做端口转发 lab1021</span><br><span class="hljs-variable">$IPT</span> -t nat -A PREROUTING -d 192.168.0.85 -p tcp --dport 44562 -j DNAT --to-destination 192.168.1.101:4456<br><span class="hljs-comment">#SNAT 做端口转发</span><br><span class="hljs-variable">$IPT</span> -t nat -A POSTROUTING -p tcp -d 192.168.1.101 --dport 4456 -j SNAT --to-source 192.168.1.103<br></code></pre></td></tr></table></figure><p>上面的注意下那个to-source后面应该接上转发机器的内网的IP的地址，如果错了就有问题</p><h3 id="使用firewalld配置"><a href="#使用firewalld配置" class="headerlink" title="使用firewalld配置"></a>使用firewalld配置</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-meta">#!/bin/sh</span><br>firewall-cmd --add-masquerade --permanent<br>firewall-cmd --add-port=44561/tcp --permanent<br>firewall-cmd --add-port=44562/tcp --permanent<br>firewall-cmd --add-forward-port=port=44561:proto=tcp:toport=4456:toaddr=192.168.1.102 --permanent<br>firewall-cmd --add-forward-port=port=44562:proto=tcp:toport=4456:toaddr=192.168.1.101 --permanent<br>firewall-cmd --reload<br>firewall-cmd --list-all<br></code></pre></td></tr></table></figure><h2 id="注意点"><a href="#注意点" class="headerlink" title="注意点"></a>注意点</h2><p>上面的配置好转发以后都不要在转发的机器本机进行测试，本机上直接测试会不通，这个需要在外部进行访问转发机器进行验证</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>自定义rock5b内核</title>
    <link href="/2023/01/31/%E8%87%AA%E5%AE%9A%E4%B9%89rock5b%E5%86%85%E6%A0%B8/"/>
    <url>/2023/01/31/%E8%87%AA%E5%AE%9A%E4%B9%89rock5b%E5%86%85%E6%A0%B8/</url>
    
    <content type="html"><![CDATA[<h1 id="自定义rock5b内核"><a href="#自定义rock5b内核" class="headerlink" title="自定义rock5b内核"></a>自定义rock5b内核</h1><h2 id="官方资料"><a href="#官方资料" class="headerlink" title="官方资料"></a>官方资料</h2><blockquote><p><a href="https://wiki.radxa.com/Rock5/guide/build-debian-from-debos-radxa">https://wiki.radxa.com/Rock5/guide/build-debian-from-debos-radxa</a></p></blockquote><p>官方的资料在这里，打包成deb包，然后进行安装的是没有问题的，手动编译成Image，和dtb的，然后替换的部分，是有问题的，如果严格按照上面的文档的方法手动去更新内核，是启动不起来的，本篇就把这块补充起来</p><p>大部分资料是参考官方的即可，小部分是补充的</p><h2 id="二者的区别"><a href="#二者的区别" class="headerlink" title="二者的区别"></a>二者的区别</h2><p>其实整体上是没有太大的区别的，deb包就是完整的内核替换流程，而手动的就是方便如果只进行部分内核模块的修改的时候，替换模块文件即可，能够做更精细的内核替换</p><h2 id="deb的内核更新方式"><a href="#deb的内核更新方式" class="headerlink" title="deb的内核更新方式"></a>deb的内核更新方式</h2><h3 id="获取内核代码"><a href="#获取内核代码" class="headerlink" title="获取内核代码"></a>获取内核代码</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs bash">apt-get update<br>apt-get install git<br><span class="hljs-built_in">mkdir</span> ~/rk3588-sdk &amp;&amp; <span class="hljs-built_in">cd</span> ~/rk3588-sdk<br>git <span class="hljs-built_in">clone</span> -b stable-5.10-rock5 https://github.com/radxa/u-boot.git<br>git <span class="hljs-built_in">clone</span> -b stable-5.10-rock5 https://github.com/radxa/kernel.git<br>git <span class="hljs-built_in">clone</span> -b master https://github.com/radxa/rkbin.git<br>git <span class="hljs-built_in">clone</span> -b debian https://github.com/radxa/build.git<br>git <span class="hljs-built_in">clone</span> -b main https://github.com/radxa/debos-radxa.git<br></code></pre></td></tr></table></figure><p>如果是在X86的环境下面编译就按官方文档安装工具链，如果就是在arm64板卡上面进行编译的，就不需要，目前我的编译环境是在arm64下面，就按arm64的写步骤</p><p>能够提供arm64编译环境的地方：</p><ul><li>1、板卡，rock4b或者rock5b</li><li>2、mac M1 安装ubuntu 虚拟机</li><li>3、大型的arm64服务器</li></ul><h3 id="安装依赖包"><a href="#安装依赖包" class="headerlink" title="安装依赖包"></a>安装依赖包</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">apt-get install device-tree-compiler libncurses5 libncurses5-dev build-essential libssl-dev mtools bc python dosfstools<br></code></pre></td></tr></table></figure><h3 id="打包成deb内核包"><a href="#打包成deb内核包" class="headerlink" title="打包成deb内核包"></a>打包成deb内核包</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">./build/pack-kernel.sh -d rockchip_linux_defconfig -r 10 <span class="hljs-comment"># rockchip_linux_defconfig: kernel defconfig; 1: release number</span><br></code></pre></td></tr></table></figure><p>编译完成以后就在下面的这个目录里面有相关的deb包生成，然后去安装即可</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">ls</span> out/packages/<br></code></pre></td></tr></table></figure><h2 id="内核启动分析"><a href="#内核启动分析" class="headerlink" title="内核启动分析"></a>内核启动分析</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs bash">root@rock-5b:~<span class="hljs-comment"># cat /boot/extlinux/extlinux.conf</span><br><span class="hljs-built_in">timeout</span> 10<br>menu title <span class="hljs-keyword">select</span> kernel<br><br>label kernel-5.10.66-11-rockchip-gc428536281d6<br>    kernel /vmlinuz-5.10.66-11-rockchip-gc428536281d6<br>    initrd /initrd.img-5.10.66-11-rockchip-gc428536281d6<br>    devicetreedir /dtbs/5.10.66-11-rockchip-gc428536281d6<br>    fdtoverlays  /dtbs/5.10.66-11-rockchip-gc428536281d6/rockchip/overlay/rk3588-uart7-m2.dtbo<br>    append   root=UUID=67ad0e7b-3914-48d6-97c2-c48e5e0e405b earlycon=uart8250,mmio32,0xfeb50000 console=ttyFIQ0 console=tty1 consoleblank=0 loglevel=0 panic=10 rootwait rw init=/sbin/init rootfstype=ext4 cgroup_enable=cpuset cgroup_memory=1 cgroup_enable=memory swapaccount=1 irqchip.gicv3_pseudo_nmi=0 switolb=1 coherent_pool=2M<br></code></pre></td></tr></table></figure><p>这个就是板卡的启动控制文件，这个跟x86的那个grub也是类似的，都是加载模块，然后根据指定的参数启动</p><ul><li>kernel 就是内核，启动的时候加载的</li><li>initrd 这个是根据&#x2F;lib&#x2F;modules&#x2F;kernel生成的，是内核的一些模块，我们编译内核的时候*就是放在内核里面，M的模块就是放在了modules里面，然后生成的initrd可以在启动的时候加载这部分的内核</li><li>devicetreedir 这个就是dtbs设备树</li><li>fdtoverlays 这部分是补充的一部分的</li><li>append 是启动的控制参数部分</li></ul><p>deb包的是会自己处理好这部分的，但是手动的，可以看到只提供了Image和fdt两个，单纯使用这两个是启动不了内核的，因为sdk提供的内核里面把一些模块是按module的方式处理的，Image里面并没有这部分，也就无法正常的启动了</p><h2 id="手动替换方法"><a href="#手动替换方法" class="headerlink" title="手动替换方法"></a>手动替换方法</h2><h3 id="下载内核"><a href="#下载内核" class="headerlink" title="下载内核"></a>下载内核</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">git <span class="hljs-built_in">clone</span> -b stable-5.10-rock5 https://github.com/radxa/kernel.git<br></code></pre></td></tr></table></figure><h3 id="检查内核"><a href="#检查内核" class="headerlink" title="检查内核"></a>检查内核</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs bash">root@ubuntu:~/rk3588-sdk/kernel<span class="hljs-comment"># git log</span><br>commit c428536281d69aeb2b3480f65b2b227210b61535 (HEAD -&gt; stable-5.10-rock5, origin/stable-5.10-rock5)<br>Author: 忘怀 &lt;otgwt@outlook.com&gt;<br>Date:   Tue Nov 1 09:08:05 2022 +0800<br><br>    rockchip_linux_defconfig: add Kubernetes support2 (<span class="hljs-comment">#36)</span><br><br>    To support kubernetes and bring more Networking feature，<br>    Need to open more Network supports.<br>    NETFILTER sets、MT_TCP、TLS、TCPacc、802.1d<br>    Fix issue : rockchip-linux<span class="hljs-comment">#273</span><br><br>    <span class="hljs-comment"># Network packet filtering framework (Netfilter)</span><br>    CONFIG_NETFILTER_XT_TARGET_CHECKSUM=m<br>    CONFIG_NETFILTER_XT_TARGET_DSCP=m<br>    CONFIG_NETFILTER_XT_TARGET_TPROXY=m<br>    CONFIG_NETFILTER_XT_TARGET_TRACE=m<br>    CONFIG_NETFILTER_XT_TARGET_TCPOPTSTRIP=m<br></code></pre></td></tr></table></figure><p>可以看到最新的是解决了k8s的运行问题，缺了一些模块，官方开启了相关的模块</p><h3 id="编译内核"><a href="#编译内核" class="headerlink" title="编译内核"></a>编译内核</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">cd</span> /root/rk3588-sdk/kernel<br>make kernelversion<br>make rockchip_linux_defconfig<br>make -j24<br></code></pre></td></tr></table></figure><p>上面的rockchip_linux_defconfig文件路径为arch&#x2F;arm64&#x2F;configs&#x2F;rockchip_linux_defconfig,这个就是内核的配置文件，开启了这些配置，如果需要更改这个默认的配置就按下面的流程，官方的脚本是调用的这个默认配置，如果是自己手动编译，就直接修改即可<br>上面的编译完成以后，并没有结束，我们需要提取我们需要的东西，官方的文档是Image和dtb文件，我们按照deb包里面的进行提取</p><h3 id="修改默认内核配置"><a href="#修改默认内核配置" class="headerlink" title="修改默认内核配置"></a>修改默认内核配置</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">make menuconfig<br>make savedefconfig<br><span class="hljs-built_in">cp</span> defconfig <span class="hljs-built_in">arch</span>/arm64/configs/rockchip_linux_defconfig<br></code></pre></td></tr></table></figure><h3 id="提取内核输出"><a href="#提取内核输出" class="headerlink" title="提取内核输出"></a>提取内核输出</h3><p>这个步骤就是上面的编译完成以后我们需要提取的东西，按下面的步骤操作</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">mkdir</span> /tmp/out/<br><span class="hljs-built_in">export</span>  INSTALL_PATH=/tmp/out/; make install<br><span class="hljs-built_in">export</span>  INSTALL_PATH=/tmp/out/; make dtbs_install<br><span class="hljs-built_in">export</span> INSTALL_MOD_PATH=/tmp/out;make modules_install<br></code></pre></td></tr></table></figure><ul><li>上面的第一个install 是安装的内核vmlinuz文件</li><li>第二个dtbs_install 是安装的dtbs相关的</li><li>第三个modules_install 是安装的内核模块的文件</li></ul><p>默认的打出来的版本号是5.10.66-267892-gc428536281d6，</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">out/lib/modules/5.10.66-267892-gc428536281d6<br></code></pre></td></tr></table></figure><p>为了避免冲突或者错误的替换，我们自己加入自己需要的版本信息</p><p>内核的版本信息在这里面</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">cat</span> kernel/include/generated/utsrelease.h <br></code></pre></td></tr></table></figure><p>是根据makefile和.config生成的</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">root@ubuntu:~/rk3588-sdk/kernel<span class="hljs-comment"># make -j 48</span><br>  UPD     include/config/kernel.release<br>  UPD     include/generated/utsrelease.h<br></code></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs bash">root@ubuntu:~/rk3588-sdk/kernel<span class="hljs-comment"># head  -n 10 Makefile</span><br><span class="hljs-comment"># SPDX-License-Identifier: GPL-2.0</span><br>VERSION = 5<br>PATCHLEVEL = 10<br>SUBLEVEL = 66<br>EXTRAVERSION = -zp1<br>NAME = Dare mighty thing<br></code></pre></td></tr></table></figure><p>我们在makefile里面增加EXTRAVERSION &#x3D; -zp1</p><p>上面的操作以后我们得到的就是下面的这些，我们拷贝到rock5b的机器上</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs bash">root@ubuntu:~<span class="hljs-comment"># ll /tmp/out/</span><br>total 37040<br>drwxr-xr-x  4 root root     4096 Nov 24 07:21 ./<br>drwxrwxrwt 14 root root     4096 Nov 24 07:22 ../<br>-rw-r--r--  1 root root   210401 Nov 24 07:20 config-5.10.66-zp1-267892-gc428536281d6-dirty<br>drwxr-xr-x  3 root root     4096 Nov 24 07:21 dtbs/<br>drwxr-xr-x  3 root root     4096 Nov 24 07:21 lib/<br>-rw-r--r--  1 root root  7165888 Nov 24 07:20 System.map-5.10.66-zp1-267892-gc428536281d6-dirty<br>-rw-r--r--  1 root root 30530048 Nov 24 07:20 vmlinuz-5.10.66-zp1-267892-gc428536281d6-dirty<br></code></pre></td></tr></table></figure><h3 id="安装内核"><a href="#安装内核" class="headerlink" title="安装内核"></a>安装内核</h3><p>拷贝模块</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">cp</span> -ra lib/modules/5.10.66-zp1-267892-gc428536281d6-dirty/ /lib/modules/<br></code></pre></td></tr></table></figure><h4 id="半自动处理方式"><a href="#半自动处理方式" class="headerlink" title="半自动处理方式"></a>半自动处理方式</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">mkdir</span> /usr/lib/linux-image-5.10.66-zp1-267892-gc428536281d6-dirty<br><span class="hljs-built_in">cp</span> -ra vmlinuz-5.10.66-zp1-267892-gc428536281d6-dirty /boot/<br><span class="hljs-built_in">cp</span> -ra dtbs/5.10.66-zp1-267892-gc428536281d6-dirty/rockchip  /usr/lib/linux-image-5.10.66-zp1-267892-gc428536281d6-dirty<br>run-parts --arg=<span class="hljs-string">&quot;5.10.66-zp1-267892-gc428536281d6-dirty&quot;</span> --arg=<span class="hljs-string">&quot;/boot/vmlinuz-5.10.66-zp1-267892-gc428536281d6-dirty&quot;</span> /etc/kernel/postinst.d<br></code></pre></td></tr></table></figure><p>上面的run-parts脚本<br>做了下面几个工作：</p><ul><li>1、生成了（&#x2F;boot&#x2F;initrd.img-5.10.66-zp1-267892-gc428536281d6-dirty）</li><li>2、把&#x2F;usr&#x2F;lib&#x2F;linux-image-5.10.66-zp1-267892-gc428536281d6-dirty里面的dtb拷贝到dtbs里面</li><li>3、更新extlinux.conf脚本</li></ul><h4 id="纯手动处理方式"><a href="#纯手动处理方式" class="headerlink" title="纯手动处理方式"></a>纯手动处理方式</h4><p>拷贝文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">cp</span> -ra dtbs/5.10.66-zp1-267892-gc428536281d6-dirty/ /boot/dtbs/<br><span class="hljs-built_in">cp</span> -ra vmlinuz-5.10.66-zp1-267892-gc428536281d6-dirty /boot/<br><span class="hljs-built_in">cp</span> -ra System.map-5.10.66-zp1-267892-gc428536281d6-dirty /boot/<br><span class="hljs-built_in">cp</span> -ra config-5.10.66-zp1-267892-gc428536281d6-dirty /boot/<br></code></pre></td></tr></table></figure><p>生成initrd</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">root@rock-5b:~/zp/out<span class="hljs-comment"># update-initramfs -k 5.10.66-zp1-267892-gc428536281d6-dirty -c</span><br>update-initramfs: Generating /boot/initrd.img-5.10.66-zp1-267892-gc428536281d6-dirty<br></code></pre></td></tr></table></figure><p>编写&#x2F;boot&#x2F;extlinux&#x2F;extlinux.conf文件</p><h4 id=""><a href="#" class="headerlink" title=""></a></h4><p>上面的手工部分是通过解压官方的image查看到的脚本</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">dpkg -e linux-image-5.10.66-11-rockchip-gc428536281d6_5.10.66-11-rockchip_arm64.deb debian/<br><span class="hljs-built_in">cat</span> debian/postinst<br></code></pre></td></tr></table></figure><h4 id="内核启动脚本内核"><a href="#内核启动脚本内核" class="headerlink" title="内核启动脚本内核"></a>内核启动脚本内核</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs bash">root@rock-5b:/boot<span class="hljs-comment"># cat /boot/extlinux/extlinux.conf</span><br><span class="hljs-comment">#timeout 10</span><br><span class="hljs-comment">#menu title select kernel</span><br><br>label kernel-5.10.66-zp1-267892-gc428536281d6-dirty<br>    kernel /vmlinuz-5.10.66-zp1-267892-gc428536281d6-dirty<br>    initrd /initrd.img-5.10.66-zp1-267892-gc428536281d6-dirty<br>    devicetreedir /dtbs/5.10.66-zp1-267892-gc428536281d6-dirty<br>    fdtoverlays  /dtbs/5.10.66-zp1-267892-gc428536281d6-dirty/rockchip/overlay/rk3588-uart7-m2.dtbo<br>    append   root=UUID=67ad0e7b-3914-48d6-97c2-c48e5e0e405b earlycon=uart8250,mmio32,0xfeb50000 console=ttyFIQ0 console=tty1 consoleblank=0 loglevel=0 panic=10 rootwait rw init=/sbin/init rootfstype=ext4 cgroup_enable=cpuset cgroup_memory=1 cgroup_enable=memory swapaccount=1 irqchip.gicv3_pseudo_nmi=0 switolb=1 coherent_pool=2M<br></code></pre></td></tr></table></figure><p>重启后检查</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">root@rock-5b:~<span class="hljs-comment"># uname  -a</span><br>Linux rock-5b 5.10.66-zp1-267892-gc428536281d6-dirty <span class="hljs-comment">#2 SMP Thu Nov 24 04:26:01 UTC 2022 aarch64 aarch64 aarch64 GNU/Linux</span><br></code></pre></td></tr></table></figure><p>可以看到内核已经替换好了</p>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>脚本禁止登录尝试</title>
    <link href="/2023/01/31/%E8%84%9A%E6%9C%AC%E7%A6%81%E6%AD%A2%E7%99%BB%E5%BD%95%E5%B0%9D%E8%AF%95/"/>
    <url>/2023/01/31/%E8%84%9A%E6%9C%AC%E7%A6%81%E6%AD%A2%E7%99%BB%E5%BD%95%E5%B0%9D%E8%AF%95/</url>
    
    <content type="html"><![CDATA[<h1 id="脚本禁止登录尝试"><a href="#脚本禁止登录尝试" class="headerlink" title="脚本禁止登录尝试"></a>脚本禁止登录尝试</h1><h2 id="需求"><a href="#需求" class="headerlink" title="需求"></a>需求</h2><p>有ip不停的尝试登录，可以通过防火墙进行屏蔽</p><span id="more"></span><h2 id="脚本"><a href="#脚本" class="headerlink" title="脚本"></a>脚本</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">cat</span> /var/log/secure |grep failure|awk <span class="hljs-string">&#x27;&#123;for (i=1;i&lt;=NF;i++)&#123;if ($i ~/rhost/) &#123;print $i&#125;&#125;&#125;&#x27;</span>| <span class="hljs-built_in">sort</span> | <span class="hljs-built_in">uniq</span>|awk -F<span class="hljs-string">&quot;=&quot;</span> <span class="hljs-string">&#x27;&#123;print $2&#125;&#x27;</span> &gt;&gt; black.list<br><br><span class="hljs-built_in">cat</span> black.list|<span class="hljs-built_in">sort</span>|<span class="hljs-built_in">uniq</span> &gt; black.list.tmp<br><span class="hljs-built_in">rm</span> -rf black.list<br><span class="hljs-built_in">mv</span> black.list.tmp black.list<br><br>firewall-cmd --list-rich-rules &gt; blackiprule.list<br>all=`<span class="hljs-built_in">cat</span> blackiprule.list`<br><br><span class="hljs-keyword">for</span> ip <span class="hljs-keyword">in</span> `<span class="hljs-built_in">cat</span> black.list|<span class="hljs-built_in">sort</span>|<span class="hljs-built_in">uniq</span>`<br><span class="hljs-keyword">do</span><br><span class="hljs-keyword">if</span> [[ <span class="hljs-variable">$all</span> =~ <span class="hljs-variable">$ip</span> ]]<br><span class="hljs-keyword">then</span><br>:<br><span class="hljs-keyword">else</span><br><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;不包含&quot;</span><br>firewall-cmd --permanent --add-rich-rule=<span class="hljs-string">&quot;rule family=&#x27;ipv4&#x27; source address=&#x27;<span class="hljs-variable">$ip</span>&#x27; reject&quot;</span><br><span class="hljs-keyword">fi</span><br><span class="hljs-keyword">done</span><br><br><span class="hljs-built_in">cat</span> blackiprule.list|<span class="hljs-built_in">wc</span> -l<br>firewall-cmd --reload<br></code></pre></td></tr></table></figure><p>定期执行脚本即可</p><p>上面的脚本存在一个问题，如果机器上面拒绝的ip太多了话，这个防火墙的列表会非常的大，而下面的这个就没有这个问题，建议用下面的方法</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@VM-4-7-centos ~]<span class="hljs-comment"># cat blackdeny.sh</span><br><span class="hljs-comment">#! /bin/sh</span><br><span class="hljs-built_in">cat</span> /var/log/secure |grep failure|awk <span class="hljs-string">&#x27;&#123;for (i=1;i&lt;=NF;i++)&#123;if ($i ~/rhost/) &#123;print $i&#125;&#125;&#125;&#x27;</span>| <span class="hljs-built_in">sort</span> | <span class="hljs-built_in">uniq</span>|awk -F<span class="hljs-string">&quot;=&quot;</span> <span class="hljs-string">&#x27;&#123;print $2&#125;&#x27;</span> &gt;&gt; /root/black.list<br><br><span class="hljs-built_in">cat</span> /root/black.list|<span class="hljs-built_in">sort</span>|<span class="hljs-built_in">uniq</span> &gt; /root/black.list.tmp<br><span class="hljs-built_in">rm</span> -rf /root/black.list<br><span class="hljs-built_in">mv</span> /root/black.list.tmp /root/black.list<br><br>&gt; /etc/hosts.deny<br><span class="hljs-keyword">for</span> ip <span class="hljs-keyword">in</span> `<span class="hljs-built_in">cat</span> black.list|<span class="hljs-built_in">sort</span>|<span class="hljs-built_in">uniq</span>`<br><span class="hljs-keyword">do</span><br><span class="hljs-built_in">echo</span> ALL:<span class="hljs-variable">$ip</span> &gt;&gt; /etc/hosts.deny<br><span class="hljs-keyword">done</span><br><br><span class="hljs-built_in">cat</span> black.list|<span class="hljs-built_in">wc</span> -l<br></code></pre></td></tr></table></figure><p>写一个定时任务</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@VM-4-7-centos ~]<span class="hljs-comment"># cat /etc/cron.d/black</span><br>*/30 * * * * root /root/blackdeny.sh<br></code></pre></td></tr></table></figure><p>下面是目前收集到的,可以预填充下列表(截止2024年12月27日 3572个ip)</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">wget https://www.zphj1987.com/images/black.list<br></code></pre></td></tr></table></figure><p>可以作为初始的ip</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>获取一段时间内ceph的osd磁盘读写情况</title>
    <link href="/2023/01/31/%E8%8E%B7%E5%8F%96%E4%B8%80%E6%AE%B5%E6%97%B6%E9%97%B4%E5%86%85ceph%E7%9A%84osd%E7%A3%81%E7%9B%98%E8%AF%BB%E5%86%99%E6%83%85%E5%86%B5/"/>
    <url>/2023/01/31/%E8%8E%B7%E5%8F%96%E4%B8%80%E6%AE%B5%E6%97%B6%E9%97%B4%E5%86%85ceph%E7%9A%84osd%E7%A3%81%E7%9B%98%E8%AF%BB%E5%86%99%E6%83%85%E5%86%B5/</url>
    
    <content type="html"><![CDATA[<h1 id="获取一段时间内ceph的osd磁盘读写情况"><a href="#获取一段时间内ceph的osd磁盘读写情况" class="headerlink" title="获取一段时间内ceph的osd磁盘读写情况"></a>获取一段时间内ceph的osd磁盘读写情况</h1><h2 id="需求"><a href="#需求" class="headerlink" title="需求"></a>需求</h2><p>这个是统计一段时间内ceph的磁盘上的数据的读取和写入的数据量以及花费的时间</p><span id="more"></span><h2 id="信息获取"><a href="#信息获取" class="headerlink" title="信息获取"></a>信息获取</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@node235 ceph]<span class="hljs-comment"># cat /sys/block/sdj/sdj1/stat</span><br>    1575        0   335680     1687   145461       46  5334992   269147        0    54562   270566<br><br>第一个域 读取磁盘的次数<br>第二个是 合并读取的次数<br>第三个域 读扇区的次数<br>第四个域 读花费的毫秒数<br>第五个域 写完成次数<br>第六个域 合并写次数<br>第七个域 写扇区的次数<br>第八个域 写操作花的毫秒数<br>第九个域 正在处理的io数目<br>第十个域 输入输出花的总毫秒数<br>第十一个域 输入输出花的加权毫秒数<br></code></pre></td></tr></table></figure><p>来源：</p><blockquote><p><a href="https://www.kernel.org/doc/html/v5.3/admin-guide/iostats.html">https://www.kernel.org/doc/html/v5.3/admin-guide/iostats.html</a></p></blockquote><h2 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@node235 ceph]<span class="hljs-comment"># cat getread.sh </span><br>[root@node235 ceph]<span class="hljs-comment"># cat getread.sh </span><br><span class="hljs-comment">#! /bin/bash</span><br>IFS=$<span class="hljs-string">&#x27;\n\n&#x27;</span> <br><span class="hljs-keyword">for</span> osd <span class="hljs-keyword">in</span> `ceph osd dump|grep osd|grep -v req|grep -v max|awk <span class="hljs-string">&#x27;&#123;print $1,$19&#125;&#x27;</span>`<br><span class="hljs-keyword">do</span> <br>osdnum=`<span class="hljs-built_in">echo</span>  <span class="hljs-variable">$osd</span>|awk <span class="hljs-string">&#x27;&#123;print $1&#125;&#x27;</span>`<br>osdid=`<span class="hljs-built_in">echo</span>  <span class="hljs-variable">$osd</span>|awk <span class="hljs-string">&#x27;&#123;print $2&#125;&#x27;</span>`<br>vgname=`lvdisplay |grep <span class="hljs-variable">$osdid</span> -A 1|<span class="hljs-built_in">tail</span>  -n 1|awk <span class="hljs-string">&#x27;&#123;print $3&#125;&#x27;</span>`<br>pvname=`pvdisplay |grep <span class="hljs-variable">$vgname</span> -B1|<span class="hljs-built_in">head</span> -n 1|awk <span class="hljs-string">&#x27;&#123;print $3&#125;&#x27;</span>`<br><br>diskname=`<span class="hljs-built_in">echo</span> <span class="hljs-variable">$&#123;pvname%?&#125;</span>|<span class="hljs-built_in">cut</span> -d / -f 3`<br>pvpath=`<span class="hljs-built_in">echo</span> <span class="hljs-variable">$pvname</span>|<span class="hljs-built_in">cut</span> -d / -f 3`<br>readsec=`<span class="hljs-built_in">cat</span> /sys/block/<span class="hljs-variable">$diskname</span>/<span class="hljs-variable">$pvpath</span>/stat|awk <span class="hljs-string">&#x27;&#123;print $3&#125;&#x27;</span>`<br>readmb=`<span class="hljs-built_in">expr</span> <span class="hljs-variable">$readsec</span> \* 512 / 1024 / 1024`<br>readtime=`<span class="hljs-built_in">cat</span> /sys/block/<span class="hljs-variable">$diskname</span>/<span class="hljs-variable">$pvpath</span>/stat|awk <span class="hljs-string">&#x27;&#123;print $4&#125;&#x27;</span>`<br>readtimesec=`<span class="hljs-built_in">expr</span> <span class="hljs-variable">$readtime</span> / 1000`<br>writesec=`<span class="hljs-built_in">cat</span> /sys/block/<span class="hljs-variable">$diskname</span>/<span class="hljs-variable">$pvpath</span>/stat|awk <span class="hljs-string">&#x27;&#123;print $7&#125;&#x27;</span>`<br>writemb=`<span class="hljs-built_in">expr</span> <span class="hljs-variable">$writesec</span> \* 512 / 1024 / 1024`<br>writetime=`<span class="hljs-built_in">cat</span> /sys/block/<span class="hljs-variable">$diskname</span>/<span class="hljs-variable">$pvpath</span>/stat|awk <span class="hljs-string">&#x27;&#123;print $8&#125;&#x27;</span>`<br>writetimesec=`<span class="hljs-built_in">expr</span> <span class="hljs-variable">$writetime</span> / 1000`<br><span class="hljs-built_in">echo</span>  <span class="hljs-variable">$osdnum</span>  <span class="hljs-variable">$osdid</span>  <span class="hljs-variable">$vgname</span>  <span class="hljs-variable">$pvname</span>  <span class="hljs-variable">$readmb</span> <span class="hljs-variable">$readtimesec</span> <span class="hljs-variable">$writemb</span> <span class="hljs-variable">$writetimesec</span><br><br><span class="hljs-keyword">done</span><br><br><br></code></pre></td></tr></table></figure><h2 id="效果"><a href="#效果" class="headerlink" title="效果"></a>效果</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@node235 ceph]<span class="hljs-comment"># bash getread.sh </span><br>osd.0 f6203dc0-7007-42da-bb1b-6ac54a9f9f74 ceph-36597766-1058-490f-bb46-ae4fd8eb141a /dev/sdd1 10063 86 13 0<br>osd.1 c4b7f141-51a0-4b97-80b5-e9681025f7db ceph-f786b5a2-82f4-4086-b0d1-7f5e6787c421 /dev/sdc1 320 1 1606 174<br>osd.2 24f68dd2-5a4f-4b41-8775-ae0211a06e5d ceph-07e22954-93d0-4bb1-9ce3-59485eab1673 /dev/sdi1 249 1 1604 155<br>osd.3 deed6a2e-456f-40be-bade-07a4b358e2c1 ceph-f1f76e61-d0d6-447b-ac26-a9a2d7fd2d5d /dev/sds1 320 1 1606 169<br>osd.4 f325a546-a6c7-4610-9916-31062ec6decc ceph-5c1357a5-036a-4f2e-83a8-4463e4349264 /dev/sdf1 320 2 1606 162<br>osd.5 3790748f-f212-4d46-8b72-b6c9ba007019 ceph-9874a1ac-9163-4936-a2f9-976260f19d6d /dev/sdr1 321 1 1606 165<br>osd.6 50140786-a01d-47c4-bca4-68ad7730863c ceph-0e4af92c-9766-467c-a7d5-db4435aadd6b /dev/sdk1 243 1 1604 149<br>osd.7 5e56f9a0-7637-477a-95cf-b995987e6270 ceph-c0de6d4b-1907-49d7-aa9b-194ccbc72465 /dev/sdb1 321 2 1606 172<br>osd.8 261ea9d8-581b-42ec-9493-1df7b74394b7 ceph-54946fde-edc4-4cf1-b59a-375d69bfff7a /dev/sdm1 237 1 1604 149<br>osd.9 a4212438-e463-48f3-be58-7668df524c35 ceph-805428c1-077c-4092-80f5-95b0ac9f5ea5 /dev/sdh1 321 2 1606 172<br>osd.10 ebfb8caf-a3e6-4715-b992-9ca560104c4e ceph-7e9809bf-c4c0-4686-84f6-4e8639f02dac /dev/sde1 322 2 1606 157<br>osd.11 7c310379-994d-4faa-9724-1ec1bf6d85bd ceph-3b602d70-99d0-496e-8b96-c34b36d973bc /dev/sdo1 231 1 1604 151<br>osd.12 cd608ef9-d491-4d09-8f7f-ac82e026b321 ceph-2a8d2275-d307-41ab-9125-17dbe610559a /dev/sdg1 322 2 1606 172<br>osd.14 782cb3ed-9fcd-40aa-9db3-1cd7babda3fb ceph-9117f02b-c6a0-43a0-ac3f-c0af75381c5f /dev/sdq1 323 1 1606 166<br>osd.15 77275ae5-d953-44fc-8196-d6b31e005fb3 ceph-c30f789a-3213-49ce-8831-2296e5697119 /dev/sdp1 322 1 1606 168<br>osd.16 1a325633-209d-4c3a-a89a-c90d6c26688f ceph-397d3bce-9f1a-4e33-9bd0-48f52804a7f6 /dev/sdj1 324 2 2629 264<br></code></pre></td></tr></table></figure><p>最右边的四个数字是：<br>读取的MB  读取的秒  写入的MB  写入的秒</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>多任务的时候如何用screen进行日志记录并留后台运行</title>
    <link href="/2021/04/30/%E5%A4%9A%E4%BB%BB%E5%8A%A1%E7%9A%84%E6%97%B6%E5%80%99%E5%A6%82%E4%BD%95%E7%94%A8screen%E8%BF%9B%E8%A1%8C%E6%97%A5%E5%BF%97%E8%AE%B0%E5%BD%95%E5%B9%B6%E7%95%99%E5%90%8E%E5%8F%B0%E8%BF%90%E8%A1%8C/"/>
    <url>/2021/04/30/%E5%A4%9A%E4%BB%BB%E5%8A%A1%E7%9A%84%E6%97%B6%E5%80%99%E5%A6%82%E4%BD%95%E7%94%A8screen%E8%BF%9B%E8%A1%8C%E6%97%A5%E5%BF%97%E8%AE%B0%E5%BD%95%E5%B9%B6%E7%95%99%E5%90%8E%E5%8F%B0%E8%BF%90%E8%A1%8C/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>多个终端执行类似的任务，想把日志都记录下来</p><h2 id="操作"><a href="#操作" class="headerlink" title="操作"></a>操作</h2><h3 id="创建任务的目录"><a href="#创建任务的目录" class="headerlink" title="创建任务的目录"></a>创建任务的目录</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@localhost ~]<span class="hljs-comment"># mkdir /chia-1</span><br>[root@localhost ~]<span class="hljs-comment"># mkdir /chia-2</span><br>[root@localhost ~]<span class="hljs-comment"># mkdir /chia-3</span><br></code></pre></td></tr></table></figure><h3 id="给每个任务创建一个screen的配置文件"><a href="#给每个任务创建一个screen的配置文件" class="headerlink" title="给每个任务创建一个screen的配置文件"></a>给每个任务创建一个screen的配置文件</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@localhost ~]<span class="hljs-comment"># scp .screenrc /chia-1/</span><br>[root@localhost ~]<span class="hljs-comment"># scp .screenrc /chia-2/</span><br>[root@localhost ~]<span class="hljs-comment"># scp .screenrc /chia-3/</span><br>[root@localhost ~]<span class="hljs-comment"># cat .screenrc</span><br>logfile ./chia.log<br></code></pre></td></tr></table></figure><h3 id="执行任务"><a href="#执行任务" class="headerlink" title="执行任务"></a>执行任务</h3><p>进入到任务的目录里面执行任务</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">cd</span> /chia-1<br>screen -c .screenrc -SL chia-1<br></code></pre></td></tr></table></figure><p>执行需要执行的命令</p><p>ctrl+a+d退出<br>恢复终端</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">scree -r chia-1<br></code></pre></td></tr></table></figure><p>然后中间看chia.log就行了</p>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>screen几个使用方法</title>
    <link href="/2021/04/26/screen%E5%87%A0%E4%B8%AA%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95/"/>
    <url>/2021/04/26/screen%E5%87%A0%E4%B8%AA%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<p>##前言<br>screen用了很久，最近有几个需求，需要进行配置，记录相关的操作</p><h2 id="需求"><a href="#需求" class="headerlink" title="需求"></a>需求</h2><h3 id="开启日志"><a href="#开启日志" class="headerlink" title="开启日志"></a>开启日志</h3><p>需要对screen里面的终端输出进行日志的记录，方便后续的使用</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">screen -c  .screenrc -LS myname<br><br>[root@node141 <span class="hljs-built_in">test</span>]<span class="hljs-comment"># cat .screenrc</span><br>logfile /root/test.log<br></code></pre></td></tr></table></figure><p>上面的命令说明-c是指定配置文件的，如果没配置，会使用默认的配置文件，我们可以创建多个目录，然后每个目录以终端名称命名，然后指定log的地方即可</p><h3 id="已经开启的终端临时开启日志"><a href="#已经开启的终端临时开启日志" class="headerlink" title="已经开启的终端临时开启日志"></a>已经开启的终端临时开启日志</h3><p>screen -r进入终端<br>然后执行ctrl+a<br>然后输入:<br>然后敲logfile  &#x2F;root&#x2F;test.log<br>然后执行ctrl+a<br>然后输入:<br>然后敲 log on</p><p>日志就会写入到指定的路径了</p>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>filecoin初探</title>
    <link href="/2021/03/31/filecoin%E5%88%9D%E6%8E%A2/"/>
    <url>/2021/03/31/filecoin%E5%88%9D%E6%8E%A2/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>filecoin最近比较火，准确来说应该2020年慢慢形成了一定的规模，初看上去，这个系统比较复杂，之前一直接触的是底层存储，逻辑上比较简单，你花钱，我提供存储，是一对一的，而filecoin这个就是基于区块链技术打造的一套系统，本篇基于目前的认知去做一些分析，后续有更新会进行一些更新</p><h2 id="ipfs"><a href="#ipfs" class="headerlink" title="ipfs"></a>ipfs</h2><p>如果是早几年的网友应该记得某播，虽然后面被取缔了，但是那套系统还是一个很先进的系统，在互联网当中，每个人既可以是用户，也是可以对外提供数据的，俗称p2p技术，这个可以大概有个理解，就是每个人是服务者也是访问者</p><p>filecoin的存储机制类似于ipfs，ip这个地方是星际的意思，可以理解为去中心化，如果你用一个进程把你的服务上线，那么你就是整个互联网的存储服务的一个提供者了，任何加入到这个链路里面的人都可以去使用里面的存储，这个从技术上已经实现了异地的访问，传统的服务的结构是cs结构，有一个服务提供者，上面存储了数据，然后我们去访问，ipfs是从存活的节点里面去访问你需要的数据，每个内容都会在网络里面形成唯一的一个地址</p><p>如果是单纯的ipfs，那么就有个问题了，如果是公共网络里面，没有好处的话，为什么要让自己的存储对外服务，所以这个只能是一个理想状态的底层技术</p><h2 id="filecoin"><a href="#filecoin" class="headerlink" title="filecoin"></a>filecoin</h2><p>那么为了保证存储可以上线，那么就需要有货币的激励了，那么FIL就产生了，需要存储的用户可以通过fil去购买存储，那么提供存储的用户，也能够得到fil，然后随着规模的扩大，会产生一些新的fil，这部分就会根据规模去分给提供存储的这些账户，fil跟真实货币是有一个换算比例的，这个换算比例是浮动的</p><p>这个里面跟比特币的挖矿区别比较大的一点就是，进行提供存储是需要质押一定的fil的，也就是类似于押金，然后提供到足够的服务的时候，这些币就会慢慢回收回来，只要提供足够稳定足够时间长的存储，你的币会增加，里面的挖矿的过程实际就是一个存储运算，提交保证等等一系列的操作的过程</p><p>如果说比特币的挖矿是浪费资源的话（挖矿过程除了计算，并没有提供什么服务），那filecoin是实实在在能够提供存储服务的，所以个人觉得未来本身这个服务是能够产生价值的</p><h2 id="怎么赚钱"><a href="#怎么赚钱" class="headerlink" title="怎么赚钱"></a>怎么赚钱</h2><p>实际上这个地方赚钱是有两个地方的<br>一个地方是对外提供存储的过程，这个是可以赚取一定的钱的<br>另外一个就是FIL币本身的浮动，如果你买了准备挖矿，200买的币，准备投进去，结果还没开始挖，币涨到400了，你还没开始挖，这个就赚了一倍了，当然运气不好，400进入的，还没开始，变成200了，那么也是亏一倍，当然因为有实体对外服务的存在，这个的稳定性会有一定的保障</p><h2 id="测试网络"><a href="#测试网络" class="headerlink" title="测试网络"></a>测试网络</h2><p>主网是要实实在在的货币去兑换才能进入，官方提供了一套完整的测试网络calibration，基本上可以把真实网络当中的操作都在这个测试网络当中进行，后续的一些研究工作主要就在这个测试网络中进行<br>这个网络的文件封装大小还是比较大，普通的机器基本没法测试，再切更小的网络测试</p>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>gimp扣图方法</title>
    <link href="/2021/03/24/gimp%E6%89%A3%E5%9B%BE%E6%96%B9%E6%B3%95/"/>
    <url>/2021/03/24/gimp%E6%89%A3%E5%9B%BE%E6%96%B9%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<h2 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h2><p>快速的从一张图片里面扣取需要的元素，背景部分去掉</p><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><p>使用的软件为gimp，免费开源的，并且客户端的，常用操作都满足</p><h3 id="倒入原始图片"><a href="#倒入原始图片" class="headerlink" title="倒入原始图片"></a>倒入原始图片</h3><p><img src="/images/blog/284415-20210324104346512-1626697998.png"></p><h3 id="模糊选择"><a href="#模糊选择" class="headerlink" title="模糊选择"></a>模糊选择</h3><p><img src="/images/blog/284415-20210324104414185-1153046343.png"></p><p>选择需要的图片轮廓，松开左键，然后按delete就把背景删除了，效果如下</p><p><img src="/images/blog/284415-20210324104513856-495743314.png"></p><p>还是有大片的透明背景需要去除</p><h3 id="去掉透明背景"><a href="#去掉透明背景" class="headerlink" title="去掉透明背景"></a>去掉透明背景</h3><p><img src="/images/blog/284415-20210324104558258-352422117.png"></p><p>点击图像，裁剪到内容</p><p><img src="/images/blog/284415-20210324104621549-922039671.png"></p><p>就完成了裁剪，然后选择导出为即可</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>上面用开源的图片工具，快速的做了扣图相关的操作</p>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>raid卡上面的磁盘对应关系查找</title>
    <link href="/2021/03/22/raid%E5%8D%A1%E4%B8%8A%E9%9D%A2%E7%9A%84%E7%A3%81%E7%9B%98%E5%AF%B9%E5%BA%94%E5%85%B3%E7%B3%BB%E6%9F%A5%E6%89%BE/"/>
    <url>/2021/03/22/raid%E5%8D%A1%E4%B8%8A%E9%9D%A2%E7%9A%84%E7%A3%81%E7%9B%98%E5%AF%B9%E5%BA%94%E5%85%B3%E7%B3%BB%E6%9F%A5%E6%89%BE/</url>
    
    <content type="html"><![CDATA[<h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>如果配置了raid，在系统层面看到的是&#x2F;dev&#x2F;sdb这样的设备，那这个设备对应到后台的物理磁盘是什么品牌的，具体到哪块设备，这个怎么查找</p><p>这个问题的来源是这样的，我有个&#x2F;dev&#x2F;sdb 我不清楚这个ssd是什么品牌的，我想用smartcl去查看这个磁盘的状态，但是过了raid以后无法直接查，需要用raid的编号查，但是又不清楚&#x2F;dev&#x2F;sdb与哪个raid对应，所以需要把这几个关系关联起来，那么下面的几个命令就是把这个关系查询到</p><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><p>获取id</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ~]<span class="hljs-comment"># lsscsi</span><br>[0:2:0:0]    disk    Intel    RMS25CB080       3.40  /dev/sda<br>[0:2:1:0]    disk    Intel    RMS25CB080       3.40  /dev/sdb<br>[0:2:2:0]    disk    Intel    RMS25CB080       3.40  /dev/sdc<br>[0:2:3:0]    disk    Intel    RMS25CB080       3.40  /dev/sdd<br>[0:2:4:0]    disk    Intel    RMS25CB080       3.40  /dev/sde<br>[0:2:5:0]    disk    Intel    RMS25CB080       3.40  /dev/sdf<br>[0:2:6:0]    disk    Intel    RMS25CB080       3.40  /dev/sdg<br></code></pre></td></tr></table></figure><p>最左边的就是编号，对应到下一步里面的target id</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ~]<span class="hljs-comment"># /opt/megaraid/megacli  -LdPdInfo -a0 -NoLog|grep &quot;Target Id\|Inquiry Data\|Raw\|Device Id&quot;</span><br>Virtual Drive: 0 (Target Id: 0)<br>Device Id: 17<br>Raw Size: 223.570 GB [0x1bf244b0 Sectors]<br>Inquiry Data: S1RGNWAG200638X     Samsung SSD 845DC EVO 240GB             EXT03X3Q<br>Virtual Drive: 1 (Target Id: 1)<br>Device Id: 19<br>Raw Size: 238.474 GB [0x1dcf32b0 Sectors]<br>Inquiry Data: G12855R000065       FORESEE 256GB SSD                       N0530A<br>Virtual Drive: 2 (Target Id: 2)<br>Device Id: 18<br>Raw Size: 238.474 GB [0x1dcf32b0 Sectors]<br>Inquiry Data: G12855R000055       FORESEE 256GB SSD                       N0530A<br>Virtual Drive: 3 (Target Id: 3)<br>Device Id: 15<br>Raw Size: 3.638 TB [0x1d1c0beb0 Sectors]<br>Inquiry Data:             ZC11HNSWST4000NM0035-1V4107                     TN02<br>Virtual Drive: 4 (Target Id: 4)<br>Device Id: 20<br>Raw Size: 3.638 TB [0x1d1c0beb0 Sectors]<br>Inquiry Data:             S1Z1H20JST4000NM0033-9ZM170                     SN04<br>Virtual Drive: 5 (Target Id: 5)<br>Device Id: 12<br>Raw Size: 3.638 TB [0x1d1c0beb0 Sectors]<br>Inquiry Data:             ZC11KJ9JST4000NM0035-1V4107                     TN02<br>Virtual Drive: 6 (Target Id: 6)<br>Device Id: 13<br>Raw Size: 3.638 TB [0x1d1c0beb0 Sectors]<br>Inquiry Data:             S1Z1G9K2ST4000NM0033-9ZM170                     SN04<br></code></pre></td></tr></table></figure><p>上面有个Device Id，对应到下面的megaraid后面的id</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ~]<span class="hljs-comment"># smartctl --scan</span><br>/dev/sda -d scsi <span class="hljs-comment"># /dev/sda, SCSI device</span><br>/dev/sdb -d scsi <span class="hljs-comment"># /dev/sdb, SCSI device</span><br>/dev/sdc -d scsi <span class="hljs-comment"># /dev/sdc, SCSI device</span><br>/dev/sdd -d scsi <span class="hljs-comment"># /dev/sdd, SCSI device</span><br>/dev/sde -d scsi <span class="hljs-comment"># /dev/sde, SCSI device</span><br>/dev/sdf -d scsi <span class="hljs-comment"># /dev/sdf, SCSI device</span><br>/dev/sdg -d scsi <span class="hljs-comment"># /dev/sdg, SCSI device</span><br>/dev/bus/0 -d megaraid,12 <span class="hljs-comment"># /dev/bus/0 [megaraid_disk_12], SCSI device</span><br>/dev/bus/0 -d megaraid,13 <span class="hljs-comment"># /dev/bus/0 [megaraid_disk_13], SCSI device</span><br>/dev/bus/0 -d megaraid,15 <span class="hljs-comment"># /dev/bus/0 [megaraid_disk_15], SCSI device</span><br>/dev/bus/0 -d megaraid,17 <span class="hljs-comment"># /dev/bus/0 [megaraid_disk_17], SCSI device</span><br>/dev/bus/0 -d megaraid,18 <span class="hljs-comment"># /dev/bus/0 [megaraid_disk_18], SCSI device</span><br>/dev/bus/0 -d megaraid,19 <span class="hljs-comment"># /dev/bus/0 [megaraid_disk_19], SCSI device</span><br>/dev/bus/0 -d megaraid,20 <span class="hljs-comment"># /dev/bus/0 [megaraid_disk_20], SCSI device</span><br></code></pre></td></tr></table></figure><p>我们看下sn</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ~]<span class="hljs-comment"># smartctl -a /dev/bus/0 -d megaraid,19|grep Serial</span><br>Serial Number:    G12855R000065<br></code></pre></td></tr></table></figure><p>可以看到关系对应上了</p><h2 id="总结一下路径"><a href="#总结一下路径" class="headerlink" title="总结一下路径"></a>总结一下路径</h2><p>1、查到  [0:2:1:0]    disk    Intel    RMS25CB080       3.40  &#x2F;dev&#x2F;sdb</p><p>2、根据上面的查到</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">Virtual Drive: 1 (Target Id: 1)<br>Device Id: 19<br>Raw Size: 238.474 GB [0x1dcf32b0 Sectors]<br>Inquiry Data: G12855R000065       FORESEE 256GB SSD                       N0530A<br></code></pre></td></tr></table></figure><p>3、根据上面的查到</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">/dev/bus/0 -d megaraid,19 <span class="hljs-comment"># /dev/bus/0 [megaraid_disk_19], SCSI device</span><br></code></pre></td></tr></table></figure><p>然后执行查询</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">smartctl -a /dev/bus/0 -d megaraid,19<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>已配置好的osd增加opencas配置</title>
    <link href="/2021/03/17/%E5%B7%B2%E9%85%8D%E7%BD%AE%E5%A5%BD%E7%9A%84osd%E5%A2%9E%E5%8A%A0opencas%E9%85%8D%E7%BD%AE/"/>
    <url>/2021/03/17/%E5%B7%B2%E9%85%8D%E7%BD%AE%E5%A5%BD%E7%9A%84osd%E5%A2%9E%E5%8A%A0opencas%E9%85%8D%E7%BD%AE/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>环境已经配置好了osd，想增加opencas的缓存加速，因为opencas支持在lvm之上部署，也支持在opencas上面创建lvm，而已经部署好osd的场景是属于前面的一种情况，也就是已经有lvm 的情况再加缓存加速</p><p>因为ceph有自己的挂载逻辑，所以这里需要梳理一下，把整个启动路径给屏蔽掉，然后加入自己的启动逻辑</p><p>本篇是讲的没有单独的db wal的部署缓存加速的方式</p><h2 id="操作方法"><a href="#操作方法" class="headerlink" title="操作方法"></a>操作方法</h2><h3 id="创建一个缓存设备"><a href="#创建一个缓存设备" class="headerlink" title="创建一个缓存设备"></a>创建一个缓存设备</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">casadm -S -i 1 -d /dev/disk/by-id/nvme-A6F1600_SW20B27P7040007-part2 -c wb<br></code></pre></td></tr></table></figure><p>opencas的配置逻辑是创建缓存设备，然后把需要加速的设备加入到缓存设备里面，然后生成新的路径，使用新的路径即可，上面就是创建了编号为1的wb模式的缓存设备</p><p>创建好了以后写配置文件&#x2F;etc&#x2F;opencas&#x2F;opencas.conf</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">[caches]<br><span class="hljs-comment">## Cache ID     Cache device                            Cache mode      Extra fields (optional)</span><br><span class="hljs-comment">## Uncomment and edit the below line for cache configuration</span><br>1               /dev/disk/by-id/nvme-A6F1600_SW20B27P7040007-part2      WB<br></code></pre></td></tr></table></figure><h3 id="默认ceph启动逻辑"><a href="#默认ceph启动逻辑" class="headerlink" title="默认ceph启动逻辑"></a>默认ceph启动逻辑</h3><p>正常的启动是lvm设备形成以后通过ceph-volume进行启动，然后进行挂载，ceph-volume实际上就是去挂载目录的操作，这两步我们需要屏蔽掉</p><h3 id="禁用自启动部分"><a href="#禁用自启动部分" class="headerlink" title="禁用自启动部分"></a>禁用自启动部分</h3><p>我们举例osd编号为3</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@node153 ~]<span class="hljs-comment"># systemctl stop ceph-osd@3</span><br>[root@node153 ~]<span class="hljs-comment"># umount /var/lib/ceph/osd/ceph-3/</span><br><br>获取osd uuid<br>[root@node153 ~]<span class="hljs-comment">#  ceph osd dump|grep osd.3</span><br><br>[root@node153 ~]<span class="hljs-comment"># systemctl disable ceph-volume@lvm-3-de370e9a-6f2c-4584-9d3d-0b6e4c891ae2</span><br>Removed symlink /etc/systemd/system/multi-user.target.wants/ceph-volume@lvm-3-de370e9a-6f2c-4584-9d3d-0b6e4c891ae2.service.<br>[root@node153 ~]<span class="hljs-comment"># systemctl disable ceph-osd@3</span><br>[root@node153 ~]<span class="hljs-comment"># umount /var/lib/ceph/osd/ceph-3</span><br></code></pre></td></tr></table></figure><p>上面的操作以后，osd.3 的自挂载，自启动就去掉了</p><h3 id="添加缓存设备"><a href="#添加缓存设备" class="headerlink" title="添加缓存设备"></a>添加缓存设备</h3><p>获取到osd.3 的lvm的路径(取osd uuid前几位grep)</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">ls</span> /dev/mapper/ceph--|grep de370e9a<br></code></pre></td></tr></table></figure><p>添加后端设备到缓存设备</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">casadm -A  -d /dev/mapper/ceph--81296ca1--c3f9--403c--974d--0058f9eba49a-osd--block--de370e9a--6f2c--4584--9d3d--0b6e4c891ae2 -i 1 -j 3<br></code></pre></td></tr></table></figure><p>注意后面的-j是后端设备的id，这个可以设置为osd的id，方便后面查找</p><p>添加配置文件到&#x2F;etc&#x2F;opencas&#x2F;opencas.conf</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">[cores]<br><span class="hljs-comment">## Cache IDCore IDCore device</span><br><span class="hljs-comment">## Uncomment and edit the below line for core configuration</span><br>13/dev/mapper/ceph--81296ca1--c3f9--403c--974d--0058f9eba49a-osd--block--de370e9a--6f2c--4584--9d3d--0b6e4c891ae2<br></code></pre></td></tr></table></figure><h3 id="手动启动缓存加速之后的设备"><a href="#手动启动缓存加速之后的设备" class="headerlink" title="手动启动缓存加速之后的设备"></a>手动启动缓存加速之后的设备</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs bash">mount -t tmpfs tmpfs /var/lib/ceph/osd/ceph-3<br>restorecon /var/lib/ceph/osd/ceph-3<br><span class="hljs-built_in">chown</span> -R ceph:ceph /var/lib/ceph/osd/ceph-3<br>ceph-bluestore-tool --cluster=ceph prime-osd-dir --dev /dev/cas1-3 --path /var/lib/ceph/osd/ceph-3<br><span class="hljs-built_in">chown</span> -h ceph:ceph /var/lib/ceph/osd/ceph-3/block<br><span class="hljs-built_in">chown</span> -R ceph:ceph /dev/cas1-3<br><span class="hljs-built_in">chown</span> -R ceph:ceph /var/lib/ceph/osd/ceph-3<br>systemctl start ceph-osd@3<br></code></pre></td></tr></table></figure><p>然后就启动了</p><h2 id="上面的操作写成脚本"><a href="#上面的操作写成脚本" class="headerlink" title="上面的操作写成脚本"></a>上面的操作写成脚本</h2><h3 id="绑定部分的处理"><a href="#绑定部分的处理" class="headerlink" title="绑定部分的处理"></a>绑定部分的处理</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-meta">#! /bin/sh</span><br><span class="hljs-keyword">for</span> osd <span class="hljs-keyword">in</span> 5 8 11<br><span class="hljs-keyword">do</span><br>systemctl stop ceph-osd@<span class="hljs-variable">$osd</span><br>umount /var/lib/ceph/osd/ceph-<span class="hljs-variable">$osd</span><br>osdid=`ceph osd dump|grep osd.|awk -v  A=osd.<span class="hljs-variable">$osd</span> <span class="hljs-string">&#x27;&#123;if($1==A) print $19&#125;&#x27;</span>`<br>systemctl <span class="hljs-built_in">disable</span> ceph-volume@lvm-<span class="hljs-variable">$osd</span>-<span class="hljs-variable">$osdid</span><br>systemctl <span class="hljs-built_in">disable</span> ceph-osd@<span class="hljs-variable">$osd</span><br>osdpathprefix=`<span class="hljs-built_in">echo</span> <span class="hljs-variable">$osdid</span>|<span class="hljs-built_in">cut</span> -d <span class="hljs-string">&quot;-&quot;</span> -f 1`<br>osdpath=`<span class="hljs-built_in">ls</span> /dev/mapper/ceph--*|grep <span class="hljs-variable">$osdpathprefix</span>`<br>casadm -A  -d <span class="hljs-variable">$osdpath</span> -i 1 -j <span class="hljs-variable">$osd</span><br><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;1           <span class="hljs-variable">$osd</span>            <span class="hljs-variable">$osdpath</span>&quot;</span><br><span class="hljs-keyword">done</span><br></code></pre></td></tr></table></figure><p>会输出几行对应关系，拷贝到配置文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-meta">#! /bin/sh</span><br><span class="hljs-comment"># start osd.5 osd.8 osd.11</span><br><span class="hljs-keyword">for</span> osd <span class="hljs-keyword">in</span> 5  8  11<br><span class="hljs-keyword">do</span><br>mount -t tmpfs tmpfs /var/lib/ceph/osd/ceph-<span class="hljs-variable">$osd</span><br>restorecon /var/lib/ceph/osd/ceph-<span class="hljs-variable">$osd</span><br><span class="hljs-built_in">chown</span> -R ceph:ceph /var/lib/ceph/osd/ceph-<span class="hljs-variable">$osd</span><br>ceph-bluestore-tool --cluster=ceph prime-osd-dir --dev /dev/cas1-<span class="hljs-variable">$osd</span> --path /var/lib/ceph/osd/ceph-<span class="hljs-variable">$osd</span><br><span class="hljs-built_in">chown</span> -h ceph:ceph /var/lib/ceph/osd/ceph-<span class="hljs-variable">$osd</span>/block<br><span class="hljs-built_in">chown</span> -R ceph:ceph /dev/cas1-<span class="hljs-variable">$osd</span><br><span class="hljs-built_in">chown</span> -R ceph:ceph /var/lib/ceph/osd/ceph-<span class="hljs-variable">$osd</span><br>systemctl start ceph-osd@<span class="hljs-variable">$osd</span><br><span class="hljs-keyword">done</span><br></code></pre></td></tr></table></figure><p>启动osd的脚本</p><p>如果重启机器，等opencas自带的服务启动好了，只用执行上面的启动osd的脚本即可</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本篇记录了配置过程，未涉及参数调优，模式改变等等，都是默认配置</p>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>时间处理相关</title>
    <link href="/2021/03/16/%E6%97%B6%E9%97%B4%E5%A4%84%E7%90%86%E7%9B%B8%E5%85%B3/"/>
    <url>/2021/03/16/%E6%97%B6%E9%97%B4%E5%A4%84%E7%90%86%E7%9B%B8%E5%85%B3/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>时间处理相关的一些记录</p><h2 id="记录"><a href="#记录" class="headerlink" title="记录"></a>记录</h2><h3 id="时间格式类型（2021-03-12-03-34-58-744522）"><a href="#时间格式类型（2021-03-12-03-34-58-744522）" class="headerlink" title="时间格式类型（2021-03-12 03:34:58.744522）"></a>时间格式类型（2021-03-12 03:34:58.744522）</h3><p>转换时间为unix时间（13位）</p><h4 id="shell下的转换"><a href="#shell下的转换" class="headerlink" title="shell下的转换"></a>shell下的转换</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">newtime=$[$(<span class="hljs-built_in">date</span> -d <span class="hljs-string">&quot;<span class="hljs-variable">$mytime</span>&quot;</span> +%s%N)/1000000]<br></code></pre></td></tr></table></figure><h4 id="python下的转换"><a href="#python下的转换" class="headerlink" title="python下的转换"></a>python下的转换</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">import time<br>mytime=<span class="hljs-string">&quot;2021-03-12 03:34:58.744522&quot;</span><br>ts=time.strptime(mytime,<span class="hljs-string">&#x27;%Y-%m-%d %H:%M:%S.%f&#x27;</span>)<br><span class="hljs-built_in">print</span> int(round(time.mktime(ts)*1000))<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>js的数组深度拷贝</title>
    <link href="/2021/03/15/js%E7%9A%84%E6%95%B0%E7%BB%84%E6%B7%B1%E5%BA%A6%E6%8B%B7%E8%B4%9D/"/>
    <url>/2021/03/15/js%E7%9A%84%E6%95%B0%E7%BB%84%E6%B7%B1%E5%BA%A6%E6%8B%B7%E8%B4%9D/</url>
    
    <content type="html"><![CDATA[<h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>数组的复制的时候，如果是多维数组，进行拷贝的时候，修改新数据会影响到老数据</p><h2 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h2><p>深度拷贝</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">let</span> newdata = JSON.parse(JSON.stringify(data.data));<br>        <span class="hljs-built_in">let</span> writedata = JSON.parse(JSON.stringify(data.data));<br>        console.log(<span class="hljs-string">&quot;newdata:&quot;</span>, newdata);<br>        <span class="hljs-keyword">for</span> (<span class="hljs-built_in">let</span> i = 0; i &lt; writedata.length; ++i) &#123;<br>              writedata[i].splice(1,1);<br>        &#125;<br>              console.log(writedata);<br>              console.log(data.data);<br></code></pre></td></tr></table></figure><p>上面的处理方法就是深度拷贝了</p>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>测试主机的整机磁盘带宽</title>
    <link href="/2021/03/12/%E6%B5%8B%E8%AF%95%E4%B8%BB%E6%9C%BA%E7%9A%84%E6%95%B4%E6%9C%BA%E7%A3%81%E7%9B%98%E5%B8%A6%E5%AE%BD/"/>
    <url>/2021/03/12/%E6%B5%8B%E8%AF%95%E4%B8%BB%E6%9C%BA%E7%9A%84%E6%95%B4%E6%9C%BA%E7%A3%81%E7%9B%98%E5%B8%A6%E5%AE%BD/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>为什么需要有整机带宽的概念，因为我们很多时候去计算性能的时候，上去都会计算你有多少块盘，每块盘有多大的带宽，那么一乘就算出来了，其实没有那么简单，整个IO路径上面任何一个地方的瓶颈都会降低总体带宽的</p><p>本篇的来源，很多年前，做机器测试的时候，总觉得机器上面的带宽不对，加起来不对，然后搜资料看到了，一个盘一个盘测试的时候带宽没问题，但是整机的磁盘IO同时下发的时候，阵列卡，背板都有可能带不动</p><p>而这个整机测试就能发现问题，最后定位到是背板有问题，刷行一下固件，总性能提升了300MB&#x2F;s</p><h2 id="测试方法"><a href="#测试方法" class="headerlink" title="测试方法"></a>测试方法</h2><p>测试很简单，就算模拟所有磁盘的同时写，而fio工具是最好的测试工具</p><p>配置文件如下</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs bash">[global]<br>bs=4M<br>iodepth=16<br>direct=1<br>ioengine=libaio<br>randrepeat=0<br>group_reporting<br>time_based<br>runtime=60<br>filesize=20G<br><br>[job1]<br>rw=write<br>filename=/dev/sdb:/dev/sdc:/dev/sdd:/dev/sde:/dev/sdf:/dev/sdg:/dev/sdh:/dev/sdi<br>name=write<br></code></pre></td></tr></table></figure><p>上面测试的是8个磁盘的并行写入的带宽</p><p>vdbench测试整机带宽</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><code class="hljs bash">hd=default,vdbench=/root/vdbench,user=root,shell=ssh<br>hd=hd1,system=66.66.66.60<br>sd=sd1,lun=/dev/sdl,host=hd1,openflags=o_direct,hitarea=0,range=(0,100),threads=2<br>sd=sd2,lun=/dev/sdc,host=hd1,openflags=o_direct,hitarea=0,range=(0,100),threads=2<br>sd=sd3,lun=/dev/sdd,host=hd1,openflags=o_direct,hitarea=0,range=(0,100),threads=2<br>sd=sd4,lun=/dev/sde,host=hd1,openflags=o_direct,hitarea=0,range=(0,100),threads=2<br>sd=sd5,lun=/dev/sdf,host=hd1,openflags=o_direct,hitarea=0,range=(0,100),threads=2<br>sd=sd6,lun=/dev/sdg,host=hd1,openflags=o_direct,hitarea=0,range=(0,100),threads=2<br>sd=sd7,lun=/dev/sdh,host=hd1,openflags=o_direct,hitarea=0,range=(0,100),threads=2<br>sd=sd8,lun=/dev/sdi,host=hd1,openflags=o_direct,hitarea=0,range=(0,100),threads=2<br>sd=sd9,lun=/dev/sdj,host=hd1,openflags=o_direct,hitarea=0,range=(0,100),threads=2<br>sd=sd10,lun=/dev/sdk,host=hd1,openflags=o_direct,hitarea=0,range=(0,100),threads=2<br>sd=sd11,lun=/dev/sdl,host=hd1,openflags=o_direct,hitarea=0,range=(0,100),threads=2<br>sd=sd12,lun=/dev/sdm,host=hd1,openflags=o_direct,hitarea=0,range=(0,100),threads=2<br>sd=sd13,lun=/dev/sdn,host=hd1,openflags=o_direct,hitarea=0,range=(0,100),threads=2<br>sd=sd14,lun=/dev/sdo,host=hd1,openflags=o_direct,hitarea=0,range=(0,100),threads=2<br>sd=sd15,lun=/dev/sdp,host=hd1,openflags=o_direct,hitarea=0,range=(0,100),threads=2<br>sd=sd16,lun=/dev/sdq,host=hd1,openflags=o_direct,hitarea=0,range=(0,100),threads=2<br>sd=sd17,lun=/dev/sdr,host=hd1,openflags=o_direct,hitarea=0,range=(0,100),threads=2<br>sd=sd18,lun=/dev/sds,host=hd1,openflags=o_direct,hitarea=0,range=(0,100),threads=2<br>sd=sd19,lun=/dev/sdt,host=hd1,openflags=o_direct,hitarea=0,range=(0,100),threads=2<br>sd=sd20,lun=/dev/sdu,host=hd1,openflags=o_direct,hitarea=0,range=(0,100),threads=2<br>sd=sd21,lun=/dev/sdv,host=hd1,openflags=o_direct,hitarea=0,range=(0,100),threads=2<br>sd=sd22,lun=/dev/sdw,host=hd1,openflags=o_direct,hitarea=0,range=(0,100),threads=2<br>sd=sd23,lun=/dev/sdx,host=hd1,openflags=o_direct,hitarea=0,range=(0,100),threads=2<br>sd=sd24,lun=/dev/sdy,host=hd1,openflags=o_direct,hitarea=0,range=(0,100),threads=2<br>sd=sd25,lun=/dev/sdz,host=hd1,openflags=o_direct,hitarea=0,range=(0,100),threads=2<br>sd=sd26,lun=/dev/sdaa,host=hd1,openflags=o_direct,hitarea=0,range=(0,100),threads=2<br>sd=sd27,lun=/dev/sdab,host=hd1,openflags=o_direct,hitarea=0,range=(0,100),threads=2<br>sd=sd28,lun=/dev/sdac,host=hd1,openflags=o_direct,hitarea=0,range=(0,100),threads=2<br>sd=sd29,lun=/dev/sdad,host=hd1,openflags=o_direct,hitarea=0,range=(0,100),threads=2<br>sd=sd30,lun=/dev/sdae,host=hd1,openflags=o_direct,hitarea=0,range=(0,100),threads=2<br>sd=sd31,lun=/dev/sdaf,host=hd1,openflags=o_direct,hitarea=0,range=(0,100),threads=2<br>sd=sd32,lun=/dev/sdag,host=hd1,openflags=o_direct,hitarea=0,range=(0,100),threads=2<br>sd=sd33,lun=/dev/sdah,host=hd1,openflags=o_direct,hitarea=0,range=(0,100),threads=2<br>sd=sd34,lun=/dev/sdai,host=hd1,openflags=o_direct,hitarea=0,range=(0,100),threads=2<br>sd=sd35,lun=/dev/sdaj,host=hd1,openflags=o_direct,hitarea=0,range=(0,100),threads=2<br>sd=sd36,lun=/dev/sdak,host=hd1,openflags=o_direct,hitarea=0,range=(0,100),threads=2<br>wd=wd1,sd=(sd1,sd2,sd3,sd4,sd5,sd6,sd7,sd8,sd9,sd10,sd11,sd12,sd13,sd14,sd15,sd16,sd17,sd18,sd19,sd20,sd21,sd22,sd23,sd24,sd25,sd26,sd27,sd28,sd29,sd30,sd31,sd32,sd33,sd34,sd35,sd36),xfersize=(1048576,100),rdpct=0,seekpct=0<br>rd=run1,wd=wd1,iorate=max,elapsed=600,warmup=300<br></code></pre></td></tr></table></figure><h2 id="文件系统主机带宽"><a href="#文件系统主机带宽" class="headerlink" title="文件系统主机带宽"></a>文件系统主机带宽</h2><p>下面的是测试文件系统的带宽</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 vdbench]<span class="hljs-comment"># cat file.conf</span><br>hd=default,vdbench=/root/vdbench,user=root,shell=ssh<br>hd=hd1,system=192.168.19.101<br>fsd=fsd1,anchor=/a/,depth=2,width=2,files=10000,size=64k<br>fsd=fsd2,anchor=/b/,depth=2,width=2,files=10000,size=64k<br>fwd=fwd1,fsd=(fsd1,fsd2),operation=write,xfersize=64k,fileio=sequential,fileselect=random,threads=2<br>rd=rd1,fwd=fwd1,fwdrate=max,format=<span class="hljs-built_in">yes</span>,elapsed=60,interval=1<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>使用MobaXterm运行centos7图形界面窗口</title>
    <link href="/2021/03/12/%E4%BD%BF%E7%94%A8MobaXterm%E8%BF%90%E8%A1%8Ccentos7%E5%9B%BE%E5%BD%A2%E7%95%8C%E9%9D%A2%E7%AA%97%E5%8F%A3/"/>
    <url>/2021/03/12/%E4%BD%BF%E7%94%A8MobaXterm%E8%BF%90%E8%A1%8Ccentos7%E5%9B%BE%E5%BD%A2%E7%95%8C%E9%9D%A2%E7%AA%97%E5%8F%A3/</url>
    
    <content type="html"><![CDATA[<h2 id="需求"><a href="#需求" class="headerlink" title="需求"></a>需求</h2><p>需要在windows下运行linux的带图形界面的程序，不想安装完整的桌面程序，并且windows上面就能运行</p><h2 id="安装方法"><a href="#安装方法" class="headerlink" title="安装方法"></a>安装方法</h2><p>安装MobaXterm</p><p>centos安装相关的软件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">yum install -y xorg-x11-xauth xorg-x11-utils xorg-x11-fonts-*<br></code></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">cat</span> /etc/ssh/sshd_config |grep X11<br>X11Forwarding <span class="hljs-built_in">yes</span><br></code></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">systemctl restart sshd<br></code></pre></td></tr></table></figure><p>测试验证</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">xclock<br></code></pre></td></tr></table></figure><p><img src="/images/blog/284415-20210312145956555-1668029993.png"></p>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>vmware开启disk-by-id</title>
    <link href="/2021/03/09/vmware%E5%BC%80%E5%90%AFdisk-by-id/"/>
    <url>/2021/03/09/vmware%E5%BC%80%E5%90%AFdisk-by-id/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>默认vmware没开启&#x2F;dev&#x2F;disk&#x2F;by-id的，测试opencas的时候需要这个路径，所以需要自己手动开启下</p><h2 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h2><p>在vmware的虚拟机配置文件vmx里面添加</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">disk.EnableUUID = <span class="hljs-string">&quot;TRUE&quot;</span><br></code></pre></td></tr></table></figure><p>然后重启下虚拟机，就可以看到路径内有磁盘编号了</p>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>简单安装配置mariadb</title>
    <link href="/2021/03/04/%E7%AE%80%E5%8D%95%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AEmariadb/"/>
    <url>/2021/03/04/%E7%AE%80%E5%8D%95%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AEmariadb/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>记录mariadb的安装配置方法</p><h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 bacula]<span class="hljs-comment"># yum install mariadb-server</span><br>[root@lab101 bacula]<span class="hljs-comment"># systemctl start mariadb</span><br>[root@lab101 bacula]<span class="hljs-comment"># systemctl enable mariadb</span><br>Created symlink from /etc/systemd/system/multi-user.target.wants/mariadb.service to /usr/lib/systemd/system/mariadb.service.<br></code></pre></td></tr></table></figure><p>##配置</p><h3 id="设置密码"><a href="#设置密码" class="headerlink" title="设置密码"></a>设置密码</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 bacula]<span class="hljs-comment"># mysql_secure_installation</span><br></code></pre></td></tr></table></figure><h3 id="测试登陆"><a href="#测试登陆" class="headerlink" title="测试登陆"></a>测试登陆</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 bacula]<span class="hljs-comment"># mysql -u root -p</span><br></code></pre></td></tr></table></figure><h3 id="配置访问权限"><a href="#配置访问权限" class="headerlink" title="配置访问权限"></a>配置访问权限</h3><p>允许所有机器以root用户名密码123456访问</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">GRANT ALL PRIVILEGES ON *.* TO <span class="hljs-string">&#x27;root&#x27;</span>@<span class="hljs-string">&#x27;%&#x27;</span> IDENTIFIED BY <span class="hljs-string">&#x27;123456&#x27;</span> WITH GRANT OPTION;<br>FLUSH PRIVILEGES;<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>存储相关招聘信息-长期</title>
    <link href="/2021/02/23/%E5%AD%98%E5%82%A8%E7%9B%B8%E5%85%B3%E6%8B%9B%E8%81%98%E4%BF%A1%E6%81%AF-%E9%95%BF%E6%9C%9F/"/>
    <url>/2021/02/23/%E5%AD%98%E5%82%A8%E7%9B%B8%E5%85%B3%E6%8B%9B%E8%81%98%E4%BF%A1%E6%81%AF-%E9%95%BF%E6%9C%9F/</url>
    
    <content type="html"><![CDATA[<h2 id="工作地点"><a href="#工作地点" class="headerlink" title="工作地点"></a>工作地点</h2><p>武汉 深圳</p><h2 id="岗位要求说明"><a href="#岗位要求说明" class="headerlink" title="岗位要求说明"></a>岗位要求说明</h2><h3 id="存储后台开发"><a href="#存储后台开发" class="headerlink" title="存储后台开发"></a>存储后台开发</h3><p>负责ceph，gluster等存储系统相关的模块开发，对相关的存储系统社区开发进展熟悉，能够独立进行backport<br>熟悉存储系统之上的一些文件系统，nfs，samba，iscsi等等<br>熟悉C语言</p><h3 id="内核相关的开发"><a href="#内核相关的开发" class="headerlink" title="内核相关的开发"></a>内核相关的开发</h3><p>熟悉ceph内核客户端相关的模块，能够对内核模块进行相关的开发或者集成功能，能够对高版本的功能进行低版本的适配<br>能够对内核进行一定程度的内核调优<br>熟悉C语言</p><h3 id="存储管理系统的中间件开发"><a href="#存储管理系统的中间件开发" class="headerlink" title="存储管理系统的中间件开发"></a>存储管理系统的中间件开发</h3><p>熟悉对存储系统的一些运维相关的操作，能够进行存储管理系统中间系统的开发，对外提供api接口进行相关的管理<br>熟悉python</p><h3 id="存储管理系统的界面开发"><a href="#存储管理系统的界面开发" class="headerlink" title="存储管理系统的界面开发"></a>存储管理系统的界面开发</h3><p>熟悉存储系统的管理，能够开发相关的管理界面<br>熟悉web开发相关的语言</p><h3 id="存储系统的运维"><a href="#存储系统的运维" class="headerlink" title="存储系统的运维"></a>存储系统的运维</h3><p>熟悉存储系统的后台，能够定位并解决存储系统的相关的问题</p><h3 id="存储系统的测试"><a href="#存储系统的测试" class="headerlink" title="存储系统的测试"></a>存储系统的测试</h3><p>熟悉存储系统的测试</p><h2 id="工作说明"><a href="#工作说明" class="headerlink" title="工作说明"></a>工作说明</h2><p>公司内的平台较多，国产化，arm等平台都在内部有相关的硬件，能够接触到最新的一些硬件平台<br>公司岗位自由，上面的岗位可根据自己情况自由调整</p><h2 id="薪资标准"><a href="#薪资标准" class="headerlink" title="薪资标准"></a>薪资标准</h2><p>薪水与能力匹配即可，未做过多限制</p><h2 id="联系方式"><a href="#联系方式" class="headerlink" title="联系方式"></a>联系方式</h2><p>QQ：199383004<br>邮件：<a href="mailto:&#x31;&#57;&#x39;&#x33;&#56;&#51;&#48;&#48;&#x34;&#64;&#x71;&#113;&#46;&#99;&#x6f;&#109;">&#x31;&#57;&#x39;&#x33;&#56;&#51;&#48;&#48;&#x34;&#64;&#x71;&#113;&#46;&#99;&#x6f;&#109;</a></p><p>武汉-磨渣</p><p>有相关的工作意向可以联系我<br>武汉工作地点为 光谷<br>深圳工作地点为 南山</p>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>强制backfill替代recover避免阻塞</title>
    <link href="/2021/01/30/%E5%BC%BA%E5%88%B6backfill%E6%9B%BF%E4%BB%A3recover%E9%81%BF%E5%85%8D%E9%98%BB%E5%A1%9E/"/>
    <url>/2021/01/30/%E5%BC%BA%E5%88%B6backfill%E6%9B%BF%E4%BB%A3recover%E9%81%BF%E5%85%8D%E9%98%BB%E5%A1%9E/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>做recover会触发阻塞，引起前端的IO卡住，可以通过参数避免这个情况</p><h2 id="调整参数"><a href="#调整参数" class="headerlink" title="调整参数"></a>调整参数</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># the number of entries to keep in the pg log when trimming it. Defaults to 3000.</span><br>osd_min_pg_log_entries = 1<br><br><span class="hljs-comment"># the max entries, say when degraded, before we trim. Defaults to 10000.</span><br>osd_max_pg_log_entries = 2<br></code></pre></td></tr></table></figure><p>是否需要做backfill是通过pg log判断的，可以通过调整上面的参数来强制做backfill</p>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>使用MegaRAID Storage Manager图形界面配置RAID</title>
    <link href="/2021/01/27/%E4%BD%BF%E7%94%A8MegaRAID%20Storage%20Manager%E5%9B%BE%E5%BD%A2%E7%95%8C%E9%9D%A2%E9%85%8D%E7%BD%AERAID/"/>
    <url>/2021/01/27/%E4%BD%BF%E7%94%A8MegaRAID%20Storage%20Manager%E5%9B%BE%E5%BD%A2%E7%95%8C%E9%9D%A2%E9%85%8D%E7%BD%AERAID/</url>
    
    <content type="html"><![CDATA[<h2 id="软件介绍"><a href="#软件介绍" class="headerlink" title="软件介绍"></a>软件介绍</h2><p>软件的名称为LSI MegaRAID Storage Manager，很多厂商都会提供，本篇是从broadcom进行的下载，有的厂商也是直接跳转到这个网站的，比如华为的服务器，这里从这里下载就可以了</p><h2 id="下载"><a href="#下载" class="headerlink" title="下载"></a>下载</h2><p>直接用下面的地址</p><blockquote><p><a href="https://www.broadcom.com/site-search?filters%5Bpages%5D%5BContent_Type%5D%5Btype%5D=and&filters%5Bpages%5D%5BContent_Type%5D%5Bvalues%5D%5B%5D=Downloads&page=1&per_page=10&q=LSI%20MegaRAID%20Storage%20Manager">https://www.broadcom.com/site-search?filters[pages][Content_Type][type]=and&amp;filters[pages][Content_Type][values][]=Downloads&amp;page=1&amp;per_page=10&amp;q=LSI%20MegaRAID%20Storage%20Manager</a></p></blockquote><p>或者进入官网<a href="https://www.broadcom.com/">https://www.broadcom.com/</a><br>搜索 LSI MegaRAID Storage Manager<br>分类里面选择download<br>然后有windows和linux的都下载下来<br>提前下好，放网盘了</p><p>链接: <a href="https://pan.baidu.com/s/1XJ4MhQ6xsBM0H4SjdCHHNg">https://pan.baidu.com/s/1XJ4MhQ6xsBM0H4SjdCHHNg</a>  密码: isti</p><h2 id="使用方法"><a href="#使用方法" class="headerlink" title="使用方法"></a>使用方法</h2><p>这个分为服务端和控制端，这里我在linux服务器上面安装服务端，执行安装的时候选择5，然后选择加密方式，这里记住这个加密选择的，后面安装客户端的时候也要选择一样的</p><p>控制端安装在windows上面，直接运行即可，注意windows不要设置多个网段的IP，可能会扫描不到，设置成单IP</p><p>然后使用图形界面进行配置即可</p><p><img src="/images/blog/284415-20210127222701086-1945707006.png"></p><h2 id="启动服务的命令"><a href="#启动服务的命令" class="headerlink" title="启动服务的命令"></a>启动服务的命令</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">/etc/init.d/vivaldiframeworkd <br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>清理多台主机集群的脚本</title>
    <link href="/2021/01/26/%E6%B8%85%E7%90%86%E5%A4%9A%E5%8F%B0%E4%B8%BB%E6%9C%BA%E9%9B%86%E7%BE%A4%E7%9A%84%E8%84%9A%E6%9C%AC/"/>
    <url>/2021/01/26/%E6%B8%85%E7%90%86%E5%A4%9A%E5%8F%B0%E4%B8%BB%E6%9C%BA%E9%9B%86%E7%BE%A4%E7%9A%84%E8%84%9A%E6%9C%AC/</url>
    
    <content type="html"><![CDATA[<h2 id="目的"><a href="#目的" class="headerlink" title="目的"></a>目的</h2><p>清理环境所有的数据，删除并恢复到初始状态</p><h2 id="脚本"><a href="#脚本" class="headerlink" title="脚本"></a>脚本</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-meta">#! /bin/sh</span><br><span class="hljs-keyword">for</span> host <span class="hljs-keyword">in</span> lab101 lab102<br><span class="hljs-keyword">do</span><br>ssh <span class="hljs-variable">$host</span>  hostname<br>ssh <span class="hljs-variable">$host</span> systemctl stop ceph-radosgw.target<br>ssh <span class="hljs-variable">$host</span> systemctl stop ceph-osd.target<br>ssh <span class="hljs-variable">$host</span> systemctl stop ceph-mgr.target<br>ssh <span class="hljs-variable">$host</span> systemctl stop ceph-mon.target<br>ssh <span class="hljs-variable">$host</span> <span class="hljs-string">&quot;for vgname in \`vgs --separator : --noheadings --units k --unbuffered --nosuffix --options &#x27;vg_name&#x27;|grep ceph\`;do echo \$vgname;vgremove -y \$vgname;done;&quot;</span><br>ssh <span class="hljs-variable">$host</span> <span class="hljs-string">&quot;for osddir in \`df -h|grep osd|grep ceph|awk &#x27;&#123;print \$6&#125;&#x27;\`;do umount \$osddir;done;&quot;</span><br>ssh <span class="hljs-variable">$host</span> <span class="hljs-string">&quot;rm -rf /var/lib/ceph/osd/*&quot;</span><br>ssh <span class="hljs-variable">$host</span> <span class="hljs-string">&quot;rm -rf /var/lib/ceph/mon/*&quot;</span><br>ssh <span class="hljs-variable">$host</span> <span class="hljs-string">&quot;rm -rf /var/lib/ceph/mgr/*&quot;</span><br>ssh <span class="hljs-variable">$host</span> <span class="hljs-string">&quot;rm -rf /var/lib/ceph/mds/*&quot;</span><br>ssh <span class="hljs-variable">$host</span> <span class="hljs-string">&quot;rm -rf /etc/ceph/*&quot;</span><br>ssh <span class="hljs-variable">$host</span> <span class="hljs-string">&quot;for pvdisk in \`pvdisplay -c|grep new|awk &#x27;&#123;print \$1&#125;&#x27;|sed &#x27;s/\&quot;//g&#x27;\`;do echo \$pvdisk;pvremove \$pvdisk;done&quot;</span><br><br><span class="hljs-keyword">done</span><br></code></pre></td></tr></table></figure><p>非脚本命令行清理lvm</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-keyword">for</span> vgname <span class="hljs-keyword">in</span> `vgs --separator : --noheadings --units k --unbuffered --nosuffix --options <span class="hljs-string">&#x27;vg_name&#x27;</span>|grep ceph`;<span class="hljs-keyword">do</span> <span class="hljs-built_in">echo</span> <span class="hljs-variable">$vgname</span>;vgremove -y <span class="hljs-variable">$vgname</span>;<span class="hljs-keyword">done</span>;<br><br><br><span class="hljs-keyword">for</span> pvdisk <span class="hljs-keyword">in</span> `pvdisplay -c|grep new|awk <span class="hljs-string">&#x27;&#123;print \$1&#125;&#x27;</span>|sed <span class="hljs-string">&#x27;s/&quot;//g&#x27;</span>`;<span class="hljs-keyword">do</span> <span class="hljs-built_in">echo</span> <span class="hljs-variable">$pvdisk</span>;pvremove <span class="hljs-variable">$pvdisk</span>;<span class="hljs-keyword">done</span><br><br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>测试数据的可视化-通用化</title>
    <link href="/2021/01/19/%E6%B5%8B%E8%AF%95%E6%95%B0%E6%8D%AE%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96-%E9%80%9A%E7%94%A8%E5%8C%96/"/>
    <url>/2021/01/19/%E6%B5%8B%E8%AF%95%E6%95%B0%E6%8D%AE%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96-%E9%80%9A%E7%94%A8%E5%8C%96/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>如果测试数据是很大量的数据，并且是基于时间变化的，需要做成一个横轴为时间，纵轴为数值的可视化，这个有很多方法<br>比如用excel做成图表形式的<br>或者用powerbi或者其它软件做成波形图<br>但是找了几个软件都没有想要的效果，grafana那种又太重了，需要配置很多东西，所以这里自己实现了一个简单但是实用的数据分析网页</p><h2 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h2><p>首先是原始数据，这里需要对原始数据进行处理，把数据处理成时间，数值的csv文件格式的，数量可以很多也没关系<br>支持通过网页导入，因为测试可能是多轮的，或者多个数据的，所以为了方便，是支持网页直接倒入的<br>网页是通过命令行启动的，不需要第三方的web服务器，所有的东西都维护在一个目录内，我们看下我们的效果</p><h2 id="使用过程"><a href="#使用过程" class="headerlink" title="使用过程"></a>使用过程</h2><p>以ceph.log为例，我们拿到的是原始打包好的数据</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 data]<span class="hljs-comment"># ll</span><br>总用量 27228<br>-rw------- 1 root root 1230784 1月  18 18:28 ceph.log-20210115.gz<br>-rw------- 1 root root 1253789 1月  18 18:28 ceph.log-20210116.gz<br>-rw------- 1 root root 6857295 1月  18 18:28 ceph.log-20210117.gz<br>-rw------- 1 root root 7443114 1月  18 18:28 ceph.log-20210118.gz<br></code></pre></td></tr></table></figure><p>解压数据</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 data]<span class="hljs-comment"># gunzip  -d *.gz</span><br>[root@lab101 data]<span class="hljs-comment"># ll</span><br>总用量 270016<br>-rw------- 1 root root 12583407 1月  18 18:28 ceph.log-20210115<br>-rw------- 1 root root 12672226 1月  18 18:28 ceph.log-20210116<br>-rw------- 1 root root 58620471 1月  18 18:28 ceph.log-20210117<br>-rw------- 1 root root 68756398 1月  18 18:28 ceph.log-20210118<br></code></pre></td></tr></table></figure><p>汇总数据</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 data]<span class="hljs-comment"># cat ceph.log-20210115 &gt; ceph.log</span><br>[root@lab101 data]<span class="hljs-comment"># cat ceph.log-20210116 &gt;&gt; ceph.log</span><br>[root@lab101 data]<span class="hljs-comment"># cat ceph.log-20210117 &gt;&gt; ceph.log</span><br>[root@lab101 data]<span class="hljs-comment"># cat ceph.log-20210118 &gt;&gt; ceph.log</span><br></code></pre></td></tr></table></figure><p>处理数据<br>我们之前的文章有篇awk的有处理数据的方法</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">cat</span> ceph.log |awk <span class="hljs-string">&#x27;&#123;for(i=1;i&lt;=NF;i++)&#123;if($i~/op\/s/)&#123;$3=$i;print $1,$2,$3&#125;&#125;&#125;&#x27;</span>|awk <span class="hljs-string">&#x27;&#123;gsub(&quot;op/s&quot;,&quot;&quot;,$3); print $1,$2,$3&#125;&#x27;</span>|awk <span class="hljs-string">&#x27;&#123;if($3~/k/) &#123;gsub(&quot;k&quot;,&quot;&quot;,$3); print $1,$2&quot;,&quot;$3*1000&#125; else &#123;print $1,$2&quot;,&quot;$3&#125;&#125;&#x27;</span> &gt; ceph.log.ops<br></code></pre></td></tr></table></figure><p>转换成平台可读文件<br>转换的时候根据数据的长度决定的，我取了四天的数据大概有17万行，这个数据量还是比较大的</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 datadisplay]<span class="hljs-comment"># cat chuli.sh</span><br><span class="hljs-built_in">cat</span> ceph.log.ops | <span class="hljs-keyword">while</span> <span class="hljs-built_in">read</span> line<br><span class="hljs-keyword">do</span><br>mytime=`<span class="hljs-built_in">echo</span> <span class="hljs-variable">$line</span>|<span class="hljs-built_in">cut</span> -d , -f 1`<br>value=`<span class="hljs-built_in">echo</span> <span class="hljs-variable">$line</span>|<span class="hljs-built_in">cut</span> -d , -f 2`<br><br>newtime=$[$(<span class="hljs-built_in">date</span> -d <span class="hljs-string">&quot;<span class="hljs-variable">$mytime</span>&quot;</span> +%s%N)/1000000]<br><span class="hljs-built_in">echo</span> <span class="hljs-variable">$newtime</span>,<span class="hljs-variable">$value</span> &gt;&gt; ceph.log.ops-chulihou<br><span class="hljs-keyword">done</span><br></code></pre></td></tr></table></figure><p>然后通过web上传上面的ceph.log.ops-chulihou文件即可</p><h2 id="效果如下"><a href="#效果如下" class="headerlink" title="效果如下"></a>效果如下</h2><div style="position: relative; padding: 30% 45%;"><iframe style="position: absolute; width: 100%; height: 100%; left: 0; top: 0;"  src="//player.bilibili.com/player.html?aid=203744854&bvid=BV1sh411y7C7&cid=284698453&page=1&as_wide=1&high_quality=1&danmaku=0" allowfullscreen="allowfullscreen" width="100%" height="500" scrolling="no" frameborder="0" ></iframe></div><h2 id="上传部分的代码"><a href="#上传部分的代码" class="headerlink" title="上传部分的代码"></a>上传部分的代码</h2><p>index.html内部的</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">&lt;form action=<span class="hljs-string">&quot;/upload&quot;</span> method=<span class="hljs-string">&quot;post&quot;</span> enctype=<span class="hljs-string">&quot;multipart/form-data&quot;</span>&gt;<br>  &lt;input <span class="hljs-built_in">type</span>=<span class="hljs-string">&quot;file&quot;</span> name=<span class="hljs-string">&quot;upload&quot;</span> /&gt;<br>  &lt;input <span class="hljs-built_in">type</span>=<span class="hljs-string">&quot;submit&quot;</span> value=<span class="hljs-string">&quot;上传&quot;</span> /&gt;<br>  &lt;a href=<span class="hljs-string">&quot;./help.html&quot;</span>&gt;查看帮助&lt;/a&gt;<br>&lt;/form&gt;<br></code></pre></td></tr></table></figure><p>server.py代码</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-meta">#! /bin/python</span><br><span class="hljs-comment"># -*- coding: UTF-8 -*-</span><br><span class="hljs-comment"># @Time :2020-01-18 18:03</span><br><span class="hljs-comment"># @Author:  zphj1987</span><br><br>import os<br>from bottle import route, request, static_file, run,template<br><br>@route(<span class="hljs-string">&#x27;/&#x27;</span>)<br>def root():<br>    <span class="hljs-built_in">return</span> static_file(<span class="hljs-string">&#x27;index.html&#x27;</span>, root=<span class="hljs-string">&#x27;.&#x27;</span>)<br><br>@route(<span class="hljs-string">&#x27;/&lt;filename&gt;&#x27;</span>)<br>def server_static(filename):<br>    <span class="hljs-built_in">return</span> static_file(filename, root=<span class="hljs-string">&#x27;.&#x27;</span>)<br><br>@route(<span class="hljs-string">&#x27;/upload&#x27;</span>, method=<span class="hljs-string">&#x27;POST&#x27;</span>)<br>def do_upload():<br>    try:<br>        upload = request.files.get(<span class="hljs-string">&#x27;upload&#x27;</span>)<br>        name, ext = os.path.splitext(upload.filename)<br>    <span class="hljs-comment">#if ext not in (&#x27;.csv&#x27;, &#x27;.jpg&#x27;, &#x27;.jpeg&#x27;):</span><br>    <span class="hljs-comment">#    return &quot;File extension not allowed.&quot;</span><br>        file_path = <span class="hljs-string">&quot;./data.csv&quot;</span><br>        os.remove(<span class="hljs-string">&quot;./data.csv&quot;</span>)<br>        upload.save(file_path)<br>    except:<br>        pass<br>    <span class="hljs-comment">#return &quot;File successfully saved to &#x27;&#123;0&#125;&#x27;.&quot;.format(file_path)</span><br>    <span class="hljs-built_in">return</span> static_file(<span class="hljs-string">&#x27;index.html&#x27;</span>, root=<span class="hljs-string">&#x27;.&#x27;</span>)<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    run(host=<span class="hljs-string">&#x27;0.0.0.0&#x27;</span>, port=8080)<br></code></pre></td></tr></table></figure><p>这部分通过bottle进行实现的，只用目录里面放一个py文件就可以了比较简单</p>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>如何构建自己的远程高带宽环境</title>
    <link href="/2021/01/19/%E5%A6%82%E4%BD%95%E6%9E%84%E5%BB%BA%E8%87%AA%E5%B7%B1%E7%9A%84%E8%BF%9C%E7%A8%8B%E9%AB%98%E5%B8%A6%E5%AE%BD%E7%8E%AF%E5%A2%83/"/>
    <url>/2021/01/19/%E5%A6%82%E4%BD%95%E6%9E%84%E5%BB%BA%E8%87%AA%E5%B7%B1%E7%9A%84%E8%BF%9C%E7%A8%8B%E9%AB%98%E5%B8%A6%E5%AE%BD%E7%8E%AF%E5%A2%83/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>公网的云机器配置最小配置也要100左右一年，然后带宽也特别贵，默认最小配置只提供1M带宽，之前需要远程都是通过frp穿透，使用公网机器作为中转机器，如果只用命令行还好，碰到需要传输数据，那1M带宽使用起来就特别的卡了</p><p>而家里面的宽带是100M+的，公司的网络也很难给个人提供到这么大的带宽，所以可以利用家里的宽带构建个人的公网环境，家庭网络建议不要提供对外的服务，否则可能直接关网的，自己用不违法，没什么问题</p><h2 id="如何构建自己的公网环境"><a href="#如何构建自己的公网环境" class="headerlink" title="如何构建自己的公网环境"></a>如何构建自己的公网环境</h2><p>以前的网络上网方式是通过路由器进行拨号，然后共享给内网机器进行共享网络，直接在路由器上面做DMZ主机或者做端口映射就可以做一些工作了，现在都是光纤网络了，到光猫的IP地址实际上是内网地址了，所以需要做下面的一些工作</p><h3 id="打电话给10000"><a href="#打电话给10000" class="headerlink" title="打电话给10000"></a>打电话给10000</h3><p>打电话给10000的客服，说自己需要一个公网的IP，然后客服就会生成工单，安排一个师傅在后台操作下，过几分钟以后你查询你本地的公网IP就是可以ping通的</p><h3 id="换光猫"><a href="#换光猫" class="headerlink" title="换光猫"></a>换光猫</h3><p>上面的操作以后，只是光猫得到一个公网的IP，但是还是没法到内网来，这个实际上是出厂的时候运营商要求光猫厂商把管理员模式关闭了，去淘宝买一个100多的光猫，这个默认开放了管理员权限的，根据提供的说明记录一些光猫认证相关的信息，拔掉光纤，插到新的光猫上，然后登陆认证，设置为桥接模式，这一步电信是不会帮你操作的，所以需要自己处理</p><h3 id="重置宽带用户名密码"><a href="#重置宽带用户名密码" class="headerlink" title="重置宽带用户名密码"></a>重置宽带用户名密码</h3><p>在光猫登陆的时候，是没有用户名密码的，用10000的app进行宽带密码重置</p><h3 id="登陆路由器"><a href="#登陆路由器" class="headerlink" title="登陆路由器"></a>登陆路由器</h3><p>这个跟很久前的电话线拨号一样了，在路由器里面进行拨号即可，到这里实际上大部分工作就完成了</p><h3 id="设置DMZ主机"><a href="#设置DMZ主机" class="headerlink" title="设置DMZ主机"></a>设置DMZ主机</h3><p>这个意思是与公网IP连接的时候，是跟内网的哪台机器进行交互，我自己是用一台虚拟机做的，这个方便关闭，也方便维护</p><h3 id="设置DDNS"><a href="#设置DDNS" class="headerlink" title="设置DDNS"></a>设置DDNS</h3><p>上面的操作以后，实际上是通过公网IP访问的，但是这个是可能变化的，需要申请一个DDNS，这个路由器提供TPLINK和花生壳的DDNS两个选择，我的TPLINK的域名出现了无法ping通的问题，这个可以两个都登陆也可以的<br>到这里，通过公网的域名已经可以连接到内网的机器了</p><p>实际上上面的工作就类似于蒲公英的工作，提供类似局域网的功能，只是这个能够直接公网，并且是自己完全可控的</p><h2 id="使用用例一："><a href="#使用用例一：" class="headerlink" title="使用用例一："></a>使用用例一：</h2><p>在家里虚拟机启动一个frps，在公司里面的机器运行一个frpc，服务器指向上面设置的动态域名，然后就有一个公网的可访问内网的IP和端口了，并且可以穿透多台机器，可以ssh的，也可以是rdp的远程桌面，并且带宽足够高<br>买个intel nuc，功耗也不高，性能够强，还可以挂盘，自己就可以构建一个自用公有云了，可以带存储，也可以是云主机了，使用minio提供个s3存储</p>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>mac常用软件记录</title>
    <link href="/2021/01/19/mac%E5%B8%B8%E7%94%A8%E8%BD%AF%E4%BB%B6%E8%AE%B0%E5%BD%95/"/>
    <url>/2021/01/19/mac%E5%B8%B8%E7%94%A8%E8%BD%AF%E4%BB%B6%E8%AE%B0%E5%BD%95/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>本片记录一些常用的软件，方便后续使用</p><h2 id="软件"><a href="#软件" class="headerlink" title="软件"></a>软件</h2><h3 id="windows远程桌面连接软件"><a href="#windows远程桌面连接软件" class="headerlink" title="windows远程桌面连接软件"></a>windows远程桌面连接软件</h3><p>由于有些操作需要连接到windows上面做，所以需要能够方便的连接windows的电脑，所以需要一个windows的远程连接软件</p><h4 id="royal-tsx"><a href="#royal-tsx" class="headerlink" title="royal tsx"></a>royal tsx</h4><p>这个软件可以连接ssh，ftp，rdp等协议，但是免费版只能连10个，这个对于我来说太少了</p><p><img src="/images/blog/284415-20210119102652820-251060924.jpg"></p><h4 id="Microsoft-Remote-Desktop-10-5-0（1841）"><a href="#Microsoft-Remote-Desktop-10-5-0（1841）" class="headerlink" title="Microsoft Remote Desktop 10.5.0（1841）"></a>Microsoft Remote Desktop 10.5.0（1841）</h4><p>这个是微软自己出的mac下面的远程连接软件</p><p><img src="/images/blog/284415-20210119102808312-602727470.jpg"></p><h3 id="IPMI连接软件"><a href="#IPMI连接软件" class="headerlink" title="IPMI连接软件"></a>IPMI连接软件</h3><p>安装java<br>安装xquartz(<a href="https://www.xquartz.org/releases/index.html)(%E8%A7%A3%E5%86%B3%E9%BB%91%E5%B1%8F)">https://www.xquartz.org/releases/index.html)(解决黑屏)</a><br>安装firefox（只有firefox可以打开下载java的启动文件）<br>打开后会被拦截，去隐私里面二次打开即可</p><p><img src="/images/blog/284415-20210127154916948-1117372952.png"></p><h3 id="linux-shell连接软件"><a href="#linux-shell连接软件" class="headerlink" title="linux shell连接软件"></a>linux shell连接软件</h3><h4 id="iterm2"><a href="#iterm2" class="headerlink" title="iterm2"></a>iterm2</h4><p>需要做一些简单的设置</p><ul><li>1、调整Title为profile名称</li><li>2、调整背景颜色021a1e</li><li>3、使用ssh-copy-id保存到本地，然后配置profile里面用command做ssh连接脚本就可以免密登陆了</li><li>4、终端不想显示中文就在profile里面的Terminal里面的Environment设置里面把自动设置变量取消</li></ul><p>快捷键：</p><ul><li>vim 移动到行尾：shift+4</li><li>vim 移动到行首：0</li></ul><h2 id="文件传输软件"><a href="#文件传输软件" class="headerlink" title="文件传输软件"></a>文件传输软件</h2><h3 id="cyberduck"><a href="#cyberduck" class="headerlink" title="cyberduck"></a>cyberduck</h3><p>就是之前测试s3使用的鸭子，这个还支持xftp的，所以直接用这个了，免费的多协议客户端</p><p><img src="/images/blog/284415-20210120000245248-1388185094.jpg"></p><h2 id="截图软件"><a href="#截图软件" class="headerlink" title="截图软件"></a>截图软件</h2><h3 id="xnip"><a href="#xnip" class="headerlink" title="xnip"></a>xnip</h3><p>截图的这块的需求不是那么高，能够跟QQ截图差不多的功能就能满足需求了，这个基本上能够满足我的需求</p><p><img src="/images/blog/284415-20210119103448752-930500126.jpg"></p><h3 id="自带的软件"><a href="#自带的软件" class="headerlink" title="自带的软件"></a>自带的软件</h3><p>shift+command+5<br>默认的这个支持截图和屏幕的录制，默认保存在桌面</p><h2 id="录屏和视频处理软件"><a href="#录屏和视频处理软件" class="headerlink" title="录屏和视频处理软件"></a>录屏和视频处理软件</h2><h3 id="Filmage-Screen-Mac"><a href="#Filmage-Screen-Mac" class="headerlink" title="Filmage Screen Mac"></a>Filmage Screen Mac</h3><p>目前这个看起来比较好，是收费软件，目前看是操作最简单，并且功能能够满足大部分的需求的</p><h3 id="自带的"><a href="#自带的" class="headerlink" title="自带的"></a>自带的</h3><p>自带的就是没有太多的附带的功能，简单使用没有问题</p><h2 id="系统软件"><a href="#系统软件" class="headerlink" title="系统软件"></a>系统软件</h2><h3 id="鼠标平滑软件mos"><a href="#鼠标平滑软件mos" class="headerlink" title="鼠标平滑软件mos"></a>鼠标平滑软件mos</h3><p>默认的触控板的滚动很平滑，使用鼠标滚轮的时候就会一卡一卡的，这个用上面的这个软件可以解决</p><h2 id="下载软件"><a href="#下载软件" class="headerlink" title="下载软件"></a>下载软件</h2><h3 id="motrix"><a href="#motrix" class="headerlink" title="motrix"></a>motrix</h3><p>用于下载用的软件，带ui界面</p>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>如何查看服务器上面的历史磁盘负载数据</title>
    <link href="/2021/01/13/%E5%A6%82%E4%BD%95%E6%9F%A5%E7%9C%8B%E6%9C%8D%E5%8A%A1%E5%99%A8%E4%B8%8A%E9%9D%A2%E7%9A%84%E5%8E%86%E5%8F%B2%E7%A3%81%E7%9B%98%E8%B4%9F%E8%BD%BD%E6%95%B0%E6%8D%AE/"/>
    <url>/2021/01/13/%E5%A6%82%E4%BD%95%E6%9F%A5%E7%9C%8B%E6%9C%8D%E5%8A%A1%E5%99%A8%E4%B8%8A%E9%9D%A2%E7%9A%84%E5%8E%86%E5%8F%B2%E7%A3%81%E7%9B%98%E8%B4%9F%E8%BD%BD%E6%95%B0%E6%8D%AE/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>服务器上面全天的负载可能存在不同，现在需要知道，之前环境上面负载的情况怎么样<br>如果没做监控工具情况下，想看历史数据，这个就需要用到sar的历史记录了</p><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><p>通过对sar的历史数据的读取可以查看整个服务器上面的io之和，虽然不能精确到每个磁盘，但是也能够反应一定的磁盘的负载情况</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs bash">sar -b -f /var/log/sa/sa05<br><br>[root@node101 zp]<span class="hljs-comment"># sar -b -f /var/log/sa/sa05</span><br><br>12:00:01 AM       tps      rtps      wtps   bread/s   bwrtn/s<br>12:10:01 AM   4578.54   3889.63    688.90 1153527.13 158204.94<br>12:20:01 AM   4691.51   3983.55    707.96 1245905.96 149193.88<br>12:30:02 AM   4408.08   3704.57    703.51 1180006.65 162309.00<br>12:40:01 AM   9830.09   9235.21    594.88 1092390.31 164432.00<br>12:50:01 AM  18801.33  18208.55    592.79 1141873.61 171874.90<br>01:00:01 AM   9836.11   9239.30    596.80 986271.99 177031.97<br>01:10:01 AM   8607.46   8031.91    575.55 962631.90 165425.27<br>01:20:01 AM  19046.61  18530.58    516.03 1044466.98 155575.49<br>01:30:01 AM   9150.17   8695.78    454.39 840282.80 132842.10<br></code></pre></td></tr></table></figure><p>上面是时间指定日期的数据，每10分钟一个，日期是循环去覆盖的，也就是能够看到最近三十天的数据的</p>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>awk处理字符串方法汇总</title>
    <link href="/2021/01/13/awk%E5%A4%84%E7%90%86%E5%AD%97%E7%AC%A6%E4%B8%B2%E6%96%B9%E6%B3%95%E6%B1%87%E6%80%BB/"/>
    <url>/2021/01/13/awk%E5%A4%84%E7%90%86%E5%AD%97%E7%AC%A6%E4%B8%B2%E6%96%B9%E6%B3%95%E6%B1%87%E6%80%BB/</url>
    
    <content type="html"><![CDATA[<h2 id="说明"><a href="#说明" class="headerlink" title="说明"></a>说明</h2><p>本篇记录一些字符串的处理方法，这里不具体讲完整的方法，只记录遇到的需要处理的情况</p><h2 id="用例"><a href="#用例" class="headerlink" title="用例"></a>用例</h2><h3 id="用例一：解析ceph-log的ops"><a href="#用例一：解析ceph-log的ops" class="headerlink" title="用例一：解析ceph.log的ops"></a>用例一：解析ceph.log的ops</h3><p>由于ops的单位是有的带k的有的不带k的，那么需要匹配处理</p><p>字符串示例：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">2021-01-13 11:09:55.664479 mgr.node103 client.994548 172.168.30.103:0/265828031 659839 : cluster [DBG] pgmap v218541: 6976 pgs: 6976 active+clean; 178TiB data, 358TiB used, 370TiB / 728TiB avail; 11.4MiB/s rd, 163MiB/s wr, 1.98kop/s<br></code></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">cat</span> ceph-20210113 |grep -v noout|awk <span class="hljs-string">&#x27;&#123;gsub(&quot;op/s&quot;,&quot;&quot;,$28); print $1,$2,$28&#125;&#x27;</span>|awk <span class="hljs-string">&#x27;&#123;if($3~/k/) &#123;gsub(&quot;k&quot;,&quot;&quot;,$3); print $1,$2&quot;,&quot;$3*1000&#125; else &#123;print $1,$2&quot;,&quot;$3&#125;&#125;&#x27;</span><br><br>2021-01-13 11:09:39.575349 928<br>2021-01-13 11:09:41.585060 919<br>2021-01-13 11:09:43.598471 1010<br>2021-01-13 11:09:45.608886 1040<br>2021-01-13 11:09:47.620040 1240<br>2021-01-13 11:09:49.630839 1460<br>2021-01-13 11:09:51.640537 1520<br>2021-01-13 11:09:53.654024 1860<br>2021-01-13 11:09:55.664479 1980<br></code></pre></td></tr></table></figure><p>语法说明：<br>通过gsub(“op&#x2F;s”,””,$28）去掉所有的op&#x2F;s字段<br>通过判断是否有k，有的话去掉k，并且把这个字段的转换成真实io，没有问题的就不转换</p><h3 id="用例2-解析非固定字段"><a href="#用例2-解析非固定字段" class="headerlink" title="用例2:解析非固定字段"></a>用例2:解析非固定字段</h3><p>我们先看下我们的例子</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 data]<span class="hljs-comment"># cat test.txt</span><br>100 100k 100<br>200 200 200k<br></code></pre></td></tr></table></figure><p>假如是上面的字段，我们不确定K出现在哪里，但是需要取得这行的值，那么就需要遍历行进行获取了，解析的方法如下</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 data]<span class="hljs-comment"># cat test.txt |awk &#x27;&#123;for(i=1;i&lt;=NF;i++)&#123;if($i~/k/)&#123;$1=$i;print $0&#125;&#125;&#125;&#x27;</span><br>100k 100k 100<br>200k 200 200k<br></code></pre></td></tr></table></figure><p>上面的例子是遍历列，我们取到了后，把值赋给变量$1的位子，整个适用于变量比较长，但是中间存在空变量的情况，也就是上面例子一当中，处于迁移状态的情况，做上面的例子一的时候解析的时候没考虑迁移的时候，字段长度发生了变化，我们重新解析一次日志</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">cat</span> ceph.log |awk <span class="hljs-string">&#x27;&#123;for(i=1;i&lt;=NF;i++)&#123;if($i~/op\/s/)&#123;$3=$i;print $1,$2,$3&#125;&#125;&#125;&#x27;</span>|awk <span class="hljs-string">&#x27;&#123;gsub(&quot;op/s&quot;,&quot;&quot;,$3); print $1,$2,$3&#125;&#x27;</span>|awk <span class="hljs-string">&#x27;&#123;if($3~/k/) &#123;gsub(&quot;k&quot;,&quot;&quot;,$3); print $1,$2&quot;,&quot;$3*1000&#125; else &#123;print $1,$2&quot;,&quot;$3&#125;&#125;&#x27;</span><br></code></pre></td></tr></table></figure><p>这次的解析的方法是先判断是否有op&#x2F;s的，有的话放在$3的变量的位置，然后后面再处理掉op&#x2F;s字段，然后根据是否有k进行转换</p><h3 id="用例三：解析ceph-log的读写数据"><a href="#用例三：解析ceph-log的读写数据" class="headerlink" title="用例三：解析ceph.log的读写数据"></a>用例三：解析ceph.log的读写数据</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># /bin/sh</span><br><br><span class="hljs-built_in">cat</span> vcfs-rd-wr.log |awk <span class="hljs-string">&#x27;</span><br><span class="hljs-string">&#123;if($0~&quot;wr&quot;)</span><br><span class="hljs-string">    &#123;</span><br><span class="hljs-string">    for(i=1;i&lt;=NF;i++)</span><br><span class="hljs-string">        &#123;if($i~/rd/)</span><br><span class="hljs-string">            &#123;</span><br><span class="hljs-string">            $3=$(i-2);</span><br><span class="hljs-string">            $4=$(i-1);</span><br><span class="hljs-string">            &#125;</span><br><span class="hljs-string">         else if($i~/wr/)</span><br><span class="hljs-string">            &#123;</span><br><span class="hljs-string">            $5=$(i-2);</span><br><span class="hljs-string">            $6=$(i-1);</span><br><span class="hljs-string">            &#125;</span><br><span class="hljs-string">        &#125;</span><br><span class="hljs-string">    print $1,$2,$3,$4,$5,$6;</span><br><span class="hljs-string">    &#125;</span><br><span class="hljs-string"></span><br><span class="hljs-string">else</span><br><span class="hljs-string">    &#123;</span><br><span class="hljs-string">    for(i=1;i&lt;=NF;i++)</span><br><span class="hljs-string">        &#123; if($i~/rd/)</span><br><span class="hljs-string">            &#123;</span><br><span class="hljs-string">            $3=$(i-2);</span><br><span class="hljs-string">            $4=$(i-1);</span><br><span class="hljs-string">            &#125;</span><br><span class="hljs-string">        &#125;</span><br><span class="hljs-string">    print $1,$2,$3,$4,&quot;0&quot;,&quot;kB/s&quot;</span><br><span class="hljs-string">    &#125;</span><br><span class="hljs-string">#print $0</span><br><span class="hljs-string">&#125;&#x27;</span>|awk <span class="hljs-string">&#x27;</span><br><span class="hljs-string">&#123;if($4==&quot;kB/s&quot;)</span><br><span class="hljs-string">    &#123;</span><br><span class="hljs-string">    $3=$3*1</span><br><span class="hljs-string">    &#125;</span><br><span class="hljs-string"> else if($4==&quot;MB/s&quot;)</span><br><span class="hljs-string">    &#123;</span><br><span class="hljs-string">    gsub(&quot;MB/s&quot;,&quot;kB/s&quot;,$4)</span><br><span class="hljs-string">    $3=$3*1024</span><br><span class="hljs-string">    &#125;</span><br><span class="hljs-string"> else if($4==&quot;GB/s&quot;)</span><br><span class="hljs-string">    &#123;</span><br><span class="hljs-string">    gsub(&quot;GB/s&quot;,&quot;kB/s&quot;,$4)</span><br><span class="hljs-string">    $3=$3*1024*1024</span><br><span class="hljs-string">    &#125;</span><br><span class="hljs-string"> if($6==&quot;B/s&quot;)</span><br><span class="hljs-string">    &#123;</span><br><span class="hljs-string">    gsub(&quot;B/s&quot;,&quot;kB/s&quot;,$6)</span><br><span class="hljs-string">    $5=$5/1024</span><br><span class="hljs-string">    &#125;</span><br><span class="hljs-string"> else if($6==&quot;kB/s&quot;)</span><br><span class="hljs-string">    &#123;</span><br><span class="hljs-string">    $5=$5</span><br><span class="hljs-string">    &#125;</span><br><span class="hljs-string"> else if($6==&quot;MB/s&quot;)</span><br><span class="hljs-string">    &#123;</span><br><span class="hljs-string">    gsub(&quot;MB/s&quot;,&quot;kB/s&quot;,$6)</span><br><span class="hljs-string">    $5=$5*1024</span><br><span class="hljs-string">    &#125;</span><br><span class="hljs-string"> else if($6==&quot;GB/s&quot;)</span><br><span class="hljs-string">    &#123;</span><br><span class="hljs-string">    gsub(&quot;GB/s&quot;,&quot;kB/s&quot;,$6)</span><br><span class="hljs-string">    $5=$5*1024*1024</span><br><span class="hljs-string">    &#125;</span><br><span class="hljs-string"></span><br><span class="hljs-string">print $0</span><br><span class="hljs-string">&#125;&#x27;</span><br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>简单的统计剩余ceph迁移时间</title>
    <link href="/2021/01/12/%E7%AE%80%E5%8D%95%E7%9A%84%E7%BB%9F%E8%AE%A1%E5%89%A9%E4%BD%99ceph%E8%BF%81%E7%A7%BB%E6%97%B6%E9%97%B4/"/>
    <url>/2021/01/12/%E7%AE%80%E5%8D%95%E7%9A%84%E7%BB%9F%E8%AE%A1%E5%89%A9%E4%BD%99ceph%E8%BF%81%E7%A7%BB%E6%97%B6%E9%97%B4/</url>
    
    <content type="html"><![CDATA[<h2 id="需求"><a href="#需求" class="headerlink" title="需求"></a>需求</h2><p>由于迁移忽快忽慢，需要知道大概的迁移的时间，做了一个简单的统计脚本，可能不准，大概能够知道在什么范围内</p><h2 id="脚本"><a href="#脚本" class="headerlink" title="脚本"></a>脚本</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-meta">#! /bin/sh</span><br><span class="hljs-keyword">while</span> ( 2&gt;1 )<br><span class="hljs-keyword">do</span> <br>start=`ceph -s|grep pgs|grep mis|awk <span class="hljs-string">&#x27;&#123;print $2&#125;&#x27;</span>|<span class="hljs-built_in">cut</span> -d / -f 1`<br><span class="hljs-built_in">sleep</span> 5<br>end=`ceph -s|grep pgs|grep mis|awk <span class="hljs-string">&#x27;&#123;print $2&#125;&#x27;</span>|<span class="hljs-built_in">cut</span> -d / -f 1`<br>speed=$((start-end))<br><span class="hljs-comment">#echo $end</span><br><span class="hljs-comment">#echo $speed</span><br>second=$((end/speed*<span class="hljs-number">5</span>))<br><br>hour=$(( <span class="hljs-variable">$second</span>/<span class="hljs-number">3600</span> ))<br>min=$(( (<span class="hljs-variable">$second</span>-<span class="hljs-variable">$&#123;hour&#125;</span>*<span class="hljs-number">3600</span>)/<span class="hljs-number">60</span> ))<br>sec=$(( <span class="hljs-variable">$second</span>-<span class="hljs-variable">$&#123;hour&#125;</span>*<span class="hljs-number">3600</span>-<span class="hljs-variable">$&#123;min&#125;</span>*<span class="hljs-number">60</span> ))<br><span class="hljs-built_in">echo</span> 当前时间:`<span class="hljs-built_in">date</span>`<br><span class="hljs-built_in">echo</span> 迁移剩余:<span class="hljs-variable">$end</span><br><span class="hljs-built_in">echo</span> 迁移速度:$((speed/<span class="hljs-number">5</span>))<br><span class="hljs-built_in">echo</span> 迁移还需要:<span class="hljs-variable">$&#123;hour&#125;</span>小时<span class="hljs-variable">$&#123;min&#125;</span>分<span class="hljs-variable">$&#123;sec&#125;</span>秒<br><br><span class="hljs-keyword">done</span><br></code></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-meta">#! /bin/sh</span><br><span class="hljs-keyword">while</span> ( 2&gt;1 )<br><span class="hljs-keyword">do</span> <br>start=`ceph -s -f json-pretty|grep misplaced_objects|<span class="hljs-built_in">cut</span> -d <span class="hljs-string">&quot;:&quot;</span> -f 2|<span class="hljs-built_in">cut</span> -d <span class="hljs-string">&#x27;,&#x27;</span> -f 1`<br><span class="hljs-built_in">sleep</span> 5<br>end=`ceph -s -f json-pretty|grep misplaced_objects|<span class="hljs-built_in">cut</span> -d <span class="hljs-string">&quot;:&quot;</span> -f 2|<span class="hljs-built_in">cut</span> -d <span class="hljs-string">&#x27;,&#x27;</span> -f 1`<br>speed=$((start-end))<br><span class="hljs-comment">#echo $end</span><br><span class="hljs-comment">#echo $speed</span><br>second=$((end/speed*<span class="hljs-number">5</span>))<br><br>hour=$(( <span class="hljs-variable">$second</span>/<span class="hljs-number">3600</span> ))<br>min=$(( (<span class="hljs-variable">$second</span>-<span class="hljs-variable">$&#123;hour&#125;</span>*<span class="hljs-number">3600</span>)/<span class="hljs-number">60</span> ))<br>sec=$(( <span class="hljs-variable">$second</span>-<span class="hljs-variable">$&#123;hour&#125;</span>*<span class="hljs-number">3600</span>-<span class="hljs-variable">$&#123;min&#125;</span>*<span class="hljs-number">60</span> ))<br><span class="hljs-built_in">echo</span> 当前时间:`<span class="hljs-built_in">date</span>`<br><span class="hljs-built_in">echo</span> 迁移剩余:<span class="hljs-variable">$end</span><br><span class="hljs-built_in">echo</span> 迁移速度:$((speed/<span class="hljs-number">5</span>))<br><span class="hljs-built_in">echo</span> 迁移还需要:<span class="hljs-variable">$&#123;hour&#125;</span>小时<span class="hljs-variable">$&#123;min&#125;</span>分<span class="hljs-variable">$&#123;sec&#125;</span>秒<br><br><span class="hljs-keyword">done</span><br></code></pre></td></tr></table></figure><p>上面的脚本有的时候获取的字段不对，后面的一个脚本获取的好一点<br>上面的统计周期为5秒，理论上设置的周期越长，统计的越接近真实的</p>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>通过shell命令行精准调整时间</title>
    <link href="/2020/12/18/%E9%80%9A%E8%BF%87shell%E5%91%BD%E4%BB%A4%E8%A1%8C%E7%B2%BE%E5%87%86%E8%B0%83%E6%95%B4%E6%97%B6%E9%97%B4/"/>
    <url>/2020/12/18/%E9%80%9A%E8%BF%87shell%E5%91%BD%E4%BB%A4%E8%A1%8C%E7%B2%BE%E5%87%86%E8%B0%83%E6%95%B4%E6%97%B6%E9%97%B4/</url>
    
    <content type="html"><![CDATA[<h2 id="需求说明"><a href="#需求说明" class="headerlink" title="需求说明"></a>需求说明</h2><p>有的时候我们做测试，需要把当前的时间，精准的往前或者往后调整个几秒钟，这个我写成了一个脚本，方便调整<br>思路是当前时间转换成unix时间，计算后，算到当前时间，然后设置</p><h2 id="脚本"><a href="#脚本" class="headerlink" title="脚本"></a>脚本</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs bash">timenow=`<span class="hljs-built_in">date</span>`<br>unixtimenow=`<span class="hljs-built_in">date</span>  +%s`<br><span class="hljs-built_in">echo</span> <span class="hljs-variable">$timenow</span><br><span class="hljs-built_in">echo</span> <span class="hljs-variable">$unixtimenow</span><br><br>unixtimenew=$((<span class="hljs-variable">$unixtimenow</span>-<span class="hljs-number">2000</span>))<br><span class="hljs-built_in">echo</span> <span class="hljs-variable">$unixtimenew</span><br><br>newtime=`<span class="hljs-built_in">date</span> -d @<span class="hljs-variable">$unixtimenew</span>  <span class="hljs-string">&quot;+%Y-%m-%d %H:%M:%S&quot;</span>`<br><br><span class="hljs-built_in">echo</span> <span class="hljs-variable">$newtime</span><br><br><span class="hljs-built_in">date</span> -s <span class="hljs-string">&quot;<span class="hljs-variable">$newtime</span>&quot;</span><br><br></code></pre></td></tr></table></figure><p>上面的-2000就是往历史调整2000秒，需要往未来调整就改成+就行了，比较简单</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>留存备用</p>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>rbd锁引起kvm虚拟机无法启动的故障</title>
    <link href="/2020/12/18/rbd%E9%94%81%E5%BC%95%E8%B5%B7kvm%E8%99%9A%E6%8B%9F%E6%9C%BA%E6%97%A0%E6%B3%95%E5%90%AF%E5%8A%A8%E7%9A%84%E6%95%85%E9%9A%9C/"/>
    <url>/2020/12/18/rbd%E9%94%81%E5%BC%95%E8%B5%B7kvm%E8%99%9A%E6%8B%9F%E6%9C%BA%E6%97%A0%E6%B3%95%E5%90%AF%E5%8A%A8%E7%9A%84%E6%95%85%E9%9A%9C/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>环境因为一些问题（网络，或者磁盘，或者其它各种异常），引起了集群的状态的一些变化，变化之后，集群的某些虚拟机正常某些虚拟机出现异常，异常现象就是无法启动<br>特别是win server2008 ,会一直卡在滚动条这里</p><p>这个问题很久前碰到过一个哥们出现过，他问我是否遇到过，这个之前遇到过一次无法启动的，通过导出导入的方式解决了，当时一直也没找到原因<br>这个哥们告诉我，通过关闭rbd的属性后，就可以正常启动了，当时就记了下，也没分析更深层次的原因<br>最近有一个环境因为时间过快，往回调整了十几分钟，集群出现mon选举，osd出现闪断以后，部分虚拟机出现无法启动的情况</p><h2 id="问题分析"><a href="#问题分析" class="headerlink" title="问题分析"></a>问题分析</h2><p>导入导出或者克隆基本可以判断数据是没有问题的，那么应该就是其它问题<br>通过关闭属性可以解决，那么大概能够定位到这几个属性相关的</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">features: layering, exclusive-lock, object-map, fast-diff, deep-flatten<br></code></pre></td></tr></table></figure><p>而锁文件是最大的怀疑，之前处理ctdb的时候，就有过锁没有加上超时时间，然后ctdb挂掉以后，锁不会自动释放的问题，怀疑这个地方类似</p><p>我们的rbd的镜像如下</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 vm]<span class="hljs-comment"># rbd info rbd/windows-server-8-base-clone</span><br>rbd image <span class="hljs-string">&#x27;windows-server-8-base-clone&#x27;</span>:<br>size 50GiB <span class="hljs-keyword">in</span> 12800 objects<br>order 22 (4MiB objects)<br>block_name_prefix: rbd_data.102376b8b4567<br>format: 2<br>features: layering, exclusive-lock, object-map, fast-diff, deep-flatten<br>flags: <br>create_timestamp: Fri Dec 18 11:56:27 2020<br>parent: rbd/windows-server-8-base@for-clone<br>overlap: 50GiB<br></code></pre></td></tr></table></figure><p>这个地方会有两个对象被上锁</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ceph]<span class="hljs-comment"># rbd lock ls rbd/windows-server-8-base-clone</span><br>There is 1 exclusive lock on this image.<br>Locker       ID                  Address                    <br>client.96545 auto 94013702760192 192.168.19.101:0/915896787 <br></code></pre></td></tr></table></figure><p>object_map的rbd_lock</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ceph]<span class="hljs-comment"># rados -p rbd lock info rbd_object_map.102376b8b4567 rbd_lock</span><br>&#123;<span class="hljs-string">&quot;name&quot;</span>:<span class="hljs-string">&quot;rbd_lock&quot;</span>,<span class="hljs-string">&quot;type&quot;</span>:<span class="hljs-string">&quot;exclusive&quot;</span>,<span class="hljs-string">&quot;tag&quot;</span>:<span class="hljs-string">&quot;&quot;</span>,<span class="hljs-string">&quot;lockers&quot;</span>:[&#123;<span class="hljs-string">&quot;name&quot;</span>:<span class="hljs-string">&quot;client.96943&quot;</span>,<span class="hljs-string">&quot;cookie&quot;</span>:<span class="hljs-string">&quot;&quot;</span>,<span class="hljs-string">&quot;description&quot;</span>:<span class="hljs-string">&quot;&quot;</span>,<span class="hljs-string">&quot;expiration&quot;</span>:<span class="hljs-string">&quot;0.000000&quot;</span>,<span class="hljs-string">&quot;addr&quot;</span>:<span class="hljs-string">&quot;192.168.19.101:0/2231653745&quot;</span>&#125;]&#125;<br></code></pre></td></tr></table></figure><p>header的rbd_lock,这个跟上面通过rbd lock查到的锁命令是同一个</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ceph]<span class="hljs-comment"># rados -p rbd lock info rbd_header.102376b8b4567 rbd_lock</span><br>&#123;<span class="hljs-string">&quot;name&quot;</span>:<span class="hljs-string">&quot;rbd_lock&quot;</span>,<span class="hljs-string">&quot;type&quot;</span>:<span class="hljs-string">&quot;exclusive&quot;</span>,<span class="hljs-string">&quot;tag&quot;</span>:<span class="hljs-string">&quot;internal&quot;</span>,<span class="hljs-string">&quot;lockers&quot;</span>:[&#123;<span class="hljs-string">&quot;name&quot;</span>:<span class="hljs-string">&quot;client.96943&quot;</span>,<span class="hljs-string">&quot;cookie&quot;</span>:<span class="hljs-string">&quot;auto 94009656832384&quot;</span>,<span class="hljs-string">&quot;description&quot;</span>:<span class="hljs-string">&quot;&quot;</span>,<span class="hljs-string">&quot;expiration&quot;</span>:<span class="hljs-string">&quot;0.000000&quot;</span>,<span class="hljs-string">&quot;addr&quot;</span>:<span class="hljs-string">&quot;192.168.19.101:0/2231653745&quot;</span>&#125;]&#125;<br></code></pre></td></tr></table></figure><h2 id="复现问题"><a href="#复现问题" class="headerlink" title="复现问题"></a>复现问题</h2><p>启动一个rbd的kvm，并且装上win server2008</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">/usr/libexec/qemu-kvm  -drive format=raw,file=rbd:rbd/windows-server-8-base-clone  -cdrom /home/vm/win2008.iso -m 8192 -vnc :0<br></code></pre></td></tr></table></figure><p>这个启动后直接用vnc连接访问即可，然后使用一个循环测试写入工具在虚拟机里面进行循环读写，模拟正常的写入，我使用的是SANergy这个工具，使用循环写模式，可以做成iso，然后上面的-cdrom参数把文件穿透进去</p><p>准备一个至少两个物理节点的集群<br>节点一为mon，节点二的时间同步指向节点一，然后调整一的物理时间往后调整20分钟（也可以往前），这个集群会出现osd闪断的情况<br>正常情况下，这个时候windows会卡死的，如果没卡死，多操作几遍往后调整时间，目的就是把虚拟机卡死</p><p>我的环境通过资源管理器看到没有读写io了，强制停止kvm进程，无法停止就kill -9</p><p>我们恢复集群的状态,然后再次检查锁状态</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ceph]<span class="hljs-comment"># cat /home/vm/checklock.sh </span><br>rados -p rbd lock info rbd_object_map.102376b8b4567 rbd_lock<br>rados -p rbd lock info rbd_header.102376b8b4567 rbd_lock<br><br>[root@lab101 ceph]<span class="hljs-comment"># sh /home/vm/checklock.sh </span><br>&#123;<span class="hljs-string">&quot;name&quot;</span>:<span class="hljs-string">&quot;rbd_lock&quot;</span>,<span class="hljs-string">&quot;type&quot;</span>:<span class="hljs-string">&quot;exclusive&quot;</span>,<span class="hljs-string">&quot;tag&quot;</span>:<span class="hljs-string">&quot;&quot;</span>,<span class="hljs-string">&quot;lockers&quot;</span>:[&#123;<span class="hljs-string">&quot;name&quot;</span>:<span class="hljs-string">&quot;client.96943&quot;</span>,<span class="hljs-string">&quot;cookie&quot;</span>:<span class="hljs-string">&quot;&quot;</span>,<span class="hljs-string">&quot;description&quot;</span>:<span class="hljs-string">&quot;&quot;</span>,<span class="hljs-string">&quot;expiration&quot;</span>:<span class="hljs-string">&quot;0.000000&quot;</span>,<span class="hljs-string">&quot;addr&quot;</span>:<span class="hljs-string">&quot;192.168.19.101:0/2231653745&quot;</span>&#125;]&#125;&#123;<span class="hljs-string">&quot;name&quot;</span>:<span class="hljs-string">&quot;rbd_lock&quot;</span>,<span class="hljs-string">&quot;type&quot;</span>:<span class="hljs-string">&quot;exclusive&quot;</span>,<span class="hljs-string">&quot;tag&quot;</span>:<span class="hljs-string">&quot;internal&quot;</span>,<span class="hljs-string">&quot;lockers&quot;</span>:[&#123;<span class="hljs-string">&quot;name&quot;</span>:<span class="hljs-string">&quot;client.96943&quot;</span>,<span class="hljs-string">&quot;cookie&quot;</span>:<span class="hljs-string">&quot;auto 94009656832384&quot;</span>,<span class="hljs-string">&quot;description&quot;</span>:<span class="hljs-string">&quot;&quot;</span>,<span class="hljs-string">&quot;expiration&quot;</span>:<span class="hljs-string">&quot;0.000000&quot;</span>,<span class="hljs-string">&quot;addr&quot;</span>:<span class="hljs-string">&quot;192.168.19.101:0/2231653745&quot;</span>&#125;]&#125;<br></code></pre></td></tr></table></figure><p>可以看到居然还在</p><p>我们尝试启动虚拟机出现无法启动的情况</p><p>我们尝试删除锁<br>处理objectmap的锁</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ceph]<span class="hljs-comment"># rados -p rbd lock break rbd_object_map.102376b8b4567 rbd_lock client.96943</span><br></code></pre></td></tr></table></figure><p>处理header的锁</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ceph]<span class="hljs-comment"># rados -p rbd lock break rbd_header.102376b8b4567 rbd_lock client.96943</span><br>ERROR: failed breaking lock: (2) No such file or directory<br>error 2: (2) No such file or directory<br></code></pre></td></tr></table></figure><p>提示没有，比较奇怪,那尝试拿锁</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ceph]<span class="hljs-comment"># rados -p rbd lock get rbd_header.102376b8b4567 rbd_lock client.96943</span><br>ERROR: failed locking: (16) Device or resource busy<br>error 16: (16) Device or resource busy<br></code></pre></td></tr></table></figure><p>可以看到Device or resource busy 这个一般就是资源占用，无法释放的情况</p><p>我们用rbd的命令试下</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ceph]<span class="hljs-comment"># rbd lock ls rbd/windows-server-8-base-clone</span><br>There is 1 exclusive lock on this image.<br>Locker       ID                  Address                     <br>client.96943 auto 94009656832384 192.168.19.101:0/2231653745 <br>[root@lab101 ceph]<span class="hljs-comment"># rbd lock rm rbd/windows-server-8-base-clone &quot;auto 94009656832384&quot; client.96943 </span><br>[root@lab101 ceph]<span class="hljs-comment"># rbd lock ls rbd/windows-server-8-base-clone</span><br></code></pre></td></tr></table></figure><p>可以看到这个是可以删除的</p><p>再次检查</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ceph]<span class="hljs-comment"># sh /home/vm/checklock.sh </span><br>&#123;<span class="hljs-string">&quot;name&quot;</span>:<span class="hljs-string">&quot;rbd_lock&quot;</span>,<span class="hljs-string">&quot;type&quot;</span>:<span class="hljs-string">&quot;exclusive&quot;</span>,<span class="hljs-string">&quot;tag&quot;</span>:<span class="hljs-string">&quot;&quot;</span>,<span class="hljs-string">&quot;lockers&quot;</span>:[]&#125;&#123;<span class="hljs-string">&quot;name&quot;</span>:<span class="hljs-string">&quot;rbd_lock&quot;</span>,<span class="hljs-string">&quot;type&quot;</span>:<span class="hljs-string">&quot;exclusive&quot;</span>,<span class="hljs-string">&quot;tag&quot;</span>:<span class="hljs-string">&quot;internal&quot;</span>,<span class="hljs-string">&quot;lockers&quot;</span>:[]&#125;[root@lab101 ceph]<span class="hljs-comment">#</span><br></code></pre></td></tr></table></figure><p>可以看到没有锁了</p><p>我们再次尝试启动</p><p>产生了新的锁文件了</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">&#123;<span class="hljs-string">&quot;name&quot;</span>:<span class="hljs-string">&quot;rbd_lock&quot;</span>,<span class="hljs-string">&quot;type&quot;</span>:<span class="hljs-string">&quot;exclusive&quot;</span>,<span class="hljs-string">&quot;tag&quot;</span>:<span class="hljs-string">&quot;&quot;</span>,<span class="hljs-string">&quot;lockers&quot;</span>:[]&#125;&#123;<span class="hljs-string">&quot;name&quot;</span>:<span class="hljs-string">&quot;rbd_lock&quot;</span>,<span class="hljs-string">&quot;type&quot;</span>:<span class="hljs-string">&quot;exclusive&quot;</span>,<span class="hljs-string">&quot;tag&quot;</span>:<span class="hljs-string">&quot;internal&quot;</span>,<span class="hljs-string">&quot;lockers&quot;</span>:[]&#125;[root@lab101 ceph]<span class="hljs-comment"># </span><br>[root@lab101 ceph]<span class="hljs-comment"># sh /home/vm/checklock.sh </span><br>&#123;<span class="hljs-string">&quot;name&quot;</span>:<span class="hljs-string">&quot;rbd_lock&quot;</span>,<span class="hljs-string">&quot;type&quot;</span>:<span class="hljs-string">&quot;exclusive&quot;</span>,<span class="hljs-string">&quot;tag&quot;</span>:<span class="hljs-string">&quot;&quot;</span>,<span class="hljs-string">&quot;lockers&quot;</span>:[&#123;<span class="hljs-string">&quot;name&quot;</span>:<span class="hljs-string">&quot;client.97312&quot;</span>,<span class="hljs-string">&quot;cookie&quot;</span>:<span class="hljs-string">&quot;&quot;</span>,<span class="hljs-string">&quot;description&quot;</span>:<span class="hljs-string">&quot;&quot;</span>,<span class="hljs-string">&quot;expiration&quot;</span>:<span class="hljs-string">&quot;0.000000&quot;</span>,<span class="hljs-string">&quot;addr&quot;</span>:<span class="hljs-string">&quot;192.168.19.101:0/1322200836&quot;</span>&#125;]&#125;&#123;<span class="hljs-string">&quot;name&quot;</span>:<span class="hljs-string">&quot;rbd_lock&quot;</span>,<span class="hljs-string">&quot;type&quot;</span>:<span class="hljs-string">&quot;exclusive&quot;</span>,<span class="hljs-string">&quot;tag&quot;</span>:<span class="hljs-string">&quot;internal&quot;</span>,<span class="hljs-string">&quot;lockers&quot;</span>:[&#123;<span class="hljs-string">&quot;name&quot;</span>:<span class="hljs-string">&quot;client.97312&quot;</span>,<span class="hljs-string">&quot;cookie&quot;</span>:<span class="hljs-string">&quot;auto 94532169500416&quot;</span>,<span class="hljs-string">&quot;description&quot;</span>:<span class="hljs-string">&quot;&quot;</span>,<span class="hljs-string">&quot;expiration&quot;</span>:<span class="hljs-string">&quot;0.000000&quot;</span>,<span class="hljs-string">&quot;addr&quot;</span>:<span class="hljs-string">&quot;192.168.19.101:0/1322200836&quot;</span>&#125;]&#125;<br></code></pre></td></tr></table></figure><p>从vnc里面可以看到桌面了</p><p>上面的是处理的方法之一，另外的一个方法是</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">rbd feature <span class="hljs-built_in">disable</span>  rbd/windows-server-8-base-clone  exclusive-lock, object-map, fast-diff<br>rbd feature <span class="hljs-built_in">enable</span>  rbd/windows-server-8-base-clone  exclusive-lock, object-map, fast-diff<br></code></pre></td></tr></table></figure><p>建议是通过方法一进行处理，理论上方法二也没什么问题，其它几个属性如果正好需要用到的话，那么关闭后，开启下就行</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>出问题总会是有原因的，如果我们提前抓到了问题，那么下次再遇到的时候就会从容的多，所以不要放弃解决问题，能够解决的问题都会成为你的经验</p>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>最简单的临时web服务器</title>
    <link href="/2020/12/17/%E6%9C%80%E7%AE%80%E5%8D%95%E7%9A%84%E4%B8%B4%E6%97%B6web%E6%9C%8D%E5%8A%A1%E5%99%A8/"/>
    <url>/2020/12/17/%E6%9C%80%E7%AE%80%E5%8D%95%E7%9A%84%E4%B8%B4%E6%97%B6web%E6%9C%8D%E5%8A%A1%E5%99%A8/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>有的时候需要做个单页的网页显示，这个直接用python启动即可，但是存在一个问题，停止进程以后，如果网页正在被访问，socket会不释放，然后再启动就会提示端口占用，实际上是没有端口占用的</p><p>这个增加一个配置项就行</p><h2 id="相关代码"><a href="#相关代码" class="headerlink" title="相关代码"></a>相关代码</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-meta">#!/usr/bin/python3</span><br><br>from __future__ import print_function<br><br>import SimpleHTTPServer<br>import SocketServer<br>import os<br>import sys<br>import signal<br><br>def quit(signal_num,frame):<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;you stop the threading&quot;</span>)<br>    sys.exit()<br><br><br>path = os.path.dirname(sys.argv[0])<br><br>os.chdir(<span class="hljs-string">&#x27;/home/cephuse/output/&#x27;</span>)<br><br>class ReusingTCPServer(SimpleHTTPServer.SimpleHTTPRequestHandler):<br>    allow_reuse_address = True<br><br>    def send_head(self):<br>        <span class="hljs-comment"># horrible kludge because SimpleHTTPServer is buggy wrt</span><br>        <span class="hljs-comment"># slash-redirecting of requests with query arguments, and will</span><br>        <span class="hljs-comment"># redirect to /foo?q=bar/ -- wrong slash placement</span><br>        self.path = self.path.split(<span class="hljs-string">&#x27;?&#x27;</span>, 1)[0]<br>        <span class="hljs-built_in">return</span> SimpleHTTPServer.SimpleHTTPRequestHandler.send_head(self)<br><br><br>SocketServer.TCPServer.allow_reuse_address = True<br>httpd = SocketServer.TCPServer(<br>    (<span class="hljs-string">&quot;&quot;</span>, 9090),<br>    ReusingTCPServer,<br>    )<br>try:<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Serving doc at port: http://localhost:9090&quot;</span>)<br>    httpd.serve_forever()<br>except KeyboardInterrupt:<br>    pass<br></code></pre></td></tr></table></figure><p>上面是参考的ceph doc的服务脚本，区别就是增加的那句</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">SocketServer.TCPServer.allow_reuse_address = True<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>预留linux系统随机端口-内核参数控制</title>
    <link href="/2020/12/16/%E9%A2%84%E7%95%99linux%E7%B3%BB%E7%BB%9F%E9%9A%8F%E6%9C%BA%E7%AB%AF%E5%8F%A3-%E5%86%85%E6%A0%B8%E5%8F%82%E6%95%B0%E6%8E%A7%E5%88%B6/"/>
    <url>/2020/12/16/%E9%A2%84%E7%95%99linux%E7%B3%BB%E7%BB%9F%E9%9A%8F%E6%9C%BA%E7%AB%AF%E5%8F%A3-%E5%86%85%E6%A0%B8%E5%8F%82%E6%95%B0%E6%8E%A7%E5%88%B6/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>随机端口可能会占用后面准备使用的端口，然后在后面进程启动的时候，无法绑定端口引起服务异常</p><h2 id="处理方法"><a href="#处理方法" class="headerlink" title="处理方法"></a>处理方法</h2><p>通过内核参数控制，在&#x2F;etc&#x2F;sysctl.conf添加</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">net.ipv4.ip_local_reserved_ports=8000,8080<br></code></pre></td></tr></table></figure><p>这个容易被冲掉，可以修改下默认的内核参数，通过tune实现</p><p>修改配置文件 &#x2F;usr&#x2F;lib&#x2F;tuned&#x2F;balanced&#x2F;tuned.conf<br>添加</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[sysctl]<br>net.ipv4.ip_local_reserved_ports=8000,8080<br></code></pre></td></tr></table></figure><p>然后执行</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">tuned-adm  profile balanced<br></code></pre></td></tr></table></figure><p>这个相当于修改了内核的默认参数，清空 &#x2F;etc&#x2F;sysctl.conf也不影响<br>执行完可以通过</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">sysctl -a|grep net.ipv4.ip_local_reserved_ports<br></code></pre></td></tr></table></figure><p>检查一下是否生效</p>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>简单的spec打包rpm包的方法</title>
    <link href="/2020/11/25/%E7%AE%80%E5%8D%95%E7%9A%84spec%E6%89%93%E5%8C%85rpm%E5%8C%85%E7%9A%84%E6%96%B9%E6%B3%95/"/>
    <url>/2020/11/25/%E7%AE%80%E5%8D%95%E7%9A%84spec%E6%89%93%E5%8C%85rpm%E5%8C%85%E7%9A%84%E6%96%B9%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>有的时候为了提供一个完整的包，需要把一些零散的文件或者脚本汇总在一起提供，rpm包就是一个很好的方法，这里我们不需要复杂的写法，就纯粹打包的，复杂的可以参考内核或者其它软件包的打包方法</p><h2 id="spec文件信息"><a href="#spec文件信息" class="headerlink" title="spec文件信息"></a>spec文件信息</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs bash">Name:   cephuse<br>Version:   1.0<br>Release:        1%&#123;?dist&#125;<br>Summary: monitor ceph use space<br><br>Group: Development/Tools<br>License:        GPL<br>URL:            http://www.zphj1987.com<br><br>BuildRequires:  python<br>Source: cephuse-1.0.tar.gz<br><br>%description<br>use to monitor ceph<br><br>%prep<br>%setup -q<br><br><br>%install<br>install -D -m 644 cephuse.service %&#123;buildroot&#125;/usr/lib/systemd/system/cephuse.service<br><span class="hljs-built_in">cp</span> -ra ./output %&#123;buildroot&#125;/<br><br>%files<br>/usr/lib/systemd/system/cephuse.service<br>/output<br>%doc<br><br>%changelog<br>* Tue Oct 30 2020 zphj1987 &lt;zphj1987@gmail.com&gt; - v1.0<br>- 新增版本<br></code></pre></td></tr></table></figure><p>打包好tar.gz包，放到源码目录，然后执行rpmbuild -bb cephuse.spec就可以打包了</p>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>内核补丁热更新ceph内核模块</title>
    <link href="/2020/11/19/%E5%86%85%E6%A0%B8%E8%A1%A5%E4%B8%81%E7%83%AD%E6%9B%B4%E6%96%B0ceph%E5%86%85%E6%A0%B8%E6%A8%A1%E5%9D%97/"/>
    <url>/2020/11/19/%E5%86%85%E6%A0%B8%E8%A1%A5%E4%B8%81%E7%83%AD%E6%9B%B4%E6%96%B0ceph%E5%86%85%E6%A0%B8%E6%A8%A1%E5%9D%97/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>内核模块的更新一般需要卸载模块再加载，但是很多时候使用场景决定了无法做卸载的操作，而linux支持了热更新内核模块的功能，这个已经支持了有一段时间了，一直没有拿ceph的相关模块进行验证</p><p>注意模块的某些函数是不支持的，init的部分是不支持的，补丁弄完验证一下就可以知道支不支持，不支持的部分会提示</p><h2 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h2><p>先检查当前的版本支持不</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 kpatch]<span class="hljs-comment"># cat /boot/config-3.10.0-1062.el7.x86_64 |grep PATCH</span><br>CONFIG_HAVE_LIVEPATCH=y<br>CONFIG_LIVEPATCH=y<br>CONFIG_DVB_BUDGET_PATCH=m<br>CONFIG_SND_HDA_PATCH_LOADER=y<br></code></pre></td></tr></table></figure><p>可以看到默认内核是支持的，这个是红帽维护的一个体系，自己的内核，肯定会很快集成进去的</p><p>安装依赖包</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab102 ~]<span class="hljs-comment"># yum install  elfutils-devel  rpm-build</span><br>[root@lab102 ~]<span class="hljs-comment"># rpm -ivh kernel-debuginfo-common-x86_64-3.10.0-1062.el7.x86_64.rpm kernel-debuginfo-3.10.0-1062.el7.x86_64.rpm</span><br></code></pre></td></tr></table></figure><p>下载软件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">git <span class="hljs-built_in">clone</span> https://github.com/dynup/kpatch.git<br>make <br>make install <br></code></pre></td></tr></table></figure><p>上面的软件提供两个命令<br>一个是kpatch<br>一个是kpatch-build<br>后面会用到</p><p>我的机器是这个版本</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab102 ~]<span class="hljs-comment"># uname  -a</span><br>Linux lab102 3.10.0-1062.el7.x86_64 <span class="hljs-comment">#1 SMP Wed Aug 7 18:08:02 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux</span><br></code></pre></td></tr></table></figure><p>那么提前下载好 </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">kernel-3.10.0-1062.el7.src.rpm<br></code></pre></td></tr></table></figure><h2 id="生成差异热更新模块"><a href="#生成差异热更新模块" class="headerlink" title="生成差异热更新模块"></a>生成差异热更新模块</h2><p>因为这个打补丁是基于差异打的补丁，所以需要知道之前的源码和现在的源码的差异，然后再进行后面的处理，所以我们要准备两份源码，一份未修改的，一份修改了的</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab102 kernel]<span class="hljs-comment"># rpm2cpio kernel-3.10.0-1062.el7.src.rpm |cpio -div</span><br>[root@lab102 kernel]<span class="hljs-comment"># xz -d linux-3.10.0-1062.el7.tar.xz</span><br>[root@lab102 kernel]<span class="hljs-comment"># tar -xvf linux-3.10.0-1062.el7.tar</span><br>[root@lab102 kernel]<span class="hljs-comment"># cp -ra linux-3.10.0-1062.el7/ linux-3.10.0-1062.el7-patch</span><br></code></pre></td></tr></table></figure><p>我们现在就有两份源码了<br>我们默认使用的是前面那套内核里面的代码，后面的是准备修改的代码</p><p>修改代码</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab102 kernel]<span class="hljs-comment"># vim linux-3.10.0-1062.el7-patch/drivers/block/rbd.c </span><br></code></pre></td></tr></table></figure><p>为了方便查看我们修改rbd map的函数</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">pr_info(<span class="hljs-string">&quot;%s: capacity %llu features 0x%llx\n&quot;</span>, rbd_dev-&gt;disk-&gt;disk_name,<br>        (unsigned long long)get_capacity(rbd_dev-&gt;disk) &lt;&lt; <span class="hljs-string">SECTOR_SHIFT,</span><br><span class="hljs-string">        rbd_dev-&gt;header.features);</span><br><span class="hljs-string">rc = count;</span><br></code></pre></td></tr></table></figure><p>改成</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">pr_info(<span class="hljs-string">&quot;%s: capacity 我改这里的显示了  %llu features 0x%llx\n&quot;</span>, rbd_dev-&gt;disk-&gt;disk_name,<br>         (unsigned long long)get_capacity(rbd_dev-&gt;disk) &lt;&lt; <span class="hljs-string">SECTOR_SHIFT,</span><br><span class="hljs-string">         rbd_dev-&gt;header.features);</span><br><span class="hljs-string"> rc = count;</span><br><span class="hljs-string"></span><br></code></pre></td></tr></table></figure><p>获取差异文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab102 kernel]<span class="hljs-comment">#  diff -u linux-3.10.0-1062.el7/drivers/block/rbd.c linux-3.10.0-1062.el7-patch/drivers/block/rbd.c &gt; rbd.patch</span><br></code></pre></td></tr></table></figure><p>得到的文件如下</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab102 kernel]<span class="hljs-comment"># cat rbd.patch </span><br>--- linux-3.10.0-1062.el7/drivers/block/rbd.c2019-07-19 03:58:03.000000000 +0800<br>+++ linux-3.10.0-1062.el7-patch/drivers/block/rbd.c2020-11-19 15:22:21.653239816 +0800<br>@@ -6299,7 +6299,7 @@<br> list_add_tail(&amp;rbd_dev-&gt;node, &amp;rbd_dev_list);<br> spin_unlock(&amp;rbd_dev_list_lock);<br> <br>-pr_info(<span class="hljs-string">&quot;%s: capacity %llu features 0x%llx\n&quot;</span>, rbd_dev-&gt;disk-&gt;disk_name,<br>+pr_info(<span class="hljs-string">&quot;%s: capacity 我改这里的显示了  %llu features 0x%llx\n&quot;</span>, rbd_dev-&gt;disk-&gt;disk_name,<br> (unsigned long long)get_capacity(rbd_dev-&gt;disk) &lt;&lt; <span class="hljs-string">SECTOR_SHIFT,</span><br><span class="hljs-string"> rbd_dev-&gt;header.features);</span><br><span class="hljs-string"> rc = count;</span><br><span class="hljs-string"></span><br></code></pre></td></tr></table></figure><p>我们需要根据这个</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab102 kernel]<span class="hljs-comment"># /usr/local/bin/kpatch-build rbd.patch  --skip-gcc-check --skip-cleanup  -r kernel-3.10.0-1062.el7.src.rpm</span><br>WARNING: Skipping gcc version matching check (not recommended)<br>Skipping cleanup<br>Fedora/Red Hat distribution detected<br>Downloading kernel <span class="hljs-built_in">source</span> <span class="hljs-keyword">for</span> 3.10.0-1062.el7.x86_64<br>Unpacking kernel <span class="hljs-built_in">source</span><br>Testing patch file(s)<br>Reading special section data<br>Building original <span class="hljs-built_in">source</span><br>Building patched <span class="hljs-built_in">source</span><br>Extracting new and modified ELF sections<br>rbd.o: changed <span class="hljs-keyword">function</span>: do_rbd_add.isra.45<br>Patched objects: drivers/block/rbd.ko<br>Building patch module: livepatch-rbd.ko<br>SUCCESS<br></code></pre></td></tr></table></figure><p>可以从提示上面看到一些信息<br>修改是drivers&#x2F;block&#x2F;rbd.ko模块，改了do_rbd_add这个函数，生成得是livepatch-rbd.ko这个ko文件</p><p>我们把这个ko文件拷贝到相同内核的，需要更新的机器</p><p>先做map的操作，检查打补丁前的输出</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 patch]<span class="hljs-comment"># rbd map testrbd</span><br>/dev/rbd0<br>[root@lab101 patch]<span class="hljs-comment"># dmesg </span><br>[3303179.423310] libceph: mon0 192.168.19.101:6789 session established<br>[3303179.423726] libceph: client20564 fsid beeb1bd5-54ed-40b6-897f-f31f43a517e6<br>[3303179.429378] rbd: rbd0: capacity 53687091200 features 0x1<br><br>[root@lab101 patch]<span class="hljs-comment"># kpatch list</span><br>Loaded patch modules:<br>Installed patch modules:<br></code></pre></td></tr></table></figure><p>可以看到没有打过补丁</p><p>加载补丁</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 patch]<span class="hljs-comment"># kpatch load livepatch-rbd.ko </span><br>loading patch module: livepatch-rbd.ko<br>waiting (up to 15 seconds) <span class="hljs-keyword">for</span> patch transition to complete...<br>transition complete (3 seconds)<br></code></pre></td></tr></table></figure><p>尝试map</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 patch]<span class="hljs-comment"># rbd map testrbda</span><br>/dev/rbd1<br>[root@lab101 patch]<span class="hljs-comment"># dmesg </span><br>[3303179.423310] libceph: mon0 192.168.19.101:6789 session established<br>[3303179.423726] libceph: client20564 fsid beeb1bd5-54ed-40b6-897f-f31f43a517e6<br>[3303179.429378] rbd: rbd0: capacity 53687091200 features 0x1<br>[3303262.296794] livepatch: enabling patch <span class="hljs-string">&#x27;livepatch_rbd&#x27;</span><br>[3303262.307782] livepatch: <span class="hljs-string">&#x27;livepatch_rbd&#x27;</span>: starting patching transition<br>[3303264.938241] livepatch: <span class="hljs-string">&#x27;livepatch_rbd&#x27;</span>: patching complete<br>[3303291.798301] rbd: rbd1: capacity 我改这里的显示了  53687091200 features 0x1<br>[root@lab101 patch]<span class="hljs-comment"># kpatch list</span><br>Loaded patch modules:<br>livepatch_rbd [enabled]<br><br>Installed patch modules:<br></code></pre></td></tr></table></figure><p>可以看到上面的操作过程中我并没有去rmmod rbd 或者重新modprobe rbd，内核模块就已经更新了</p><p>上面的是加载了补丁，如果需要安装补丁是需要执行</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 patch]<span class="hljs-comment"># kpatch install livepatch-rbd.ko </span><br>installing livepatch-rbd.ko (3.10.0-1062.el7.x86_64)<br>Created symlink from /etc/systemd/system/multi-user.target.wants/kpatch.service to /usr/local/lib/systemd/system/kpatch.service.<br></code></pre></td></tr></table></figure><p>实际上上面的操作是把patch的ko拷贝到了路径</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">/var/lib/kpatch/3.10.0-1062.el7.x86_64/livepatch-rbd.ko<br></code></pre></td></tr></table></figure><p>install的操作就是启动的时候把这个加载进去<br>如果觉得不满意，补丁是支持回退的</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 patch]<span class="hljs-comment"># kpatch unload livepatch-rbd</span><br>disabling patch module: livepatch_rbd<br>waiting (up to 15 seconds) <span class="hljs-keyword">for</span> patch transition to complete...<br>transition complete (3 seconds)<br>unloading patch module: livepatch_rbd<br></code></pre></td></tr></table></figure><p>基于以上就完成了rbd的一次热更新的过程了，通常来说模块的更新并不需要重启机器，但是如果模块提供的服务上面加载了其它服务，服务又被客户端连接的话，这个更新步骤就比较麻烦了，如果能够热更新，能够省很多事情，当然内核模块的更新要测试验证没有问题再去动，否则很容易把机器搞死机了</p><h2 id="自有内核模块的热更新"><a href="#自有内核模块的热更新" class="headerlink" title="自有内核模块的热更新"></a>自有内核模块的热更新</h2><p>如果内核模块是自己改过的，或者并不是内核树里面的，需要打补丁，可以用下面的命令处理</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">/usr/local/bin/kpatch-build -s ./block/ -t all -e ./block/rbd.ko block-rbd.patch  --skip-gcc-check<br></code></pre></td></tr></table></figure><p>上面的block为源码的目录，需要准备好Makefile的，后面的-e后面接的是之前版本编译出来的内核模块，block-rbd.patch 就是源码的差异，然后编译出来的就是patch模块<br>这个地方内核的版本就再block的里面的Makefile里面去控制了</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab102 kernel]<span class="hljs-comment"># /usr/local/bin/kpatch-build -s ./block/ -t all -e ./block/rbd.ko block-rbd.patch  --skip-gcc-check</span><br>WARNING: Skipping gcc version matching check (not recommended)<br>Using <span class="hljs-built_in">source</span> directory at /root/kernel/block<br>Testing patch file(s)<br>Reading special section data<br>Building original <span class="hljs-built_in">source</span><br>Building patched <span class="hljs-built_in">source</span><br>Extracting new and modified ELF sections<br>rbd.o: changed <span class="hljs-keyword">function</span>: do_rbd_add.isra.45<br>Patched objects: rbd.ko<br>Building patch module: livepatch-block-rbd.ko<br>SUCCESS<br></code></pre></td></tr></table></figure><p>这个方式的编译就快很多了，如果是更新内核自带的模块，用上面的整个编译的，如果是自己改过的，就可以用后面的方式去实现了</p>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>rgw使用boto3生成可以访问的预签名url</title>
    <link href="/2020/11/03/rgw%E4%BD%BF%E7%94%A8boto3%E7%94%9F%E6%88%90%E5%8F%AF%E4%BB%A5%E8%AE%BF%E9%97%AE%E7%9A%84%E9%A2%84%E7%AD%BE%E5%90%8Durl/"/>
    <url>/2020/11/03/rgw%E4%BD%BF%E7%94%A8boto3%E7%94%9F%E6%88%90%E5%8F%AF%E4%BB%A5%E8%AE%BF%E9%97%AE%E7%9A%84%E9%A2%84%E7%AD%BE%E5%90%8Durl/</url>
    
    <content type="html"><![CDATA[<p>##前言<br>如果想访问一个ceph里面的s3地址，但是又不想直接提供secrect key的时候，可以通过预签名的方式生成url</p><h2 id="生成方法"><a href="#生成方法" class="headerlink" title="生成方法"></a>生成方法</h2><p>下载boto3</p><p>脚本如下</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">cat</span> s3.py<br><br>import boto3<br>from botocore.client import Config<br><br>s3_host=<span class="hljs-string">&quot;http://192.168.0.201:7481&quot;</span><br><span class="hljs-comment"># Get the service client with sigv4 configured</span><br>s3 = boto3.client(<span class="hljs-string">&#x27;s3&#x27;</span>, aws_access_key_id = <span class="hljs-string">&quot;test&quot;</span>,aws_secret_access_key = <span class="hljs-string">&quot;test&quot;</span>,endpoint_url = s3_host, config=Config(signature_version=<span class="hljs-string">&#x27;s3v4&#x27;</span>))<br><br><span class="hljs-comment"># Generate the URL to get &#x27;key-name&#x27; from &#x27;bucket-name&#x27;</span><br><span class="hljs-comment"># URL expires in 604800 seconds (seven days)</span><br>url = s3.generate_presigned_url(<br>    ClientMethod=<span class="hljs-string">&#x27;get_object&#x27;</span>,<br>    Params=&#123;<br>        <span class="hljs-string">&#x27;Bucket&#x27;</span>: <span class="hljs-string">&#x27;movie&#x27;</span>,<br>        <span class="hljs-string">&#x27;Key&#x27;</span>: <span class="hljs-string">&#x27;20200919211055.ps&#x27;</span><br>    &#125;,<br>    ExpiresIn=604800<br>)<br><br><span class="hljs-built_in">print</span>(url)<br></code></pre></td></tr></table></figure><p>执行以后</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab201 s3]<span class="hljs-comment"># python s3.py </span><br>http://192.168.0.201:7481/movie/20200919211055.ps?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Expires=604800&amp;X-Amz-Credential=<span class="hljs-built_in">test</span>%2F20201103%2Fus-east-1%2Fs3%2Faws4_request&amp;X-Amz-SignedHeaders=host&amp;X-Amz-Date=20201103T062621Z&amp;X-Amz-Signature=b008a40341800b1ab7f701490e56b0597a05fe2e5c796d063c798e20564ef4a2<br></code></pre></td></tr></table></figure><p>生成上面的地址就可以直接下载需要权限才能访问的文件了</p><p>上面的签名的是V4的，如果想生成V2的修改配置文件如下</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">config=Config(signature_version=<span class="hljs-string">&#x27;s3&#x27;</span>)<br></code></pre></td></tr></table></figure><p>再次执行：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">http://192.168.0.201:7481/movie/20200919211055.ps?AWSAccessKeyId=<span class="hljs-built_in">test</span>&amp;Expires=1604989885&amp;Signature=mpGtOaHn0NHLIY5PXqqD6TJrDSE%3D<br></code></pre></td></tr></table></figure><p>生成上面的形式的url</p>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>ceph的df容量显示计算</title>
    <link href="/2020/10/28/ceph%E7%9A%84df%E5%AE%B9%E9%87%8F%E6%98%BE%E7%A4%BA%E8%AE%A1%E7%AE%97/"/>
    <url>/2020/10/28/ceph%E7%9A%84df%E5%AE%B9%E9%87%8F%E6%98%BE%E7%A4%BA%E8%AE%A1%E7%AE%97/</url>
    
    <content type="html"><![CDATA[<h2 id="显示数据"><a href="#显示数据" class="headerlink" title="显示数据"></a>显示数据</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab201 ~]<span class="hljs-comment"># ceph df</span><br>GLOBAL:<br>    SIZE       AVAIL    RAW USED     %RAW USED <br>    1092T     404T       688T         63.01%<br>POOLS:<br> NAME                 ID     USED        %USED    MAX AVAIL        OBJECTS <br> rbd                  0       17483G      15.88    92633G          4616231<br>data                  1       212T        70.09    92633G          246574395<br>metadata              2      88677K        0        1400G          407249 <br></code></pre></td></tr></table></figure><p>总used的63% ，data的总used为70%</p><p>先说下容量的计算规则</p><ul><li>1、根据每个osd计算可用容量：osd可用的容量&#x2F;osd的crush权重占存储池所有osd的总crush的权重的百分比</li><li>2、然后用上面计算得到的最小的可用容量&#x2F;pool_size(存储池副本数)，还要乘以mon_osd_full_ratio（默认0.95），得到的就是存储池的max avail</li></ul><h3 id="举个例子"><a href="#举个例子" class="headerlink" title="举个例子"></a>举个例子</h3><p>2个osd 每个为4T，crush weight 为4，osd.1可用容量为1.5T，osd.2可用容量为1T</p><p>那么我们来根据上面的公式来计算<br>osd.1计算1.5T&#x2F;0.5&#x3D;3T<br>osd.2计算1T&#x2F;0.5&#x3D;2T<br>取最小值2T<br>副本为2的话，那么可用容量就是2T&#x2F;2为1T</p><p>为什么要这么计算，实际上这个地方计算的时候，分母是占用的百分比，分子是当前剩余的空间，因为总容量是由最短板的osd决定的，这个跟水桶灌水一样</p><p>分母相当于自己需要承担的容量，所以，分子越小的然后，分母越大，算的就会越小，最终最小的那个决定了最终容量</p><p>上面的如果没动过crush weight，那么简单的计算方式就是</p><p>剩余最小的那个osd的可用容量X总的osd个数就是存储池的可用容量</p><p>上面的例子的模拟计算</p><p>总容量，总的剩余容量，总的使用的容量<br>8T              2.5T              5.5T                        已使用百分比  68.75%<br>按存储池计算<br>rbd    已用2.75T              可用1T               已使用百分比    73%</p>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>ceph bluestore的db分区应该预留多大的空间</title>
    <link href="/2020/10/26/ceph%20bluestore%E7%9A%84db%E5%88%86%E5%8C%BA%E5%BA%94%E8%AF%A5%E9%A2%84%E7%95%99%E5%A4%9A%E5%A4%A7%E7%9A%84%E7%A9%BA%E9%97%B4/"/>
    <url>/2020/10/26/ceph%20bluestore%E7%9A%84db%E5%88%86%E5%8C%BA%E5%BA%94%E8%AF%A5%E9%A2%84%E7%95%99%E5%A4%9A%E5%A4%A7%E7%9A%84%E7%A9%BA%E9%97%B4/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>关于bluestore的db应该预留多少空间，网上有很多资料<br>如果采用默认的</p><blockquote><p>write_buffer_size&#x3D;268435456</p></blockquote><p>大小的话<br>那么几个rocksdb的数据等级是</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">L0: <span class="hljs-keyword">in</span> memory<br>L1: 256MB<br>L2: 2.56 GB<br>L3: 25.6 GB<br>L4: 256 GB<br></code></pre></td></tr></table></figure><p>设置L4那么大的ssd可以给一个osd使用有点不划算，那么空间一般计算就是L1+L2+L3将近30GB<br>这个可以参考下面的文章</p><blockquote><p><a href="https://blog.csdn.net/NewTyun/article/details/103379694">https://blog.csdn.net/NewTyun/article/details/103379694</a></p></blockquote><p>关于block.db大小调整,只需为所有Bluestore OSD保留30 GB</p><p>那么这个大小对不对，如果你直接参考30GB这个，并且按照常规的去分区来说，就会带来问题了，我们看下具体什么问题</p><h2 id="实际测试验证"><a href="#实际测试验证" class="headerlink" title="实际测试验证"></a>实际测试验证</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">parted -s /dev/sdb mkpart primaru 1 31G<br></code></pre></td></tr></table></figure><p>上面的命令已经放大了1GB了，但是实际上还是不行</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab102 ~]<span class="hljs-comment"># ceph daemon osd.0 perf dump|grep bluefs -A 10</span><br>    <span class="hljs-string">&quot;bluefs&quot;</span>: &#123;<br>        <span class="hljs-string">&quot;gift_bytes&quot;</span>: 0,<br>        <span class="hljs-string">&quot;reclaim_bytes&quot;</span>: 0,<br>        <span class="hljs-string">&quot;db_total_bytes&quot;</span>: 30999044096,<br>        <span class="hljs-string">&quot;db_used_bytes&quot;</span>: 3258966016,<br>        <span class="hljs-string">&quot;wal_total_bytes&quot;</span>: 1999630336,<br>        <span class="hljs-string">&quot;wal_used_bytes&quot;</span>: 501215232,<br>        <span class="hljs-string">&quot;slow_total_bytes&quot;</span>: 160000114688,<br>        <span class="hljs-string">&quot;slow_used_bytes&quot;</span>: 7837319168,<br>        <span class="hljs-string">&quot;num_files&quot;</span>: 194,<br>        <span class="hljs-string">&quot;log_bytes&quot;</span>: 10485760,<br></code></pre></td></tr></table></figure><p>上面是我测试环境记录的值，db只使用了3.2G实际上已经开始使用slow 了,所以这个大小实际上不满足的我的预设的，这个跟parted命令分区的GB转换也存在的一定的关系</p><p>看下parted的问题</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab102 ~]<span class="hljs-comment"># parted -s  /dev/sdf mkpart primary 1 1GB</span><br>[root@lab102 ~]<span class="hljs-comment"># parted -s  /dev/sdf print</span><br>Model: Intel RMS25CB080 (scsi)<br>Disk /dev/sdf: 4000GB<br>Sector size (logical/physical): 512B/4096B<br>Partition Table: gpt<br>Disk Flags: <br><br>Number  Start   End     Size   File system  Name     Flags<br> 1      1049kB  1000MB  999MB               primary<br></code></pre></td></tr></table></figure><p>可以看到上面创建1GB的时候实际上只创建了999MB，加上我指定的从1MB开始，实际上这个地方设置是按1000进制处理容量的，而对容量的需求的是真正的1024的去算的，这个地方就存在误差了</p><p>那么我们简单点处理，就是直接放大到35GB即可</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">parted -s  /dev/sdf mkpart primary 1 35GB<br></code></pre></td></tr></table></figure><p>按这个容量设置的，能够保证上面的L3没有先满的时候不会提前溢出了</p><p>红帽的官方的建议是留1T 40GB左右，而suse是建议db大小为64GB</p><blockquote><p><a href="https://documentation.suse.com/zh-tw/ses/6/single-html/ses-deployment/index.html#:~:text=%E5%A6%82%E9%9C%80BlueStore%20%E7%9A%84%E8%A9%B3%E7%B4%B0,%E4%BD%BF%E7%94%A8%E5%96%AE%E7%8D%A8%E7%9A%84%E5%88%86%E5%89%B2%E5%8D%80%E3%80%82">https://documentation.suse.com/zh-tw/ses/6/single-html/ses-deployment/index.html#:~:text=%E5%A6%82%E9%9C%80BlueStore%20%E7%9A%84%E8%A9%B3%E7%B4%B0,%E4%BD%BF%E7%94%A8%E5%96%AE%E7%8D%A8%E7%9A%84%E5%88%86%E5%89%B2%E5%8D%80%E3%80%82</a><br><a href="https://access.redhat.com/documentation/en-us/red_hat_ceph_storage/4/html/administration_guide/osd-bluestore">https://access.redhat.com/documentation/en-us/red_hat_ceph_storage/4/html/administration_guide/osd-bluestore</a></p></blockquote><p>如果没有调整write_buffer_size的情况下，建议是35GB，40GB或者64GB，这个都存在一些放大设置，如果磁盘空间足够的情况下，多分一点也没什么关系的，尽量避免转换不正确带来的未知的降速</p><p>WAL大小，suse建议是4GB的</p><h2 id="测试模型构建"><a href="#测试模型构建" class="headerlink" title="测试模型构建"></a>测试模型构建</h2><p>准备一个4TB的sata盘，准备一个db分区，准备一个wal分区(测试环境为2GB)<br>db分区设置为你需要的大小，上面的环境当中，我测试了db 30GB和35GB两组大小的情况</p><p>设置35GB写入600万文件的时候osd的db情况如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs bash">ceph daemon osd.0 perf dump|grep bluefs -A 10<br>    <span class="hljs-string">&quot;bluefs&quot;</span>: &#123;<br>        <span class="hljs-string">&quot;gift_bytes&quot;</span>: 0,<br>        <span class="hljs-string">&quot;reclaim_bytes&quot;</span>: 0,<br>        <span class="hljs-string">&quot;db_total_bytes&quot;</span>: 34999361536,<br>        <span class="hljs-string">&quot;db_used_bytes&quot;</span>: 10392428544,<br>        <span class="hljs-string">&quot;wal_total_bytes&quot;</span>: 1999630336,<br>        <span class="hljs-string">&quot;wal_used_bytes&quot;</span>: 492826624,<br>        <span class="hljs-string">&quot;slow_total_bytes&quot;</span>: 160000114688,<br>        <span class="hljs-string">&quot;slow_used_bytes&quot;</span>: 0,<br>        <span class="hljs-string">&quot;num_files&quot;</span>: 177,<br>        <span class="hljs-string">&quot;log_bytes&quot;</span>: 3944448,<br></code></pre></td></tr></table></figure><p>创建osd的命令</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">ceph-deploy osd create --data /dev/sdc1 --block-db /dev/sdb1  --block-wal /dev/sdb2 lab102<br></code></pre></td></tr></table></figure><p>创建一个rgw网关<br>然后用cosbench往网关打数据<br>200个worker，64KB的文件，写入600万文件</p><p>测试一轮的时间大概为2小时就可以复现上面的情况，测试过程还带出了另外的一个问题</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">rgw_dynamic_resharding = <span class="hljs-literal">true</span><br></code></pre></td></tr></table></figure><p>这个动态分片过程中会有一定的概率阻塞住请求的，通过cosbench里面的压测图形也可以看到分片后的性能比没分片是好很多的，所以如果抢时间的话</p><blockquote><p>最好是关闭动态分片，设置好需要的分片数目</p></blockquote><p>测试完需要改db的时候，直接删存储池，然后重新创建即可，推掉的操作也很快的</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>网上的文章都是用来参考的，实际是一定需要去复测验证的，一般分享的文章也不会细化到一个parted的命令也记录，只会从原理上面出发去分析，并且环境调整了什么参数，都是不同的结果的，比如上面的<br>write_buffer_size如果调整到512MB，那么预留的空间差不多需要翻一倍的</p><p>所以参数的调整，一定要实测</p>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>ceph扩展bluestore的db分区</title>
    <link href="/2020/10/26/ceph%E6%89%A9%E5%B1%95bluestore%E7%9A%84db%E5%88%86%E5%8C%BA/"/>
    <url>/2020/10/26/ceph%E6%89%A9%E5%B1%95bluestore%E7%9A%84db%E5%88%86%E5%8C%BA/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>在ceph 14版本里面才加入了bluefs-bdev-migrate，分区迁移相关的命令，那么在12版本里面其实也是可以扩展分区的<br>测试的版本</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab102 ceph-0]<span class="hljs-comment"># ceph -v</span><br>ceph version 12.2.13 (584a20eb0237c657dc0567da126be145106aa47e) luminous (stable)<br></code></pre></td></tr></table></figure><h2 id="操作方法"><a href="#操作方法" class="headerlink" title="操作方法"></a>操作方法</h2><p>如果db分区之前有做独立独立分区，但是发现db的大小设置小了，想把这个db调大，或者是从ssd设备迁移到nvme的分区，那么可以通过dd命令来实现的</p><h3 id="停止osd"><a href="#停止osd" class="headerlink" title="停止osd"></a>停止osd</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">systemctl stop ceph-osd@0<br></code></pre></td></tr></table></figure><h3 id="替换分区"><a href="#替换分区" class="headerlink" title="替换分区"></a>替换分区</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">dd</span> <span class="hljs-keyword">if</span>=/dev/sdb1 of=/dev/sda1 bs=64M<br><span class="hljs-built_in">ln</span> -s /dev/sda1 /var/lib/ceph/osd/ceph-0/block.db<br><span class="hljs-built_in">chown</span> ceph:ceph  -R /var/lib/ceph/osd/ceph-0/*<br></code></pre></td></tr></table></figure><p>把原来的db分区整体复制一份，注意后面的分区要大于或者等于&#x2F;dev&#x2F;sdb1这个原始的db分区，加上后面的bs，复制的速度会快很多</p><p>扩展db分区的大小，如果不使用扩展命令，显示的就是跟原始的分区大小是一致的</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab102 ceph-0]<span class="hljs-comment"># ceph-bluestore-tool  bluefs-bdev-expand --path /var/lib/ceph/osd/ceph-0/</span><br>infering bluefs devices from bluestore path<br> slot 0 /var/lib/ceph/osd/ceph-0//block.wal<br> slot 1 /var/lib/ceph/osd/ceph-0//block.db<br> slot 2 /var/lib/ceph/osd/ceph-0//block<br>0 : size 0x77300000 : own 0x[1000~772ff000]<br>1 : size 0x950200000 : own 0x[2000~737afe000]<br>2 : size 0x3a352400000 : own 0x[1bf08c00000~2540c00000]<br>Expanding...<br>0 : no changes detected. Bypassed.<br>1 : expanding  from 0x737b00000 to 0x950200000<br>1 : size label updated to 39998980096<br>2 : unable to <span class="hljs-built_in">expand</span>. Bypassed.<br></code></pre></td></tr></table></figure><p>检测是否使用了慢分区和检测db分区大小的命令</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab102 ~]<span class="hljs-comment"># ceph daemon osd.0 perf dump|grep bluefs -A 10</span><br>    <span class="hljs-string">&quot;bluefs&quot;</span>: &#123;<br>        <span class="hljs-string">&quot;gift_bytes&quot;</span>: 0,<br>        <span class="hljs-string">&quot;reclaim_bytes&quot;</span>: 0,<br>        <span class="hljs-string">&quot;db_total_bytes&quot;</span>: 39998971904,<br>        <span class="hljs-string">&quot;db_used_bytes&quot;</span>: 2918178816,<br>        <span class="hljs-string">&quot;wal_total_bytes&quot;</span>: 1999630336,<br>        <span class="hljs-string">&quot;wal_used_bytes&quot;</span>: 223342592,<br>        <span class="hljs-string">&quot;slow_total_bytes&quot;</span>: 160000114688,<br>        <span class="hljs-string">&quot;slow_used_bytes&quot;</span>: 7261519872,<br>        <span class="hljs-string">&quot;num_files&quot;</span>: 181,<br>        <span class="hljs-string">&quot;log_bytes&quot;</span>: 178040832,<br></code></pre></td></tr></table></figure><p>db_total_bytes是分区的大小<br>slow_used_bytes是使用了慢分区的大小</p><p>做下压缩</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">ceph tell osd.0 compact<br></code></pre></td></tr></table></figure><p>再次检查</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs bash">ceph daemon osd.0 perf dump|grep bluefs -A 10<br>    <span class="hljs-string">&quot;bluefs&quot;</span>: &#123;<br>        <span class="hljs-string">&quot;gift_bytes&quot;</span>: 0,<br>        <span class="hljs-string">&quot;reclaim_bytes&quot;</span>: 0,<br>        <span class="hljs-string">&quot;db_total_bytes&quot;</span>: 39998971904,<br>        <span class="hljs-string">&quot;db_used_bytes&quot;</span>: 10199490560,<br>        <span class="hljs-string">&quot;wal_total_bytes&quot;</span>: 1999630336,<br>        <span class="hljs-string">&quot;wal_used_bytes&quot;</span>: 82833408,<br>        <span class="hljs-string">&quot;slow_total_bytes&quot;</span>: 160000114688,<br>        <span class="hljs-string">&quot;slow_used_bytes&quot;</span>: 0,<br>        <span class="hljs-string">&quot;num_files&quot;</span>: 160,<br>        <span class="hljs-string">&quot;log_bytes&quot;</span>: 25575424,<br></code></pre></td></tr></table></figure><p>可以看到上面的慢分区的数据都刷到新替换的快分区里面去了</p><p>###注意事项<br>替换的时候一定操作谨慎，一个个替换，确认数据没有问题再做相关的处理，比较安全的方式是下线osd，再新创建osd</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>分区替换可以换一个大的盘<br>替换后需要compact才会把slow的数据刷掉</p>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>重构克隆rbd的数据</title>
    <link href="/2020/10/12/%E9%87%8D%E6%9E%84%E5%85%8B%E9%9A%86rbd%E7%9A%84%E6%95%B0%E6%8D%AE/"/>
    <url>/2020/10/12/%E9%87%8D%E6%9E%84%E5%85%8B%E9%9A%86rbd%E7%9A%84%E6%95%B0%E6%8D%AE/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>之前写过一篇重构rbd的元数据的文章，讲的是如果rbd的元数据丢失了，数据还在的时候怎么恢复相关的元数据，但是实际使用的场景是，集群可能崩溃了，数据还在，osd无法拉起来，数据又很重要，需要把数据拼接回来，这个也是最底层的一种数据还原方式了</p><p>网上有非快照的rbd的拼接的文章，对于快照使用方式的拼接这个没有太多信息，而实际上很多使用场景就是克隆了一个原始系统后，再使用的，本篇就是把还原需要的细节都写出来了</p><h2 id="重构的步骤"><a href="#重构的步骤" class="headerlink" title="重构的步骤"></a>重构的步骤</h2><h3 id="获取基本的信息"><a href="#获取基本的信息" class="headerlink" title="获取基本的信息"></a>获取基本的信息</h3><ul><li>1、找到rbd_directory，通过这个找到整个环境里面的rbd的名称和prefix的对应关系</li><li>2、根据rbd_header的元数据信息找到rbd下面的信息</li><li>rbd的大小</li><li>rbd的块大小</li><li>rbd是否有parent（判断这个是通过哪个快照创建的）</li><li>3、rbd是否做了快照（做了快照有两种可能，本身的快照或者本身做的快照被克隆了）,不是快照的image的对象，直接取head的对象进行拼接即可，通过快照克隆的对象，需要判断每个对象的状态来进行拼接（后面讲）</li></ul><p>正常情况</p><p>正常的情况就是通过命令能够获取到上面的信息1，2的信息，这个的前提是，相关的对象所在的osd能够启动，因为数据存储在omap里面的，而无法启动的时候，就只能去底层读取了，这里先讲正常读取的情况，这个正常读取的情况其实就是几个命令就能获取的</p><p>异常情况<br>异常情况就是无法启动osd了，需要去底层读取了，下面会介绍正常和异常的两种方式的读取</p><p>通过命令获取关联信息</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab102 opt]<span class="hljs-comment"># rados  -p rbd listomapvals rbd_directory</span><br>id_5f7a6b8b4567<br>value (11 bytes) :<br>00000000  07 00 00 00 74 65 73 74  72 62 64                 |....testrbd|<br>0000000b<br><br>id_5f856b8b4567<br>value (10 bytes) :<br>00000000  06 00 00 00 6e 65 77 72  62 64                    |....newrbd|<br>0000000a<br><br>name_newrbd<br>value (16 bytes) :<br>00000000  0c 00 00 00 35 66 38 35  36 62 38 62 34 35 36 37  |....5f856b8b4567|<br>00000010<br><br>name_testrbd<br>value (16 bytes) :<br>00000000  0c 00 00 00 35 66 37 61  36 62 38 62 34 35 36 37  |....5f7a6b8b4567|<br>00000010<br></code></pre></td></tr></table></figure><p>上面可以找到rbd的名称和prefix的对应关系，无法通过命令找到的时候，我们通过底层rocksdb查找</p><p>通过底层命令获取信息<br>这个需要在rbd_directory对象所在的osd里面执行，查询的时候需要先停止osd</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab102 opt]<span class="hljs-comment"># ceph-kvstore-tool rocksdb /var/lib/ceph/osd/ceph-0/current/omap/ list|grep id</span><br>_USER_0000000000000065_USER_:id_5f7a6b8b4567<br>_USER_0000000000000065_USER_:id_5f856b8b4567<br><br>[root@lab102 opt]<span class="hljs-comment"># ceph-kvstore-tool rocksdb /var/lib/ceph/osd/ceph-0/current/omap/ list|grep name</span><br>_USER_0000000000000065_USER_:name_newrbd<br>_USER_0000000000000065_USER_:name_testrbd<br><br>[root@lab102 opt]<span class="hljs-comment"># ceph-kvstore-tool rocksdb /var/lib/ceph/osd/ceph-0/current/omap/ get _USER_0000000000000065_USER_ name_newrbd</span><br>(_USER_0000000000000065_USER_, name_newrbd)<br>00000000  0c 00 00 00 35 66 38 35  36 62 38 62 34 35 36 37  |....5f856b8b4567|<br>00000010<br>[root@lab102 opt]<span class="hljs-comment"># ceph-kvstore-tool rocksdb /var/lib/ceph/osd/ceph-0/current/omap/ get _USER_0000000000000065_USER_ name_testrbd</span><br>00000000  0c 00 00 00 35 66 37 61  36 62 38 62 34 35 36 37  |....5f7a6b8b4567|<br>00000010<br></code></pre></td></tr></table></figure><p>可以看到通过底层找到的信息跟上层命令获取的信息是一致的</p><h3 id="查询rbd的元数据信息"><a href="#查询rbd的元数据信息" class="headerlink" title="查询rbd的元数据信息"></a>查询rbd的元数据信息</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab102 opt]<span class="hljs-comment"># rbd info testrbd</span><br>rbd image <span class="hljs-string">&#x27;testrbd&#x27;</span>:<br>size 16384 kB <span class="hljs-keyword">in</span> 4 objects<br>order 22 (4096 kB objects)<br>block_name_prefix: rbd_data.5f7a6b8b4567<br>format: 2<br>features: layering<br>flags: <br>[root@lab102 opt]<span class="hljs-comment"># rbd info newrbd</span><br>rbd image <span class="hljs-string">&#x27;newrbd&#x27;</span>:<br>size 16384 kB <span class="hljs-keyword">in</span> 4 objects<br>order 22 (4096 kB objects)<br>block_name_prefix: rbd_data.5f856b8b4567<br>format: 2<br>features: layering<br>flags: <br>parent: rbd/testrbd@testrbd-write3obj<br>overlap: 16384 kB<br>[root@lab102 opt]<span class="hljs-comment"># rbd snap ls testrbd</span><br>SNAPID NAME                  SIZE <br>    10 testrbd-write3obj 16384 kB <br>    11 writemany         16384 kB <br>    12 writemany1        16384 kB <br>    13 writemany2        16384 kB <br>    14 writemany3        16384 kB <br>    15 writemany4        16384 kB <br>    16 writemany2a       16384 kB <br>    17 writemany2h       16384 kB <br>    18 writemany2l       16384 kB <br></code></pre></td></tr></table></figure><p>通过上面的命令我们可以查询到相关的对应关系，newrbd是通过testrbd的快照进行克隆创建的，并且snapid为10，快照名称为testrbd-write3obj，这个是正常获得的，我们通过底层获取</p><p>上面的信息我们已经获取到了两个rbd的信息，下面就查询两个rbd的信息<br>查询命令</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab102 opt]<span class="hljs-comment"># rados  -p rbd listomapvals rbd_header.5f7a6b8b4567</span><br>features<br>value (8 bytes) :<br>00000000  01 00 00 00 00 00 00 00                           |........|<br>00000008<br><br>object_prefix<br>value (25 bytes) :<br>00000000  15 00 00 00 72 62 64 5f  64 61 74 61 2e 35 66 37  |....rbd_data.5f7|<br>00000010  61 36 62 38 62 34 35 36  37                       |a6b8b4567|<br>00000019<br><br>order<br>value (1 bytes) :<br>00000000  16                                                |.|<br>00000001<br><br>size<br>value (8 bytes) :<br>00000000  00 00 00 01 00 00 00 00                           |........|<br>00000008<br><br>snap_seq<br>value (8 bytes) :<br>00000000  12 00 00 00 00 00 00 00                           |........|<br>00000008<br><br>snapshot_000000000000000a<br>value (94 bytes) :<br>00000000  04 01 58 00 00 00 0a 00  00 00 00 00 00 00 11 00  |..X.............|<br>00000010  00 00 74 65 73 74 72 62  64 2d 77 72 69 74 65 33  |..testrbd-write3|<br>00000020  6f 62 6a 00 00 00 01 00  00 00 00 01 00 00 00 00  |obj.............|<br>00000030  00 00 00 01 01 1c 00 00  00 ff ff ff ff ff ff ff  |................|<br>00000040  ff 00 00 00 00 fe ff ff  ff ff ff ff ff 00 00 00  |................|<br>00000050  00 00 00 00 00 02 00 00  00 00 00 00 00 00        |..............|<br>0000005e<br><br>snapshot_000000000000000b<br>value (86 bytes) :<br>00000000  04 01 50 00 00 00 0b 00  00 00 00 00 00 00 09 00  |..P.............|<br>00000010  00 00 77 72 69 74 65 6d  61 6e 79 00 00 00 01 00  |..writemany.....|<br>00000020  00 00 00 01 00 00 00 00  00 00 00 01 01 1c 00 00  |................|<br>00000030  00 ff ff ff ff ff ff ff  ff 00 00 00 00 fe ff ff  |................|<br>00000040  ff ff ff ff ff 00 00 00  00 00 00 00 00 00 00 00  |................|<br>00000050  00 00 00 00 00 00                                 |......|<br>00000056<br><br>snapshot_000000000000000c<br>value (87 bytes) :<br>00000000  04 01 51 00 00 00 0c 00  00 00 00 00 00 00 0a 00  |..Q.............|<br>00000010  00 00 77 72 69 74 65 6d  61 6e 79 31 00 00 00 01  |..writemany1....|<br>00000020  00 00 00 00 01 00 00 00  00 00 00 00 01 01 1c 00  |................|<br>00000030  00 00 ff ff ff ff ff ff  ff ff 00 00 00 00 fe ff  |................|<br>00000040  ff ff ff ff ff ff 00 00  00 00 00 00 00 00 00 00  |................|<br>00000050  00 00 00 00 00 00 00                              |.......|<br>00000057<br><br>snapshot_000000000000000d<br>value (87 bytes) :<br>00000000  04 01 51 00 00 00 0d 00  00 00 00 00 00 00 0a 00  |..Q.............|<br>00000010  00 00 77 72 69 74 65 6d  61 6e 79 32 00 00 00 01  |..writemany2....|<br>00000020  00 00 00 00 01 00 00 00  00 00 00 00 01 01 1c 00  |................|<br>00000030  00 00 ff ff ff ff ff ff  ff ff 00 00 00 00 fe ff  |................|<br>00000040  ff ff ff ff ff ff 00 00  00 00 00 00 00 00 00 00  |................|<br>00000050  00 00 00 00 00 00 00                              |.......|<br>00000057<br><br>snapshot_000000000000000e<br>value (87 bytes) :<br>00000000  04 01 51 00 00 00 0e 00  00 00 00 00 00 00 0a 00  |..Q.............|<br>00000010  00 00 77 72 69 74 65 6d  61 6e 79 33 00 00 00 01  |..writemany3....|<br>00000020  00 00 00 00 01 00 00 00  00 00 00 00 01 01 1c 00  |................|<br>00000030  00 00 ff ff ff ff ff ff  ff ff 00 00 00 00 fe ff  |................|<br>00000040  ff ff ff ff ff ff 00 00  00 00 00 00 00 00 00 00  |................|<br>00000050  00 00 00 00 00 00 00                              |.......|<br>00000057<br><br>snapshot_000000000000000f<br>value (87 bytes) :<br>00000000  04 01 51 00 00 00 0f 00  00 00 00 00 00 00 0a 00  |..Q.............|<br>00000010  00 00 77 72 69 74 65 6d  61 6e 79 34 00 00 00 01  |..writemany4....|<br>00000020  00 00 00 00 01 00 00 00  00 00 00 00 01 01 1c 00  |................|<br>00000030  00 00 ff ff ff ff ff ff  ff ff 00 00 00 00 fe ff  |................|<br>00000040  ff ff ff ff ff ff 00 00  00 00 00 00 00 00 00 00  |................|<br>00000050  00 00 00 00 00 00 00                              |.......|<br>00000057<br><br>snapshot_0000000000000010<br>value (88 bytes) :<br>00000000  04 01 52 00 00 00 10 00  00 00 00 00 00 00 0b 00  |..R.............|<br>00000010  00 00 77 72 69 74 65 6d  61 6e 79 32 61 00 00 00  |..writemany2a...|<br>00000020  01 00 00 00 00 01 00 00  00 00 00 00 00 01 01 1c  |................|<br>00000030  00 00 00 ff ff ff ff ff  ff ff ff 00 00 00 00 fe  |................|<br>00000040  ff ff ff ff ff ff ff 00  00 00 00 00 00 00 00 00  |................|<br>00000050  00 00 00 00 00 00 00 00                           |........|<br>00000058<br><br>snapshot_0000000000000011<br>value (88 bytes) :<br>00000000  04 01 52 00 00 00 11 00  00 00 00 00 00 00 0b 00  |..R.............|<br>00000010  00 00 77 72 69 74 65 6d  61 6e 79 32 68 00 00 00  |..writemany2h...|<br>00000020  01 00 00 00 00 01 00 00  00 00 00 00 00 01 01 1c  |................|<br>00000030  00 00 00 ff ff ff ff ff  ff ff ff 00 00 00 00 fe  |................|<br>00000040  ff ff ff ff ff ff ff 00  00 00 00 00 00 00 00 00  |................|<br>00000050  00 00 00 00 00 00 00 00                           |........|<br>00000058<br><br>snapshot_0000000000000012<br>value (88 bytes) :<br>00000000  04 01 52 00 00 00 12 00  00 00 00 00 00 00 0b 00  |..R.............|<br>00000010  00 00 77 72 69 74 65 6d  61 6e 79 32 6c 00 00 00  |..writemany2l...|<br>00000020  01 00 00 00 00 01 00 00  00 00 00 00 00 01 01 1c  |................|<br>00000030  00 00 00 ff ff ff ff ff  ff ff ff 00 00 00 00 fe  |................|<br>00000040  ff ff ff ff ff ff ff 00  00 00 00 00 00 00 00 02  |................|<br>00000050  00 00 00 00 00 00 00 00                           |........|<br>00000058<br><br></code></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab102 opt]<span class="hljs-comment"># rados  -p rbd listomapvals rbd_header.5f856b8b4567</span><br>features<br>value (8 bytes) :<br>00000000  01 00 00 00 00 00 00 00                           |........|<br>00000008<br><br>object_prefix<br>value (25 bytes) :<br>00000000  15 00 00 00 72 62 64 5f  64 61 74 61 2e 35 66 38  |....rbd_data.5f8|<br>00000010  35 36 62 38 62 34 35 36  37                       |56b8b4567|<br>00000019<br><br>order<br>value (1 bytes) :<br>00000000  16                                                |.|<br>00000001<br><br>parent<br>value (46 bytes) :<br>00000000  01 01 28 00 00 00 00 00  00 00 00 00 00 00 0c 00  |..(.............|<br>00000010  00 00 35 66 37 61 36 62  38 62 34 35 36 37 0a 00  |..5f7a6b8b4567..|<br>00000020  00 00 00 00 00 00 00 00  00 01 00 00 00 00        |..............|<br>0000002e<br><br>size<br>value (8 bytes) :<br>00000000  00 00 00 01 00 00 00 00                           |........|<br>00000008<br><br>snap_seq<br>value (8 bytes) :<br>00000000  00 00 00 00 00 00 00 00                           |........|<br>00000008<br></code></pre></td></tr></table></figure><p>上面的信息可以看到，查询到的信息是不同的，一个有parent，一个没有，一个有快照，一个没有，通过这个信息可以推断，有parent的img是通过快照创建的，我们解析下这个数据</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs bash">parent<br>value (46 bytes) :<br>00000000  01 01 28 00 00 00 00 00  00 00 00 00 00 00 0c 00  |..(.............|<br>00000010  00 00 35 66 37 61 36 62  38 62 34 35 36 37 0a 00  |..5f7a6b8b4567..|<br>00000020  00 00 00 00 00 00 00 00  00 01 00 00 00 00        |..............|<br>0000002e<br></code></pre></td></tr></table></figure><p>上面是16进制的字符串，右边是这个字符串对应的文本，实际上这个是固定结构的，开头的00 00可以忽略，35 66 37 61 36 62  38 62 34 35 36 37这一段对应的就是后面的5f7a6b8b4567，这个是rbd的prefix，而之前的信息我们知道5f7a6b8b4567就是testrbd的prefix，而后面的0a就是这个快照的编号，这个是16进制的数字，也就是snapid 10编号，0a快照的</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs bash">snapshot_000000000000000a<br>value (94 bytes) :<br>00000000  04 01 58 00 00 00 0a 00  00 00 00 00 00 00 11 00  |..X.............|<br>00000010  00 00 74 65 73 74 72 62  64 2d 77 72 69 74 65 33  |..testrbd-write3|<br>00000020  6f 62 6a 00 00 00 01 00  00 00 00 01 00 00 00 00  |obj.............|<br>00000030  00 00 00 01 01 1c 00 00  00 ff ff ff ff ff ff ff  |................|<br>00000040  ff 00 00 00 00 fe ff ff  ff ff ff ff ff 00 00 00  |................|<br>00000050  00 00 00 00 00 02 00 00  00 00 00 00 00 00        |..............|<br>0000005e<br></code></pre></td></tr></table></figure><p>可以看到跟我们命令查询的信息也是一致的，上面是通过命令查询的信息，我们从底层查询一次</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab102 opt]<span class="hljs-comment"># ceph-kvstore-tool rocksdb /var/lib/ceph/osd/ceph-0/current/omap/  list</span><br>_USER_0000000000000075_USER_:features<br>_USER_0000000000000075_USER_:object_prefix<br>_USER_0000000000000075_USER_:order<br>_USER_0000000000000075_USER_:size<br>_USER_0000000000000075_USER_:snap_seq<br>_USER_0000000000000075_USER_:snapshot_000000000000000a<br>_USER_0000000000000075_USER_:snapshot_000000000000000b<br>_USER_0000000000000075_USER_:snapshot_000000000000000c<br>_USER_0000000000000075_USER_:snapshot_000000000000000d<br>_USER_0000000000000075_USER_:snapshot_000000000000000e<br>_USER_0000000000000075_USER_:snapshot_000000000000000f<br>_USER_0000000000000075_USER_:snapshot_0000000000000010<br>_USER_0000000000000075_USER_:snapshot_0000000000000011<br>_USER_0000000000000075_USER_:snapshot_0000000000000012<br>_USER_0000000000000076_USER_:features<br>_USER_0000000000000076_USER_:object_prefix<br>_USER_0000000000000076_USER_:order<br>_USER_0000000000000076_USER_:parent<br>_USER_0000000000000076_USER_:size<br>_USER_0000000000000076_USER_:snap_seq<br></code></pre></td></tr></table></figure><p>上面省略了一些无用的信息，找到带object_prefix的信息，然后进行其它信息获取</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab102 opt]<span class="hljs-comment"># ceph-kvstore-tool rocksdb /var/lib/ceph/osd/ceph-0/current/omap/  get _USER_0000000000000075_USER_ object_prefix</span><br>(_USER_0000000000000075_USER_, object_prefix)<br>00000000  15 00 00 00 72 62 64 5f  64 61 74 61 2e 35 66 37  |....rbd_data.5f7|<br>00000010  61 36 62 38 62 34 35 36  37                       |a6b8b4567|<br>00000019<br>[root@lab102 opt]<span class="hljs-comment"># ceph-kvstore-tool rocksdb /var/lib/ceph/osd/ceph-0/current/omap/  get _USER_0000000000000076_USER_ parent</span><br>(_USER_0000000000000076_USER_, parent)<br>00000000  01 01 28 00 00 00 00 00  00 00 00 00 00 00 0c 00  |..(.............|<br>00000010  00 00 35 66 37 61 36 62  38 62 34 35 36 37 0a 00  |..5f7a6b8b4567..|<br>00000020  00 00 00 00 00 00 00 00  00 01 00 00 00 00        |..............|<br>0000002e<br></code></pre></td></tr></table></figure><p>可以看到跟上面的信息获取的信息一致的，我们解析下其它几个信息</p><h3 id="块大小的获取"><a href="#块大小的获取" class="headerlink" title="块大小的获取"></a>块大小的获取</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab102 opt]<span class="hljs-comment"># ceph-kvstore-tool rocksdb /var/lib/ceph/osd/ceph-0/current/omap/  get _USER_0000000000000076_USER_ order</span><br>(_USER_0000000000000076_USER_, order)<br>00000000  16                                                |.|<br>00000001<br></code></pre></td></tr></table></figure><p>可以看到上面显示的是16，这个是16进制，实际就是16+6&#x3D;22,对应的就是下表的数值中的4M的</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs bash">order 15 (32768 bytes objects)<br>order 16 (64 kB objects)<br>order 17 (128 kB objects)<br>order 18 (256 kB objects)<br>order 19 (512 kB objects)<br>order 20 (1024 kB objects)<br>order 21 (2048 kB objects)<br>order 22 (4096 kB objects)<br>order 23 (8192 kB objects)<br>order 24 (16384 kB objects)<br>order 25 (32768 kB objects)<br></code></pre></td></tr></table></figure><p>块大小获取到了</p><p>获取rbd的大小</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab102 opt]<span class="hljs-comment"># ceph-kvstore-tool rocksdb /var/lib/ceph/osd/ceph-0/current/omap/  get _USER_0000000000000076_USER_ size</span><br>(_USER_0000000000000076_USER_, size)<br>00000000  00 00 00 01 00 00 00 00                           |........|<br>00000008<br></code></pre></td></tr></table></figure><p>这个大小的也是通过16进制来确定的</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs bash">1B<br>01 00 00 00 00 00 00 00<br>256B<br>00 01 00 00 00 00 00 00<br>64K<br>00 00 01 00 00 00 00 00<br>16M<br>00 00 00 01 00 00 00 00<br>4G<br>00 00 00 00 01 00 00 00<br>1T<br>00 00 00 00 00 01 00 00<br>256T<br>00 00 00 00 00 00 01 00<br></code></pre></td></tr></table></figure><p>每两位可以表示256个数字，初始为1B，也就可以得到上面的数值了<br>查询到的 00 00 00 01 00 00 00 00   按计算为16M</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab102 opt]<span class="hljs-comment"># rbd info newrbd</span><br>rbd image <span class="hljs-string">&#x27;newrbd&#x27;</span>:<br>size 16384 kB <span class="hljs-keyword">in</span> 4 objects<br></code></pre></td></tr></table></figure><p>可以看到是匹配的，如果是其它数值，进行计算即可，注意是16进制的，有了上面的信息，就可以进行下一步了</p><h3 id="快照的写入方式"><a href="#快照的写入方式" class="headerlink" title="快照的写入方式"></a>快照的写入方式</h3><p>做了快照以后，如果没有修改对象数据，那么对象数据是以head结尾的数据形式存在的，如果修改了数据，那么数据就会复制一份并且以快照id的方式存储一份，最新的数据写入到head对象里面</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">./0.38_head/rbd\udata.5f7a6b8b4567.0000000000000001__head_D4B551B8__0<br>./0.38_head/rbd\udata.5f7a6b8b4567.0000000000000001__a_D4B551B8__0<br>./0.38_head/rbd\udata.5f7a6b8b4567.0000000000000001__b_D4B551B8__0<br>./0.38_head/rbd\udata.5f7a6b8b4567.0000000000000001__c_D4B551B8__0<br></code></pre></td></tr></table></figure><p>而克隆以后，对相同索引的对象进行修改的时候，就会生成自己prefix的相同的索引id的对象，如果没有修改就去读取parent里面的对象，这个地方实际我们找对象就有判断顺序了</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">./0.3d_head/rbd\udata.5f856b8b4567.0000000000000000__head_538F72BD__0<br></code></pre></td></tr></table></figure><p>以这个对象为例子</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">如果有5f856b8b4567.0000000000000000__head就读这个<br>如果没有就读取5f7a6b8b4567.0000000000000001__a<br>如果没有5f7a6b8b4567.0000000000000001__a就读取5f7a6b8b4567.0000000000000001_head<br></code></pre></td></tr></table></figure><p>这里第三步实际上存在一个问题来了，如果原始镜像里面，没有修改对象，是有5f7a6b8b4567.0000000000000001_head这个的，如果做快照的时候，没有写过5f7a6b8b4567.0000000000000001_head这个对象，是空的，然后做了快照之后，再写的这个对象，这个时候也不会生成a后缀的对象，那么我们如果这个时候读取的是新写的对象，那么数据实际就是错的了，这个地方需要加一步判断了，一个对象是原始对象，还是后写入的对象，实际上是可以通过对象的扩展属性来判断的，这个查询的方式如下</p><h3 id="判断对象的属性"><a href="#判断对象的属性" class="headerlink" title="判断对象的属性"></a>判断对象的属性</h3><p>获取扩展属性ceph.snapset</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">attr -q -g <span class="hljs-string">&quot;ceph.snapset&quot;</span> ./0.34_head/rbd\\udata.5f7a6b8b4567.0000000000000003__head_2F5129B4__0 &gt; 3.txt<br></code></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab102 current]<span class="hljs-comment"># ceph-dencoder import 3.txt type SnapSet decode dump_json</span><br>&#123;<br>    <span class="hljs-string">&quot;snap_context&quot;</span>: &#123;<br>        <span class="hljs-string">&quot;seq&quot;</span>: 10,<br>        <span class="hljs-string">&quot;snaps&quot;</span>: [<br>            10<br>        ]<br>    &#125;,<br>    <span class="hljs-string">&quot;head_exists&quot;</span>: 1,<br>    <span class="hljs-string">&quot;clones&quot;</span>: []<br>&#125;<br></code></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab102 current]<span class="hljs-comment"># attr -q -g &quot;ceph.snapset&quot; ./0.2a_head/rbd\\udata.5f7a6b8b4567.0000000000000002__head_D519586A__0 &gt; 2.txt</span><br>[root@lab102 current]<span class="hljs-comment"># ceph-dencoder import 2.txt type SnapSet decode dump_json</span><br>&#123;<br>    <span class="hljs-string">&quot;snap_context&quot;</span>: &#123;<br>        <span class="hljs-string">&quot;seq&quot;</span>: 18,<br>        <span class="hljs-string">&quot;snaps&quot;</span>: [<br>            18,<br>            17,<br>            16,<br>            15,<br>            14,<br>            13,<br>            12,<br>            11,<br>            10<br>        ]<br>    &#125;,<br>    <span class="hljs-string">&quot;head_exists&quot;</span>: 1,<br>    <span class="hljs-string">&quot;clones&quot;</span>: [<br>        &#123;<br>            <span class="hljs-string">&quot;snap&quot;</span>: 15,<br>            <span class="hljs-string">&quot;size&quot;</span>: 4194304,<br>            <span class="hljs-string">&quot;overlap&quot;</span>: <span class="hljs-string">&quot;[]&quot;</span><br>        &#125;,<br>        &#123;<br>            <span class="hljs-string">&quot;snap&quot;</span>: 16,<br>            <span class="hljs-string">&quot;size&quot;</span>: 4194304,<br>            <span class="hljs-string">&quot;overlap&quot;</span>: <span class="hljs-string">&quot;[]&quot;</span><br>        &#125;,<br>        &#123;<br>            <span class="hljs-string">&quot;snap&quot;</span>: 17,<br>            <span class="hljs-string">&quot;size&quot;</span>: 4194304,<br>            <span class="hljs-string">&quot;overlap&quot;</span>: <span class="hljs-string">&quot;[]&quot;</span><br>        &#125;,<br>        &#123;<br>            <span class="hljs-string">&quot;snap&quot;</span>: 18,<br>            <span class="hljs-string">&quot;size&quot;</span>: 4194304,<br>            <span class="hljs-string">&quot;overlap&quot;</span>: <span class="hljs-string">&quot;[]&quot;</span><br>        &#125;<br>    ]<br>&#125;<br></code></pre></td></tr></table></figure><p>下面的数据就是clone后写入的数据，也就是打快照后新写入的数据，上面的是新写入的数据，下面是有覆盖写的情况，我们来比较，之前有但是数据没有动，和新写入的数据，这两个情况是怎样的，测试方法如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab102 current]<span class="hljs-comment"># rbd create testrbd --size 16M</span><br>[root@lab102 current]<span class="hljs-comment"># rbd map testrbd</span><br>/dev/rbd0<br>[root@lab102 current]<span class="hljs-comment"># dd if=/dev/urandom of=/dev/rbd/rbd/testrbd bs=4M count=2</span><br>2+0 records <span class="hljs-keyword">in</span><br>2+0 records out<br>8388608 bytes (8.4 MB) copied, 0.111296 s, 75.4 MB/s<br>rbd snap create --image testrbd --snap overwrite<br>rbd snap protect --image testrbd --snap overwrite<br>rbd <span class="hljs-built_in">clone</span> --image testrbd --snap overwrite newrbd<br>[root@lab102 current]<span class="hljs-comment"># ceph-dencoder import 0.txt type SnapSet decode dump_json</span><br>&#123;<br>    <span class="hljs-string">&quot;snap_context&quot;</span>: &#123;<br>        <span class="hljs-string">&quot;seq&quot;</span>: 0,<br>        <span class="hljs-string">&quot;snaps&quot;</span>: []<br>    &#125;,<br>    <span class="hljs-string">&quot;head_exists&quot;</span>: 1,<br>    <span class="hljs-string">&quot;clones&quot;</span>: []<br>&#125;<br><br>[root@lab102 current]<span class="hljs-comment"># ceph-dencoder import 1.txt type SnapSet decode dump_json</span><br>&#123;<br>    <span class="hljs-string">&quot;snap_context&quot;</span>: &#123;<br>        <span class="hljs-string">&quot;seq&quot;</span>: 0,<br>        <span class="hljs-string">&quot;snaps&quot;</span>: []<br>    &#125;,<br>    <span class="hljs-string">&quot;head_exists&quot;</span>: 1,<br>    <span class="hljs-string">&quot;clones&quot;</span>: []<br>&#125;<br></code></pre></td></tr></table></figure><p>上面的对象是快照原始数据未修改的数据的，我们新写入一个数据到原始镜像</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab102 current]<span class="hljs-comment"># dd if=/dev/urandom of=/dev/rbd/rbd/testrbd bs=4M count=1 seek=2</span><br>1+0 records <span class="hljs-keyword">in</span><br>1+0 records out<br>4194304 bytes (4.2 MB) copied, 0.06408 s, 65.5 MB/s<br>[root@lab102 current]<span class="hljs-comment"># ceph-dencoder import 2.txt type SnapSet decode dump_json</span><br>&#123;<br>    <span class="hljs-string">&quot;snap_context&quot;</span>: &#123;<br>        <span class="hljs-string">&quot;seq&quot;</span>: 32,<br>        <span class="hljs-string">&quot;snaps&quot;</span>: [<br>            32<br>        ]<br>    &#125;,<br>    <span class="hljs-string">&quot;head_exists&quot;</span>: 1,<br>    <span class="hljs-string">&quot;clones&quot;</span>: []<br>&#125;<br></code></pre></td></tr></table></figure><p>新写入的地方标记了个snap 32，而上面的snaps没有标记的是原始的数据，那么判断一个对象，没有快照数据，需要判断是原始数据，还是新数据的时候，snaps里面有数据的就是新写入的数据，没有snaps的标记的时候，就是老的数据，读取这个原始数据，可以理解为创建可快照32之后写入的这个对象，那么就是新对象，对于快照后克隆的rbd，就不要读取这个数据了</p><p>还有一种情况，做了多个快照，对象对于快照1来说是老数据，对于快照2来说是新数据，那么怎么判断</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab102 current]<span class="hljs-comment"># ceph-dencoder import 0.txt type SnapSet decode dump_json</span><br>&#123;<br>    <span class="hljs-string">&quot;snap_context&quot;</span>: &#123;<br>        <span class="hljs-string">&quot;seq&quot;</span>: 0,<br>        <span class="hljs-string">&quot;snaps&quot;</span>: []<br>    &#125;,<br>    <span class="hljs-string">&quot;head_exists&quot;</span>: 1,<br>    <span class="hljs-string">&quot;clones&quot;</span>: []<br>&#125;<br><br>[root@lab102 current]<span class="hljs-comment"># ceph-dencoder import 1.txt type SnapSet decode dump_json</span><br>&#123;<br>    <span class="hljs-string">&quot;snap_context&quot;</span>: &#123;<br>        <span class="hljs-string">&quot;seq&quot;</span>: 34,<br>        <span class="hljs-string">&quot;snaps&quot;</span>: [<br>            34<br>        ]<br>    &#125;,<br>    <span class="hljs-string">&quot;head_exists&quot;</span>: 1,<br>    <span class="hljs-string">&quot;clones&quot;</span>: []<br>&#125;<br><br>[root@lab102 current]<span class="hljs-comment"># ceph-dencoder import 2.txt type SnapSet decode dump_json</span><br>&#123;<br>    <span class="hljs-string">&quot;snap_context&quot;</span>: &#123;<br>        <span class="hljs-string">&quot;seq&quot;</span>: 35,<br>        <span class="hljs-string">&quot;snaps&quot;</span>: [<br>            35,<br>            34<br>        ]<br>    &#125;,<br>    <span class="hljs-string">&quot;head_exists&quot;</span>: 1,<br>    <span class="hljs-string">&quot;clones&quot;</span>: []<br>&#125;<br>[root@lab102 current]<span class="hljs-comment"># rbd snap ls testrbd</span><br>SNAPID NAME      SIZE <br>    34 snap1 16384 kB <br>    35 snap2 16384 kB <br></code></pre></td></tr></table></figure><p>写入第一个对象，做快照snap1，然后写入第二个对象，做快照2，再写入第三个对象<br>那么第二个对象对于快照1来说是新对象，对于快照2是老对象<br>我们从上面的snaps里面可以看到的，如果这个对象里面没有包含快照的id，那么这个对象就是属于快照的原始数据，如果有快照的id，那么就是原始快照新写入的数据<br>比如上面的1.txt的信息，有快照34，那么这个对于快照34来说是新数据，不要读取，2.txt里面有34，35,那么对于快照34和35都是新的数据，都不要读取，0.txt没有记录，那么这个就是快照的相关的原始数据，是要去读取的</p><p>根据上面的流程以后，就能判断出一个img需要的是哪个对象的数据了，有了这些信息之后，创建一个空img，然后把对象数据塞进去就可以了，上面的这些操作对于一个大的集群来说，一个个操作肯定不现实，所以需要去用脚本来实现就会快很多，这个上面的原理都清楚了，再写脚本就很简单了，这个后面再写个脚本</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本篇文章是分析了快照以后的元数据如何从底层读取，一个对象如何判断是不是克隆后可以读取的对象，基于以上的操作，即使集群破坏的很厉害，只要底层的数据没有删除，还是有进行重构的可能的</p>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>centos7单独编译nbd内核模块</title>
    <link href="/2020/10/09/centos7%E5%8D%95%E7%8B%AC%E7%BC%96%E8%AF%91nbd%E5%86%85%E6%A0%B8%E6%A8%A1%E5%9D%97/"/>
    <url>/2020/10/09/centos7%E5%8D%95%E7%8B%AC%E7%BC%96%E8%AF%91nbd%E5%86%85%E6%A0%B8%E6%A8%A1%E5%9D%97/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>centos7默认内核没有带nbd的模块，可以通过下载跟当前版本匹配的内核源码，编译源码指定的模块，然后加载到系统</p><h2 id="步骤"><a href="#步骤" class="headerlink" title="步骤"></a>步骤</h2><h3 id="判断版本"><a href="#判断版本" class="headerlink" title="判断版本"></a>判断版本</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab201 linux-3.10.0-957.el7]<span class="hljs-comment"># uname -a</span><br>Linux lab201 3.10.0-957.el7.x86_64 <span class="hljs-comment">#1 SMP Thu Nov 8 23:39:32 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux</span><br>[root@lab201 linux-3.10.0-957.el7]<span class="hljs-comment"># lsb_release -a</span><br>LSB Version::core-4.1-amd64:core-4.1-noarch<br>Distributor ID:CentOS<br>Description:CentOS Linux release 7.6.1810 (Core) <br>Release:7.6.1810<br>Codename:Core<br></code></pre></td></tr></table></figure><p>没更新过内核的话，就是centos7.6的3.10.0-957版本的内核</p><h3 id="获取源码"><a href="#获取源码" class="headerlink" title="获取源码"></a>获取源码</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">wget https://mirrors.tuna.tsinghua.edu.cn/centos-vault/7.6.1810/os/Source/SPackages/kernel-3.10.0-957.el7.src.rpm<br></code></pre></td></tr></table></figure><p>清华的源支持vault的库，比centos自带的vault要快，用这个下载</p><h3 id="安装匹配版本的devel包"><a href="#安装匹配版本的devel包" class="headerlink" title="安装匹配版本的devel包"></a>安装匹配版本的devel包</h3><p>这个编译模块的时候需要</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">wget https://mirrors.tuna.tsinghua.edu.cn/centos-vault/7.6.1810/os/x86_64/Packages/kernel-devel-3.10.0-957.el7.x86_64.rpm<br>rpm -ivh kernel-devel-3.10.0-957.el7.x86_64.rpm<br></code></pre></td></tr></table></figure><p>安装完了检查下,下面的目录应该不为空</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">ls</span> /lib/modules/3.10.0-957.el7.x86_64/build<br></code></pre></td></tr></table></figure><h3 id="解压源码"><a href="#解压源码" class="headerlink" title="解压源码"></a>解压源码</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">rpm2cpio kernel-3.10.0-957.el7.src.rpm |cpio -div<br>xz -d linux-3.10.0-957.el7.tar.xz<br><span class="hljs-built_in">cd</span> linux-3.10.0-957.el7/drivers/block/<br></code></pre></td></tr></table></figure><p>默认编译不会通过<br>报错如下</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab201 block]<span class="hljs-comment"># make CONFIG_BLK_DEV_NBD=m -C /lib/modules/3.10.0-957.el7.x86_64/build M=`pwd` modules</span><br>make: Entering directory `/usr/src/kernels/3.10.0-957.el7.x86_64<span class="hljs-string">&#x27;</span><br><span class="hljs-string">  CC [M]  /root/rbd/kernel/linux-3.10.0-957.el7/drivers/block/floppy.o</span><br><span class="hljs-string">  CC [M]  /root/rbd/kernel/linux-3.10.0-957.el7/drivers/block/brd.o</span><br><span class="hljs-string">  CC [M]  /root/rbd/kernel/linux-3.10.0-957.el7/drivers/block/loop.o</span><br><span class="hljs-string">  CC [M]  /root/rbd/kernel/linux-3.10.0-957.el7/drivers/block/pktcdvd.o</span><br><span class="hljs-string">  CC [M]  /root/rbd/kernel/linux-3.10.0-957.el7/drivers/block/osdblk.o</span><br><span class="hljs-string">  CC [M]  /root/rbd/kernel/linux-3.10.0-957.el7/drivers/block/nbd.o</span><br><span class="hljs-string">/root/rbd/kernel/linux-3.10.0-957.el7/drivers/block/nbd.c: In function ‘__nbd_ioctl’:</span><br><span class="hljs-string">/root/rbd/kernel/linux-3.10.0-957.el7/drivers/block/nbd.c:619:19: error: ‘REQ_TYPE_SPECIAL’ undeclared (first use in this function)</span><br><span class="hljs-string">   sreq.cmd_type = REQ_TYPE_SPECIAL;</span><br><span class="hljs-string">                   ^</span><br><span class="hljs-string">/root/rbd/kernel/linux-3.10.0-957.el7/drivers/block/nbd.c:619:19: note: each undeclared identifier is reported only once for each function it appears in</span><br><span class="hljs-string">make[1]: *** [/root/rbd/kernel/linux-3.10.0-957.el7/drivers/block/nbd.o] Error 1</span><br><span class="hljs-string">make: *** [_module_/root/rbd/kernel/linux-3.10.0-957.el7/drivers/block] Error 2</span><br><span class="hljs-string">make: Leaving directory `/usr/src/kernels/3.10.0-957.el7.x86_64&#x27;</span><br><br></code></pre></td></tr></table></figure><p>修改代码<br>增加下面内容<br>这个是从..&#x2F;..&#x2F;include&#x2F;linux&#x2F;blkdev.h里面提取的</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs bash">                mutex_lock(&amp;nbd-&gt;tx_lock);<br>                blk_rq_init(NULL, &amp;sreq);<br><span class="hljs-comment">#ifdef __GENKSYMS__</span><br>        REQ_TYPE_SPECIAL,               /* driver defined <span class="hljs-built_in">type</span> */<br><span class="hljs-comment">#else</span><br>        REQ_TYPE_DRV_PRIV,              /* driver defined <span class="hljs-built_in">type</span> */<br><span class="hljs-comment">#endif</span><br>                sreq.cmd_type = REQ_TYPE_SPECIAL;<br>                nbd_cmd(&amp;sreq) = NBD_CMD_DISC;<br></code></pre></td></tr></table></figure><p>修改makefile</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab201 block]<span class="hljs-comment"># cat Makefile</span><br><span class="hljs-comment">#</span><br><span class="hljs-comment"># Makefile for the kernel block device drivers.</span><br><span class="hljs-comment">#</span><br><span class="hljs-comment"># 12 June 2000, Christoph Hellwig &lt;hch@infradead.org&gt;</span><br><span class="hljs-comment"># Rewritten to use lists instead of if-statements.</span><br><span class="hljs-comment">#</span><br>obj-$(CONFIG_BLK_DEV_NBD)       += nbd.o <br></code></pre></td></tr></table></figure><p>只保留一个需要的</p><p>再次编译</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab201 block]<span class="hljs-comment"># make CONFIG_BLK_DEV_NBD=m -C /lib/modules/3.10.0-957.el7.x86_64/build M=`pwd` modules</span><br>make: Entering directory `/usr/src/kernels/3.10.0-957.el7.x86_64<span class="hljs-string">&#x27;</span><br><span class="hljs-string">  Building modules, stage 2.</span><br><span class="hljs-string">  MODPOST 1 modules</span><br><span class="hljs-string">  CC      /root/rbd/kernel/linux-3.10.0-957.el7/drivers/block/nbd.mod.o</span><br><span class="hljs-string">  LD [M]  /root/rbd/kernel/linux-3.10.0-957.el7/drivers/block/nbd.ko</span><br><span class="hljs-string">make: Leaving directory `/usr/src/kernels/3.10.0-957.el7.x86_64&#x27;</span><br></code></pre></td></tr></table></figure><p>拷贝模块到当前的内核</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab201 block]<span class="hljs-comment"># cp -ra nbd.ko /lib/modules/3.10.0-957.el7.x86_64/kernel/drivers/block/</span><br>[root@lab201 block]<span class="hljs-comment"># depmod -a</span><br>[root@lab201 block]<span class="hljs-comment"># modprobe nbd</span><br>[root@lab201 block]<span class="hljs-comment"># modinfo nbd</span><br>filename:       /lib/modules/3.10.0-957.el7.x86_64/kernel/drivers/block/nbd.ko<br>···<br>[root@lab201 block]<span class="hljs-comment"># lsmod |grep nbd</span><br>nbd                    17554  0 <br></code></pre></td></tr></table></figure><p>完成模块的加载了</p>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>通过tuned-adm调整服务器的各种参数</title>
    <link href="/2020/09/25/%E9%80%9A%E8%BF%87tuned-adm%E8%B0%83%E6%95%B4%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%9A%84%E5%90%84%E7%A7%8D%E5%8F%82%E6%95%B0/"/>
    <url>/2020/09/25/%E9%80%9A%E8%BF%87tuned-adm%E8%B0%83%E6%95%B4%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%9A%84%E5%90%84%E7%A7%8D%E5%8F%82%E6%95%B0/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>centos7 服务器上面支持通过tuned进行统一的系统参数调整，以前我们可能会通过修改内核配置文件修改内核参数，通过echo去调整磁盘预读，等等很多调整会分散在各处<br>而系统实际上是可以通过一个地方进行固化的，这个就是tuned-adm做的事情，本篇是一个集合贴，会持续补充各种调整的方法</p><h2 id="操作"><a href="#操作" class="headerlink" title="操作"></a>操作</h2><h3 id="调整配置"><a href="#调整配置" class="headerlink" title="调整配置"></a>调整配置</h3><h3 id="生效配置"><a href="#生效配置" class="headerlink" title="生效配置"></a>生效配置</h3><h3 id="整体调整"><a href="#整体调整" class="headerlink" title="整体调整"></a>整体调整</h3><p>系统提供了很多模式的调整参数，默认的是最平衡的，如果不是很清楚调整了什么，建议不要整体模式应用，可能引起性能巨大的衰减或者抖动，需要根据实际情况进行调整</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本篇是对服务器进行一个调整，如果使用过不同的os的会清楚，同样的硬件在不同的环境下的性能会有所不通，有的是软件版本的问题，有的是内核版本的问题，有的是参数的问题，本篇是从参数角度来进行一些调整</p>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>测试cephfs写入海量文件</title>
    <link href="/2020/09/22/%E6%B5%8B%E8%AF%95cephfs%E5%86%99%E5%85%A5%E6%B5%B7%E9%87%8F%E6%96%87%E4%BB%B6/"/>
    <url>/2020/09/22/%E6%B5%8B%E8%AF%95cephfs%E5%86%99%E5%85%A5%E6%B5%B7%E9%87%8F%E6%96%87%E4%BB%B6/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>测试cephfs的写入大量文件，通过mdtest写入1K大小的文件1亿个，每个目录里面文件为1万，目录总数为1万，总文件数目就为1亿了</p><h2 id="写入的命令"><a href="#写入的命令" class="headerlink" title="写入的命令"></a>写入的命令</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">mdtest -C -F -L -z 4 -b 10 -I 10000 -d /mnt/1yi/ -w 1024<br></code></pre></td></tr></table></figure><ul><li>-C 只创建</li><li>-F  只创建文件</li><li>-L 只在叶子层目录创建文件</li><li>-z 目录深度层级为4</li><li>-b 每个树下面子目录为10</li><li>-I 指定目录里面的文件数目</li><li>-w指定文件大小为1K<br>每层的目录数目</li></ul><blockquote><p>1，10，100，1000，10000</p></blockquote><h2 id="统计写入的情况"><a href="#统计写入的情况" class="headerlink" title="统计写入的情况"></a>统计写入的情况</h2><p>通过扩展属性统计</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 <span class="hljs-comment">#test-dir.0]# getfattr -d -m ceph.dir.* mdtest_tree.0/</span><br><span class="hljs-comment"># file: mdtest_tree.0/</span><br>ceph.dir.entries=<span class="hljs-string">&quot;10&quot;</span><br>ceph.dir.files=<span class="hljs-string">&quot;0&quot;</span><br>ceph.dir.rbytes=<span class="hljs-string">&quot;1000307712&quot;</span><br>ceph.dir.rctime=<span class="hljs-string">&quot;1600762346.09161429944&quot;</span><br>ceph.dir.rentries=<span class="hljs-string">&quot;1001670&quot;</span><br>ceph.dir.rfiles=<span class="hljs-string">&quot;990559&quot;</span><br>ceph.dir.rsubdirs=<span class="hljs-string">&quot;11111&quot;</span><br>ceph.dir.subdirs=<span class="hljs-string">&quot;10&quot;</span><br></code></pre></td></tr></table></figure><p>参数解析</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs bash">ceph.dir.entries 指定目录下面文件和目录总个数<br>ceph.dir.files  指定目录下面没有文件<br>ceph.dir.rbytes  指定目录下面的文件总文件容量为1000307712 bytes<br>ceph.dir.rctime  目录的访问时间戳<br>ceph.dir.rentries  目录下面的文件和目录的总个数<br>ceph.dir.rfiles  目录下面的文件的个数<br>ceph.dir.rsubdirs 目录下面的子目录总个数（递归统计）<br>ceph.dir.subdirs   目录下面的子目录的个数（一层的）<br></code></pre></td></tr></table></figure><h2 id="mds内存占用统计"><a href="#mds内存占用统计" class="headerlink" title="mds内存占用统计"></a>mds内存占用统计</h2><p>200万文件   内存3.3G<br>300万文件    内存4.3G</p><p>统计脚本</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ~]<span class="hljs-comment"># cat getmdsmem.sh </span><br><span class="hljs-comment">#! /bin/sh</span><br><br><span class="hljs-keyword">while</span> [ 2 &gt; 1 ]<br><span class="hljs-keyword">do</span><br>    mem=`ps -p 49808  -o rsz|<span class="hljs-built_in">tail</span> -n 1`<br>    file_num=`getfattr -d -m ceph.dir.*  /mnt/1yi/|grep ceph.dir.rfiles|<span class="hljs-built_in">cut</span> -d <span class="hljs-string">&quot;=&quot;</span> -f 2|<span class="hljs-built_in">cut</span> -d <span class="hljs-string">&quot;\&quot;&quot;</span> -f 2`<br>    <span class="hljs-built_in">date</span>=`<span class="hljs-built_in">date</span> <span class="hljs-string">&quot;+%Y%m%d%H%M%S&quot;</span>` <br>    <span class="hljs-built_in">echo</span> <span class="hljs-variable">$date</span>,<span class="hljs-variable">$file_num</span>,<span class="hljs-variable">$mem</span>&gt;&gt;/opt/recode.txt<br>    <br>    <span class="hljs-built_in">sleep</span>  2<br><span class="hljs-keyword">done</span>  <br><br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>iscsi一致性的测试验证方法</title>
    <link href="/2020/09/18/iscsi%E4%B8%80%E8%87%B4%E6%80%A7%E7%9A%84%E6%B5%8B%E8%AF%95%E9%AA%8C%E8%AF%81%E6%96%B9%E6%B3%95/"/>
    <url>/2020/09/18/iscsi%E4%B8%80%E8%87%B4%E6%80%A7%E7%9A%84%E6%B5%8B%E8%AF%95%E9%AA%8C%E8%AF%81%E6%96%B9%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>如果使用场景是多路径iscsi，那么数据一致性的就需要去验证一致性，就需要一个比较通用的测试方法，最近在处理这块，记录下简单的测试方法</p><h2 id="测试方法"><a href="#测试方法" class="headerlink" title="测试方法"></a>测试方法</h2><h3 id="写入数据"><a href="#写入数据" class="headerlink" title="写入数据"></a>写入数据</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">dd</span> <span class="hljs-keyword">if</span>=/dev/urandom   of=/dev/sdb bs=512 count=8000 oflag=direct<br>或者<br><span class="hljs-built_in">dd</span> <span class="hljs-keyword">if</span>=/dev/urandom   of=/dev/sdb bs=512 count=8000<br></code></pre></td></tr></table></figure><p>通常来说需要用下面的那种，上面的带参数的是模拟的direct的请求，如果在当前设备之上的软件会处理好这个，这个地方就不要用direct的请求的，用direct是模拟软件的写，不带direct模拟正常读写</p><h3 id="验证数据"><a href="#验证数据" class="headerlink" title="验证数据"></a>验证数据</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">sha512sum</span> /dev/sdb1<br></code></pre></td></tr></table></figure><p>如果设备被多路径软件接管以后，那么sha512sum就不要对着多路径接管的设备去操作了，多路径接管后的数据多路径自己去处理的，否则测试方法本身就是存在问题的，如果要确认一致性，可用先停到多路径软件，确认没问题之后，启动多路径软件从多路径设备路径检查</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>实际就是两条命令，一个写一个读，然后设备路径的问题，这个路径选择错误可能引起测试结论的错误的</p><h2 id="更新历史"><a href="#更新历史" class="headerlink" title="更新历史"></a>更新历史</h2><table><thead><tr><th>why</th><th>when</th></tr></thead><tbody><tr><td>创建</td><td>2020年09月18日</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>ceph通过tgt配置多路径的数据一致性问题</title>
    <link href="/2020/09/16/ceph%E9%80%9A%E8%BF%87tgt%E9%85%8D%E7%BD%AE%E5%A4%9A%E8%B7%AF%E5%BE%84%E7%9A%84%E6%95%B0%E6%8D%AE%E4%B8%80%E8%87%B4%E6%80%A7%E9%97%AE%E9%A2%98/"/>
    <url>/2020/09/16/ceph%E9%80%9A%E8%BF%87tgt%E9%85%8D%E7%BD%AE%E5%A4%9A%E8%B7%AF%E5%BE%84%E7%9A%84%E6%95%B0%E6%8D%AE%E4%B8%80%E8%87%B4%E6%80%A7%E9%97%AE%E9%A2%98/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>使用librbd通过tgt出iscsi，多路径数据是一致的，由于环境的原因，切换为kernel rbd 出iscsi，然后通过不同的iscsi路径写入后，发现在rbd端就出现了数据没有刷新的问题</p><h2 id="问题分析"><a href="#问题分析" class="headerlink" title="问题分析"></a>问题分析</h2><p>这里有个类似的问题</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">https://github.com/ceph/ceph-csi/issues/461<br></code></pre></td></tr></table></figure><p>这个是通过把rbd map到不同的节点，通过dd写入的时候发现数据不同步，需要通过增加参数direct才能保证数据一致，我们通过官网的描述和这个issue里面的回答可以看到</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">https://docs.ceph.com/docs/luminous/rbd/rbd-config-ref/<br></code></pre></td></tr></table></figure><blockquote><p>The kernel driver for Ceph block devices can use the Linux page cache to improve performance.</p></blockquote><p>rbd内核客户端是会通过linux page cache来改善性能的，而librbd是可以通过参数关闭缓存的，这个在我以前的理解里面krbd是没有缓存的，实际上这个是存在缓存的</p><p>而通过tgt的客户端写入，数据到了tgt，而tgt可以利用kernel的rbd page缓存一点数据，然后就没下刷到后端了，从另外一个客户端读取数据的时候就看到数据是没刷过去的，这个跟把rbd格式化文件系统，然后map到两台机器，一台写入，另外一台查看，也是存在看不到数据的情况</p><p>而issue里面也说的很清楚，这个同步写的过程应该交给上层软件去控制，也就是软件去控制o_direct的写入</p><h2 id="问题复现"><a href="#问题复现" class="headerlink" title="问题复现"></a>问题复现</h2><p>复现问题比较容易，配置kernel rbd，map到服务器，然后配置tgt，两台机器配置相同的tgt，然后通过iscsi客户端连接两台tgt的iscsi server，那么iscsi的客户端机器就增加了两个盘符<br>直接对裸盘进行dd的写入</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">dd</span> <span class="hljs-keyword">if</span>=/dev/urandom   of=/dev/sdc bs=512 count=8000<br></code></pre></td></tr></table></figure><p>写完以后在 rbdmap的机器上面执行</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">sha512sum</span> /dev/rbd/rbd/testrbd<br></code></pre></td></tr></table></figure><p>两台机器都执行检查，这个时候默认情况下，得到的输出是不一致的</p><h2 id="解决问题"><a href="#解决问题" class="headerlink" title="解决问题"></a>解决问题</h2><p>配置文件增加一个配置</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">&lt;target iqn.2008-09.com.example:server.target2&gt;<br>    backing-store /dev/rbd/rbd/testrbd<br>    bsoflags direct<br>    <span class="hljs-comment">#write-cache off</span><br>&lt;/target&gt;<br></code></pre></td></tr></table></figure><p>增加一个bsoflags direct就可以保证数据一致性了，下面的写缓存应该是客户端那边的缓存，缓存客户端的数据还没来到rbd这边，来到以后也是direct的写，也是能保证一致性的，所以可以保留</p><p>测试过程中还遇到另外一个问题backing-store如果写成direct-store，下面的配置的参数就不能生效了，所以这个地方需要注意下</p>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>ceph各个版本之间参数变化分析</title>
    <link href="/2020/09/15/ceph%E5%90%84%E4%B8%AA%E7%89%88%E6%9C%AC%E4%B9%8B%E9%97%B4%E5%8F%82%E6%95%B0%E5%8F%98%E5%8C%96%E5%88%86%E6%9E%90/"/>
    <url>/2020/09/15/ceph%E5%90%84%E4%B8%AA%E7%89%88%E6%9C%AC%E4%B9%8B%E9%97%B4%E5%8F%82%E6%95%B0%E5%8F%98%E5%8C%96%E5%88%86%E6%9E%90/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>本篇主要是分析ceph的版本之间参数的变化，参数变化意味着功能的变化，通过参数来分析增加，删除，修改了哪些功能，以及版本之间的变化，本篇主要通过导出参数，然后通过脚本去比对不同的版本的参数变化</p><p>14版本之前安装一个ceph-common,然后touch一个空配置文件就可以通过ceph –show-config拿到版本的配置文件<br>14版本之后需要安装好mon，并且这个命令取消了，通过下面的命令获取</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">ceph config show-with-defaults mon.lab204|awk <span class="hljs-string">&#x27;&#123;print $1,&quot;=&quot;,$2&#125;&#x27;</span><br></code></pre></td></tr></table></figure><h2 id="处理过程"><a href="#处理过程" class="headerlink" title="处理过程"></a>处理过程</h2><p>选取了6个版本的配置文件做分析</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab204 cephconf]<span class="hljs-comment"># ll</span><br>total 304<br>-rw-r--r-- 1 root root 35475 Sep 14 15:30 ceph.conf.10.2.11<br>-rw-r--r-- 1 root root 47857 Sep 14 15:32 ceph.conf.12.2.13<br>-rw-r--r-- 1 root root 51393 Sep 14 17:28 ceph.conf.14.1.0<br>-rw-r--r-- 1 root root 51535 Sep 14 16:12 ceph.conf.14.2.0<br>-rw-r--r-- 1 root root 53575 Sep 14 17:42 ceph.conf.14.2.11<br>-rw-r--r-- 1 root root 55400 Sep 14 16:25 ceph.conf.15.2.4<br>-rwxrwxrwx 1 root root   986 Sep 14 17:40 compare-conf.sh<br></code></pre></td></tr></table></figure><p>通过执行脚本输出比对结果</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">sh ./compare-conf.sh  ceph.conf.10.2.11 ceph.conf.12.2.13  &gt; /tmp/compare.log<br></code></pre></td></tr></table></figure><h2 id="比对结果如下"><a href="#比对结果如下" class="headerlink" title="比对结果如下"></a>比对结果如下</h2><table><thead><tr><th align="center">原始版本</th><th align="center">比对版本</th><th align="center">删除配置</th><th align="center">改动配置</th><th align="center">增加配置</th></tr></thead><tbody><tr><td align="center">ceph.conf.10.2.11</td><td align="center">ceph.conf.12.2.13</td><td align="center">78</td><td align="center">182</td><td align="center">394</td></tr><tr><td align="center">ceph.conf.12.2.13</td><td align="center">ceph.conf.14.1.0</td><td align="center">75</td><td align="center">143</td><td align="center">170</td></tr><tr><td align="center">ceph.conf.14.1.0</td><td align="center">ceph.conf.14.2.0</td><td align="center">5</td><td align="center">7</td><td align="center">8</td></tr><tr><td align="center">ceph.conf.14.2.0</td><td align="center">ceph.conf.14.2.11</td><td align="center">5</td><td align="center">24</td><td align="center">61</td></tr><tr><td align="center">ceph.conf.14.2.11</td><td align="center">ceph.conf.15.2.4</td><td align="center">55</td><td align="center">14</td><td align="center">105</td></tr></tbody></table><p>得到版本的参数变化值以后，再挑选出来进行具体的分析即可，通过上面的版本可以看到从10的最后一个版本到现在差不多增加了600多个参数，也删除了200个参数左右，从整体上来说我们需要重点关注下新增的参数和变动的参数</p><h2 id="ceph-conf-10-2-11到ceph-conf-12-2-13主要变化"><a href="#ceph-conf-10-2-11到ceph-conf-12-2-13主要变化" class="headerlink" title="ceph.conf.10.2.11到ceph.conf.12.2.13主要变化"></a>ceph.conf.10.2.11到ceph.conf.12.2.13主要变化</h2><ul><li>mon_pg_stuck_threshold 从300秒调整为60秒，判断pg为stuck状态的</li><li>mon_stat_smooth_intervals 统计pgmap从最近的2个调整为最近的6个</li><li>mon_osd_cache_size  mon保留的osdmap从10调整为500</li><li>mon_osd_down_out_interval 从down到out的时间从300秒调整为600秒</li><li>mon_keyvaluedb  从leveldb换成了rocksdb</li><li>mds_cache_size 从100000调整为0也就是不限制</li><li>mds_log_max_segments 从30调整为128 控制Behind on trimming的</li><li>osd_backfill_retry_interval backfill的重试间隔从10调整为30</li><li>osd_map_max_advance pg检查OSDMap增量版本时每次检查的最大版本数量从150调整为40</li><li>osd_map_cache_size 从200调整为50</li><li>osd_map_message_max 从100调整为40  这个在老版本里面大了会影响稳定性（<a href="https://tracker.ceph.com/issues/3804">3804</a>）</li><li>osd_map_share_max_epochs 从100 调整为40</li><li>osd_op_num_threads_per_shard 每个缓存队列的线程数从2调整为0</li><li>osd_op_num_shards 缓存队列从5调整为0</li><li>osd_pg_epoch_persisted_max_stale 从150调整为40</li><li>osd_max_object_size  从107374182400调整为128M，超大对象可能影响稳定性，cephfs的可能调整这个设置不成功</li><li>rbd_localize_parent_reads rbd的本地就近读取功能从开启调整为关闭了，官方有个pr说关闭更安全（<a href="https://github.com/ceph/ceph/pull/16882">16882</a>）</li><li>rgw_thread_pool_size 线程池从100调整为512，rgw的并发能力提高了</li></ul><p>上面的是主要的一些变化，bluestore的一些参数就不记录了，因为jewel里面的bluestore基本是实验版本的<br>增加了394个参数，大部分是bluestore的，也有其它的参数的，增加的参数后面再分析</p><h2 id="ceph-conf-12-2-13到ceph-conf-14-1-0主要变化"><a href="#ceph-conf-12-2-13到ceph-conf-14-1-0主要变化" class="headerlink" title="ceph.conf.12.2.13到ceph.conf.14.1.0主要变化"></a>ceph.conf.12.2.13到ceph.conf.14.1.0主要变化</h2><ul><li><p>bluefs_buffered_io 从关闭得到开启，但是这个开启后会在后期引起swap的调用，引起整体性能降低（<a href="https://access.redhat.com/solutions/4967421">Redhat4967421</a>），红帽自己是准备把这个参数关闭（<a href="https://github.com/ceph/ceph/pull/34297">GitHub pr 34297</a>）后面版本应该也关闭了</p></li><li><p>fuse_big_writes 从true改成了false，false的时候是4K的写入，true的时候是128K的写入，对性能影响特别大（<a href="http://lists.ceph.com/pipermail/ceph-users-ceph.com/2019-April/034107.html">提出下降的问题</a>），这个参数L版本还是true，这个版本改成了false，<a href="https://github.com/ceph/ceph/pull/16562">16562 pr</a>这个pr可以看到，是因为libfuse去掉了这个参数了， 还要看下这个参数-o max_write ，用了fuse需要关注下版本，CentOS软件包libfuse2.9.2。因此，使用ceph-fuse在CentOS上升级到Mimi 导致fuse中的4k写 性能非常差的层</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab204 cephconf]<span class="hljs-comment"># rpm -qa|grep fuse</span><br>fuse-libs-2.9.2-11.el7.x86_64<br>fuse-2.9.2-11.el7.x86_64<br></code></pre></td></tr></table></figure><p>也就是如果fuse是这个版本，就手动开启，如果把fuse升级了，那么性能就不用这个参数控制了</p></li><li><p>log_max_recent log最近的事件从500调整为10000</p></li><li><p>mon_client_hunt_parallel 客户端寻找mon的时候可以控制寻找mon的数量，从2改成了3了</p></li><li><p>mon config key max entry size 配置的key的大小从4096调整为65536（单位bytes）</p></li><li><p>mon_mgr_mkfs_grace  从60调整为120  如果120s没有活跃的mgr就用warn警告调整为error警告</p></li><li><p>osd_deep_scrub_large_omap_object_key_threshold判断omap超大的标准，从20万调整为200万，这个参数存在争议，后面看下最终是多少<a href="https://tracker.ceph.com/issues/40583">issue 40583</a></p></li><li><p>osd_max_pg_log_entries 从10000调整为3000 （这个参数是控制recover和backfill的，调整到足够小，就是强制做backfill了（<a href="https://access.redhat.com/documentation/en-us/red_hat_ceph_storage/3/html-single/administration_guide/index">红帽文档</a>）） To force backfill rather than recovery, set osd_min_pg_log_entries to 1, and set osd_max_pg_log_entries to 2</p></li><li><p>osd_min_pg_log_entries 从1500调整为3000  （也有相关的问题<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1608060">内存占用</a>）</p></li><li><p>rgw_reshard_bucket_lock_duration reshard的时候，锁定对象的时间从120秒调整为360秒</p></li></ul><h2 id="ceph-conf-14-1-0到ceph-conf-14-2-0版本变化"><a href="#ceph-conf-14-1-0到ceph-conf-14-2-0版本变化" class="headerlink" title="ceph.conf.14.1.0到ceph.conf.14.2.0版本变化"></a>ceph.conf.14.1.0到ceph.conf.14.2.0版本变化</h2><ul><li>rbd_skip_partial_discard 从false改成了true，跟discard 有关的，If true, the block device will skip zeroing a range when trying to discard a range inside an object.</li><li>rgw_frontends 默认从civetweb变成了beast</li></ul><h2 id="ceph-conf-14-2-0到ceph-conf-14-2-11版本变化"><a href="#ceph-conf-14-2-0到ceph-conf-14-2-11版本变化" class="headerlink" title="ceph.conf.14.2.0到ceph.conf.14.2.11版本变化"></a>ceph.conf.14.2.0到ceph.conf.14.2.11版本变化</h2><ul><li>bluefs_allocator 从bitmap调整为hybrid，V14.2.11 新加入的功能（<a href="https://github.com/ceph/ceph/pull/35500">相关pr</a>）</li><li>bluefs_buffered_io这个版本调整为false了</li><li>bluestore_block_size 从10G调整为100G了</li><li>bluestore_fsck_on_mount_deep 从true调整为false了</li><li>bluestore_fsck_on_umount_deep 从true调整为false了</li><li>bluestore_log_omap_iterator_age 从1调整为5</li><li>bluestore_min_alloc_size_ssd从16384调整为4096这个跟性能有关的，The default value of bluestore_min_alloc_size_ssd has been changed to 4K to improve performance across all workloads.</li><li>fuse_big_writes又从false调整为true了</li><li>mon_pg_warn_min_per_osd从30调整为0了</li><li>osd_client_message_cap从100调整为0</li><li>osd_deep_scrub_large_omap_object_key_threshold 这个又从2000000调整为200000了</li><li>rocksdb_pin_l0_filter_and_index_blocks_in_cache从true调整为false了</li></ul><h2 id="ceph-conf-14-2-11到ceph-conf-15-2-4版本变化"><a href="#ceph-conf-14-2-11到ceph-conf-15-2-4版本变化" class="headerlink" title="ceph.conf.14.2.11到ceph.conf.15.2.4版本变化"></a>ceph.conf.14.2.11到ceph.conf.15.2.4版本变化</h2><ul><li>mds_cache_memory_limit mds的缓存从1073741824调整为4294967296</li><li>osd_client_message_cap 从0 调整为100了</li><li>osd_max_omap_entries_per_request从131072调整为1024了</li><li>osd_max_pg_log_entries 从3000调整为10000 这个上个版本调整过，又动了</li><li>osd_min_pg_log_entries 从3000调整为250了  这个上个版本调整过，又动了</li><li>osd_op_queue_cut_off 从low调整为high</li><li>osd_pool_default_pg_autoscale_mode 从warn调整为了on了</li><li>rgw_bucket_index_max_aio从9调整为128</li><li>rgw_lc_lock_max_time 从60调整为90</li></ul><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本篇是提供了一个查找功能变化点的思路，当然也可以通过github的pr来分析，配置文件过多的时候，我们就分析重点的地方即可</p><h2 id="变更记录"><a href="#变更记录" class="headerlink" title="变更记录"></a>变更记录</h2><table><thead><tr><th align="center">Why</th><th align="center">Who</th><th align="center">When</th></tr></thead><tbody><tr><td align="center">创建</td><td align="center">武汉-运维-磨渣</td><td align="center">2020-09-15</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>rgw的rgw_thread_pool_size配置调整</title>
    <link href="/2020/09/11/rgw%E7%9A%84rgw_thread_pool_size%E9%85%8D%E7%BD%AE%E8%B0%83%E6%95%B4/"/>
    <url>/2020/09/11/rgw%E7%9A%84rgw_thread_pool_size%E9%85%8D%E7%BD%AE%E8%B0%83%E6%95%B4/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>在比对rgw的不同前端的区别的时候，官方说civetweb是通过线程池来控制连接的，beast是后面加入了流控相关的，这块一直也没有调整过相关的参数，然后通过ab压测了一下，还是有很明显的区别的</p><p>测试很简单，虚拟机下面就可以完成</p><h2 id="测试过程"><a href="#测试过程" class="headerlink" title="测试过程"></a>测试过程</h2><p>rgw_thread_pool_size &#x3D; 512<br>默认参数为这个</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab201 ~]<span class="hljs-comment"># ps -ef|grep radosgw</span><br>root      242156  239377  0 11:42 pts/1    00:00:02 tailf /var/log/ceph/ceph-client.radosgw1.<span class="hljs-built_in">log</span><br>ceph      247068       1  3 11:53 ?        00:00:00 /usr/bin/radosgw -f --cluster ceph --name client.radosgw1 --setuser ceph --setgroup ceph<br>root      247654  234613  0 11:53 pts/0    00:00:00 grep --color=auto radosgw<br>[root@lab201 ~]<span class="hljs-comment"># cat /proc/247068/status|grep Thread</span><br>Threads:579<br></code></pre></td></tr></table></figure><p>启动rgw进程后可以去查看这个进程的线程数目，基本接近设置值的，还有一些其它需要用的线程</p><p>我们往s3接口里面传输一个很小的文件 260 bytes的文件，让这个文件可以公共访问，然后使用ab去并发连接这个文件，这里不是去看整个集群的负载多大，而是看下在相同的环境情况下，参数的改变会有什么区别</p><h3 id="ab测试"><a href="#ab测试" class="headerlink" title="ab测试"></a>ab测试</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">ab -c 1000 -n 50000 http://192.168.0.201:7481/test/index.txt<br></code></pre></td></tr></table></figure><p>1000个并发请求50000次<br>测试输出</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs bash">Percentage of the requests served within a certain time (ms)<br>  50%    180<br>  66%    186<br>  75%    193<br>  80%    198<br>  90%    214<br>  95%   1182<br>  98%   1205<br>  99%   2019<br> 100%   7589 (longest request)<br></code></pre></td></tr></table></figure><p>我们调整rgw_thread_pool_size &#x3D; 128再测试一轮</p><p>基本无法完成测试</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab204 ~]<span class="hljs-comment"># ab -c 1000 -n 50000 http://192.168.0.201:7481/test/index.txt</span><br>This is ApacheBench, Version 2.3 &lt;<span class="hljs-variable">$Revision</span>: 1430300 $&gt;<br>Copyright 1996 Adam Twiss, Zeus Technology Ltd, http://www.zeustech.net/<br>Licensed to The Apache Software Foundation, http://www.apache.org/<br><br>Benchmarking 192.168.0.201 (be patient)<br>Completed 5000 requests<br>Completed 10000 requests<br>apr_socket_recv: Connection reset by peer (104)<br>Total of 14351 requests completed<br><br></code></pre></td></tr></table></figure><p>我们再加大参数配置rgw_thread_pool_size &#x3D; 1024</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab204 ~]<span class="hljs-comment"># ab -c 1000 -n 50000 http://192.168.0.201:7481/test/index.txt</span><br>Percentage of the requests served within a certain time (ms)<br>  50%    281<br>  66%    286<br>  75%    290<br>  80%    293<br>  90%    299<br>  95%    305<br>  98%    310<br>  99%    313<br> 100%   1309 (longest request)<br></code></pre></td></tr></table></figure><p>可以看到与512线程的时候明显的有所改善了</p><p>基于以上可以看到，线程如果设置小了，而外部的请求的并发连接大了，可能出现服务不响应的情况，而适当的加大线程数，也能一定程度上改善请求的效果，上面的测试仅测试验证参数的影响，而实际落地到真实环境，还需要慢慢调整</p><p>luminous版本的beast也是需要调整上面的线程池的参数来应对超多连接的情况的，否则客户端很容易就异常了</p><h2 id="客户端并发请求最大值调整"><a href="#客户端并发请求最大值调整" class="headerlink" title="客户端并发请求最大值调整"></a>客户端并发请求最大值调整</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab204 ~]<span class="hljs-comment"># ab -c 3000 -n 50000 http://192.168.0.201:7481/test/index.txt</span><br>This is ApacheBench, Version 2.3 &lt;<span class="hljs-variable">$Revision</span>: 1430300 $&gt;<br>Copyright 1996 Adam Twiss, Zeus Technology Ltd, http://www.zeustech.net/<br>Licensed to The Apache Software Foundation, http://www.apache.org/<br><br>Benchmarking 192.168.0.201 (be patient)<br>socket: Too many open files (24)<br></code></pre></td></tr></table></figure><p>默认的为1024，需要调整下参数</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab204 ~]<span class="hljs-comment"># ulimit -n 4096</span><br></code></pre></td></tr></table></figure><p>调整后就可以加大并发请求了</p><h2 id="变更记录"><a href="#变更记录" class="headerlink" title="变更记录"></a>变更记录</h2><table><thead><tr><th align="center">Why</th><th align="center">Who</th><th align="center">When</th></tr></thead><tbody><tr><td align="center">创建</td><td align="center">武汉-运维-磨渣</td><td align="center">2020-09-11</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>rgw前端替换civetweb为beast</title>
    <link href="/2020/09/11/rgw%E5%89%8D%E7%AB%AF%E6%9B%BF%E6%8D%A2civetweb%E4%B8%BAbeast/"/>
    <url>/2020/09/11/rgw%E5%89%8D%E7%AB%AF%E6%9B%BF%E6%8D%A2civetweb%E4%B8%BAbeast/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>ceph的rgw现在提供了两种前端， civetweb和beast</p><h2 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h2><p>修改配置文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">rgw_frontends = civetweb port=7481<br></code></pre></td></tr></table></figure><p>为</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">rgw frontends = beast  port=7481<br></code></pre></td></tr></table></figure><p>然后重启rgw进程，查看日志</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs bash">2020-09-11 10:23:52.117738 7f90b45f8000  0 ceph version 12.2.13 (584a20eb0237c657dc0567da126be145106aa47e) luminous (stable), process radosgw, pid 235781<br>2020-09-11 10:23:52.204891 7f90b45f8000 -1 *** experimental feature <span class="hljs-string">&#x27;rgw-beast-frontend&#x27;</span> is not enabled ***<br>This feature is marked as experimental, <span class="hljs-built_in">which</span> means it<br> - is untested<br> - is unsupported<br> - may corrupt your data<br> - may <span class="hljs-built_in">break</span> your cluster is an unrecoverable fashion<br>To <span class="hljs-built_in">enable</span> this feature, add this to your ceph.conf:<br>  <span class="hljs-built_in">enable</span> experimental unrecoverable data corrupting features = rgw-beast-frontend<br></code></pre></td></tr></table></figure><p>这个在 ceph version 12.2.13还是这个提示，后面的版本里面就没这个了</p><p>增加配置文件</p><blockquote><p>enable experimental unrecoverable data corrupting features &#x3D; rgw-beast-frontend<br>到global里面</p></blockquote><p>在这个<a href="https://github.com/ceph/ceph/pull/21272/commits/f1e826ad7f074195b87b7bbd37fcb74d4ca626cb%E6%8F%90%E4%BA%A4%E4%B9%8B%E5%90%8E%E5%8E%BB%E6%8E%89%E5%AE%9E%E9%AA%8C%E6%80%A7%E8%B4%A8%E7%9A%84">https://github.com/ceph/ceph/pull/21272/commits/f1e826ad7f074195b87b7bbd37fcb74d4ca626cb提交之后去掉实验性质的</a></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">master (<span class="hljs-comment">#21272)  v16.0.0  v15.2.4 v15.2.3 v15.2.2 v15.2.1 v15.2.0 v15.1.1 v15.1.0 v15.0.0 v14.2.11 v14.2.10 v14.2.9 v14.2.8 v14.2.7 v14.2.6 v14.2.5 v14.2.4 v14.2.3 v14.2.2 v14.2.1 v14.2.0 v14.1.1 v14.1.0 v14.0.1 v14.0.0 v13.2.10 v13.2.9 v13.2.8 v13.2.7 v13.2.6 v13.2.5 v13.2.4 v13.2.3 v13.2.2 v13.2.1 v13.2.0 v13.1.1 v13.1.0</span><br></code></pre></td></tr></table></figure><p>在这些版本里面去掉了这个标记的</p><p>重启rgw进程</p><p>再次查看日志</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">2020-09-11 10:35:18.189630 7fe5891bf000  0 starting handler: beast<br>2020-09-11 10:35:18.192804 7fe5891bf000  0 <span class="hljs-built_in">set</span> uid:gid to 167:167 (ceph:ceph)<br>2020-09-11 10:35:18.205123 7fe5891bf000  1 mgrc service_daemon_register rgw.radosgw1 metadata &#123;<span class="hljs-built_in">arch</span>=x86_64,ceph_version=ceph version 12.2.13 (584a20eb0237c657dc0567da126be145106aa47e) luminous (stable),cpu=Intel(R) Core(TM) i7-10710U CPU @ 1.10GHz,distro=centos,distro_description=CentOS Linux 7 (Core),distro_version=7,frontend_config<span class="hljs-comment">#0=beast  port=7481,frontend_type#0=beast,hostname=lab201,kernel_description=#1 SMP Thu Nov 8 23:39:32 UTC 2018,kernel_version=3.10.0-957.el7.x86_64,mem_swap_kb=0,mem_total_kb=3861512,num_handles=1,os=Linux,pid=236518,zone_id=80576aa5-448e-470d-aee4-ac8662d35e62,zone_name=default,zonegroup_id=5eced993-d226-4e15-9a4f-5362a3da3c19,zonegroup_name=default&#125;</span><br></code></pre></td></tr></table></figure><p>没有问题了，然后直接访问即可</p><h2 id="待确认问题"><a href="#待确认问题" class="headerlink" title="待确认问题"></a>待确认问题</h2><ul><li>性能的变化</li><li>稳定性的变化</li><li>资源占用的变化</li></ul><p>使用前需要看下是不是有这个问题(周期性的崩溃)</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">https://tracker.ceph.com/issues/39660<br></code></pre></td></tr></table></figure><p>已经解决了的问题</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">https://github.com/ceph/ceph/pull/30746<br>https://github.com/ceph/ceph/pull/30746/files<br></code></pre></td></tr></table></figure><p>这个问题应该是14.2.4这个版本还存在，14.2.5以及之后的版本就解决这个崩溃的问题</p><p>beast增加参数来控制并发请求的</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">rgw max concurrent requests<br></code></pre></td></tr></table></figure><p>默认1024,最开始出来的适合8192后面调整了，这个可以控制一下高负债下的内存占用，相当于qos的作用<br>civetweb是通过前端的线程来控制的</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">max_connection_backlog<br></code></pre></td></tr></table></figure><p>这个参数也是后面增加来控制连接数的，12的版本还没有这个功能</p><h2 id="luminous版本"><a href="#luminous版本" class="headerlink" title="luminous版本"></a>luminous版本</h2><p>从测试来看，ab压3000的连接的时候，开始通过rgw上面的ss查询连接数目稳定在500作用，过一会就增加了，然后到1000左右就崩掉了，这个应该跟beast使用的boost库有关系，上面提到了后面的版本增加了参数的配置，默认的就是boost自己自带的参数的</p><h2 id="变更记录"><a href="#变更记录" class="headerlink" title="变更记录"></a>变更记录</h2><table><thead><tr><th align="center">Why</th><th align="center">Who</th><th align="center">When</th></tr></thead><tbody><tr><td align="center">创建</td><td align="center">武汉-运维-磨渣</td><td align="center">2020-09-11</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>配置内网访问的TV</title>
    <link href="/2020/09/10/%E9%85%8D%E7%BD%AE%E5%86%85%E7%BD%91%E8%AE%BF%E9%97%AE%E7%9A%84TV/"/>
    <url>/2020/09/10/%E9%85%8D%E7%BD%AE%E5%86%85%E7%BD%91%E8%AE%BF%E9%97%AE%E7%9A%84TV/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>通过内网模式访问tv远程机器</p><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><h3 id="云主机配置"><a href="#云主机配置" class="headerlink" title="云主机配置"></a>云主机配置</h3><p>一台云主机，云主机申请两个公网IP</p><p>云主机启动两个frps进程绑定到两个内网的ip</p><h3 id="客户端配置"><a href="#客户端配置" class="headerlink" title="客户端配置"></a>客户端配置</h3><p>远程一台linux跳板机运行frpc，启动两个进程，分别访问不同的公网IP</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">[tv]<br><span class="hljs-built_in">type</span> = tcp<br>local_ip = 192.168.1.118<br>local_port = 5938<br>remote_port = 5938<br></code></pre></td></tr></table></figure><p>绑定好端口5938</p><p>开启好TV的lan访问<br>通过TV客户端输入公网的IP就可以连接到内网的tv上面了</p><h2 id="好处"><a href="#好处" class="headerlink" title="好处"></a>好处</h2><ul><li>避免商业检测</li><li>不用找破解版本</li><li>访问流畅</li></ul><h2 id="变更记录"><a href="#变更记录" class="headerlink" title="变更记录"></a>变更记录</h2><table><thead><tr><th align="center">Why</th><th align="center">Who</th><th align="center">When</th></tr></thead><tbody><tr><td align="center">创建</td><td align="center">武汉-运维-磨渣</td><td align="center">2020-09-10</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>关于vm.min_free_kbytes的合理设置推测</title>
    <link href="/2020/09/09/%E5%85%B3%E4%BA%8Evm.min_free_kbytes%E7%9A%84%E5%90%88%E7%90%86%E8%AE%BE%E7%BD%AE%E6%8E%A8%E6%B5%8B/"/>
    <url>/2020/09/09/%E5%85%B3%E4%BA%8Evm.min_free_kbytes%E7%9A%84%E5%90%88%E7%90%86%E8%AE%BE%E7%BD%AE%E6%8E%A8%E6%B5%8B/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>之前系统出现过几次hung住的情况，没有oom，也没有其它内存相关的信息，而linux设计就是去尽量吃满内存，然后再回收清理的机制</p><h2 id="探讨"><a href="#探讨" class="headerlink" title="探讨"></a>探讨</h2><p>目前这个参数还没有找到合适的处理这个预留的参数，一般也没有去调整的<br>系统是默认根据物理内存进行计算得到一个数值得</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">sysctl -a|grep min_free_kbytes<br>vm.min_free_kbytes = 45056<br></code></pre></td></tr></table></figure><p>查看内核参数，这个小环境是保留的45M</p><h2 id="网上的一些说法"><a href="#网上的一些说法" class="headerlink" title="网上的一些说法"></a>网上的一些说法</h2><h3 id="Aerospike-的说法"><a href="#Aerospike-的说法" class="headerlink" title="Aerospike 的说法"></a>Aerospike 的说法</h3><p><a href="https://discuss.aerospike.com/t/how-to-tune-the-linux-kernel-for-memory-performance/4195">https://discuss.aerospike.com/t/how-to-tune-the-linux-kernel-for-memory-performance/4195</a></p><blockquote><p>The standard RedHat recommendation 204 is to keep min_free_kbytes at 1-3% of the total memory on the system, with Aerospike advising to keep at least 1.1GB, even if that is above the official recommended total memory percentage.</p></blockquote><blockquote><p>On a system with over 37GB of total RAM, you should leave no more than 3% of spare memory to min_free_kbytes in order to avoid the kernel spending too much time unnecessarily reclaiming memory. This would equal anywhere between 1.1GB and 3% of total RAM on such systems.</p></blockquote><p>上面的说法是如果环境内存超过37G的情况下，按3%算就是1.1G，我们一般的环境也超过了40G，那么基本就是建议最少留个1.1G的，100G的可以保留到3G左右</p><p>内核参数</p><p>vm.min_free_kbytes &#x3D; 1153434<br>vm.min_free_kbytes &#x3D; 3145728 </p><h3 id="红帽的说法"><a href="#红帽的说法" class="headerlink" title="红帽的说法"></a>红帽的说法</h3><p><a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/performance_tuning_guide/sect-red_hat_enterprise_linux-performance_tuning_guide-configuration_tools-configuring_system_memory_capacity">https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/performance_tuning_guide/sect-red_hat_enterprise_linux-performance_tuning_guide-configuration_tools-configuring_system_memory_capacity</a></p><blockquote><p>Setting min_free_kbytes too low prevents the system from reclaiming memory. This can <strong>result in system hangs</strong> and OOM-killing multiple processes.</p></blockquote><blockquote><p>However, setting min_free_kbytes too high (for example, to 5–10% of total system memory) causes the system to enter an out-of-memory state immediately, resulting in the system spending too much time reclaiming memory.</p></blockquote><p>红帽的说法是需要低于总内存的5%</p><h3 id="ltp测试里面的参数控制"><a href="#ltp测试里面的参数控制" class="headerlink" title="ltp测试里面的参数控制"></a>ltp测试里面的参数控制</h3><p><a href="https://sourceforge.net/p/ltp/mailman/message/29738250/">https://sourceforge.net/p/ltp/mailman/message/29738250/</a></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">Setting min_free_kbytes too high will cause system hangs,<br>especially <span class="hljs-keyword">in</span> i386 <span class="hljs-built_in">arch</span>, using less than 5% of total memory<br>can avoid it, so choose %5 of free memory or 2% of total memory.<br>Thanks Shuang pointed out it.<br></code></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs bash">* Description:<br>*<br>* The <span class="hljs-keyword">case</span> is designed to <span class="hljs-built_in">test</span> min_free_kbytes tunable.<br>*<br>* The tune is used to control free memory, and system always<br>* reserve min_free_kbytes memory at least.<br>*<br>* Since the tune is not too large or too little, <span class="hljs-built_in">which</span> will<br>* lead to the system hang, so I choose two cases, and <span class="hljs-built_in">test</span> them<br>* on all overcommit_memory policy, at the same time, compare<br>* the current free memory with the tunable value repeatedly.<br><br><br>* a) default min_free_kbytes with all overcommit memory policy<br>* b) 2x default value with all overcommit memory policy<br>* c) 5% of MemFree or %2 MemTotal with all overcommit memory policy<br></code></pre></td></tr></table></figure><p>测试用例里面测试内存过载情况下的几种参数，默认，两倍默认，5%空闲内存，或者总内存的2%，理论上，这几个都不会导致机器hung死</p><h2 id="其它知识"><a href="#其它知识" class="headerlink" title="其它知识"></a>其它知识</h2><p>通过slabtop查看内核的缓存空间占用</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@VM_0_17_centos ~]<span class="hljs-comment"># slabtop -o|grep Total</span><br> Active / Total Objects (% used)    : 550057 / 573695 (95.9%)<br> Active / Total Slabs (% used)      : 22507 / 22507 (100.0%)<br> Active / Total Caches (% used)     : 101 / 135 (74.8%)<br> Active / Total Size (% used)       : 102508.62K / 106202.81K (96.5%)<br></code></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@VM_0_17_centos ~]<span class="hljs-comment"># grep Slab /proc/meminfo</span><br>Slab:             108676 kB<br></code></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@VM_0_17_centos ~]<span class="hljs-comment"># sysctl -a|grep min_free_kbytes</span><br>vm.min_free_kbytes = 45056<br></code></pre></td></tr></table></figure><p>这个上面是腾讯云主机的，看到内核自身的占用应该在100M以上了，而我自己的vmware里面的虚拟机这个数值是47MB，这个数值可能跟不同的内核有关</p><p>系统还保留了一定的内存防止</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">Reserving 161MB of memory at 688MB <span class="hljs-keyword">for</span> crashkernel (System RAM: 2047MB)<br></code></pre></td></tr></table></figure><p>系统启动的时候看到的内存占用</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab204 ~]<span class="hljs-comment"># dmesg |grep Memory:</span><br>[    0.000000] Memory: 1841584k/2097152k available (7784k kernel code, 524k absent, 255044k reserved, 5958k data, 1980k init)<br></code></pre></td></tr></table></figure><p>内核文档关于这个参数的解释<br><a href="https://www.kernel.org/doc/Documentation/sysctl/vm.txt">https://www.kernel.org/doc/Documentation/sysctl/vm.txt</a></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs bash">min_free_kbytes:<br><br>This is used to force the Linux VM to keep a minimum number<br>of kilobytes free.  The VM uses this number to compute a<br>watermark[WMARK_MIN] value <span class="hljs-keyword">for</span> each lowmem zone <span class="hljs-keyword">in</span> the system.<br>Each lowmem zone gets a number of reserved free pages based<br>proportionally on its size.<br><br>Some minimal amount of memory is needed to satisfy PF_MEMALLOC<br>allocations; <span class="hljs-keyword">if</span> you <span class="hljs-built_in">set</span> this to lower than 1024KB, your system will<br>become subtly broken, and prone to deadlock under high loads.<br><br>Setting this too high will OOM your machine instantly.<br></code></pre></td></tr></table></figure><h2 id="基于以上暂时推测"><a href="#基于以上暂时推测" class="headerlink" title="基于以上暂时推测"></a>基于以上暂时推测</h2><p>建议能保留1G以上的空间</p><h2 id="变更记录"><a href="#变更记录" class="headerlink" title="变更记录"></a>变更记录</h2><table><thead><tr><th align="center">Why</th><th align="center">Who</th><th align="center">When</th></tr></thead><tbody><tr><td align="center">创建</td><td align="center">武汉-运维-磨渣</td><td align="center">2020-09-09</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>rbd的删除回收站功能</title>
    <link href="/2020/09/09/rbd%E7%9A%84%E5%88%A0%E9%99%A4%E5%9B%9E%E6%94%B6%E7%AB%99%E5%8A%9F%E8%83%BD/"/>
    <url>/2020/09/09/rbd%E7%9A%84%E5%88%A0%E9%99%A4%E5%9B%9E%E6%94%B6%E7%AB%99%E5%8A%9F%E8%83%BD/</url>
    
    <content type="html"><![CDATA[<p>##前言<br>rbd 提供了一个回收站功能，这个是属于防呆设计，防止误操作删除rbd引起无法恢复的情况，rbd正常情况下的删除是马上会在后台回收空间的，这个也听说过有人做过误删除的操作，那么这个设计就是从操作逻辑上来尽量避免这个失误的</p><h2 id="相关操作"><a href="#相关操作" class="headerlink" title="相关操作"></a>相关操作</h2><p>命令比较简单就几条命令，luminous版本就支持了，提供几个命令</p><ul><li>trash list (trash ls) 列出回收站的rbd</li><li>trash move (trash mv)通过回收接口删除rbd</li><li>trash remove (trash rm)删除回收站里面的rbd</li><li>trash restore 还原回收站的rbd</li></ul><p>其中move操作里面是支持delay的</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab201 ~]<span class="hljs-comment"># rbd  help  trash move |grep delay</span><br>usage: rbd trash move [--pool &lt;pool&gt;] [--image &lt;image&gt;] [--delay &lt;delay&gt;] <br>  --delay arg          time delay <span class="hljs-keyword">in</span> seconds <span class="hljs-keyword">until</span> effectively remove the image<br></code></pre></td></tr></table></figure><p>这个delay不是说delay多久后会删除，而是设置进入回收站以后，在多长时间之后是可以去删除回收站的rbd的，如果没到这个delay的时间是不能直接删除的，需要加上force强制命令的，这个地方也就是可以设置一个时间，比如一天，今天删除的，今天不能正常清理回收站，明天再清理，也提供了强制命令，但是操作逻辑上面是默认有个宽限期的</p><h2 id="相关的测试"><a href="#相关的测试" class="headerlink" title="相关的测试"></a>相关的测试</h2><h3 id="创建rbd"><a href="#创建rbd" class="headerlink" title="创建rbd"></a>创建rbd</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">rbd create <span class="hljs-built_in">test</span> --size 8G<br></code></pre></td></tr></table></figure><h3 id="删除到回收站并设置延时时间"><a href="#删除到回收站并设置延时时间" class="headerlink" title="删除到回收站并设置延时时间"></a>删除到回收站并设置延时时间</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">rbd    trash move <span class="hljs-built_in">test</span> --delay  7200<br></code></pre></td></tr></table></figure><h3 id="查看回收站列表"><a href="#查看回收站列表" class="headerlink" title="查看回收站列表"></a>查看回收站列表</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab201 ~]<span class="hljs-comment"># rbd trash list</span><br>166a16b8b4567 <span class="hljs-built_in">test</span><br></code></pre></td></tr></table></figure><h3 id="删除回收站的rbd"><a href="#删除回收站的rbd" class="headerlink" title="删除回收站的rbd"></a>删除回收站的rbd</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab201 ~]<span class="hljs-comment"># rbd    trash remove 166a16b8b4567</span><br>Deferment time has not expired, please use --force <span class="hljs-keyword">if</span> you really want to remove the image<br>Removing image: 0% complete...failed.<br>2020-09-09 11:40:15.571122 7f51d7677d40 -1 librbd: error: deferment time has not expired.<br></code></pre></td></tr></table></figure><h3 id="强制删除回收站的rbd"><a href="#强制删除回收站的rbd" class="headerlink" title="强制删除回收站的rbd"></a>强制删除回收站的rbd</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab201 ~]<span class="hljs-comment"># rbd    trash remove 166a16b8b4567 --force</span><br>Removing image: 100% complete...done.<br></code></pre></td></tr></table></figure><h3 id="还原操作"><a href="#还原操作" class="headerlink" title="还原操作"></a>还原操作</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab201 ~]<span class="hljs-comment"># rbd    trash restore 166a76b8b4567</span><br></code></pre></td></tr></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>回收站的功能能够比较好的去避免误操作，这样即使误删除了，也留了一定的时间可以去处理，从而从逻辑上更安全了一些</p><h2 id="变更记录"><a href="#变更记录" class="headerlink" title="变更记录"></a>变更记录</h2><table><thead><tr><th align="center">Why</th><th align="center">Who</th><th align="center">When</th></tr></thead><tbody><tr><td align="center">创建</td><td align="center">武汉-运维-磨渣</td><td align="center">2020-09-09</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>ceph打印出每秒的IO和pg状态</title>
    <link href="/2020/09/09/ceph%E6%89%93%E5%8D%B0%E5%87%BA%E6%AF%8F%E7%A7%92%E7%9A%84IO%E5%92%8Cpg%E7%8A%B6%E6%80%81/"/>
    <url>/2020/09/09/ceph%E6%89%93%E5%8D%B0%E5%87%BA%E6%AF%8F%E7%A7%92%E7%9A%84IO%E5%92%8Cpg%E7%8A%B6%E6%80%81/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>在ceph 的jewel版本以及之前的版本，通过ceph -w命令是可以拿到每秒钟ceph集群的io状态的，现在的版本是ceph -s一秒秒手动去刷，ceph -w也不监控io的状态了，有的时候需要看io是否平滑，或者恢复还剩多少，能够比较直观的去看</p><p>实际上通过简单的脚本就可以实现之前差不多的效果</p><h2 id="每秒查看状态"><a href="#每秒查看状态" class="headerlink" title="每秒查看状态"></a>每秒查看状态</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab201 ~]<span class="hljs-comment"># sh ceph-s.sh </span><br>Wed Sep  9 10:44:57 CST 2020192 pgs: 192 active+clean; 1.30GiB data, 13.3GiB used, 287GiB / 300GiB avail; 93.3KiB/s rd, 0B/s wr, 155op/s<br>Wed Sep  9 10:44:58 CST 2020192 pgs: 192 active+clean; 1.30GiB data, 13.3GiB used, 287GiB / 300GiB avail; 91.7KiB/s rd, 0B/s wr, 152op/s<br>Wed Sep  9 10:45:00 CST 2020192 pgs: 192 active+clean; 1.30GiB data, 13.3GiB used, 287GiB / 300GiB avail; 94.6KiB/s rd, 0B/s wr, 157op/s<br>Wed Sep  9 10:45:01 CST 2020192 pgs: 192 active+clean; 1.30GiB data, 13.3GiB used, 287GiB / 300GiB avail; 94.6KiB/s rd, 0B/s wr, 157op/s<br>Wed Sep  9 10:45:02 CST 2020192 pgs: 192 active+clean; 1.30GiB data, 13.3GiB used, 287GiB / 300GiB avail; 88.3KiB/s rd, 0B/s wr, 147op/s<br>Wed Sep  9 10:45:03 CST 2020192 pgs: 192 active+clean; 1.30GiB data, 13.3GiB used, 287GiB / 300GiB avail; 88.3KiB/s rd, 0B/s wr, 147op/s<br>Wed Sep  9 10:45:04 CST 2020192 pgs: 192 active+clean; 1.30GiB data, 13.3GiB used, 287GiB / 300GiB avail; 69.6KiB/s rd, 0B/s wr, 115op/s<br>Wed Sep  9 10:45:06 CST 2020192 pgs: 192 active+clean; 1.30GiB data, 13.3GiB used, 287GiB / 300GiB avail; 92.6KiB/s rd, 0B/s wr, 154op/s<br>Wed Sep  9 10:45:07 CST 2020192 pgs: 192 active+clean; 1.30GiB data, 13.3GiB used, 287GiB / 300GiB avail; 92.6KiB/s rd, 0B/s wr, 154op/s<br>Wed Sep  9 10:45:08 CST 2020192 pgs: 192 active+clean; 1.30GiB data, 13.3GiB used, 287GiB / 300GiB avail; 90.0KiB/s rd, 0B/s wr, 150op/s<br>Wed Sep  9 10:45:09 CST 2020192 pgs: 192 active+clean; 1.30GiB data, 13.3GiB used, 287GiB / 300GiB avail; 90.0KiB/s rd, 0B/s wr, 150op/s<br>Wed Sep  9 10:45:10 CST 2020192 pgs: 192 active+clean; 1.30GiB data, 13.3GiB used, 287GiB / 300GiB avail; 90.1KiB/s rd, 0B/s wr, 150op/s<br>Wed Sep  9 10:45:12 CST 2020192 pgs: 192 active+clean; 1.30GiB data, 13.3GiB used, 287GiB / 300GiB avail; 92.8KiB/s rd, 0B/s wr, 154op/s<br>Wed Sep  9 10:45:13 CST 2020192 pgs: 192 active+clean; 1.30GiB data, 13.3GiB used, 287GiB / 300GiB avail; 92.8KiB/s rd, 0B/s wr, 154op/s<br></code></pre></td></tr></table></figure><p>需要有时间，需要每秒的状态方便比对，如上所示</p><h2 id="脚本"><a href="#脚本" class="headerlink" title="脚本"></a>脚本</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab201 ~]<span class="hljs-comment"># cat ceph-s.sh </span><br><span class="hljs-comment">#!/bin/bash</span><br>LANG=C<br>PATH=/sbin:/usr/sbin:/bin:/usr/bin<br>interval=1<br>length=86400<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> $(<span class="hljs-built_in">seq</span> 1 $(<span class="hljs-built_in">expr</span> <span class="hljs-variable">$&#123;length&#125;</span> / <span class="hljs-variable">$&#123;interval&#125;</span>));<span class="hljs-keyword">do</span><br><span class="hljs-built_in">date</span>=`<span class="hljs-built_in">date</span>`<br><span class="hljs-built_in">echo</span> -n <span class="hljs-string">&quot;<span class="hljs-variable">$date</span>&quot;</span><br>ceph pg <span class="hljs-built_in">stat</span><br><span class="hljs-built_in">sleep</span> <span class="hljs-variable">$&#123;interval&#125;</span><br><span class="hljs-keyword">done</span><br></code></pre></td></tr></table></figure><p>可以自行调整中间的间隔</p><h2 id="变更记录"><a href="#变更记录" class="headerlink" title="变更记录"></a>变更记录</h2><table><thead><tr><th align="center">Why</th><th align="center">Who</th><th align="center">When</th></tr></thead><tbody><tr><td align="center">创建</td><td align="center">武汉-运维-磨渣</td><td align="center">2020-09-09</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>rgw配置删除快速回收对象</title>
    <link href="/2020/09/08/rgw%E9%85%8D%E7%BD%AE%E5%88%A0%E9%99%A4%E5%BF%AB%E9%80%9F%E5%9B%9E%E6%94%B6%E5%AF%B9%E8%B1%A1/"/>
    <url>/2020/09/08/rgw%E9%85%8D%E7%BD%AE%E5%88%A0%E9%99%A4%E5%BF%AB%E9%80%9F%E5%9B%9E%E6%94%B6%E5%AF%B9%E8%B1%A1/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>做rgw测试的时候，经常会有删除文件的操作，而用默认的参数的时候，rgw是通过gc回收机制来处理删除对象的，这个对于生产环境是有好处的，把删除对业务系统的压力分摊到不同的时间点，但是测试的时候，可能需要反复的写入删除，而这种情况下，可能希望能够迅速的回收对象，那么就需要修改几个参数了</p><h2 id="参数"><a href="#参数" class="headerlink" title="参数"></a>参数</h2><p>主要调整下面的几个参数</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">rgw_gc_obj_min_wait = 30<br>rgw_gc_processor_max_time = 180<br>rgw_gc_processor_period = 2<br></code></pre></td></tr></table></figure><p>删除的步骤如下</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">1、删除以后进入gc队列<br>2、等待rgw_gc_obj_min_wait时间后开始处理<br>3、执行rgw_gc_processor_max_time时间的gc操作<br>4、等待rgw_gc_processor_period时间进入下一次gc操作<br></code></pre></td></tr></table></figure><p>调整以后，删除就能够很快的回收了</p><p>查询删除以后执行gc的时间</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab201 ~]<span class="hljs-comment"># radosgw-admin gc list --include-all|grep time;date</span><br>        <span class="hljs-string">&quot;time&quot;</span>: <span class="hljs-string">&quot;2020-09-08 17:26:20.0.873326s&quot;</span>,<br>Tue Sep  8 17:25:52 CST 2020<br></code></pre></td></tr></table></figure><p>可以看到上面执行删除操作后，通过命令查询下一次gc的时间与当前的时间差就是30s左右</p><p>##上面的gc相关的参数是哪里控制的<br>做了一个实验，通过给不同的rgw网关配置不同的gc参数，然后通过不同的网关去删除数据，可以看到，这个删除是由网关决定的,下面的是默认的参数做了删除操作以后查看删除的时间，可以看到是两个小时以后才开始删除，也就是rgw_gc_obj_min_wait &#x3D; 7200</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab201 ~]<span class="hljs-comment"># radosgw-admin gc list --include-all|grep time;date</span><br>        <span class="hljs-string">&quot;time&quot;</span>: <span class="hljs-string">&quot;2020-09-08 19:31:01.0.24764s&quot;</span>,<br>Tue Sep  8 17:31:10 CST 2020<br></code></pre></td></tr></table></figure><p>那么这里实际上是可以给不同的网关配置不同的参数的，如果想要立刻回收空间的时候，可以配置回收快的参数，然后通过这个网关的端口去删除</p><p>控制删除线程的参数</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">rgw_gc_max_concurrent_io = 10<br></code></pre></td></tr></table></figure><p>大量删除的时候可以考虑增大这个参数</p><h2 id="变更记录"><a href="#变更记录" class="headerlink" title="变更记录"></a>变更记录</h2><table><thead><tr><th align="center">Why</th><th align="center">Who</th><th align="center">When</th></tr></thead><tbody><tr><td align="center">创建</td><td align="center">武汉-运维-磨渣</td><td align="center">2020-09-08</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>ipmi常用的命令行命令</title>
    <link href="/2020/09/07/ipmi%E5%B8%B8%E7%94%A8%E7%9A%84%E5%91%BD%E4%BB%A4%E8%A1%8C%E5%91%BD%E4%BB%A4/"/>
    <url>/2020/09/07/ipmi%E5%B8%B8%E7%94%A8%E7%9A%84%E5%91%BD%E4%BB%A4%E8%A1%8C%E5%91%BD%E4%BB%A4/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>记录一些常用的命令行操作</p><h2 id="命令"><a href="#命令" class="headerlink" title="命令"></a>命令</h2><h3 id="查询机器的电源状态"><a href="#查询机器的电源状态" class="headerlink" title="查询机器的电源状态"></a>查询机器的电源状态</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">ipmitool -I lanplus -U admin -P admin -H 172.16.21.215 power status<br></code></pre></td></tr></table></figure><h3 id="硬重启机器"><a href="#硬重启机器" class="headerlink" title="硬重启机器"></a>硬重启机器</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">ipmitool -I lanplus -U admin -P admin -H 172.16.21.215 power reset<br></code></pre></td></tr></table></figure><h3 id="关闭机器"><a href="#关闭机器" class="headerlink" title="关闭机器"></a>关闭机器</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">ipmitool -I lanplus -U admin -P admin -H 172.16.21.215 power off<br></code></pre></td></tr></table></figure><h3 id="开启机器"><a href="#开启机器" class="headerlink" title="开启机器"></a>开启机器</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">ipmitool -I lanplus -U admin -P admin -H 172.16.21.215 power on<br></code></pre></td></tr></table></figure><h2 id="变更记录"><a href="#变更记录" class="headerlink" title="变更记录"></a>变更记录</h2><table><thead><tr><th align="center">Why</th><th align="center">Who</th><th align="center">When</th></tr></thead><tbody><tr><td align="center">创建</td><td align="center">武汉-运维-磨渣</td><td align="center">2020-09-07</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>自动化翻译ceph文档</title>
    <link href="/2020/09/04/%E8%87%AA%E5%8A%A8%E5%8C%96%E7%BF%BB%E8%AF%91ceph%E6%96%87%E6%A1%A3/"/>
    <url>/2020/09/04/%E8%87%AA%E5%8A%A8%E5%8C%96%E7%BF%BB%E8%AF%91ceph%E6%96%87%E6%A1%A3/</url>
    
    <content type="html"><![CDATA[<p>需求很简单，翻译官网的操作文档</p><h2 id="下载ceph代码luminous版本"><a href="#下载ceph代码luminous版本" class="headerlink" title="下载ceph代码luminous版本"></a>下载ceph代码luminous版本</h2><p>这个只用来编译doc的，我们只需要最新的这个分支即可，拉最少的代码</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">git <span class="hljs-built_in">clone</span> -b v12.2.13 --single-branch --depth 1  git://github.com/ceph/ceph.git ceph12<br><span class="hljs-built_in">cd</span> ceph12<br>git checkout -b v12.2.13<br></code></pre></td></tr></table></figure><p>代码就下好了</p><p>根据官网的文档可以知道，官网的文档的语法是Sphinx写的文档，然后通过工具可以渲染成固定的网页</p><blockquote><p><a href="https://docs.ceph.com/docs/luminous/dev/generatedocs/">https://docs.ceph.com/docs/luminous/dev/generatedocs/</a></p></blockquote><p>而Sphinx写的文档是支持国际化的，也就是我们只需要通过固定的文件去修改翻译文本就可以实现整个文档的翻译，而翻译的过程是可以通过发送到google，然后拿到结果再填写回去的方式来处理</p><p>安装依赖包</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">cd</span> admin<br>./build-doc<br></code></pre></td></tr></table></figure><p>根据提示安装依赖包，有一个ditaa安装不上</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">wget ftp://ftp.pbone.net/mirror/archive.fedoraproject.org/fedora-secondary/releases/20/Everything/ppc64/os/Packages/d/ditaa-0.9-10.r74.fc20.noarch.rpm<br>wget https://archives.fedoraproject.org/pub/archive/fedora/linux/releases/20/Everything/x86_64/os/Packages/j/jericho-html-3.2-6.fc20.noarch.rpm<br>rpm -ivh jericho-html-3.2-6.fc20.noarch.rpm<br>yum localinstall ditaa-0.9-10.r74.fc20.noarch.rpm<br></code></pre></td></tr></table></figure><p>修改build-doc脚本的60行<br>virtualenv修改为</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">virtualenv-3<br></code></pre></td></tr></table></figure><p>生成的文档在</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 admin]<span class="hljs-comment"># ls ../build-doc/output/html/</span><br></code></pre></td></tr></table></figure><p>执行</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">./serve-doc<br></code></pre></td></tr></table></figure><p>访问主机的8080端口</p><p>就可以看到一个原版的文档了</p><p>我们把这个备份下，方便后面比对翻译的效果</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 admin]<span class="hljs-comment"># unalias cp</span><br>[root@lab101 admin]<span class="hljs-comment"># cp -ra  ../build-doc/output/html/* /usr/share/nginx/html/</span><br>[root@lab101 admin]<span class="hljs-comment"># systemctl start nginx</span><br></code></pre></td></tr></table></figure><p>脚本里面还有其它的一些操作，实际上主要的编译命令是这个命令 </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">/root/ceph/ceph12/build-doc/virtualenv/bin/sphinx-build -a -b dirhtml -d doctrees /root/ceph/ceph12/doc /root/ceph/ceph12/build-doc/output/html<br></code></pre></td></tr></table></figure><p>我们的版本</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 admin]<span class="hljs-comment"># /root/ceph/ceph12/build-doc/virtualenv/bin/sphinx-build  --version</span><br>sphinx-build 2.1.2<br></code></pre></td></tr></table></figure><p>安装国际化的处理软件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">/root/ceph/ceph12/build-doc/virtualenv/bin/pip3 install sphinx-intl<br></code></pre></td></tr></table></figure><p>提取pot文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">/root/ceph/ceph12/build-doc/virtualenv/bin/sphinx-build -b gettext /root/ceph/ceph12/doc locale<br></code></pre></td></tr></table></figure><p>根据pot文件生成zh的po文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">/root/ceph/ceph12/build-doc/virtualenv/bin/sphinx-intl update -p locale/ -l zh<br></code></pre></td></tr></table></figure><p>然后需要修改翻译就修改po文件</p><p>会在本地生成locales文件夹，把文件夹拷贝到doc目录下面</p><p>修改doc&#x2F;conf.py文件</p><p>添加三个设置配置</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">locale_dirs = [<span class="hljs-string">&#x27;locales/&#x27;</span>]<br>gettext_compact = False<br>language= <span class="hljs-string">&#x27;zh_CN&#x27;</span><br></code></pre></td></tr></table></figure><p>然后编译的就会取翻译中文的html</p><h3 id="处理po文件"><a href="#处理po文件" class="headerlink" title="处理po文件"></a>处理po文件</h3><p>下一步处理po文件，通过python处理</p><p>安装软件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">pip-3 install polib<br></code></pre></td></tr></table></figure><p>po文件的完整路径</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">ll /root/ceph/ceph12/doc/locales/zh_CN/LC_MESSAGES/<br></code></pre></td></tr></table></figure><p>获取po文件的完整列表</p><h3 id="polib的使用方法"><a href="#polib的使用方法" class="headerlink" title="polib的使用方法"></a>polib的使用方法</h3><p>这里用翻译po文件的代码举例</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-meta">#! /usr/bin/python3</span><br><span class="hljs-comment"># -* coding:UTF-8 -*-</span><br>import time<br>import sys<br>import polib<br>import sys<br> <br>import add_space<br><br>sys.path.append(<span class="hljs-string">&#x27;../&#x27;</span>)<br>from google_translate import translate_google<br><br><span class="hljs-comment">#translate_google.translate_text_with_glossary(&quot;mozha,hello world&quot;)</span><br><br>def translate_po(file_path):<br>    po = polib.pofile(file_path)<br>    <span class="hljs-keyword">for</span> entry <span class="hljs-keyword">in</span> po:<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;原始字符串:&quot;</span>)<br>        <span class="hljs-built_in">print</span>(entry.msgid)<br>        mystringa=entry.msgid<br>        newmsg=translate_google.translate_text_with_glossary(<span class="hljs-string">&quot;%s&quot;</span> %(mystringa))<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;翻译后未处理的字符串:&quot;</span>)<br>        <span class="hljs-built_in">print</span>(newmsg)<br>        newmsg=add_space.add_string_space(newmsg)<br>        entry.msgstr = newmsg<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;翻译后的字符串:&quot;</span>)<br>        <span class="hljs-built_in">print</span>(entry.msgstr)<br>        time.sleep(0.1)<br>        po.save(file_path)<br><br>translate_po(sys.argv[1])<br></code></pre></td></tr></table></figure><p>这里用的比较少，流程是读取po文件，拿到原始字符串，然后翻译，再回写po文件，再进行保存，就完成一个po文件的处理了</p><h3 id="google-的translate的用法"><a href="#google-的translate的用法" class="headerlink" title="google 的translate的用法"></a>google 的translate的用法</h3><p>在凭据里面的服务账号里面创建一个密钥用于访问翻译的api<br>保存下来是一个json的文件</p><p>设置环境变量</p><p>我的是</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">export</span> GOOGLE_APPLICATION_CREDENTIALS=<span class="hljs-string">&quot;/root/ceph/ceph12/admin/translate/3-google-translate/zphj1987-translate-797fe0ff0849.json&quot;</span><br></code></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 google_translate]<span class="hljs-comment"># cat /etc/bashrc |tail -n 1</span><br><span class="hljs-built_in">export</span> GOOGLE_APPLICATION_CREDENTIALS=<span class="hljs-string">&quot;/root/ceph/ceph12/admin/translate/google_translate/zphj1987-translate-797fe0ff0849.json&quot;</span><br></code></pre></td></tr></table></figure><p>可以写到bashrc文件里面，登录命令行就直接可以使用了</p><p>安装python的翻译库</p><p>我们使用的是高级版本的</p><p>构建自己的术语表，需要上传到google的存储上面，然后告诉google翻译，去存储上面拿最新的存储的术语的文件，然后处理好了以后，返回成功</p><p>安装存储<br>官网推荐的是如下</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">pip-3 install --upgrade google-cloud-storage<br></code></pre></td></tr></table></figure><p>上面的版本有问题尝试用之前的版本</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">pip-3 install google-cloud-translate==2.0.2<br></code></pre></td></tr></table></figure><p>官网的文档没有更新到最新，匹配的是上面的2版本的</p><p>上传代码</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 3-google-translate]<span class="hljs-comment"># cat uoload-csv.py </span><br>from google.cloud import storage<br><br>def upload_blob(bucket_name, source_file_name, destination_blob_name):<br>    <span class="hljs-string">&quot;&quot;</span><span class="hljs-string">&quot;Uploads a file to the bucket.&quot;</span><span class="hljs-string">&quot;&quot;</span><br>    <span class="hljs-comment"># bucket_name = &quot;your-bucket-name&quot;</span><br>    <span class="hljs-comment"># source_file_name = &quot;local/path/to/file&quot;</span><br>    <span class="hljs-comment"># destination_blob_name = &quot;storage-object-name&quot;</span><br><br>    storage_client = storage.Client()<br>    bucket = storage_client.bucket(bucket_name)<br>    blob = bucket.blob(destination_blob_name)<br><br>    blob.upload_from_filename(source_file_name)<br><br>    <span class="hljs-built_in">print</span>(<br>        <span class="hljs-string">&quot;File &#123;&#125; uploaded to &#123;&#125;.&quot;</span>.format(<br>            source_file_name, destination_blob_name<br>        )<br>    )<br><br>upload_blob(<span class="hljs-string">&#x27;mytranslate&#x27;</span>,<span class="hljs-string">&#x27;./ceph-glo.csv&#x27;</span>, <span class="hljs-string">&#x27;ceph-glo.csv&#x27;</span>)<br></code></pre></td></tr></table></figure><p>上传好自己的词库，还要告诉Google 翻译进行更新，字库的处理需要写脚本去读取po文件，然后保留一些固定的格式</p><p>安装gloud</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">tee</span> -a /etc/yum.repos.d/google-cloud-sdk.repo &lt;&lt; <span class="hljs-string">EOM</span><br><span class="hljs-string">[google-cloud-sdk]</span><br><span class="hljs-string">name=Google Cloud SDK</span><br><span class="hljs-string">baseurl=https://packages.cloud.google.com/yum/repos/cloud-sdk-el7-x86_64</span><br><span class="hljs-string">enabled=1</span><br><span class="hljs-string">gpgcheck=1</span><br><span class="hljs-string">repo_gpgcheck=1</span><br><span class="hljs-string">gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg</span><br><span class="hljs-string">       https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg</span><br><span class="hljs-string">EOM</span><br></code></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">yum install google-cloud-sdk<br></code></pre></td></tr></table></figure><p>设置一个request文件来指定需要更新到哪个词库文件,然后先删除再更新即可</p><p>翻译的代码</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-meta">#! /usr/bin/python3</span><br><span class="hljs-comment"># -* coding:UTF-8 -*-</span><br>from google.cloud import translate_v3 as translate<br><br>def translate_text_with_glossary(<br>    text=<span class="hljs-string">&quot;YOUR_TEXT_TO_TRANSLATE&quot;</span>,<br>    project_id=<span class="hljs-string">&quot;zphj1987-translate&quot;</span>,<br>    glossary_id=<span class="hljs-string">&quot;myceph-en-zh&quot;</span>,<br>):<br>    client = translate.TranslationServiceClient()<br>    parent = client.location_path(project_id, <span class="hljs-string">&quot;us-central1&quot;</span>)<br>    glossary = client.glossary_path(<br>        project_id, <span class="hljs-string">&quot;us-central1&quot;</span>, glossary_id  <span class="hljs-comment"># The location of the glossary</span><br>    )<br><br>    glossary_config = translate.types.TranslateTextGlossaryConfig(<br>        glossary=glossary)<br>    response = client.translate_text(<br>        contents=[text],<br>        target_language_code=<span class="hljs-string">&quot;zh&quot;</span>,<br>        source_language_code=<span class="hljs-string">&quot;en&quot;</span>,<br>        parent=parent,<br>        glossary_config=glossary_config,<br>    )<br>    <span class="hljs-keyword">for</span> translation <span class="hljs-keyword">in</span> response.glossary_translations:<br>        <span class="hljs-built_in">print</span>(u<span class="hljs-string">&quot;&#123;&#125;&quot;</span>.format(translation.translated_text))<br>translate_text_with_glossary(<span class="hljs-string">&quot;mozha,hello world&quot;</span>)<br><br></code></pre></td></tr></table></figure><p>可以去google 官网查询api的用法</p><p>词库的提取方法</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 glocsv]<span class="hljs-comment"># cat get_po_link_single.py</span><br><span class="hljs-comment">#! /usr/bing/python3</span><br><span class="hljs-comment">#-*- coding:UTF-8 -*-</span><br><br><span class="hljs-comment">#这个是处理`xxxx`_链接的</span><br><br>import polib<br>import re<br>import sys<br><br>po = polib.pofile(sys.argv[1])<br><span class="hljs-keyword">for</span> entry <span class="hljs-keyword">in</span> po:<br>    ms_string=entry.msgid<br>    <span class="hljs-keyword">for</span> rst_link <span class="hljs-keyword">in</span> re.findall(r<span class="hljs-string">&quot;`(.*?)`_&quot;</span>,ms_string):<br>        <span class="hljs-keyword">if</span> <span class="hljs-string">&#x27;`&#x27;</span> <span class="hljs-keyword">in</span> rst_link:<br>            <span class="hljs-keyword">if</span> rst_link.split(<span class="hljs-string">&#x27;`&#x27;</span>)[-1] == <span class="hljs-string">&quot;&quot;</span>:<br>                pass<br>            <span class="hljs-keyword">else</span>:<br>                <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;\&quot;`&quot;</span>+rst_link.split(<span class="hljs-string">&#x27;`&#x27;</span>)[-1]+<span class="hljs-string">&quot;`_\&quot;&quot;</span>+<span class="hljs-string">&quot;,&quot;</span>+<span class="hljs-string">&quot;\&quot;`&quot;</span>+rst_link.split(<span class="hljs-string">&#x27;`&#x27;</span>)[-1]+<span class="hljs-string">&quot;`_\&quot;&quot;</span> )<br>        <span class="hljs-keyword">elif</span> rst_link == <span class="hljs-string">&quot;&quot;</span>:<br>            pass<br>        <span class="hljs-keyword">else</span>:    <br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;\&quot;`&quot;</span>+rst_link+<span class="hljs-string">&quot;`_\&quot;&quot;</span>+<span class="hljs-string">&quot;,&quot;</span>+<span class="hljs-string">&quot;\&quot;`&quot;</span>+rst_link+<span class="hljs-string">&quot;`_\&quot;&quot;</span>)<br><br></code></pre></td></tr></table></figure><p>上面是处理单个po文件的，可以做好po文件的list，然后脚本去处理</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 glocsv]<span class="hljs-comment"># cat get_po_link_single.sh</span><br><span class="hljs-comment">#! /bin/sh</span><br><span class="hljs-built_in">rm</span> -rf po_link.csv<br><span class="hljs-keyword">for</span> pofile <span class="hljs-keyword">in</span> `<span class="hljs-built_in">cat</span> polist-do`<br><span class="hljs-keyword">do</span><br><span class="hljs-built_in">echo</span> <span class="hljs-variable">$pofile</span><br>python3 get_po_link_single.py <span class="hljs-variable">$pofile</span> &gt;&gt; po_link.csv.tmp<br><span class="hljs-keyword">done</span><br><br><span class="hljs-built_in">cat</span> po_link.csv.tmp|<span class="hljs-built_in">sort</span>|<span class="hljs-built_in">uniq</span> &gt; po_link.csv<br>sed -i <span class="hljs-string">&#x27;1 s/^/\xef\xbb\xbf&amp;/&#x27;</span> po_link.csv<br><span class="hljs-built_in">rm</span> -rf po_link.csv.tmp<br></code></pre></td></tr></table></figure><p>上面的是其中一个例子的情况</p><h3 id="需要特殊的处理哪些情况"><a href="#需要特殊的处理哪些情况" class="headerlink" title="需要特殊的处理哪些情况"></a>需要特殊的处理哪些情况</h3><p>翻译回来的中文会有问题，需要处理一些情况，主要的代码如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 translate_po]<span class="hljs-comment"># cat add_space.py </span><br><span class="hljs-comment">#! /usr/bin/python3</span><br><span class="hljs-comment"># -* coding:UTF-8 -*-</span><br>import re<br><br><span class="hljs-comment">##处理双引号无空格的问题</span><br><span class="hljs-comment"># 处理 **xxxxx**</span><br>def add_string_space(mystring):<br>    <span class="hljs-keyword">for</span> rst_link <span class="hljs-keyword">in</span> re.findall(r<span class="hljs-string">&quot;\*\*(.*?)\*\*&quot;</span>,mystring):<br>        newstring=<span class="hljs-string">&quot;**&quot;</span>+rst_link+<span class="hljs-string">&quot;**&quot;</span><br>        mystring=mystring.replace(<span class="hljs-string">&#x27;%s&#x27;</span> %(newstring),<span class="hljs-string">&#x27; %s &#x27;</span> %(newstring))<br><br><span class="hljs-comment"># 处理`xxxx`_</span><br>    <span class="hljs-keyword">for</span> rst_link <span class="hljs-keyword">in</span> re.findall(r<span class="hljs-string">&quot;`(.*?)`_&quot;</span>,mystring):<br>       <span class="hljs-comment"># print(rst_link)</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-string">&#x27;`&#x27;</span> <span class="hljs-keyword">in</span> rst_link:<br>            <span class="hljs-keyword">if</span> rst_link.split(<span class="hljs-string">&#x27;`&#x27;</span>)[-1] == <span class="hljs-string">&quot;&quot;</span>:<br>                pass<br>            <span class="hljs-keyword">else</span>:<br>                newstring=<span class="hljs-string">&#x27;`&#x27;</span>+rst_link.split(<span class="hljs-string">&#x27;`&#x27;</span>)[-1]+<span class="hljs-string">&quot;`_&quot;</span><br>                mystring=mystring.replace(<span class="hljs-string">&#x27;%s&#x27;</span> %(newstring),<span class="hljs-string">&#x27; %s &#x27;</span> %(newstring))<br>        <span class="hljs-keyword">elif</span> rst_link == <span class="hljs-string">&quot;&quot;</span>:<br>            pass<br>        <span class="hljs-keyword">else</span>:<br>            newstring=<span class="hljs-string">&quot;`&quot;</span>+rst_link+<span class="hljs-string">&quot;`_&quot;</span><br>            mystring=mystring.replace(<span class="hljs-string">&#x27;%s&#x27;</span> %(newstring),<span class="hljs-string">&#x27; %s &#x27;</span> %(newstring))<br>    <span class="hljs-comment">#print(mystring)</span><br><br><span class="hljs-comment"># 处理 :term:`xxx`</span><br>    <span class="hljs-keyword">for</span> rst_link <span class="hljs-keyword">in</span> re.findall(r<span class="hljs-string">&quot;:term:`(.*?)`&quot;</span>,mystring):<br>        newstring=<span class="hljs-string">&quot;:term:`&quot;</span>+rst_link+<span class="hljs-string">&quot;`&quot;</span><br>        mystring=mystring.replace(<span class="hljs-string">&#x27;%s&#x27;</span> %(newstring),<span class="hljs-string">&#x27; %s &#x27;</span> %(newstring))<br><br><span class="hljs-comment"># 处理``xxxx``</span><br>    <span class="hljs-keyword">for</span> rst_link <span class="hljs-keyword">in</span> re.findall(r<span class="hljs-string">&quot;``(.*?)``&quot;</span>,mystring):<br>        newstring=<span class="hljs-string">&quot;``&quot;</span>+rst_link+<span class="hljs-string">&quot;``&quot;</span><br>        mystring=mystring.replace(<span class="hljs-string">&#x27;%s&#x27;</span> %(newstring),<span class="hljs-string">&#x27; %s &#x27;</span> %(newstring))<br><span class="hljs-comment"># 处理:doc:`xxxx`</span><br>    <span class="hljs-keyword">for</span> rst_link <span class="hljs-keyword">in</span> re.findall(r<span class="hljs-string">&quot;:doc:`(.*?)`&quot;</span>,mystring):<br>        newstring=<span class="hljs-string">&quot;:doc:`&quot;</span>+rst_link+<span class="hljs-string">&quot;`&quot;</span><br>        mystring=mystring.replace(<span class="hljs-string">&#x27;%s&#x27;</span> %(newstring),<span class="hljs-string">&#x27; %s &#x27;</span> %(newstring))<br><br><span class="hljs-comment"># 处理其它情况</span><br>    mystring=mystring.replace(<span class="hljs-string">&#x27;。&#x27;</span>,<span class="hljs-string">&#x27;.&#x27;</span>)<br>    mystring=mystring.replace(<span class="hljs-string">&#x27;，&#x27;</span>,<span class="hljs-string">&#x27;,&#x27;</span>)<br>    mystring=mystring.replace(<span class="hljs-string">&#x27;”&#x27;</span>,<span class="hljs-string">&#x27;&#x27;</span>)<br>    mystring=mystring.replace(<span class="hljs-string">&#x27;“&#x27;</span>,<span class="hljs-string">&#x27;&#x27;</span>)<br>    mystring=mystring.replace(<span class="hljs-string">&#x27;（&#x27;</span>,<span class="hljs-string">&#x27;(&#x27;</span>)<br>    mystring=mystring.replace(<span class="hljs-string">&#x27;）&#x27;</span>,<span class="hljs-string">&#x27;)&#x27;</span>)<br>    mystring=mystring.replace(<span class="hljs-string">&#x27;\&#x27;</span>\&#x27;<span class="hljs-string">&#x27;,&#x27;</span><span class="hljs-string">&#x27;)</span><br><span class="hljs-string">    mystring=mystring.replace(&#x27;</span> ：<span class="hljs-string">&#x27;,&#x27;</span>:<span class="hljs-string">&#x27;)</span><br><span class="hljs-string">    mystring=mystring.replace(&#x27;</span>_   _<span class="hljs-string">&#x27;,&#x27;</span>_ <span class="hljs-string">&#x27;)</span><br><span class="hljs-string">    mystring=mystring.replace(&#x27;</span>的Ubuntu<span class="hljs-string">&#x27;,&#x27;</span>Ubuntu<span class="hljs-string">&#x27;)</span><br><span class="hljs-string">    mystring=mystring.replace(&#x27;</span>CentOS的<span class="hljs-string">&#x27;,&#x27;</span>CentOS<span class="hljs-string">&#x27;)    </span><br><span class="hljs-string">    mystring=mystring.replace(&#x27;</span>：项：` <span class="hljs-string">&#x27;,&#x27;</span><span class="hljs-string">&#x27;)</span><br><span class="hljs-string">    mystring=mystring.replace(&#x27;</span> 。<span class="hljs-string">&#x27;,&#x27;</span>. <span class="hljs-string">&#x27;)</span><br><span class="hljs-string">    mystring=mystring.replace(&#x27;</span>：<span class="hljs-string">&#x27;,&#x27;</span>:<span class="hljs-string">&#x27;)</span><br><span class="hljs-string">    mystring=mystring.replace(&#x27;</span>,<span class="hljs-string">&#x27;,&#x27;</span>，<span class="hljs-string">&#x27;)</span><br><span class="hljs-string">    mystring=mystring.replace(&#x27;</span>/  ``<span class="hljs-string">&#x27;,&#x27;</span> ``<span class="hljs-string">&#x27;)</span><br><span class="hljs-string">    mystring=mystring.replace(&#x27;</span>Bug Tracker_<span class="hljs-string">&#x27;,&#x27;</span> Bug Tracker_ <span class="hljs-string">&#x27;)</span><br><span class="hljs-string">    mystring=mystring.replace(&#x27;</span>ceph-users@ceph.com<span class="hljs-string">&#x27;,&#x27;</span> ceph-users@ceph.com <span class="hljs-string">&#x27;)</span><br><span class="hljs-string">    mystring=mystring.replace(&#x27;</span>http://ceph.com/docs<span class="hljs-string">&#x27;,&#x27;</span> http://ceph.com/docs <span class="hljs-string">&#x27;)</span><br><span class="hljs-string">    mystring=mystring.replace(&#x27;</span>ceph-devel@vger.kernel.org<span class="hljs-string">&#x27;,&#x27;</span> ceph-devel@vger.kernel.org <span class="hljs-string">&#x27;)</span><br><span class="hljs-string">    mystring=mystring.replace(&#x27;</span>ceph-commit@ceph.com<span class="hljs-string">&#x27;,&#x27;</span> ceph-commit@ceph.com <span class="hljs-string">&#x27;)</span><br><span class="hljs-string">    mystring=mystring.replace(&#x27;</span>http://github.com<span class="hljs-string">&#x27;,&#x27;</span> http://github.com <span class="hljs-string">&#x27;)</span><br><span class="hljs-string"></span><br><span class="hljs-string">    mystring=mystring.lstrip()</span><br><span class="hljs-string"></span><br><span class="hljs-string">    return(mystring)</span><br><span class="hljs-string"></span><br><span class="hljs-string"></span><br></code></pre></td></tr></table></figure><h2 id="流程总结"><a href="#流程总结" class="headerlink" title="流程总结"></a>流程总结</h2><p>第一次处理全局翻译</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">1、获得一个google cloud translate的api，能够通过api进行翻译，这个也可以尝试其它的api，能够给英文返回中文及即可<br>2、提取ceph doc的po文件，能够修改po文件来实现翻译<br>3、维护一个字库，通过分析po文件的内容，脚本提取自己需要保留的固定格式或者固定的翻译，处理好了以后，提交到google 翻译里面更新<br>4、用脚本处理翻译返回的结果，然后再保留到po文件里面去<br></code></pre></td></tr></table></figure><p>需要更新的情况</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">1、更新字库<br>2、更新处理脚本<br>3、翻译更新po文件<br></code></pre></td></tr></table></figure><p>基于以上就可以打造一个自动翻译文档的系统了，当然这个翻译本身是需要一个校对的过程，所以使用过程中应该是不断晚上后面说的部分，就是维护好字库，维护好更新处理脚本，这个打造好了以后，再来新的版本，直接进行翻译即可，而校对的过程可以边学习文档边处理</p><p>翻译的api高级功能一般都是需要收费的，这个通常来说通过人工一个个去翻译，肯定花费的时间功夫是超过了api处理的，并且来新版本是能够很快处理完成的</p><h2 id="更详细的代码"><a href="#更详细的代码" class="headerlink" title="更详细的代码"></a>更详细的代码</h2><p>更详细的代码存放地址：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">https://github.com/zphj1987/translate-ceph-doc-2020<br></code></pre></td></tr></table></figure><p>由于对账户的依赖性比较高，上面的文章只记录了相关的思路，和重要的处理点，具体的可以自己实现</p><h2 id="变更记录"><a href="#变更记录" class="headerlink" title="变更记录"></a>变更记录</h2><table><thead><tr><th align="center">Why</th><th align="center">Who</th><th align="center">When</th></tr></thead><tbody><tr><td align="center">创建</td><td align="center">武汉-运维-磨渣</td><td align="center">2020-09-04</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>希捷powerchoice磁盘休眠功能arm打包</title>
    <link href="/2020/08/25/%E5%B8%8C%E6%8D%B7powerchoice%E7%A3%81%E7%9B%98%E4%BC%91%E7%9C%A0%E5%8A%9F%E8%83%BDarm%E6%89%93%E5%8C%85/"/>
    <url>/2020/08/25/%E5%B8%8C%E6%8D%B7powerchoice%E7%A3%81%E7%9B%98%E4%BC%91%E7%9C%A0%E5%8A%9F%E8%83%BDarm%E6%89%93%E5%8C%85/</url>
    
    <content type="html"><![CDATA[<p>官方只提供了x86下面的包，没有提供arm下面的包，而我们的arm机器是32位的，需要编译一个支持armhf的二进制文件，这个文件只需要一个即可，但是编译是整套编译的，并且我们需要选定指定的版本，关闭nvme的支持（arm的缺库，也用不上），不带debug信息的</p><h2 id="准备编译环境"><a href="#准备编译环境" class="headerlink" title="准备编译环境"></a>准备编译环境</h2><p>编译环境选择的是ubuntu 18.04 (X86),在centos下面编译可能出现arm库不对的情况，通常情况下，ubuntu的跨平台编译要好一些，并且我们的arm也是ubuntu的</p><p>安装编译软件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">apt-get install gcc-arm-linux-gnueabihf<br></code></pre></td></tr></table></figure><p>支持arm的编译环境</p><h2 id="下载代码"><a href="#下载代码" class="headerlink" title="下载代码"></a>下载代码</h2><p>注意要下载这个版本的，其它版本可能发生命令变化，这个无法去一个个确认，这个版本确认可以的</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">git <span class="hljs-built_in">clone</span> --recursive -b  Release-19.06.02 https://github.com/Seagate/openSeaChest.git<br></code></pre></td></tr></table></figure><p>进入执行编译命令的目录</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">root@ubuntu-KVM:~/sea<span class="hljs-comment"># cd openSeaChest/Make/gcc</span><br>root@ubuntu-KVM:~/sea/openSeaChest/Make/gcc<span class="hljs-comment"># CC=arm-linux-gnueabihf-gcc make</span><br></code></pre></td></tr></table></figure><p>修改Makefile-方便调试编译</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">clean:<br>        <span class="hljs-built_in">rm</span> -f *.o *.a $(FILE_OUTPUT_<br>改成<br>clean:<br>        <span class="hljs-built_in">rm</span> -rf *.o *.a $(FILE_OUTPUT_<br></code></pre></td></tr></table></figure><p>这个是可以让每次编译的时候能够清理好环境，有个是目录，这个需要改成r才能删除</p><p>报错</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs bash">In file included from ../../include/platform_helper.h:22:0,<br>                 from ../../src/cmds.c:21:<br>../../include/sg_helper.h:68:35: error: Please define one of the following to include the correct NVMe header: SEA_NVME_IOCTL_H, SEA_NVME_H, or SEA_UAPI_NVME_H<br>These specify whether the NVMe IOCTL is <span class="hljs-keyword">in</span> /usr/include/linux/nvme_ioctl.h, /usr/include/linux/nvme.h, or /usr/include/uapi/nvme.h<br>                 <span class="hljs-comment">#pragma GCC error &quot;Please define one of the following to include the correct NVMe header: SEA_NVME_IOCTL_H, SEA_NVME_H, or SEA_UAPI_NVME_H\nThese specify whether the NVMe IOCTL is in /usr/include/linux/nvme_ioctl.h, /usr/include/linux/nvme.h, or /usr/include/uapi/nvme.h&quot;</span><br>                                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~<br>Makefile:99: recipe <span class="hljs-keyword">for</span> target <span class="hljs-string">&#x27;../../src/cmds.o&#x27;</span> failed<br></code></pre></td></tr></table></figure><p>这个报错是出现在编译opensea-transport里面的，我们检查下编译opensea-transport的Makefile</p><p>vim .&#x2F;opensea-transport&#x2F;Make&#x2F;gcc&#x2F;Makefile</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment">#determine the proper NVMe include file. SEA_NVME_IOCTL_H, SEA_NVME_H, or SEA_UAPI_NVME_H</span><br>NVME_IOCTL_H = /usr/include/linux/nvme_ioctl.h<br>NVME_H = /usr/include/linux/nvme.h<br>UAPI_NVME_H = /usr/include/uapi/nvme.h<br></code></pre></td></tr></table></figure><p>可以看到这个地方是引用了本地的头文件，而我们的编译环境是跨平台编译，肯定没这个的arm的引用的，我们可以屏蔽掉这个nvme相关的，这个里面是提供了屏蔽的参数的</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs bash">root@ubuntu-KVM:~/sea/openSeaChest/Make/gcc<span class="hljs-comment"># vim Makefile</span><br><span class="hljs-comment">#add any defines needed for tool release.</span><br><span class="hljs-comment">#PROJECT_DEFINES += -DDISABLE_NVME_PASSTHROUGH -DDISABLE_TCG_SUPPORT</span><br>PROJECT_DEFINES += -DDISABLE_TCG_SUPPORT<br>ifeq ($(UNAME),FreeBSD)<br>PROJECT_DEFINES += -DDISABLE_NVME_PASSTHROUGH<br>endif<br>修改为<br><span class="hljs-comment">#add any defines needed for tool release.</span><br>PROJECT_DEFINES += -DDISABLE_NVME_PASSTHROUGH -DDISABLE_TCG_SUPPORT<br><span class="hljs-comment">#PROJECT_DEFINES += -DDISABLE_TCG_SUPPORT</span><br>ifeq ($(UNAME),FreeBSD)<br>PROJECT_DEFINES += -DDISABLE_NVME_PASSTHROUGH<br>endif<br></code></pre></td></tr></table></figure><p>再次编译</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">root@ubuntu-KVM:~/sea/openSeaChest/Make/gcc<span class="hljs-comment">#  CC=arm-linux-gnueabihf-gcc make</span><br></code></pre></td></tr></table></figure><p>报错如下</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs bash">make[1]: Entering directory <span class="hljs-string">&#x27;/root/sea/openSeaChest/Make/gcc&#x27;</span><br><span class="hljs-built_in">mkdir</span> -p openseachest_exes<br>gcc -Wall -c -std=gnu99 -g -I../../opensea-common/include -I../../opensea-transport/include -I../../opensea-transport/include/vendor -I../../include -I../../opensea-operations/include -DDISABLE_NVME_PASSTHROUGH -DDISABLE_TCG_SUPPORT -D_DEBUG -D_DEBUG ../../utils/C/openSeaChest/openSeaChest_Firmware.c -o ../../utils/C/openSeaChest/openSeaChest_Firmware.o<br>make[1]: gcc: Command not found<br>Makefile.openSeaChest_firmware:109: recipe <span class="hljs-keyword">for</span> target <span class="hljs-string">&#x27;../../utils/C/openSeaChest/openSeaChest_Firmware.o&#x27;</span> failed<br>make[1]: *** [../../utils/C/openSeaChest/openSeaChest_Firmware.o] Error 127<br>make[1]: Leaving directory <span class="hljs-string">&#x27;/root/sea/openSeaChest/Make/gcc&#x27;</span><br>Makefile:210: recipe <span class="hljs-keyword">for</span> target <span class="hljs-string">&#x27;openSeaChest_Firmware&#x27;</span> failed<br></code></pre></td></tr></table></figure><p>可以看到上面出现了gcc，并且有openseachest_exes，这个是调用的另外一个makefile文件出现的，我们检查下makefile文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">root@ubuntu-KVM:~/sea/openSeaChest/Make/gcc<span class="hljs-comment"># vim Makefile.openSeaChest_firmware </span><br>可以看到写死了两个值<br>CC = gcc<br><br>STRIP = strip<br></code></pre></td></tr></table></figure><p>我们修改为我们想编译的平台的</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">CC = arm-linux-gnueabihf-gcc<br>STRIP = arm-linux-gnueabihf-strip<br></code></pre></td></tr></table></figure><p>再次编译，顺利编译成功</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">root@ubuntu-KVM:~/sea/openSeaChest/Make/gcc<span class="hljs-comment">#  CC=arm-linux-gnueabihf-gcc make </span><br></code></pre></td></tr></table></figure><p>这里编译完成的时候，默认开启了debug，我们需要有个没有debug信息的版本</p><p>再次编译</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">root@ubuntu-KVM:~/sea/openSeaChest/Make/gcc<span class="hljs-comment">#  CC=arm-linux-gnueabihf-gcc make release</span><br></code></pre></td></tr></table></figure><p>注意我们的命令发生了变化，加上了release的后缀，这个是能够提供两个二进制文件的，一个是dbg的一个是剥离了dbg的</p><p>检查生成的文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs bash">root@ubuntu-KVM:~/sea/openSeaChest/Make/gcc<span class="hljs-comment"># ll openseachest_exes/openSeaChest_PowerControl*</span><br>-rwxr-xr-x 1 root root  822808 8月  24 17:55 openseachest_exes/openSeaChest_PowerControl*<br>-rwxr-xr-x 1 root root 1872988 8月  24 17:49 openseachest_exes/openSeaChest_PowerControl_dbg*<br><br>root@ubuntu-KVM:~/sea/openSeaChest/Make/gcc<span class="hljs-comment"># file openseachest_exes/openSeaChest_PowerControl</span><br>openseachest_exes/openSeaChest_PowerControl: ELF 32-bit LSB shared object, ARM, EABI5 version 1 (SYSV), dynamically linked, interpreter /lib/ld-linux-armhf.so.3, <span class="hljs-keyword">for</span> GNU/Linux 3.2.0, BuildID[sha1]=400fe0fa246e1b57115f6b7a3ea70569fd64efae, with debug_info, not stripped<br></code></pre></td></tr></table></figure><p>我们把openSeaChest_PowerControl拷贝到arm的机器上面执行</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs bash">root@arm23:~<span class="hljs-comment"># ./openSeaChest_PowerControl -d /dev/sdb  --checkPowerMode</span><br>==========================================================================================<br> openSeaChest_PowerControl - openSeaChest drive utilities<br> Copyright (c) 2014-2020 Seagate Technology LLC and/or its Affiliates, All Rights Reserved<br> openSeaChest_PowerControl Version: 1.10.0-1_19_24 ARM<br> Build Date: Aug 24 2020<br> Today: Mon Aug 24 18:01:55 2020<br>==========================================================================================<br><br>/dev/sdb - ST10000NM0016-1TT101 - ZA2CS3KZ - ATA<br>Device is <span class="hljs-keyword">in</span> the PM1: Idle state and the device is <span class="hljs-keyword">in</span> the Idle_b power condition<br></code></pre></td></tr></table></figure><p>可以看到命令可以执行成功，并且上面也显示的是arm的版本，剩下的具体的设置根据配置文档进行设置即可</p><h2 id="变更记录"><a href="#变更记录" class="headerlink" title="变更记录"></a>变更记录</h2><table><thead><tr><th align="center">Why</th><th align="center">Who</th><th align="center">When</th></tr></thead><tbody><tr><td align="center">创建</td><td align="center">武汉-运维-磨渣</td><td align="center">2020-08-25</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>希捷powerchoice磁盘休眠功能配置方法</title>
    <link href="/2020/08/25/%E5%B8%8C%E6%8D%B7powerchoice%E7%A3%81%E7%9B%98%E4%BC%91%E7%9C%A0%E5%8A%9F%E8%83%BD%E9%85%8D%E7%BD%AE%E6%96%B9%E6%B3%95/"/>
    <url>/2020/08/25/%E5%B8%8C%E6%8D%B7powerchoice%E7%A3%81%E7%9B%98%E4%BC%91%E7%9C%A0%E5%8A%9F%E8%83%BD%E9%85%8D%E7%BD%AE%E6%96%B9%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<p>本篇关于希捷磁盘休眠的配置方法</p><h2 id="准备设置的软件"><a href="#准备设置的软件" class="headerlink" title="准备设置的软件"></a>准备设置的软件</h2><p>下载地址</p><blockquote><p><a href="https://raw.githubusercontent.com/Seagate/ToolBin/master/SeaChest/PowerControl/v1.10.0/Linux/SeaChest_PowerControl_1100_11923_64">https://raw.githubusercontent.com/Seagate/ToolBin/master/SeaChest/PowerControl/v1.10.0/Linux/SeaChest_PowerControl_1100_11923_64</a></p></blockquote><p>或者通过</p><blockquote><p><a href="http://support.seagate.com/seachest/SeaChestUtilities.zip">http://support.seagate.com/seachest/SeaChestUtilities.zip</a></p></blockquote><p>下载后解压拿到二进制文件</p><p>注意版本尽量用这个版本，不同的版本命令不同，可以有的重要命令不支持，所以限定这个版本即可</p><h2 id="磁盘休眠相关命令"><a href="#磁盘休眠相关命令" class="headerlink" title="磁盘休眠相关命令"></a>磁盘休眠相关命令</h2><p>默认磁盘开启的是idle_a的节能模式，这个模式实际上不省电</p><p>服务器的操作是禁用了一些内部服务，减少处理器和通道功耗，磁盘还是全速转动</p><p>idle b 模式<br>是卸载磁头到硬盘斜坡</p><p>官方给出的功耗数据</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">https://www.seagate.com/www-content/product-content/enterprise-hdd-fam/enterprise-capacity-3-5-hdd/enterprise-capacity-3-5-hdd/en-us/docs/100791104c.pdf<br></code></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">idle_A      4.36<br>idle_B      2.98<br>idle_C      2.34<br>Standby     0.80<br></code></pre></td></tr></table></figure><p>考虑恢复时间和功耗的节省问题，我们选择idle_B</p><p>关闭AB模式，也就是关闭省电模式</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">./SeaChest_PowerControl_1100_11923_64 -d /dev/sdp  --disableMode --changePower --powerMode idle_b<br>./SeaChest_PowerControl_1100_11923_64 -d /dev/sdp  --disableMode --changePower --powerMode idle_a<br></code></pre></td></tr></table></figure><p>开启B模式的省电模式，并设置休眠时间为60s<br>后面的单位为100ms</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">./SeaChest_PowerControl_1100_11923_64 -d /dev/sdp  --disableMode --changePower --powerMode idle_a<br>./SeaChest_PowerControl_1100_11923_64 -d /dev/sdp  --enableMode --changePower --powerMode idle_b  --modeTimer 600<br></code></pre></td></tr></table></figure><p>这个可以不管什么情况，都把A模式关闭了，否则同时设置的时候，会优先进入a模式</p><p>检查当前的省电设置情况</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@node120 ~]<span class="hljs-comment"># ./SeaChest_PowerControl_1100_11923_64 -d /dev/sdp --showEPCSettings</span><br>==========================================================================================<br> SeaChest_PowerControl - Seagate drive utilities - NVMe Enabled<br> Copyright (c) 2014-2019 Seagate Technology LLC and/or its Affiliates, All Rights Reserved<br> SeaChest_PowerControl Version: 1.10.0-1_19_23 X86_64<br> Build Date: Jun 10 2019<br> Today: Mon Aug 24 10:51:36 2020<br>==========================================================================================<br><br>/dev/sg17 - ST10000NM0016-1TT101 - ZA2CRY9G - ATA<br>.<br><br>===EPC Settings===<br>* = timer is enabled<br>C column = Changeable<br>S column = Saveable<br>All <span class="hljs-built_in">times</span> are <span class="hljs-keyword">in</span> 100 milliseconds<br><br>Name       Current Timer Default Timer Saved Timer   Recovery Time C S<br>Idle A      0            *1             1            1             Y Y<br>Idle B     *10           *1200         *10           4             Y Y<br>Idle C      0             6000          6000         50            Y Y<br>Standby Z   0             9000          9000         120           Y Y<br></code></pre></td></tr></table></figure><p>可以看到我的设置是1s的，这个是我方便测试看是否能进入到省电B模式设置的1s的，实际按照我们自己的设计是60s的</p><p>查询当前磁盘的省电状态的命令</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs bash">./SeaChest_PowerControl_1100_11923_64 -d /dev/sdp  --checkPowerMode;<br>[root@node120 ~]<span class="hljs-comment"># ./SeaChest_PowerControl_1100_11923_64 -d /dev/sdp  --checkPowerMode;</span><br>==========================================================================================<br> SeaChest_PowerControl - Seagate drive utilities - NVMe Enabled<br> Copyright (c) 2014-2019 Seagate Technology LLC and/or its Affiliates, All Rights Reserved<br> SeaChest_PowerControl Version: 1.10.0-1_19_23 X86_64<br> Build Date: Jun 10 2019<br> Today: Mon Aug 24 10:54:49 2020<br>==========================================================================================<br><br>/dev/sg17 - ST10000NM0016-1TT101 - ZA2CRY9G - ATA<br>Device is <span class="hljs-keyword">in</span> the PM1: Idle state and the device is <span class="hljs-keyword">in</span> the Idle_b power condition<br><br><br>[root@node120 ~]<span class="hljs-comment"># ./SeaChest_PowerControl_1100_11923_64 -d /dev/sdp  --checkPowerMode;</span><br>==========================================================================================<br> SeaChest_PowerControl - Seagate drive utilities - NVMe Enabled<br> Copyright (c) 2014-2019 Seagate Technology LLC and/or its Affiliates, All Rights Reserved<br> SeaChest_PowerControl Version: 1.10.0-1_19_23 X86_64<br> Build Date: Jun 10 2019<br> Today: Mon Aug 24 10:55:04 2020<br>==========================================================================================<br><br>/dev/sg17 - ST10000NM0016-1TT101 - ZA2CRY9G - ATA<br>Device is <span class="hljs-keyword">in</span> the PM0: Active state or PM1: Idle State<br></code></pre></td></tr></table></figure><p>在省电模式的提示是</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">Device is <span class="hljs-keyword">in</span> the PM1: Idle state and the device is <span class="hljs-keyword">in</span> the Idle_b power condition<br></code></pre></td></tr></table></figure><p>磁盘正在运行的模式是</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">Device is <span class="hljs-keyword">in</span> the PM0: Active state or PM1: Idle State<br></code></pre></td></tr></table></figure><p>注意，这个命令</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">./SeaChest_PowerControl_1100_11923_64 -d /dev/sdp --showEPCSettings<br></code></pre></td></tr></table></figure><p>会唤醒磁盘也就是中断了省电模式，所以只有确定知晓会中断省电模式的时候才使用</p><p>管理平台不要频繁调用</p><p>如果需要检查当前模式就用上面的checkPowerMode命令，checkPowerMode命令不会唤醒磁盘，并且需要注意是否有本地的意外进程把磁盘省电模式中断了</p>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>公网临时传输大文件</title>
    <link href="/2020/07/31/%E5%85%AC%E7%BD%91%E4%B8%B4%E6%97%B6%E4%BC%A0%E8%BE%93%E5%A4%A7%E6%96%87%E4%BB%B6/"/>
    <url>/2020/07/31/%E5%85%AC%E7%BD%91%E4%B8%B4%E6%97%B6%E4%BC%A0%E8%BE%93%E5%A4%A7%E6%96%87%E4%BB%B6/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>我们经常遇到的一个场景是需要把一个大文件通过互联网传递到另外一个地方，类似日志，或者大的安装包，有的时候是传输给自己，有的时候传输给别人</p><h2 id="传输方式"><a href="#传输方式" class="headerlink" title="传输方式"></a>传输方式</h2><p>百度盘我们都知道速度限制到怀疑人生，没会员的时候基本就是浪费时间，这种基本就不考虑做传输工具，备份下还可以，这里介绍的是几款比较方便的传输方式，这几款都是类似的处理，上传到网站，网站生成链接，拿到链接直接下载</p><p>一个开源的传输平台，服务器应该在国外，速度比较慢</p><blockquote><p><a href="https://transfer.sh/#">https://transfer.sh/#</a></p></blockquote><p><img src="/images/blog/284415-20200731144423025-464029052.png"></p><p>firefox出品的临时传输平台，速度一般</p><blockquote><p><a href="https://send.firefox.com/">https://send.firefox.com/</a></p></blockquote><p><img src="/images/blog/284415-20200731144434276-1251931581.png"></p><p>奶牛快传，基本满速</p><blockquote><p><a href="https://cowtransfer.com/">https://cowtransfer.com</a></p></blockquote><p><img src="/images/blog/284415-20200731144443263-2097600962.png"></p><p>推荐使用第三个奶牛快传，基本上是网速有多快，传输就多快，接收方也不用下客户端什么的拿到地址就可以下载</p>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>arm64大服务器安装ubuntu18看不到安装界面</title>
    <link href="/2020/07/14/arm64%E5%A4%A7%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%AE%89%E8%A3%85ubuntu18%E7%9C%8B%E4%B8%8D%E5%88%B0%E5%AE%89%E8%A3%85%E7%95%8C%E9%9D%A2/"/>
    <url>/2020/07/14/arm64%E5%A4%A7%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%AE%89%E8%A3%85ubuntu18%E7%9C%8B%E4%B8%8D%E5%88%B0%E5%AE%89%E8%A3%85%E7%95%8C%E9%9D%A2/</url>
    
    <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>最近在使用arm的大服务器需要用到ubuntu相关的一些东西，在操作系统安装过程中遇到了一些问题</p><h2 id="记录"><a href="#记录" class="headerlink" title="记录"></a>记录</h2><h3 id="华为鲲鹏服务器"><a href="#华为鲲鹏服务器" class="headerlink" title="华为鲲鹏服务器"></a>华为鲲鹏服务器</h3><p>这个默认安装centos的都很顺利，安装ubuntu18最新的，impi就花屏了，然后找各种地方都没找到原因，看到官网的，用18.04.01写的文档，然后试了下18.04.01可以，其它版本都花屏，直接使用即可</p><h3 id="安培服务器"><a href="#安培服务器" class="headerlink" title="安培服务器"></a>安培服务器</h3><p>同样的基本找不到相关的文档，网上的都是禁用的一些参数什么的，实际上操作如下：</p><p>在grub编辑界面，在—后面增加</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">console=tty0 iommu.passthrough=1<br></code></pre></td></tr></table></figure><p>然后crtl+x启动就好了，不清楚这个在上面的华为机器上是否可以，机器被拿走了有机会再试了</p>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>ceph的pg平衡插件balancer</title>
    <link href="/2020/06/17/ceph%E7%9A%84pg%E5%B9%B3%E8%A1%A1%E6%8F%92%E4%BB%B6balancer/"/>
    <url>/2020/06/17/ceph%E7%9A%84pg%E5%B9%B3%E8%A1%A1%E6%8F%92%E4%BB%B6balancer/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>ceph比较老的版本使用的reweight或者osd weight来调整平衡的，本篇介绍的是ceph新的自带的插件balancer的使用，官网有比较详细的操作手册可以查询</p><h2 id="使用方法"><a href="#使用方法" class="headerlink" title="使用方法"></a>使用方法</h2><p>查询插件的开启情况</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@node1 ceph]<span class="hljs-comment"># ceph mgr module ls</span><br>&#123;<br>    <span class="hljs-string">&quot;enabled_modules&quot;</span>: [<br>        <span class="hljs-string">&quot;balancer&quot;</span>,<br>        <span class="hljs-string">&quot;restful&quot;</span>,<br>        <span class="hljs-string">&quot;status&quot;</span><br>    ],<br>    <span class="hljs-string">&quot;disabled_modules&quot;</span>: [<br>        <span class="hljs-string">&quot;dashboard&quot;</span>,<br>        <span class="hljs-string">&quot;influx&quot;</span>,<br>        <span class="hljs-string">&quot;localpool&quot;</span>,<br>        <span class="hljs-string">&quot;prometheus&quot;</span>,<br>        <span class="hljs-string">&quot;selftest&quot;</span>,<br>        <span class="hljs-string">&quot;telemetry&quot;</span>,<br>        <span class="hljs-string">&quot;zabbix&quot;</span><br>    ]<br>&#125;<br></code></pre></td></tr></table></figure><p>默认balancer就是enable的</p><p>查询balancer活动情况</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@node1 ceph]<span class="hljs-comment"># ceph  balancer status</span><br>&#123;<br>    <span class="hljs-string">&quot;last_optimize_duration&quot;</span>: <span class="hljs-string">&quot;&quot;</span>, <br>    <span class="hljs-string">&quot;plans&quot;</span>: [], <br>    <span class="hljs-string">&quot;mode&quot;</span>: <span class="hljs-string">&quot;none&quot;</span>, <br>    <span class="hljs-string">&quot;active&quot;</span>: <span class="hljs-literal">false</span>, <br>    <span class="hljs-string">&quot;optimize_result&quot;</span>: <span class="hljs-string">&quot;&quot;</span>, <br>    <span class="hljs-string">&quot;last_optimize_started&quot;</span>: <span class="hljs-string">&quot;&quot;</span><br>&#125;<br></code></pre></td></tr></table></figure><p>可以看到active是false，这里有手动的方法和自动的方法，我一般使用自动的，然后调整完了关闭</p><p>首先设置兼容模式</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">ceph balancer mode crush-compat<br></code></pre></td></tr></table></figure><p>开启调整前，我们需要先看下我们的调整的效果，这里可以用</p><p><a href="https://zphj1987.com/2015/10/14/%e6%9f%a5%e8%af%a2osd%e4%b8%8a%e7%9a%84pg%e6%95%b0/">查询osd上的pg数</a><br>提供的脚本来进行查询，效果如下</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@node1 ceph]<span class="hljs-comment"># sh getpg.sh </span><br>dumped all<br><br>pool :6| SUM <br>------------------------<br>osd.0159| 159<br>osd.1136| 136<br>osd.2167| 167<br>osd.3163| 163<br>osd.4143| 143<br>------------------------<br>SUM :768|<br>Osd :5|<br>AVE :153.60|<br>Max :167|<br>Osdid :osd.2|<br>per:8.7%|<br>------------------------<br>min :136|<br>osdid :osd.1|<br>per:-11.5%|<br></code></pre></td></tr></table></figure><p>之所以要这个脚本，是因为自带的提供的是osd上面的pg之和，有的时候我们的存储池混用物理osd的，上面的有的空存储池的pg会影响查看效果，所以需要分存储池去计算统计</p><p>开启调整</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">ceph balancer on<br></code></pre></td></tr></table></figure><p>查看情况</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@node1 ceph]<span class="hljs-comment"># ceph  balancer status</span><br>&#123;<br>    <span class="hljs-string">&quot;last_optimize_duration&quot;</span>: <span class="hljs-string">&quot;0:00:00.989178&quot;</span>, <br>    <span class="hljs-string">&quot;plans&quot;</span>: [], <br>    <span class="hljs-string">&quot;mode&quot;</span>: <span class="hljs-string">&quot;crush-compat&quot;</span>, <br>    <span class="hljs-string">&quot;active&quot;</span>: <span class="hljs-literal">true</span>, <br>    <span class="hljs-string">&quot;optimize_result&quot;</span>: <span class="hljs-string">&quot;Optimization plan created successfully&quot;</span>, <br>    <span class="hljs-string">&quot;last_optimize_started&quot;</span>: <span class="hljs-string">&quot;Wed Jun 17 14:34:53 2020&quot;</span><br>&#125;<br></code></pre></td></tr></table></figure><p>现在的这个状态查询比以前做的好了，还带上了最后的执行时间，从监控来看，1分钟会触发一次，差不多等个几分钟，基本就调整完了，这个的前提是空的环境，有数据的环境，那就看每一轮的需要迁移的数据量了，所以搭建完集群，一定需要调整平衡<br>我们检查下我们的环境</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@node1 ceph]<span class="hljs-comment"># sh getpg.sh </span><br>dumped all<br><br>pool :6| SUM <br>------------------------<br>osd.0153| 153<br>osd.1153| 153<br>osd.2154| 154<br>osd.3154| 154<br>osd.4154| 154<br>------------------------<br>SUM :768|<br>Osd :5|<br>AVE :153.60|<br>Max :154|<br>Osdid :osd.2|<br>per:0.3%|<br>------------------------<br>min :153|<br>osdid :osd.1|<br>per:-0.4%|<br></code></pre></td></tr></table></figure><p>再次查询</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@node1 ceph]<span class="hljs-comment"># ceph  balancer status</span><br>&#123;<br>    <span class="hljs-string">&quot;last_optimize_duration&quot;</span>: <span class="hljs-string">&quot;0:00:00.114673&quot;</span>, <br>    <span class="hljs-string">&quot;plans&quot;</span>: [], <br>    <span class="hljs-string">&quot;mode&quot;</span>: <span class="hljs-string">&quot;crush-compat&quot;</span>, <br>    <span class="hljs-string">&quot;active&quot;</span>: <span class="hljs-literal">true</span>, <br>    <span class="hljs-string">&quot;optimize_result&quot;</span>: <span class="hljs-string">&quot;Unable to find further optimization, change balancer mode and retry might help&quot;</span>, <br>    <span class="hljs-string">&quot;last_optimize_started&quot;</span>: <span class="hljs-string">&quot;Wed Jun 17 14:40:57 2020&quot;</span><br>&#125;<br><br></code></pre></td></tr></table></figure><p>效果相当惊人，结果提示这个无法更好了，这个调整看自己接受的程度了，之前遇到过一次主机不对称的crush，实际上会出现永远调不平的情况，所以自己判断下即可，目前的情况非常的均衡了，这个时候我个人的操作是关闭掉这个调整，以免后面有变化，有数据的时候自动触发了调整引起不必要的麻烦</p><p>关闭自动平衡</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@node1 ceph]<span class="hljs-comment"># ceph balancer off</span><br>[root@node1 ceph]<span class="hljs-comment"># ceph  balancer status</span><br>&#123;<br>    <span class="hljs-string">&quot;last_optimize_duration&quot;</span>: <span class="hljs-string">&quot;0:00:00.114152&quot;</span>, <br>    <span class="hljs-string">&quot;plans&quot;</span>: [], <br>    <span class="hljs-string">&quot;mode&quot;</span>: <span class="hljs-string">&quot;crush-compat&quot;</span>, <br>    <span class="hljs-string">&quot;active&quot;</span>: <span class="hljs-literal">false</span>, <br>    <span class="hljs-string">&quot;optimize_result&quot;</span>: <span class="hljs-string">&quot;Unable to find further optimization, change balancer mode and retry might help&quot;</span>, <br>    <span class="hljs-string">&quot;last_optimize_started&quot;</span>: <span class="hljs-string">&quot;Wed Jun 17 14:44:58 2020&quot;</span><br>&#125;<br></code></pre></td></tr></table></figure><h2 id="手动调整方法"><a href="#手动调整方法" class="headerlink" title="手动调整方法"></a>手动调整方法</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">ceph balancer optimize tune<br>ceph balancer execute tune<br></code></pre></td></tr></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>现在的平衡跟之前通过weight的调整，最终的效果比之前会好很多，之前H版本J版本一个集群能调整到5%，基本就是调整极限了</p><h2 id="变更记录"><a href="#变更记录" class="headerlink" title="变更记录"></a>变更记录</h2><table><thead><tr><th align="center">Why</th><th align="center">Who</th><th align="center">When</th></tr></thead><tbody><tr><td align="center">创建</td><td align="center">武汉-运维-磨渣</td><td align="center">2020-06-17</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>python-redmine获取redmine平台的信息</title>
    <link href="/2020/04/03/python-redmine%E8%8E%B7%E5%8F%96redmine%E5%B9%B3%E5%8F%B0%E7%9A%84%E4%BF%A1%E6%81%AF/"/>
    <url>/2020/04/03/python-redmine%E8%8E%B7%E5%8F%96redmine%E5%B9%B3%E5%8F%B0%E7%9A%84%E4%BF%A1%E6%81%AF/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>最近做redmine的bug平台的分析，统计一些需要用到的数据，这里把相关调用的地方记录下来以备后用</p><h2 id="相关的获取接口"><a href="#相关的获取接口" class="headerlink" title="相关的获取接口"></a>相关的获取接口</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-meta">#! /usr/bin/python</span><br><span class="hljs-comment"># -*- coding:UTF-8 -*-</span><br><br><br>from redminelib import Redmine<br>redmine = Redmine(<span class="hljs-string">&#x27;redmin的地址&#x27;</span>,username=<span class="hljs-string">&#x27;用户名&#x27;</span>,password=<span class="hljs-string">&#x27;密码&#x27;</span>,requests=&#123;<span class="hljs-string">&#x27;timeout&#x27;</span>: 5 &#125;)<br>issues = redmine.issue.filter(project_id=<span class="hljs-string">&#x27;项目id&#x27;</span>,status_id=<span class="hljs-string">&#x27;open&#x27;</span>,offset=0,<span class="hljs-built_in">limit</span>=100)<br><br><span class="hljs-comment"># 遍历bug单</span><br><span class="hljs-keyword">for</span> issue <span class="hljs-keyword">in</span> issues:<br>    <span class="hljs-comment">#获取当前BUG编号</span><br>    <span class="hljs-built_in">print</span> issue.id<br>    <span class="hljs-comment">#获取bug单标题</span><br>    <span class="hljs-built_in">print</span> issue.subject<br>    <span class="hljs-comment">#获取bug单的提单人</span><br>    <span class="hljs-built_in">print</span> issue.author<br>    <span class="hljs-comment">#获取issue创建时间</span><br>    <span class="hljs-built_in">print</span> issue.created_on<br>    <span class="hljs-comment">#获取更新时间</span><br>    <span class="hljs-built_in">print</span> issue.updated_on<br><span class="hljs-comment">#遍历追加评论（BUG流转过程）</span><br>    myissue=redmine.issue.get(issue.id, include=[<span class="hljs-string">&#x27;children&#x27;</span>, <span class="hljs-string">&#x27;journals&#x27;</span>, <span class="hljs-string">&#x27;watchers&#x27;</span>])<br>    <span class="hljs-keyword">for</span> journal <span class="hljs-keyword">in</span> myissue.journals:<br><span class="hljs-comment">#获取评论人</span><br>        <span class="hljs-built_in">print</span> journal.user.name<br><span class="hljs-comment">#获取评论时间</span><br>        <span class="hljs-built_in">print</span> journal.created_on<br></code></pre></td></tr></table></figure><p>因为默认的时间是UTC时间，我们转换成本地时间</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs bash">def utc2local(utc_dtm):<br>    local_tm = datetime.fromtimestamp( 0 )<br>    utc_tm = datetime.utcfromtimestamp( 0 )<br>    offset = local_tm - utc_tm<br>    <span class="hljs-built_in">return</span> utc_dtm + offset<br><br>local_issue_tm = utc2local(issue.created_on)<br></code></pre></td></tr></table></figure><p>这样可以把时间转换成当前的时区的时间<br>想格式化也行</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">issue_create_time=local_issue_tm.strftime(<span class="hljs-string">&quot;%Y-%m-%d %H:%M:%S&quot;</span>)<br></code></pre></td></tr></table></figure><h2 id="超过100条的问题的处理"><a href="#超过100条的问题的处理" class="headerlink" title="超过100条的问题的处理"></a>超过100条的问题的处理</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">issues = redmine.issue.filter(project_id=<span class="hljs-string">&#x27;项目id&#x27;</span>,status_id=<span class="hljs-string">&#x27;open&#x27;</span>,offset=0,<span class="hljs-built_in">limit</span>=100)<br>issues = redmine.issue.filter(project_id=<span class="hljs-string">&#x27;项目id&#x27;</span>,status_id=<span class="hljs-string">&#x27;open&#x27;</span>,offset=100,<span class="hljs-built_in">limit</span>=100)<br></code></pre></td></tr></table></figure><p>这个是因为redmine内部把单个最大请求数限制死了是100个，因此可以通过偏移量来获取即可，有几百个bug就写几条就行了，也不会太多</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>上面的就是获取一些信息的接口，获取以后可以到出为csv或者excel，然后做更多的数据处理，这里就不做过多的记录</p><h2 id="变更记录"><a href="#变更记录" class="headerlink" title="变更记录"></a>变更记录</h2><table><thead><tr><th align="center">Why</th><th align="center">Who</th><th align="center">When</th></tr></thead><tbody><tr><td align="center">创建</td><td align="center">武汉-运维-磨渣</td><td align="center">2020-04-03</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>python脚本打包成rpm软件包</title>
    <link href="/2020/04/03/python%E8%84%9A%E6%9C%AC%E6%89%93%E5%8C%85%E6%88%90rpm%E8%BD%AF%E4%BB%B6%E5%8C%85/"/>
    <url>/2020/04/03/python%E8%84%9A%E6%9C%AC%E6%89%93%E5%8C%85%E6%88%90rpm%E8%BD%AF%E4%BB%B6%E5%8C%85/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>软件最终都会有交付的形式，有的是用tar包，有个是以目录，有的是封成一个文件包，从大多数使用场景来说，直接打包成软件包的方式是最简单，也是最不容易出错的，路径可以在包里面写死了</p><h2 id="实践"><a href="#实践" class="headerlink" title="实践"></a>实践</h2><p>关于打包的资料网上很多，而封包的方式也很多有spec文件方式，有fpm打包方式，本篇记录的是通过setup.py的方式打包的，因为最近出了小工具，所以进行了相关的尝试，这里记录一下</p><p>首先创建一个setup.py文件，写上一些基础内容</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@<span class="hljs-built_in">test</span> redmine]<span class="hljs-comment"># cat setup.py </span><br><span class="hljs-comment"># coding:utf-8</span><br><span class="hljs-comment">#from distutils.core import setup</span><br>from setuptools import setup, find_packages<br><br>setup(<br>        name=<span class="hljs-string">&#x27;issue-check&#x27;</span>,<br>        version=<span class="hljs-string">&#x27;2020-0403-1419&#x27;</span>,<br>        description=<span class="hljs-string">&#x27;This redmine issue-check&#x27;</span>, <br>        author=<span class="hljs-string">&#x27;zphj1987&#x27;</span>,  <br>        author_email=<span class="hljs-string">&#x27;zphj1987@gmail.com&#x27;</span>,<br>        license=<span class="hljs-string">&#x27;MIT&#x27;</span>,<br>        url=<span class="hljs-string">&#x27;https://github.com/zphj1987/issue-check.git&#x27;</span>,<br>        py_modules=[<span class="hljs-string">&#x27;issue-check&#x27;</span>,<span class="hljs-string">&#x27;issue-check-pretty&#x27;</span>],<br>        data_files=[(<span class="hljs-string">&#x27;/opt/issue-check/&#x27;</span>,[<span class="hljs-string">&#x27;issue.conf.sample&#x27;</span>]),(<span class="hljs-string">&#x27;/opt/issue-check/&#x27;</span>,[<span class="hljs-string">&#x27;README.md&#x27;</span>])]<br>)<br></code></pre></td></tr></table></figure><p>上面的就是把当前目录的issue-check.py和issue-check-pretty.py打包进rpm，把issue.conf.sample打包到&#x2F;opt&#x2F;issue-check&#x2F;这个路径下面</p><p>通过data_files可以进行安装路径的控制，而上面的py_modules则不在这里进行控制了，需要增加一个配置文件setup.cfg</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[install]<br>install-lib=/opt/issue-check<br></code></pre></td></tr></table></figure><p>修改以后，安装路径就指定到&#x2F;opt&#x2F;issue-check下面了，如果不做控制，就会默认安装到python的执行目录里面去，上面的这些简单的控制以后就可以开始封包的命令了</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">python setup.py bdist_rpm<br></code></pre></td></tr></table></figure><p>执行完了后在当前目录的dist目录下面就会生成相关的rpm包和源码的rpm包，这个实际上也是调用了spec，然后进行的相关打包工作，只是在这个之上再封装了一层</p><h2 id="暂时未解决的问题"><a href="#暂时未解决的问题" class="headerlink" title="暂时未解决的问题"></a>暂时未解决的问题</h2><p>默认这样的打包方式会生成egg相关的文件，并且打包过程中会编译pyc，pyo等文件，暂时还没找到方法简单的去把这些文件给去掉，不过也不会太大的影响</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>如果是做py的软件包，建议还是能够封包后再输出</p><h2 id="变更记录"><a href="#变更记录" class="headerlink" title="变更记录"></a>变更记录</h2><table><thead><tr><th align="center">Why</th><th align="center">Who</th><th align="center">When</th></tr></thead><tbody><tr><td align="center">创建</td><td align="center">武汉-运维-磨渣</td><td align="center">2020-04-03</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>brctl 增加桥接网卡</title>
    <link href="/2020/01/09/brctl%20%E5%A2%9E%E5%8A%A0%E6%A1%A5%E6%8E%A5%E7%BD%91%E5%8D%A1/"/>
    <url>/2020/01/09/brctl%20%E5%A2%9E%E5%8A%A0%E6%A1%A5%E6%8E%A5%E7%BD%91%E5%8D%A1/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>之前有一篇介绍配置桥接网卡的，这个桥接网卡一般是手动做虚拟化的时候会用到，通过修改网卡的配置文件的方式会改变环境的原有的配置，而很多情况，我只是简单的用一下，并且尽量不要把网络搞断了，万一有问题，远程把机器重启一下也就恢复了，不至于反复去定位哪里改错了，当然如果是能够直连的修改的时候，还是建议通过配置文件的方式去修改</p><p>安装必要的软件包</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">yum install bridge-utils<br></code></pre></td></tr></table></figure><p>选择想要修改的网卡</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ~]<span class="hljs-comment"># ifconfig </span><br>ens33: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  mtu 1500<br>        inet 192.168.0.101  netmask 255.255.255.0  broadcast 192.168.0.255<br>        inet6 fe80::20c:29ff:fe19:3efb  prefixlen 64  scopeid 0x20&lt;<span class="hljs-built_in">link</span>&gt;<br>        ether 00:0c:29:19:3e:fb  txqueuelen 1000  (Ethernet)<br>        RX packets 181  bytes 16447 (16.0 KiB)<br>        RX errors 0  dropped 0  overruns 0  frame 0<br>        TX packets 98  bytes 16871 (16.4 KiB)<br>        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0<br></code></pre></td></tr></table></figure><p>我的环境为ens33</p><p>修改配置文件，把onboot改错no，也就是开机不启动</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">/etc/sysconfig/network-scripts/ifcfg-ens33 <br>ONBOOT=<span class="hljs-string">&quot;no&quot;</span><br></code></pre></td></tr></table></figure><p>修改&#x2F;etc&#x2F;rc.local</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs bash">ifconfig ens33 down<br>ifconfig ens33 0.0.0.0<br>brctl addbr br0<br>brctl addif br0 ens33<br>ifconfig br0 192.168.0.101/24 up<br>brctl stp br0 off<br>route add default gw 192.168.0.1 br0<br></code></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">chmod</span> +x /etc/rc.d/rc.local<br></code></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ~]<span class="hljs-comment"># brctl show</span><br>bridge name     bridge <span class="hljs-built_in">id</span>               STP enabled     interfaces<br>br0             8000.000c29193efb       no              ens33<br></code></pre></td></tr></table></figure><p>可以看到br0已经桥接到了ens33上面去了，并且网络也没有中断</p><p>如果需要还原，就把&#x2F;etc&#x2F;rc.local这些注释掉，并且把onboot改成yes就可以了</p><p>或者通过脚本还原</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs bash">ifconfig ens33 down<br>ifconfig br0 down<br>brctl delif br0 ens33<br>brctl delbr br0<br>ifconfig ens33 192.168.0.101/24 up<br>route add default gw 192.168.0.1 ens33<br></code></pre></td></tr></table></figure><p>然后去掉rc.local和onboot改成yes就可以了</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>修改网卡的方式很多，本篇记录的是怎么方便快捷，怎么避免出错，并且比较好还原，不中断的修改网卡桥接的方式</p><h2 id="变更记录"><a href="#变更记录" class="headerlink" title="变更记录"></a>变更记录</h2><table><thead><tr><th align="center">Why</th><th align="center">Who</th><th align="center">When</th></tr></thead><tbody><tr><td align="center">创建</td><td align="center">武汉-运维-磨渣</td><td align="center">2020-01-09</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>计算机结构</title>
    <link href="/2019/12/09/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%93%E6%9E%84/"/>
    <url>/2019/12/09/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%93%E6%9E%84/</url>
    
    <content type="html"><![CDATA[<h2>计算机结构</h2><p>从大的结构来说，计算机方面的东西主要分为两个大的部分，一个为硬件的部分，也就是组成计算机的一些硬件的</p><p>一个是软件的部分，就是我们基于硬件之上的做的一些东西，通常来说，硬件的东西比较固定，而软件的东西就比较多了</p><h3>计算机硬件部分</h3><ul><li>CPU</li><li>硬盘</li><li>内存</li><li>主板</li><li>网络</li></ul><h3>计算机的软件部分</h3><ul><li>操作系统</li><li>内核</li><li>系统相关的软件</li><li>网络相关</li><li>linux基础操作</li><li>文件系统</li><li>分布式文件系统</li><li>san相关</li><li>高可用和负载均衡相关</li></ul><p>上面应该还有很多很多的内容，这个目录结构会随着时间的变化，慢慢填充起来</p><h2>软件开发</h2><p>这个章节是记录一些软件开发过程中的一些知识</p><h2>测试软件</h2><p>这个是一些测试相关的软件</p><p>基于以上的内容会慢慢的进行扩展,博客里面的内容会关于上面的这些方面</p><h2>更新历史</h2><table><thead><tr><th>why</th><th>when</th></tr></thead><tbody><tr><td>创建</td><td>2019年12月9日</td></tr><tr><td>更新</td><td>2019年12月9日</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>经验总结</category>
      
    </categories>
    
    
    <tags>
      
      <tag>计算机</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>ceph osd tree的可视化</title>
    <link href="/2019/09/19/ceph%20osd%20tree%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96/"/>
    <url>/2019/09/19/ceph%20osd%20tree%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>很久没有处理很大的集群，在接触一个新集群的时候，如果集群足够大，需要比较长的时间才能去理解这个集群的结构，而直接去看ceph osd tree的结果，当然是可以的，这里是把osd tree的结构进行了一个结构化输出，也可以理解为画出一个简单的结构图，比较适合给其它人讲解你对crush做了哪些改变，这个如果指着文字来讲估计很多人会听的云里雾里，如果有比较方便的方式出图就比较好了</p><p>为此写了一个小工具自己用，正好也可以看看我们对结构做简单调整后的效果</p><h2 id="创建一个模拟集群"><a href="#创建一个模拟集群" class="headerlink" title="创建一个模拟集群"></a>创建一个模拟集群</h2><p>环境就一台机器，不需要用到磁盘，这里是模拟结构<br>创建一个大集群40台机器</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">seq</span> 1 40 |xargs -i ceph  osd crush add-bucket lab&#123;&#125; host<br><span class="hljs-built_in">seq</span> 1 40|xargs -i ceph osd crush move lab&#123;&#125;  root=default<br></code></pre></td></tr></table></figure><p>创建一个960个的集群</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">seq</span> 1 960 |xargs -i ceph osd create<br></code></pre></td></tr></table></figure><p>放到指定的主机</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-meta">#! /bin/sh</span><br><span class="hljs-keyword">for</span> osd <span class="hljs-keyword">in</span> `<span class="hljs-built_in">seq</span> 0 959`<br><span class="hljs-keyword">do</span><br>host=$(( ((<span class="hljs-variable">$osd</span> / <span class="hljs-number">24</span>)) + 1 ))<br>ceph osd  crush create-or-move  osd.<span class="hljs-variable">$osd</span> 3.6  host=lab<span class="hljs-variable">$host</span> &amp;<br><span class="hljs-keyword">done</span><br></code></pre></td></tr></table></figure><p>这里后台执行会快点,不然得等15分钟，执行完检查下有没有还在处理的进程即可</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">ps -ef|grep create-or-move<br></code></pre></td></tr></table></figure><h2 id="osd的一些结构"><a href="#osd的一些结构" class="headerlink" title="osd的一些结构"></a>osd的一些结构</h2><p>机器如果在远程可以上传临时文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">ceph osd tree -f json-pretty &gt; zp.json<br>curl --upload-file ./zp.json https://transfer.sh/zp.json<br></code></pre></td></tr></table></figure><p>得到的地址下载即可</p><h3 id="默认结构，主机分组"><a href="#默认结构，主机分组" class="headerlink" title="默认结构，主机分组"></a>默认结构，主机分组</h3><p><img src="/images/blog/o_200901101152cephosdtree-host.png"></p><p>960个osd的效果如图，节点太多了，图片本身输出的时候是高清图片可以放大去看的，并且主机如果不是这么多，显示效果会好一点，这里我们看的清楚是主机分组的</p><h3 id="调整下rack结构"><a href="#调整下rack结构" class="headerlink" title="调整下rack结构"></a>调整下rack结构</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">seq</span> 1 8|xargs -i ceph  osd crush add-bucket rack&#123;&#125; rack<br><span class="hljs-built_in">seq</span> 1 8|xargs -i ceph osd crush move rack&#123;&#125; root=default<br></code></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-meta">#! /bin/sh</span><br><span class="hljs-keyword">for</span> host <span class="hljs-keyword">in</span> `<span class="hljs-built_in">seq</span> 1 40`<br><span class="hljs-keyword">do</span><br>rack=$((  ((((<span class="hljs-variable">$host</span> - <span class="hljs-number">1</span>)) / 5)) + 1 ))<br>ceph osd  crush move lab<span class="hljs-variable">$host</span> rack=rack<span class="hljs-variable">$rack</span> &amp;<br><span class="hljs-keyword">done</span><br></code></pre></td></tr></table></figure><h3 id="调整完结构我们再看下"><a href="#调整完结构我们再看下" class="headerlink" title="调整完结构我们再看下"></a>调整完结构我们再看下</h3><p><img src="/images/blog/o_200901101158cephosdtree-rack.png"></p><p>可以看到已经是多层的结构了，这个机器太多，我们来看下稍微少点机器的效果</p><h3 id="12台机器每台12个osd的效果"><a href="#12台机器每台12个osd的效果" class="headerlink" title="12台机器每台12个osd的效果"></a>12台机器每台12个osd的效果</h3><p>主机分组</p><p><img src="/images/blog/o_200901101205cephosdtree-smallhost.png"></p><p>rack分组</p><p><img src="/images/blog/o_200901101211cephosdtree-smallrack.png"></p><p>后面的两个图的就比较清晰了</p><p>工具的一个作用是，在调整结构后，能够比较方便的去检查osd的结构，是不是均匀的，有没有调整错误，有没有某个host里面的osd个数跟其他不一样，这几种情况之前都遇到过</p><ul><li>分rack的时候rack里面主机数目不一致，怎么调整pg都是不平衡</li><li>加osd的时候，某个主机的osd给漏掉了</li></ul><p>如果你面对的是几十个osd的时候，你还会看看，当超过一百个的时候，一般来说就是走过场了，都会侥幸的认为，应该没事吧，好像是对的，当然通过各种方式都能实现</p><p>目前这个小工具是直接运行直接出图的,配色后面再处理下效果</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>这个tree的图一直以为之前有写过，但是找不到代码了，重新实现了一次，也算一个梳理，这个图只是作为一个更大的体系里面很小的一部分的模块</p><p>本篇同样给出了模拟大集群的方式</p><p>Just for Fun</p><h2 id="变更记录"><a href="#变更记录" class="headerlink" title="变更记录"></a>变更记录</h2><table><thead><tr><th align="center">Why</th><th align="center">Who</th><th align="center">When</th></tr></thead><tbody><tr><td align="center">创建</td><td align="center">武汉-运维-磨渣</td><td align="center">2018-09-19</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>高性能arm运行ceph存储基准测试</title>
    <link href="/2019/09/09/%E9%AB%98%E6%80%A7%E8%83%BDarm%E8%BF%90%E8%A1%8Cceph%E5%AD%98%E5%82%A8%E5%9F%BA%E5%87%86%E6%B5%8B%E8%AF%95/"/>
    <url>/2019/09/09/%E9%AB%98%E6%80%A7%E8%83%BDarm%E8%BF%90%E8%A1%8Cceph%E5%AD%98%E5%82%A8%E5%9F%BA%E5%87%86%E6%B5%8B%E8%AF%95/</url>
    
    <content type="html"><![CDATA[<h2 id="关于arm"><a href="#关于arm" class="headerlink" title="关于arm"></a>关于arm</h2><p>之前wdlab对外发布过一次约500个节点的arm的ceph集群，那个采用的是微集群的结构，使用的是双核的cortex-a9 ARM处理器，运行速度为1.3 GHz，内存为1 GB，直接焊接到驱动器的PCB上，选项包括2 GB内存和ECC保护</p><p>这个在国内也有类似的实现，深圳瑞驰商用Arm存储NxCells</p><p><img src="/images/blog/o_200901101035smallarm.png" alt="small-arm"></p><p>这个采用的是微集群的架构，能够比较好的应对一些冷存场景，但是今天要说的不是这种架构，而是一个比较新的平台，采用的是高性能的arm的架构，也就是类似X86的大主板结构</p><p>很多人了解的arm的特点是小，功耗低，主频低，这个是以前的arm想发力的场景，类似于intel做的一款atom，在很早期的时候，我在的公司也尝试过基于atom主板做过1U的ceph存储，但是后来各种原因没有继续下去</p><p>实际上arm也在发力高性能的场景，但是这个比较新，并不是每个人都能接触的到，在这里，我把我们的硬件设备的测试数据发一部分出来，也许能改变你对arm的印象，在未来硬件选型的时候，也许就多了一层选择</p><h2 id="高性能arm设备说明"><a href="#高性能arm设备说明" class="headerlink" title="高性能arm设备说明"></a>高性能arm设备说明</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs bash">System Information<br>  PROCESSOR:          Ampere eMAG ARMv8 @ 3.00GHz<br>    Core Count:       32                        <br>    Scaling Driver:   cppc_cpufreq conservative <br><br>  GRAPHICS:           ASPEED<br>    Screen:           1024x768         <br><br>  MOTHERBOARD:        MiTAC RAPTOR<br>    BIOS Version:     0.11                                               <br>    Chipset:          Ampere Computing LLC Skylark                       <br>    Network:          2 x Intel 82599ES 10-Gigabit SFI/SFP+ + Intel I210 <br><br>  MEMORY:             2 x 32 GB DDR4-2667MT/s Samsung M393A4K40BB2-CTD<br><br>  DISK:               FORESEE 256GB SS + 6001GB Seagate ST6000NM0115-1YZ<br>    File-System:      xfs                               <br>    Mount Options:    attr2 inode64 noquota relatime rw <br>    Disk Scheduler:   DEADLINE                          <br><br>  OPERATING SYSTEM:   CentOS Linux 7<br>    Kernel:           4.14.0-115.el7a.0.1.aarch64 (aarch64) 20181125          <br>    Compiler:         GCC 4.8.5 20150623                                      <br>    Security:         meltdown: Mitigation of PTI                             <br>                      + spec_store_bypass: Not affected                       <br>                      + spectre_v1: Mitigation of __user pointer sanitization <br>                      + spectre_v2: Vulnerable             <br><br></code></pre></td></tr></table></figure><p>机器采用的是ampere的最新的平台RAPTOR平台进行的测试，这里只是做了最小环境的测试，因为测试设备被占用了，就利用有限的资源进行测试即可,机器上面的插口都没有什么限制，支持pcie插槽，能够自己扩展到多盘位机器，也支持万兆网卡，本次测试采用的就是36盘位的机器，对于36盘位的机器来说，底层的磁盘的总资源肯定是大于网络带宽的，这样也有个好处就是磁盘在前端业务满载的时候，不会那么忙</p><h2 id="测试环境"><a href="#测试环境" class="headerlink" title="测试环境"></a>测试环境</h2><p>测试环境说明</p><p>环境为单机36盘位的ceph12版本，配置的12.2.10版本的ceph，使用的是bluestore，设置的副本1，然后用一台X86机器作为客户端，客户端和服务器之间通过万兆相连，对比测试的机器同样为36盘位的机器，为了保证环境的一致性，测试过程中集群没有重搭，直接把盘换了一个平台进行了启动后进行测试</p><p>磁盘有36块6T的sata盘</p><p>本次测试测试了两组数据</p><ul><li>机器的主机带宽（vdbench测试arm的）</li><li>机器带上ceph以后的输出带宽</li></ul><p>本次测试只在现有的环境下对比，测试模型很多，同样的X86选型都可以选出各种各样的，这里我只拿我现有的机器进行的测试，X86和arm的都为32 processor的服务器，内存一致</p><h3 id="主机带宽"><a href="#主机带宽" class="headerlink" title="主机带宽"></a>主机带宽</h3><p>主机vdbench测试</p><table><thead><tr><th align="center">测试项目</th><th align="center">测试时长</th><th align="center">IOPS</th><th align="center">response</th><th align="center">带宽</th></tr></thead><tbody><tr><td align="center">4K随机写</td><td align="center">600s</td><td align="center">13123.1</td><td align="center">5.400</td><td align="center">51.26M&#x2F;s</td></tr><tr><td align="center">4K随机读</td><td align="center">600s</td><td align="center">3463.0</td><td align="center">20.782</td><td align="center">13.53M&#x2F;s</td></tr><tr><td align="center">1M顺序写</td><td align="center">600s</td><td align="center">3661.6</td><td align="center">19.360</td><td align="center">3661.63M&#x2F;s</td></tr><tr><td align="center">1M顺序读</td><td align="center">600s</td><td align="center">3857.8</td><td align="center">18.657</td><td align="center">3857.82M&#x2F;s</td></tr></tbody></table><p>这个是vdbench 对主机带宽进行的测试，为什么需要这个测试，很久以前看过一篇博客，是讲fio测试的，博主提出了一个概念，你如果只是测试一个磁盘的数据，然后相加，这个数据可能跟实际偏离很多，还有个情况就是cpu的处理io的并发能力同样会影响最终输出，也就是你的写入越接近最终的并发写入，越能反应最终的可能的最大输出带宽，所以这个地方通过fio或者vdbench都能测出主机带宽，在知道可能存在这个问题以后，每次都会测一测，到目前为止已经发现了两次问题</p><p>第一次是前面的硬盘的面板带宽的，之前有次测试的数据24个盘不管怎么样都是1.9G，在到了19个盘之后的数据就不再上升，磁盘utils一直都是100%，后来查出来是面板驱动问题，找硬件厂商刷了下驱动后，带宽就恢复正常</p><p>一次是本次测试同样36盘一直跑出来发现只有2G&#x2F;s,排查以后发现是内部的lsi的卡到面板只用了一根线，虽然看到的是36个盘，但是受限于线的带宽，只有2G&#x2F;s，把线换成2根以后，性能就到了上面的3.6G&#x2F;s了，这个如果不跑下整机带宽，也许没发现，或者后续会怀疑是不是本身软件处理慢了</p><p>本次测试主要是sata的盘的，也就是跑的带宽模式，大IO的场景，需要小io的场景的需要ssd的环境了，不在本篇讨论范围内</p><h3 id="集群的测试数据"><a href="#集群的测试数据" class="headerlink" title="集群的测试数据"></a>集群的测试数据</h3><p>arm和X86测试数据对比</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">rados -p rbd -t 32 -b 64K bench  300 write --no-cleanup --run-name 64kt32<br>rados -p rbd -t 8 -b 4096K bench  300 write --no-cleanup --run-name 4Mt8<br></code></pre></td></tr></table></figure><p>命令的例子，根据不同的用例进行调整参数</p><h4 id="64K块大小性能"><a href="#64K块大小性能" class="headerlink" title="64K块大小性能"></a>64K块大小性能</h4><table><thead><tr><th align="center">读&#x2F;写</th><th align="center">块大小&#x2F;并发数</th><th align="center">Bandwidth(MB&#x2F;s)</th><th align="center">Average IOPS</th><th align="center">Average Latency(ms)</th></tr></thead><tbody><tr><td align="center">arm写</td><td align="center">64K&#x2F;32</td><td align="center">39.8914</td><td align="center">638</td><td align="center">0.0501339</td></tr><tr><td align="center">X86写</td><td align="center">64K&#x2F;32</td><td align="center">39.3184</td><td align="center">629</td><td align="center">0.0508632</td></tr><tr><td align="center">arm写</td><td align="center">64K&#x2F;64</td><td align="center">67.283</td><td align="center">1076</td><td align="center">0.0594479</td></tr><tr><td align="center">X86写</td><td align="center">64K&#x2F;64</td><td align="center">66.0471</td><td align="center">1056</td><td align="center">0.0605586</td></tr><tr><td align="center">arm写</td><td align="center">64K&#x2F;128</td><td align="center">110.14</td><td align="center">1762</td><td align="center">0.0726323</td></tr><tr><td align="center">X86写</td><td align="center">64K&#x2F;128</td><td align="center">108.525</td><td align="center">1736</td><td align="center">0.0737129</td></tr><tr><td align="center">arm写</td><td align="center">64K&#x2F;256</td><td align="center">177.103</td><td align="center">2833</td><td align="center">0.0903411</td></tr><tr><td align="center">X86写</td><td align="center">64K&#x2F;256</td><td align="center">190.263</td><td align="center">3044</td><td align="center">0.0840909</td></tr><tr><td align="center">arm写</td><td align="center">64K&#x2F;512</td><td align="center">260.715</td><td align="center">4171</td><td align="center">0.122732</td></tr><tr><td align="center">X86写</td><td align="center">64K&#x2F;512</td><td align="center">261.358</td><td align="center">4181</td><td align="center">0.122422</td></tr><tr><td align="center">arm读</td><td align="center">64K&#x2F;32</td><td align="center">1060.96</td><td align="center">16975</td><td align="center">0.00186769</td></tr><tr><td align="center">X86读</td><td align="center">64K&#x2F;32</td><td align="center">958.661</td><td align="center">15338</td><td align="center">0.00206133</td></tr><tr><td align="center">arm读</td><td align="center">64K&#x2F;64</td><td align="center">1026.75</td><td align="center">16428</td><td align="center">0.00387973</td></tr><tr><td align="center">X86读</td><td align="center">64K&#x2F;64</td><td align="center">946.426</td><td align="center">15142</td><td align="center">0.00419585</td></tr><tr><td align="center">arm读</td><td align="center">64K&#x2F;128</td><td align="center">1082.39</td><td align="center">17318</td><td align="center">0.007375</td></tr><tr><td align="center">X86读</td><td align="center">64K&#x2F;128</td><td align="center">931.589</td><td align="center">14905</td><td align="center">0.00855418</td></tr><tr><td align="center">arm读</td><td align="center">64K&#x2F;256</td><td align="center">1116.87</td><td align="center">17869</td><td align="center">0.0143076</td></tr><tr><td align="center">X86读</td><td align="center">64K&#x2F;256</td><td align="center">1001.93</td><td align="center">16030</td><td align="center">0.0159373</td></tr><tr><td align="center">arm读</td><td align="center">64K&#x2F;512</td><td align="center">1116.81</td><td align="center">17868</td><td align="center">0.0286263</td></tr><tr><td align="center">X86读</td><td align="center">64K&#x2F;512</td><td align="center">1008.51</td><td align="center">16136</td><td align="center">0.0316968</td></tr></tbody></table><h4 id="4M块大小性能"><a href="#4M块大小性能" class="headerlink" title="4M块大小性能"></a>4M块大小性能</h4><table><thead><tr><th align="center">读&#x2F;写</th><th align="center">块大小&#x2F;并发数</th><th align="center">Bandwidth(MB&#x2F;s)</th><th align="center">Average IOPS</th><th align="center">Average Latency(ms)</th></tr></thead><tbody><tr><td align="center">arm写</td><td align="center">4M&#x2F;8</td><td align="center">377.74</td><td align="center">94</td><td align="center">0.0847078</td></tr><tr><td align="center">X86写</td><td align="center">4M&#x2F;8</td><td align="center">377.604</td><td align="center">94</td><td align="center">0.0847369</td></tr><tr><td align="center">arm写</td><td align="center">4M&#x2F;16</td><td align="center">676.168</td><td align="center">169</td><td align="center">0.094649</td></tr><tr><td align="center">X86写</td><td align="center">4M&#x2F;16</td><td align="center">670.021</td><td align="center">167</td><td align="center">0.0955143</td></tr><tr><td align="center">arm写</td><td align="center">4M&#x2F;32</td><td align="center">900.391</td><td align="center">225</td><td align="center">0.142123</td></tr><tr><td align="center">X86写</td><td align="center">4M&#x2F;32</td><td align="center">902.573</td><td align="center">225</td><td align="center">0.1418</td></tr><tr><td align="center">arm写</td><td align="center">4M&#x2F;64</td><td align="center">901.906</td><td align="center">225</td><td align="center">0.283723</td></tr><tr><td align="center">X86写</td><td align="center">4M&#x2F;64</td><td align="center">902.953</td><td align="center">225</td><td align="center">0.28335</td></tr><tr><td align="center">arm写</td><td align="center">4M&#x2F;128</td><td align="center">903.476</td><td align="center">225</td><td align="center">0.566172</td></tr><tr><td align="center">X86写</td><td align="center">4M&#x2F;128</td><td align="center">904.942</td><td align="center">226</td><td align="center">0.56528</td></tr><tr><td align="center">arm读</td><td align="center">4M&#x2F;8</td><td align="center">906.495</td><td align="center">226</td><td align="center">0.0347298</td></tr><tr><td align="center">X86读</td><td align="center">4M&#x2F;8</td><td align="center">784.499</td><td align="center">196</td><td align="center">0.0395726</td></tr><tr><td align="center">arm读</td><td align="center">4M&#x2F;16</td><td align="center">968.325</td><td align="center">242</td><td align="center">0.0655441</td></tr><tr><td align="center">X86读</td><td align="center">4M&#x2F;16</td><td align="center">1091.42</td><td align="center">272</td><td align="center">0.0570873</td></tr><tr><td align="center">arm读</td><td align="center">4M&#x2F;32</td><td align="center">1074.49</td><td align="center">268</td><td align="center">0.118411</td></tr><tr><td align="center">X86读</td><td align="center">4M&#x2F;32</td><td align="center">1108.76</td><td align="center">277</td><td align="center">0.113665</td></tr><tr><td align="center">arm读</td><td align="center">4M&#x2F;64</td><td align="center">1053.77</td><td align="center">263</td><td align="center">0.242012</td></tr><tr><td align="center">X86读</td><td align="center">4M&#x2F;64</td><td align="center">1116.84</td><td align="center">279</td><td align="center">0.227442</td></tr><tr><td align="center">arm读</td><td align="center">4M&#x2F;128</td><td align="center">1121.86</td><td align="center">280</td><td align="center">0.4553</td></tr><tr><td align="center">X86读</td><td align="center">4M&#x2F;128</td><td align="center">1102.44</td><td align="center">275</td><td align="center">0.462227</td></tr></tbody></table><p>从上面的两大组数据可以看到，在这个测试模型下面，这个arm的机器的性能并不差，两款硬件在同样的测试压力，和同等磁盘的情况下，基本维持了跟X86一致的水平</p><p>这个如果硬要说哪款好，我觉得那也不是一下两下说的清楚的，只能说给出一个测试模型，然后两个同样的压力同样环境去做比较，这样就太多场景了，并且也还会考虑成本的问题，兼容性的问题，如果在各方面都差不多的情况下，这个不失为一种选择</p><p>硬盘还分为sata，sas，ssd，高铁也有普通列车，和谐号，复兴号的差别，这个看怎么去定位硬件本身的输出能力了，这里从测试数据来看，还是一款不错的高性能arm硬件，当然需要到更多的环境下面去适应和磨合了</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>在安培的arm高性能机器下，也能跑出跟X86下的差不多的性能了，虽然还不能说绝对能去完全替换X86，但是目前看性能还是很不错的，值得一试，如果价格合适，又满足需求，还是可行的</p><h2 id="变更记录"><a href="#变更记录" class="headerlink" title="变更记录"></a>变更记录</h2><table><thead><tr><th align="center">Why</th><th align="center">Who</th><th align="center">When</th></tr></thead><tbody><tr><td align="center">创建</td><td align="center">武汉-运维-磨渣</td><td align="center">2018-09-09</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>bluestore的osd自启动</title>
    <link href="/2019/09/04/bluestore%E7%9A%84osd%E8%87%AA%E5%90%AF%E5%8A%A8/"/>
    <url>/2019/09/04/bluestore%E7%9A%84osd%E8%87%AA%E5%90%AF%E5%8A%A8/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>自启动相关的文章很多，有分析的很详细的文章，这里就不做赘述，本篇讲述的是什么情况下用，怎么用的问题</p><h2 id="使用场景"><a href="#使用场景" class="headerlink" title="使用场景"></a>使用场景</h2><p>一台机器的系统盘坏了，需要重装系统，相关的一些信息没有了，但是上面的数据盘还是在的，所以需要保留</p><p>某个磁盘需要换台机器进行启动，但是那台机器上没有相关的信息</p><h2 id="处理过程"><a href="#处理过程" class="headerlink" title="处理过程"></a>处理过程</h2><h3 id="自启动的相关处理"><a href="#自启动的相关处理" class="headerlink" title="自启动的相关处理"></a>自启动的相关处理</h3><p>先扫描下lvm</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">vgscan<br>pvscan<br>lvscan<br></code></pre></td></tr></table></figure><p>本篇的场景是lvm没有损坏的情况，如果lvm本身损坏了，那么就是去恢复lvm的问题，本篇的基础是有一个完整的osd的数据盘，也就是磁盘本身是没问题的</p><h3 id="查询osd相关的磁盘信息"><a href="#查询osd相关的磁盘信息" class="headerlink" title="查询osd相关的磁盘信息"></a>查询osd相关的磁盘信息</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs bash">lvdisplay |grep <span class="hljs-string">&quot;LV Path&quot;</span>|grep ceph<br>    LV Path                /dev/ceph-b748833c-b646-4b1c-a2ef-f50576b0a165/osd-block-38657557-5ce3-43a1-861a-e690c880ddf6<br>    LV Path                /dev/ceph-aa2304f1-a098-4990-8f3a-46f176d4cece/osd-block-f8a30c38-48fd-465c-9982-14cd22d00d21<br>    LV Path                /dev/ceph-8b987af1-f10a-4c9a-a096-352e63c7ef83/osd-block-07d1c423-8777-4eea-8a1d-34dc06f840ae<br>    LV Path                /dev/ceph-f39ac1da-2811-4486-8690-4ccfb1e45e18/osd-block-0cb9186e-6512-4582-a30d-9fb4cf03c964<br>    LV Path                /dev/ceph-6167d452-a121-4602-836a-ab378cf6eccc/osd-block-2e77e3b5-9d5c-4d5f-bf18-c33ddf0bbc0a<br></code></pre></td></tr></table></figure><p>注意osd-block后面的字段，这个信息是会记录在osd dump输出信息的，我们查询下osd-block-38657557-5ce3-43a1-861a-e690c880ddf6这个的信息</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@node1 ~]<span class="hljs-comment"># ceph osd dump|grep 38657557-5ce3-43a1-861a-e690c880ddf6</span><br>osd.31 down <span class="hljs-keyword">in</span>  weight 1 up_from 395 up_thru 395 down_at 399 last_clean_interval [391,392) 66.66.66.60:6830/10392 66.66.66.60:6847/10392 66.66.66.60:6875<br>/10392 66.66.66.60:6882/10392 exists 38657557-5ce3-43a1-861a-e690c880ddf6<br></code></pre></td></tr></table></figure><h3 id="做自动挂载"><a href="#做自动挂载" class="headerlink" title="做自动挂载"></a>做自动挂载</h3><p>可以得到如下信息，osd.31的lvm标记为38657557-5ce3-43a1-861a-e690c880ddf6</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">systemctl start ceph-volume@lvm-31-38657557-5ce3-43a1-861a-e690c880ddf6<br>systemctl <span class="hljs-built_in">enable</span> ceph-volume@lvm-31-38657557-5ce3-43a1-861a-e690c880ddf6<br></code></pre></td></tr></table></figure><p>检查下挂载</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@node1 ~]<span class="hljs-comment"># df -h|grep osd|grep 31</span><br>tmpfs                     48G   24K   48G   1% /var/lib/ceph/osd/ceph-31<br></code></pre></td></tr></table></figure><p>可以看到挂载的操作是通过下面这个命令进行挂载的，然后enable下就是自启动了</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">systemctl start ceph-volume@lvm-osdid-osdfsid<br></code></pre></td></tr></table></figure><h3 id="做自启动osd"><a href="#做自启动osd" class="headerlink" title="做自启动osd"></a>做自启动osd</h3><p>启动osd</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@node1 ~]<span class="hljs-comment"># systemctl start ceph-osd@31</span><br>[root@node1 ~]<span class="hljs-comment"># systemctl enable ceph-osd@31</span><br>Created symlink from /etc/systemd/system/ceph-osd.target.wants/ceph-osd@31.service to /usr/lib/systemd/system/ceph-osd@.service.<br></code></pre></td></tr></table></figure><h3 id="检查启动情况"><a href="#检查启动情况" class="headerlink" title="检查启动情况"></a>检查启动情况</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@node1 ~]<span class="hljs-comment"># ps -ef|grep osd|grep 31</span><br>ceph       31177       1  1 10:42 ?        00:00:02 /usr/bin/ceph-osd -f --cluster ceph --<span class="hljs-built_in">id</span> 31 --setuser ceph --setgroup ceph<br></code></pre></td></tr></table></figure><p>那么自挂载，自启动的过程就是上面的操作</p><h2 id="脚本处理整机的osd"><a href="#脚本处理整机的osd" class="headerlink" title="脚本处理整机的osd"></a>脚本处理整机的osd</h2><p>如果觉得一个个去查询太麻烦了，那就准备脚本就好了<br>创建startosd.sh脚本写入下面的内容</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-meta">#! /usr/bin/bash</span><br><span class="hljs-comment"># @Author:momo</span><br><span class="hljs-comment"># @Time: 2019/9/4 11:05</span><br>vgscan<br>pvscan<br>lvscan<br>osddump=`ceph osd dump`<br><br><span class="hljs-keyword">for</span> osdfsid <span class="hljs-keyword">in</span> `lvdisplay |grep <span class="hljs-string">&quot;LV Path&quot;</span>|grep ceph|awk <span class="hljs-string">&#x27;&#123;print $3&#125;&#x27;</span>|<span class="hljs-built_in">cut</span> -d <span class="hljs-string">&quot;/&quot;</span> -f 4|<span class="hljs-built_in">cut</span> -d - -f 3-7`<br><span class="hljs-keyword">do</span><br>    osdid=`<span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;<span class="hljs-variable">$&#123;osddump&#125;</span>&quot;</span>|grep <span class="hljs-variable">$osdfsid</span>|awk <span class="hljs-string">&#x27;&#123;print $1&#125;&#x27;</span>|<span class="hljs-built_in">cut</span> -d . -f 2`<br>    <span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;start <span class="hljs-variable">$osdid</span>  with fsid <span class="hljs-variable">$osdfsid</span>&quot;</span><br>    <span class="hljs-comment"># auto mount</span><br>    systemctl start ceph-volume@lvm-<span class="hljs-variable">$osdid</span>-<span class="hljs-variable">$osdfsid</span><br>    systemctl <span class="hljs-built_in">enable</span> ceph-volume@lvm-<span class="hljs-variable">$osdid</span>-<span class="hljs-variable">$osdfsid</span><br>    <span class="hljs-comment"># auto start </span><br>    systemctl start ceph-osd@<span class="hljs-variable">$osdid</span><br>    systemctl <span class="hljs-built_in">enable</span> ceph-osd@<span class="hljs-variable">$osdid</span><br><span class="hljs-keyword">done</span><br></code></pre></td></tr></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本篇是因为做对比测试，不想重搭集群，把36盘位的磁盘全部换一个平台测试，想直接启动起来测试，然后就有了这篇文章记录，在filestore下面的处理逻辑比较简单，可以用fstab，可以用trigger，可以用mount tmp检查后手动挂载，方式很多，从Jewel版本开始启动相关的都慢慢集成到数据本身，用服务去控制了</p><h2 id="变更记录"><a href="#变更记录" class="headerlink" title="变更记录"></a>变更记录</h2><table><thead><tr><th align="center">Why</th><th align="center">Who</th><th align="center">When</th></tr></thead><tbody><tr><td align="center">创建</td><td align="center">武汉-运维-磨渣</td><td align="center">2018-09-04</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>ceph与flashcache的around模式结合启动问题</title>
    <link href="/2019/09/04/ceph%E4%B8%8Eflashcache%E7%9A%84around%E6%A8%A1%E5%BC%8F%E7%BB%93%E5%90%88%E5%90%AF%E5%8A%A8%E9%97%AE%E9%A2%98/"/>
    <url>/2019/09/04/ceph%E4%B8%8Eflashcache%E7%9A%84around%E6%A8%A1%E5%BC%8F%E7%BB%93%E5%90%88%E5%90%AF%E5%8A%A8%E9%97%AE%E9%A2%98/</url>
    
    <content type="html"><![CDATA[<h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>通过对我们的启动流程看了下，目前是穿到一个脚本里面的，然后这个脚本是用无限循环的方式去执行一些事情，这个地方不符合松耦合的设计，一个模块做一个事情，两个并不相关的功能不要嵌入另一个脚本，否则出现问题的时候，不好更改不好优化</p><h2 id="解决方式"><a href="#解决方式" class="headerlink" title="解决方式"></a>解决方式</h2><p>首先分析ceph自身的启动方式</p><p>ceph的启动方式是通过去enable的一个service的方式这个enable会传入参数，osd的id和osd的fsid，这两个都是集群的定值，也就是每个osd的这块都是各自独立的，所以就是一个总控脚本去通过调用参数的方式进行服务的启动和挂载</p><p>那么最佳的处理方式应该也是近似处理，我们做结合启动的时候，先禁用相关的服务，这个后面脚本里面内部会处理，我们先写出来怎么禁用ceph的挂载和启动</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">systemctl stop ceph-osd@<span class="hljs-variable">$osd_id</span><br>systemctl <span class="hljs-built_in">disable</span> ceph-osd@<span class="hljs-variable">$osd_id</span><br>systemctl <span class="hljs-built_in">disable</span> ceph-volume@lvm-<span class="hljs-variable">$osd_id</span>-<span class="hljs-variable">$osd_dev</span><br>umount /var/lib/ceph/osd/ceph-0/<br></code></pre></td></tr></table></figure><p>上面一个是osd id  一个是fsid<br>fsid可以从ceph osd dump|grep osdid获得<br>初始部署成功后，机器的上面的磁盘也会打上上面的相关的标签</p><p>启动禁止了，开始写我们的启动服务<br>我们看下原生的服务</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ~]<span class="hljs-comment"># cat /usr/lib/systemd/system/ceph-volume@.service</span><br>[Unit]<br>Description=Ceph Volume activation: %i<br>After=local-fs.target<br>Wants=local-fs.target<br><br>[Service]<br>Type=oneshot<br>KillMode=none<br>Environment=CEPH_VOLUME_TIMEOUT=10000<br>ExecStart=/bin/sh -c <span class="hljs-string">&#x27;timeout $CEPH_VOLUME_TIMEOUT /usr/sbin/ceph-volume-systemd %i&#x27;</span><br>TimeoutSec=0<br><br>[Install]<br>WantedBy=multi-user.target<br></code></pre></td></tr></table></figure><p>我们写我们自己的服务</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ~]<span class="hljs-comment"># cat /usr/lib/systemd/system/ceph-volume-flashcache@.service</span><br>[Unit]<br>Description=Ceph flash cache Volume activation: %i<br>After=local-fs.target<br>Wants=local-fs.target<br><br>[Service]<br>Type=oneshot<br>KillMode=none<br>ExecStart=/bin/sh -c <span class="hljs-string">&#x27;timeout 10000 /usr/lib/ceph/ceph-load-flashcache.sh  %i&#x27;</span><br>TimeoutSec=0<br><br>[Install]<br>WantedBy=multi-user.target<br></code></pre></td></tr></table></figure><p>这是总控服务，我们传参进去，用加载脚本处理</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ~]<span class="hljs-comment"># cat  /usr/lib/ceph/ceph-load-flashcache.sh</span><br><span class="hljs-comment">#! /usr/bin/sh</span><br><br><span class="hljs-function"><span class="hljs-title">startflashcache</span></span>()&#123;<br>    ssd_dev=`<span class="hljs-built_in">echo</span> <span class="hljs-variable">$1</span>|awk -F <span class="hljs-string">&#x27;--&#x27;</span> <span class="hljs-string">&#x27;&#123;print $1&#125;&#x27;</span>`<br>    osd_id=`<span class="hljs-built_in">echo</span> <span class="hljs-variable">$1</span>|awk -F <span class="hljs-string">&#x27;--&#x27;</span> <span class="hljs-string">&#x27;&#123;print $2&#125;&#x27;</span>`<br>    osd_dev=`<span class="hljs-built_in">echo</span> <span class="hljs-variable">$1</span>|awk -F <span class="hljs-string">&#x27;--&#x27;</span> <span class="hljs-string">&#x27;&#123;print $3&#125;&#x27;</span>`<br><br>    <span class="hljs-built_in">echo</span> <span class="hljs-variable">$ssd_dev</span><br>    <span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;<span class="hljs-variable">$osd_id</span>&quot;</span><br>    <span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;<span class="hljs-variable">$osd_dev</span>&quot;</span><br><br>    <span class="hljs-comment"># 先停掉osd</span><br>    systemctl <span class="hljs-built_in">disable</span> ceph-volume@lvm-0-bcdb55b0-e95b-4833-8362-18f633782632<br>    systemctl stop ceph-osd@<span class="hljs-variable">$osd_id</span><br>    systemctl <span class="hljs-built_in">disable</span> ceph-osd@<span class="hljs-variable">$osd_id</span><br>    systemctl <span class="hljs-built_in">disable</span> ceph-volume@lvm-<span class="hljs-variable">$osd_id</span>-<span class="hljs-variable">$osd_dev</span><br>    <span class="hljs-comment"># umount osd</span><br>    umount /var/lib/ceph/osd/ceph-<span class="hljs-variable">$osd_id</span><br><br>    <span class="hljs-comment">#remove 原来的虚拟设备 我们确定arond的</span><br>    <span class="hljs-keyword">if</span> [ ! -f <span class="hljs-string">&quot;/dev/mapper/<span class="hljs-variable">$osd_id</span>&quot;</span> ]; <span class="hljs-keyword">then</span><br>      dmsetup remove osd<span class="hljs-variable">$osd_id</span><br>      <span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;remove old flashcache /dev/mapper/<span class="hljs-variable">$osd_id</span>&quot;</span><br>    <span class="hljs-keyword">fi</span><br><br>    <span class="hljs-comment"># if osd exit</span><br>    ssd_path=<span class="hljs-string">&quot;/dev/disk/by-partuuid/<span class="hljs-variable">$ssd_dev</span>&quot;</span><br>    osd_lv_path=`lvdisplay |grep <span class="hljs-variable">$osd_dev</span> |grep <span class="hljs-string">&quot;LV Path&quot;</span>|awk <span class="hljs-string">&#x27;&#123;print $3&#125;&#x27;</span>`<br><br>    <span class="hljs-comment">#if path exist</span><br>    <span class="hljs-built_in">ls</span> -al <span class="hljs-variable">$ssd_path</span><br>    <span class="hljs-built_in">ls</span> -al <span class="hljs-variable">$osd_lv_path</span><br><br>    <span class="hljs-comment">#创建 around flashcache</span><br>    flashcache_create -p around osd<span class="hljs-variable">$osd_id</span> <span class="hljs-variable">$ssd_path</span> <span class="hljs-variable">$osd_lv_path</span><br><br>    <span class="hljs-built_in">ls</span> -al  /dev/mapper/osd<span class="hljs-variable">$osd_id</span><br>    flashcache_dev_dm=/dev/`<span class="hljs-built_in">ls</span> -al /dev/mapper/osd0|awk <span class="hljs-string">&#x27;&#123;print $11&#125;&#x27;</span>|<span class="hljs-built_in">cut</span> -d <span class="hljs-string">&quot;/&quot;</span> -f 2`<br><br>    <span class="hljs-built_in">echo</span> <span class="hljs-variable">$flashcache_dev_dm</span><br><br>    mount -t tmpfs tmpfs /var/lib/ceph/osd/ceph-<span class="hljs-variable">$osd_id</span><br>    restorecon /var/lib/ceph/osd/ceph-<span class="hljs-variable">$osd_id</span><br>    <span class="hljs-built_in">chown</span> -R ceph:ceph <span class="hljs-variable">$flashcache_dev_dm</span><br>    <span class="hljs-built_in">chown</span> -R ceph:ceph /var/lib/ceph/osd/ceph-<span class="hljs-variable">$osd_id</span><br><br>    <span class="hljs-comment">#进入部署流程</span><br><br>    ceph-bluestore-tool --cluster=ceph prime-osd-dir --dev /dev/mapper/osd<span class="hljs-variable">$osd_id</span> --path /var/lib/ceph/osd/ceph-<span class="hljs-variable">$osd_id</span><br>    <span class="hljs-built_in">ln</span> -snf /dev/mapper/osd<span class="hljs-variable">$osd_id</span> /var/lib/ceph/osd/ceph-0/block<br>    <span class="hljs-built_in">chown</span> -h ceph:ceph /var/lib/ceph/osd/ceph-0/block<br>    <span class="hljs-built_in">chown</span> ceph:ceph -R /var/lib/ceph/osd/ceph-0<br>    <span class="hljs-built_in">chown</span> -R ceph:ceph /dev/mapper/osd<span class="hljs-variable">$osd_id</span><br>    systemctl start ceph-osd@<span class="hljs-variable">$osd_id</span><br>&#125;<br><br>startflashcache <span class="hljs-variable">$1</span><br></code></pre></td></tr></table></figure><p>脚本内容的内容全部是从ceph内部启动流程给剥离出来的，也就是全部按照ceph的自身的启动方式处理，只是加入了flashcache的处理，每次启动前，去掉缓存设备，这个flashcache的arond的模式是每次新加载的</p><h2 id="怎么用"><a href="#怎么用" class="headerlink" title="怎么用"></a>怎么用</h2><p>预制前提是部署好了一个osd<br>我们准备添加flashcache<br>获取缓存设备的uuid,就是PARTUUID</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ~]<span class="hljs-comment"># blkid /dev/sdc1</span><br>/dev/sdc1: PARTLABEL=<span class="hljs-string">&quot;primary&quot;</span> PARTUUID=<span class="hljs-string">&quot;3b3546e5-65e5-426e-9659-f2e0d37a0895&quot;</span> <br></code></pre></td></tr></table></figure><p>获取准备加缓存的osd id</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ~]<span class="hljs-comment"># cat /var/lib/ceph/osd/ceph-0/whoami </span><br>0<br>[root@lab101 ~]<span class="hljs-comment"># cat /var/lib/ceph/osd/ceph-0/fsid </span><br>bcdb55b0-e95b-4833-8362-18f633782632<br></code></pre></td></tr></table></figure><p>得到0 和bcdb55b0-e95b-4833-8362-18f633782632</p><p>我们写入启动服务</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">systemctl <span class="hljs-built_in">enable</span> ceph-volume-flashcache@3b3546e5-65e5-426e-9659-f2e0d37a0895--0--bcdb55b0-e95b-4833-8362-18f633782632.service<br></code></pre></td></tr></table></figure><p>注意@后面有三个值，第一个是cache盘的uuid，第二个值为0，就是osd的id，第三个值就是osd的fsid，中间用–相连</p><p>如果想查询本机做了多少个flashcache的自启动</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ~]<span class="hljs-comment">#ls /etc/systemd/system/multi-user.target.wants/ceph-volume-flashcache*</span><br>/etc/systemd/system/multi-user.target.wants/ceph-volume-flashcache@3b3546e5-65e5-426e-9659-f2e0d37a0895--0--bcdb55b0-e95b-4833-8362-18f633782632.service<br></code></pre></td></tr></table></figure><p>执行加缓存的操作</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">systemctl start ceph-volume-flashcache@3b3546e5-65e5-426e-9659-f2e0d37a0895--0--bcdb55b0-e95b-4833-8362-18f633782632.service<br></code></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ~]<span class="hljs-comment"># ll /var/lib/ceph/osd/ceph-0</span><br>total 24<br>lrwxrwxrwx. 1 ceph ceph 16 Oct 25 14:13 block -&gt; /dev/mapper/osd0<br>-rw-------. 1 ceph ceph 37 Oct 25 14:13 ceph_fsid<br>-rw-------. 1 ceph ceph 37 Oct 25 14:13 fsid<br>-rw-------. 1 ceph ceph 55 Oct 25 14:13 keyring<br>-rw-------. 1 ceph ceph  6 Oct 25 14:13 ready<br>-rw-------. 1 ceph ceph 10 Oct 25 14:13 <span class="hljs-built_in">type</span><br>-rw-------. 1 ceph ceph  2 Oct 25 14:13 <span class="hljs-built_in">whoami</span><br></code></pre></td></tr></table></figure><p>检查可以看到block的路径变更了</p><p>如果想去掉缓存，恢复没有缓存怎么处理，很简单</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs bash">systemctl <span class="hljs-built_in">disable</span> ceph-volume-flashcache@3b3546e5-65e5-426e-9659-f2e0d37a0895--0--bcdb55b0-e95b-4833-8362-18f633782632.service<br>systemctl stop ceph-osd@0<br>umount /var/lib/ceph/osd/ceph-0<br><br>systemctl start ceph-volume@lvm-0-bcdb55b0-e95b-4833-8362-18f633782632<br>systemctl <span class="hljs-built_in">enable</span> ceph-volume@lvm-0-bcdb55b0-e95b-4833-8362-18f633782632<br>systemctl start ceph-osd@0<br>systemctl <span class="hljs-built_in">enable</span> ceph-osd@0<br></code></pre></td></tr></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>用这种方式每个osd的服务自己记录自己的，没有配置文件，启动服务就是记录配置的地方，需要就启动，不需要disable，都是通用标准操作<br>并且不干扰其它没有配置flashcache的osd</p><p>限制</p><p>目前这个只适用于arond的模式的，因为这个模式的cache设备是随时可分离的，随时新建，少了很多盘符续用的问题</p><h2 id="更新历史"><a href="#更新历史" class="headerlink" title="更新历史"></a>更新历史</h2><table><thead><tr><th>why</th><th>when</th></tr></thead><tbody><tr><td>创建</td><td>2019年09月04日</td></tr><tr><td>更新</td><td>2019年12月9日</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>如何通过iptables代理访问内网</title>
    <link href="/2019/09/04/%E5%A6%82%E4%BD%95%E9%80%9A%E8%BF%87iptables%E4%BB%A3%E7%90%86%E8%AE%BF%E9%97%AE%E5%86%85%E7%BD%91/"/>
    <url>/2019/09/04/%E5%A6%82%E4%BD%95%E9%80%9A%E8%BF%87iptables%E4%BB%A3%E7%90%86%E8%AE%BF%E9%97%AE%E5%86%85%E7%BD%91/</url>
    
    <content type="html"><![CDATA[<h2 id="场景"><a href="#场景" class="headerlink" title="场景"></a>场景</h2><p>A机器能够联通内网机器，B机器能够联通A机器，但是访问不到内网机器，场景是希望通过A机器能够转发直接联通局域网内的其它机器</p><h2 id="机器IP"><a href="#机器IP" class="headerlink" title="机器IP"></a>机器IP</h2><p>内网为172.0.0.x&#x2F;24</p><p>A机器为172.0.0.10&#x2F;24</p><p>A机器为192.168.1.10&#x2F;24</p><p>B机器IP为192.168.1.20&#x2F;24</p><h2 id="进行设置"><a href="#进行设置" class="headerlink" title="进行设置"></a>进行设置</h2><p>在A机器上进行设置</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@jenkins ~]<span class="hljs-comment"># cat ip.sh </span><br><span class="hljs-comment">#!/bin/sh </span><br><span class="hljs-comment"># </span><br>IPT=<span class="hljs-string">&quot;/sbin/iptables&quot;</span><br><br>/bin/echo <span class="hljs-string">&quot;1&quot;</span> &gt; /proc/sys/net/ipv4/ip_forward<br><br>/sbin/modprobe ip_tables<br>/sbin/modprobe iptable_filter<br>/sbin/modprobe iptable_nat<br>/sbin/modprobe ip_conntrack<br>/sbin/modprobe ip_conntrack_ftp<br>/sbin/modprobe ip_nat_ftp<br><br><span class="hljs-variable">$IPT</span> -F<br><span class="hljs-variable">$IPT</span> -t nat -F<br><span class="hljs-variable">$IPT</span> -X<br><span class="hljs-variable">$IPT</span> -t nat -X<br><span class="hljs-variable">$IPT</span> -Z<br><span class="hljs-variable">$IPT</span> -t nat -Z<br><br><span class="hljs-variable">$IPT</span> -t nat -A POSTROUTING -s  192.168.1.0/255.255.255.0 -j SNAT --to 172.0.0.10<br><span class="hljs-variable">$IPT</span> -A FORWARD -s 192.168.1.20 -j ACCEPT<br></code></pre></td></tr></table></figure><p>意思是来自192.168.1.0网段的数据通过172.0.0.10这个进行转发，这个172.0.0.10是转发机器本机的ip</p><p>在客户端192.168.1.20机器执行</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">route  add  -p  172.0.0.0 mask 255.255.255.0 192.168.1.10<br></code></pre></td></tr></table></figure><p>-p 参数：p 即 persistent 的意思</p><p>-p 表示将路由表项永久加入系统注册表</p><p>如果要删除</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">route delete 172.0.0.0<br></code></pre></td></tr></table></figure><p>通过添加静态路由，把发往172.0.0.0网段的数据通过192.168.1.10这个网关进行转发</p><p>然后这个192.168.1.20的机器就能跟内网的172.0.0.x的机器进行通信了</p><h2 id="备注"><a href="#备注" class="headerlink" title="备注"></a>备注</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment">#songtao to 254</span><br><span class="hljs-variable">$IPT</span> -t nat -A POSTROUTING -s  66.66.66.64/255.255.255.255 -j SNAT --to 192.168.188.247<br><span class="hljs-variable">$IPT</span> -A FORWARD -s 66.66.66.64 -d 192.168.188.12 -j ACCEPT<br></code></pre></td></tr></table></figure><p>如果是内网的，并且只代理一个ip的，可以这样处理<br>66.66.66.64为客户端机器<br>192.168.188.12为目标机器<br>192.168.188.247为代理机器本地的ip，能够连通目标机器的那个ip<br>第二行为指定可以访问的ip</p><h2 id="更新历史"><a href="#更新历史" class="headerlink" title="更新历史"></a>更新历史</h2><table><thead><tr><th>why</th><th>when</th></tr></thead><tbody><tr><td>创建</td><td>2019年09月04日</td></tr><tr><td>更新</td><td>2019年12月9日</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>ceph luminous版本限制osd的内存使用</title>
    <link href="/2019/09/03/ceph%20luminous%E7%89%88%E6%9C%AC%E9%99%90%E5%88%B6osd%E7%9A%84%E5%86%85%E5%AD%98%E4%BD%BF%E7%94%A8/"/>
    <url>/2019/09/03/ceph%20luminous%E7%89%88%E6%9C%AC%E9%99%90%E5%88%B6osd%E7%9A%84%E5%86%85%E5%AD%98%E4%BD%BF%E7%94%A8/</url>
    
    <content type="html"><![CDATA[<h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>ceph自从到了L版本以后，L版本的启用，对性能本身有了极大的提高，一直对这个比较不放心的就是内存的占用，刚开始的时候记得大量dd就可以把内存搞崩掉，这个应该是内部的设计逻辑需要更多的内存的占用</p><p>最近在做ARM版本的服务器的测试，机器为36盘位的机器，内存需要自然多，但是36盘位的机器，按之前想法是4G预留，那得需要144G内存了，这个还没有算迁移的时候的内存消耗，而很多时候，我们并不需要速度，只需要稳定就好</p><h2 id="测试环境说明"><a href="#测试环境说明" class="headerlink" title="测试环境说明"></a>测试环境说明</h2><p>测试环境比较简单，一台36盘位的arm机器，一台X86机器，通过万兆相连，设置集群为副本1，然后再X86上面通</p><h2 id="限制前后对比"><a href="#限制前后对比" class="headerlink" title="限制前后对比"></a>限制前后对比</h2><p>我们先按默认的来一组测试</p><p>用读取命令进行测试</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs bash">rados  -p rbd  -t 64  bench 300  <span class="hljs-built_in">seq</span>  --run-name  4Mt16<br>···<br>2019-09-03 15:19:20.478841 min lat: 0.188154 max lat: 0.658198 avg lat: 0.227437<br>  sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)<br>   20      63      5620      5557   1111.24      1124    0.223682    0.227437<br>   21      63      5901      5838   1111.84      1124    0.232894      0.2274<br>   22      63      6179      6116   1111.84      1112    0.210126    0.227447<br>   23      63      6459      6396   1112.19      1120    0.209931    0.227353<br>   24      63      6742      6679   1113.01      1132    0.210041    0.227428<br>   25      63      7023      6960   1113.44      1124    0.447199    0.227417<br>   26      63      7302      7239   1113.54      1116    0.213757    0.227367<br>   27      63      7585      7522   1114.19      1132    0.222248    0.227416<br>   28      63      7870      7807   1115.08      1140    0.198665    0.227351<br>   29      63      8148      8085   1114.97      1112    0.222645    0.227402<br>   30      63      8430      8367    1115.4      1128    0.205243    0.227394<br>   31      63      8707      8644   1115.16      1108     0.22508    0.227397<br>   32      63      8987      8924   1115.31      1120    0.198234    0.227326<br>   33      63      9267      9204   1115.41      1120    0.232074    0.227447<br>   34      63      9551      9488   1115.99      1136    0.230373    0.227413<br>   35      63      9829      9766   1115.88      1112    0.214755    0.227405<br>   36      63     10107     10044   1115.77      1112    0.233391    0.227403<br>   37      63     10390     10327   1116.18      1132    0.223244    0.227412<br>   38      63     10673     10610    1116.6      1132    0.221128     0.22742<br>···<br></code></pre></td></tr></table></figure><p>通过top看下内存占用</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs bash">Tasks: 526 total,   1 running, 237 sleeping,   0 stopped,   0 zombie<br>%Cpu(s):  1.6 us,  1.5 sy,  0.0 ni, 95.3 <span class="hljs-built_in">id</span>,  1.5 wa,  0.0 hi,  0.1 si,  0.0 st<br>KiB Mem : 13316204+total, 31199616 free, 10079264+used,  1169792 buff/cache<br>KiB Swap:        0 total,        0 free,        0 used. 21086208 avail Mem <br><br>    PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND                      <br>  43967 ceph      20   0 4176384   3.1g  26624 S   1.3  2.4   0:08.79 ceph-osd                     <br>  43890 ceph      20   0 4116992   3.0g  26688 S   1.0  2.4   0:08.69 ceph-osd                     <br>  43753 ceph      20   0 4091712   3.0g  26752 S   2.3  2.4   0:08.88 ceph-osd                     <br>  43873 ceph      20   0 4160000   3.0g  26688 S   2.0  2.4   0:08.77 ceph-osd                     <br>  43998 ceph      20   0 4275776   3.0g  26688 S   2.3  2.4   0:08.91 ceph-osd                     <br>  43716 ceph      20   0 4128960   3.0g  26688 S   2.3  2.3   0:08.95 ceph-osd                     <br>  43899 ceph      20   0 3911232   3.0g  26688 S   1.7  2.3   0:08.44 ceph-osd                     <br>  44022 ceph      20   0 4133120   3.0g  26752 S   2.0  2.3   0:09.29 ceph-osd                     <br>  43617 ceph      20   0 4100608   2.9g  26688 S   2.6  2.3   0:09.13 ceph-osd                     <br>  43868 ceph      20   0 4110016   2.9g  26688 S   2.6  2.3   0:08.70 ceph-osd                     <br>  43792 ceph      20   0 4095552   2.9g  26752 S   2.3  2.3   0:09.02 ceph-osd                     <br>  43985 ceph      20   0 4090368   2.9g  26624 S   2.3  2.3   0:08.84 ceph-osd                     <br>  43707 ceph      20   0 4177472   2.9g  26624 S   2.3  2.3   0:08.79 ceph-osd                     <br>  44028 ceph      20   0 4084288   2.9g  26624 S   2.3  2.3   0:08.93 ceph-osd                     <br>  43995 ceph      20   0 4092480   2.8g  26688 S   2.3  2.2   0:08.95 ceph-osd                     <br>  43852 ceph      20   0 4054528   2.8g  26752 S   3.6  2.2   0:08.75 ceph-osd                     <br>  44038 ceph      20   0 3966528   2.8g  26688 S   2.6  2.2   0:08.51 ceph-osd                     <br>  43755 ceph      20   0 4092096   2.8g  26752 S   3.0  2.2   0:09.10 ceph-osd                     <br>  43718 ceph      20   0 4045376   2.8g  26688 S   1.3  2.2   0:08.84 ceph-osd                     <br>  43901 ceph      20   0 3931648   2.7g  26688 S   1.7  2.2   0:08.67 ceph-osd                     <br>  43880 ceph      20   0 4028992   2.7g  26688 S   2.6  2.1   0:08.82 ceph-osd                     <br>  43897 ceph      20   0 3978752   2.7g  26624 S   2.3  2.1   0:08.59 ceph-osd                     <br>  43858 ceph      20   0 4019776   2.7g  26560 S   1.3  2.1   0:08.89 ceph-osd       <br></code></pre></td></tr></table></figure><p>可以看到内存占用还是很可观的，我128G内存，基本要吃空了，内存需要的太大，还出现了无法分配内存的情况，我们限制下我们的内存</p><p>限制osd内存的参数<br>默认值为</p><blockquote><p>osd_memory_target &#x3D; 4294967296</p></blockquote><p>我们限制到1G</p><blockquote><p>osd_memory_target &#x3D; 1073741824</p></blockquote><p>设置好了后重启下所有的osd</p><p>再来一轮测试</p><p>同样的命令</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs bash">2019-09-03 15:28:41.369259 min lat: 0.189981 max lat: 0.88615 avg lat: 0.227472<br>  sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)<br>   40      63     11232     11169   1116.67      1116    0.221146    0.227472<br>   41      63     11513     11450   1116.85      1124     0.22655    0.227467<br>   42      63     11794     11731   1117.01      1124    0.213789    0.227451<br>   43      63     12073     12010   1116.98      1116     0.21521    0.227447<br>   44      63     12353     12290   1117.05      1120    0.232574    0.227455<br>   45      63     12632     12569   1117.02      1116    0.217199    0.227449<br>   46      63     12917     12854   1117.52      1140    0.211747    0.227425<br>   47      63     13194     13131   1117.31      1108    0.229418    0.227473<br>   48      63     13476     13413   1117.52      1128    0.229639    0.227456<br>   49      63     13756     13693   1117.57      1120    0.228479     0.22745<br>   50      63     14041     13978      1118      1140     0.21689    0.227463<br>   51      63     14316     14253   1117.64      1100     0.23947    0.227468<br>   52      63     14599     14536   1117.92      1132    0.215251    0.227452<br>   53      63     14880     14817   1118.03      1124    0.227617    0.227461<br>   54      63     15160     15097   1118.05      1120    0.224964    0.227449<br>   55      63     15442     15379   1118.22      1128    0.219148    0.227451<br>   56      63     15721     15658   1118.18      1116    0.223736    0.227459<br>   57      63     16001     15938   1118.21      1120     0.23183    0.227468<br>   58      63     16282     16219    1118.3      1124    0.217697    0.227465<br>   59      63     16566     16503    1118.6      1136    0.217804    0.227465<br></code></pre></td></tr></table></figure><p>查看修改后的内存占用</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><code class="hljs bash">top - 15:29:10 up  4:54,  6 <span class="hljs-built_in">users</span>,  load average: 1.88, 2.74, 2.25<br>Tasks: 506 total,   1 running, 239 sleeping,   0 stopped,   0 zombie<br>%Cpu(s):  1.2 us,  0.9 sy,  0.0 ni, 97.4 <span class="hljs-built_in">id</span>,  0.0 wa,  0.0 hi,  0.4 si,  0.0 st<br>KiB Mem : 13316204+total, 12275264+free,  9211264 used,  1198144 buff/cache<br>KiB Swap:        0 total,        0 free,        0 used. 11264793+avail Mem <br><br>    PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND                      <br>  48575 ceph      20   0  908544 262144  26688 S   2.0  0.2   0:09.71 ceph-osd                     <br>  48547 ceph      20   0  904448 252800  26752 S   1.0  0.2   0:09.36 ceph-osd                     <br>  48480 ceph      20   0  902336 247552  26624 S   1.3  0.2   0:09.11 ceph-osd                     <br>  48485 ceph      20   0  906048 242048  26688 S   1.7  0.2   0:09.38 ceph-osd                     <br>  48423 ceph      20   0  901312 241984  26688 S   1.3  0.2   0:09.62 ceph-osd                     <br>  48534 ceph      20   0  896576 234752  26688 S   1.3  0.2   0:09.22 ceph-osd                     <br>  48395 ceph      20   0  902656 231360  26688 S   2.0  0.2   0:08.96 ceph-osd                     <br>  48474 ceph      20   0  899584 225920  26624 S   1.3  0.2   0:09.07 ceph-osd                     <br>  48369 ceph      20   0  891456 223232  26688 S   1.3  0.2   0:09.31 ceph-osd                     <br>  48375 ceph      20   0  899648 222016  26624 S   1.7  0.2   0:08.72 ceph-osd                     <br>  48365 ceph      20   0  885312 220928  26688 S   1.3  0.2   0:09.06 ceph-osd                     <br>  48421 ceph      20   0  899200 218944  26688 S   1.7  0.2   0:09.31 ceph-osd                     <br>  48478 ceph      20   0  889344 218496  26880 S   1.3  0.2   0:09.46 ceph-osd                     <br>  48347 ceph      20   0  879104 218112  26688 S   1.7  0.2   0:08.52 ceph-osd                     <br>  48515 ceph      20   0  891456 215680  26688 S   2.0  0.2   0:09.21 ceph-osd                     <br>  48465 ceph      20   0  884224 214336  26688 S   1.7  0.2   0:08.82 ceph-osd                     <br>  48560 ceph      20   0  892480 214144  26624 S   1.3  0.2   0:08.87 ceph-osd                     <br>  48552 ceph      20   0  900992 213952  26752 S   1.7  0.2   0:09.32 ceph-osd                     <br>  48599 ceph      20   0  890432 213760  26624 S   2.0  0.2   0:09.19 ceph-osd                     <br>  48613 ceph      20   0  882176 213056  26752 S   1.0  0.2   0:09.12 ceph-osd                     <br>  48430 ceph      20   0  893440 212672  26688 S   1.7  0.2   0:09.18 ceph-osd                     <br>  48503 ceph      20   0  885312 211456  26688 S   2.3  0.2   0:09.52 ceph-osd                     <br>  48444 ceph      20   0  889408 211200  26752 S   2.0  0.2   0:09.60 ceph-osd                     <br>  48635 ceph      20   0  898624 210944  26688 S   2.0  0.2   0:09.26 ceph-osd                     <br>  48526 ceph      20   0  898624 210752  26816 S   2.6  0.2   0:09.20 ceph-osd                     <br>  48491 ceph      20   0  898368 210304  26816 S   2.3  0.2   0:09.57 ceph-osd     <br></code></pre></td></tr></table></figure><p>可以看到内存控的好好的，并且在这个场景下面，我们的性能并没有太多的损失，基本一致的，因为我的环境是单万兆的，输出的网络带宽是小于底层磁盘可以提供的带宽的，这个情况下磁盘的占用也就没那么满</p><p>这个也是我在做配置的时候推荐的一点，各项资源不要打的那么满，对于带宽场景来说，多留几个磁盘的输出冗余，那么在出现一些异常的时候，底层也不会影响到那么大</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本篇通过一个参数的介绍，讲述了osd的内存的占用的控制，一般情况来说，默认的参数已经足够好了，当然你在很确定你修改的参数会影响什么的情况下，可以根据需要做一些调整，如果在你的IO模型下面，你调整了都看不到效果，那还是不动为好</p><p>我们手上有高性能的arm，就是上面的这款测试用到的，有32个processor，内存最大可以支持到</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">Maximum Capacity: 512 GB<br>Number Of Devices: 16<br></code></pre></td></tr></table></figure><p>盘位的机箱我们有能力自己设计，也就是可以根据需要进行定制，有需要的可以找我</p><h2 id="变更记录"><a href="#变更记录" class="headerlink" title="变更记录"></a>变更记录</h2><table><thead><tr><th align="center">Why</th><th align="center">Who</th><th align="center">When</th></tr></thead><tbody><tr><td align="center">创建</td><td align="center">武汉-运维-磨渣</td><td align="center">2018-09-03</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>登陆系统的设计</title>
    <link href="/2019/09/03/%E7%99%BB%E9%99%86%E7%B3%BB%E7%BB%9F%E7%9A%84%E8%AE%BE%E8%AE%A1/"/>
    <url>/2019/09/03/%E7%99%BB%E9%99%86%E7%B3%BB%E7%BB%9F%E7%9A%84%E8%AE%BE%E8%AE%A1/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>在做一个管理平台的时候，面向客户的最开始的第一步就是管理平台的登陆系统，而由于面向的是企业用户，内网用户，因此，往往有很多我们预想不到的情况出现，而这些都会去影响着管理平台的体验，或者会完全中断掉你的操作</p><p>这里在分析一些做的比较好的平台后，以及结合我们自身可能遇到的情况，对登陆系统进行一个新的设计，这个是通用的设计并不针对某款产品</p><h2 id="范例"><a href="#范例" class="headerlink" title="范例"></a>范例</h2><p>zabbix的登陆系统是做的比较好的，我们先来看看，然后再去给出我的设计</p><p>首页欢迎页面</p><p><img src="/images/blog/o_2009010957381571714306711.png"></p><p>通过欢迎页面可以看到，整个初始化大概有几个步骤，这个只要有明显的告知即可</p><p>检查先决条件</p><p><img src="/images/blog/o_2009010958381571714889266.png"></p><p>这个步骤是去检查环境是不是有问题，依赖的选项是不是正确的，这里可以把一些比较重要的调整参数或者默认的参数放到这里做检查，一个是给出的推荐值和一个当前的值，方便去检查</p><p>配置数据库连接</p><p><img src="/images/blog/o_2009010958501571715194942.png"></p><p>这一步是比较重要的一步，跟管理平台的可用性比较大，下面我们会介绍，这里给出了多种数据库的选择，这是因为平台支持，一般情况下就写死就行，数据库的主机名称，端口，数据库的名称，用户名，密码</p><p>这些都没问题</p><p>总览配置</p><p><img src="/images/blog/o_2009010958581571715297194.png"></p><p>刚刚做的配置，在这里做一次查看，确认下相关的信息，这样也方便做整体的二次审查</p><p>最终完成的提醒</p><p><img src="/images/blog/o_2009011000041571715353383.png"></p><p>这里会告诉你上面的配置信息写到了哪里，这个配置信息应该是可配置信息的，并且应该是保存在本地文件里面，可修改可重写的</p><h2 id="设计思路"><a href="#设计思路" class="headerlink" title="设计思路"></a>设计思路</h2><p>从上面zabbix的登陆初始系统的设计我们可以分析得到一些东西，面临的问题，以及怎么去解决这些问题</p><h3 id="最开始欢迎页"><a href="#最开始欢迎页" class="headerlink" title="最开始欢迎页"></a>最开始欢迎页</h3><p>欢迎页面这个地方，可以提供一些比较核心方便的信息，版本，发布日期，这些信息方便从安装的时候就确认自己的版本是不是有问题，需要几个步骤，这个页面的功能就完成了</p><h3 id="第二个检查页"><a href="#第二个检查页" class="headerlink" title="第二个检查页"></a>第二个检查页</h3><p>这里可以去放置一些可能需要经常改动的核心参数，或者是一些比较核心的版本，比如一些内核参数的等等，这个就不在这里详细说，有需要检查的放进去就好</p><h3 id="第三个数据库连接页"><a href="#第三个数据库连接页" class="headerlink" title="第三个数据库连接页"></a>第三个数据库连接页</h3><p>这个是登陆系统里面比较核心的设计了，为什么需要把这个独立出来，我们来看看怎么处理下面的几个问题</p><ul><li>平台是跟其它平台共用的mysql数据库，大平台需要统一接管数据</li><li>平台的数据库并不在本机，在其它机器上面</li><li>平台所在的主机机器坏了，重新安装了，数据库在其它机器上面，怎么连接起来</li><li>数据库在其它机器，想要多台机器的管理平台都能连接同一个数据库，从而实现高可用，数据库的高可用可以通过mysql的高可用实现</li><li>平台有很多垃圾数据，不想用之前的数据，想直接重新初始化，用新的数据</li></ul><p>可以看到在实际平台的运行过程中上面的问题都可能遇到，那么我们就需要做的是</p><ul><li>平台web和数据库是可以分离的</li><li>可以配置平台的数据库连接，可以连接到之前的数据，连接的数据库要指定数据库的</li><li>可以通过连接新建数据库</li><li>后台应该有个文件来判断是不是要进入数据库初始化这个界面（想重新初始化的时候删除那个文件，就可以到初始界面）</li></ul><p>基于以上的设计我们来看下原型设计</p><h2 id="自己原型设计"><a href="#自己原型设计" class="headerlink" title="自己原型设计"></a>自己原型设计</h2><p><img src="/images/blog/o_2009011000151571729531551.png"></p><p>本页包含版本信息和发布的日期</p><p>配置检查</p><p><img src="/images/blog/o_2009011000231571730277716.png"></p><p>新建数据库</p><p><img src="/images/blog/o_2009011000311571729600004.png"></p><p>本页的下拉选项是包含两种的，新建数据库和连接数据库，本页是新建数据库的，默认会生成一些信息</p><p>新建数据库</p><p><img src="/images/blog/o_2009011000381571729660496.png"></p><p>本页是连接数据的，连接数据库是填写好用户名密码机器，点击查询后得到机器上面的几个数据库，然后选择确定的那个数据库，然后进入下一步</p><p>登陆设置</p><p><img src="/images/blog/o_2009011000451571730382778.png"></p><p>这里注意一下，如果是新建数据库，那么就有设置登陆用户名密码的操作，如图是连接数据库，那么就没有设置用户名密码的这一步，这个用之前数据库里面存储的用户名密码</p><p>本次配置信息</p><p><img src="/images/blog/o_2009011001011571730410340.png"></p><p>这里把之前的配置都再次显示了，建议留一个配置信息的页面，可以好保留，以免后面忘记了</p><p>配置完成页面</p><p><img src="/images/blog/o_2009011001081571730454470.png"></p><p>这里有个配置文件的信息，这里面保留了刚刚填写的信息，除了登陆用户名密码密码信息不保留以外，其余的信息都保留在里面，平台也是通过这个配置文件来进行数据库连接的，如果想要重新配置，直接把这个文件进行删除即可</p><p>基于以上的设计以后就是基本能够满足我们需求的登陆连接系统了</p><h2 id="更新历史"><a href="#更新历史" class="headerlink" title="更新历史"></a>更新历史</h2><table><thead><tr><th>why</th><th>when</th></tr></thead><tbody><tr><td>创建</td><td>2019年09月03日</td></tr><tr><td>更新</td><td>2019年12月9日</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>检查邮箱是否可登陆和发送邮件</title>
    <link href="/2019/09/03/%E6%A3%80%E6%9F%A5%E9%82%AE%E7%AE%B1%E6%98%AF%E5%90%A6%E5%8F%AF%E7%99%BB%E9%99%86%E5%92%8C%E5%8F%91%E9%80%81%E9%82%AE%E4%BB%B6/"/>
    <url>/2019/09/03/%E6%A3%80%E6%9F%A5%E9%82%AE%E7%AE%B1%E6%98%AF%E5%90%A6%E5%8F%AF%E7%99%BB%E9%99%86%E5%92%8C%E5%8F%91%E9%80%81%E9%82%AE%E4%BB%B6/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>邮箱系统在提供了用户名和密码以后可以发送邮件，而在发送之前我们有个需求是需要验证下这个密码是不是正确的，本篇的内容就是用python默认的库来检测邮箱是不是可以登录的</p><h2 id="验证"><a href="#验证" class="headerlink" title="验证"></a>验证</h2><p>这个采用的是python默认带的库smtplib，是一个发送邮件的库，脚本文件如下</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment">#! usr/bin/python</span><br><span class="hljs-comment"># -*- coding: UTF-8 -*-</span><br>import sys<br>import os<br><br>username=sys.argv[1]<br>password=sys.argv[2]<br>chinesname=sys.argv[3]<br><br>import smtplib<br>from email.mime.text import MIMEText<br>mail_host = <span class="hljs-string">&#x27;smtp.qq.com&#x27;</span><br>mail_user = <span class="hljs-string">&#x27;%s@qq.com&#x27;</span>   % username<br><br>mail_pass = <span class="hljs-string">&#x27;%s&#x27;</span> % password<br><br><span class="hljs-comment">#print mail_user</span><br><span class="hljs-comment">#print mail_pass</span><br><span class="hljs-comment">#os._exit(0)</span><br>sender = <span class="hljs-string">&#x27;199383004@qq.com&#x27;</span><br>receivers = [<span class="hljs-string">&#x27;199383004@qq.com&#x27;</span>]<br><br>message = MIMEText(<span class="hljs-string">&#x27;python content&#x27;</span>,<span class="hljs-string">&#x27;plain&#x27;</span>,<span class="hljs-string">&#x27;utf-8&#x27;</span>)<br>message[<span class="hljs-string">&#x27;From&#x27;</span>] = sender<br>message[<span class="hljs-string">&#x27;To&#x27;</span>] = receivers[0]<br><br>try:<br><br>smtpObj = smtplib.SMTP()<br>smtpObj.connect(mail_host,25)<br>smtpObj.login(mail_user,mail_pass)<br>smtpObj.sendmail(<br>sender,receivers,message.as_string()) <br><br>smtpObj.quit() <br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;可以登录并发送测试邮件  %s&#x27;</span> %(chinesname))<br>except smtplib.SMTPException as e:<br>    <span class="hljs-comment">#os._exit(0)</span><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;error&#x27;</span>,e) <br></code></pre></td></tr></table></figure><p>上面的python脚本是验证是否能登陆的，这个地方传递三个参数进去<br>用户名，密码或授权码，中文名称</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">python /tmp/send.py 199383004 xxxxxxx 磨渣<br></code></pre></td></tr></table></figure><p>如果不想上面的发送就注释掉sendmail部分即可<br>QQ邮箱密码是授权码,去邮箱设置里面生成，企业邮箱可以直接使用密码即可</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>linux发送邮件的方式有很多，越简单的越好，能实现自己想要的效果即可</p><h2 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-meta">#! /bin/sh</span><br><span class="hljs-built_in">cat</span> lianxirenwithname.txt | <span class="hljs-keyword">while</span> <span class="hljs-built_in">read</span> line; <br><span class="hljs-keyword">do</span> <br>name=`<span class="hljs-built_in">echo</span>  <span class="hljs-variable">$line</span>|awk <span class="hljs-string">&#x27;&#123;print $1&#125;&#x27;</span>`<br>chinesname=`<span class="hljs-built_in">echo</span>  <span class="hljs-variable">$line</span>|awk <span class="hljs-string">&#x27;&#123;print $2&#125;&#x27;</span>`<br><br>python testemail.py <span class="hljs-variable">$name</span> default-password <span class="hljs-variable">$chinesname</span><br></code></pre></td></tr></table></figure><p>如果是有一个邮件列表，想去检查很多邮件是不是可以登录，可以通过一个列表，然后调用python的脚本去验证即可</p><h2 id="更新历史"><a href="#更新历史" class="headerlink" title="更新历史"></a>更新历史</h2><table><thead><tr><th>why</th><th>when</th></tr></thead><tbody><tr><td>创建</td><td>2019年09月03日</td></tr><tr><td>更新</td><td>2019年12月9日</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>CTDB使用radosobject作为lockfile</title>
    <link href="/2019/09/02/CTDB%E4%BD%BF%E7%94%A8radosobject%E4%BD%9C%E4%B8%BAlockfile/"/>
    <url>/2019/09/02/CTDB%E4%BD%BF%E7%94%A8radosobject%E4%BD%9C%E4%B8%BAlockfile/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>服务器的服务做HA有很多种方式，其中有一种就是是用CTDB，之前这个是独立的软件来做HA的，现在已经跟着SAMBA主线里面了，也就是跟着samba发行包一起发行</p><p>之前CTDB的模式是需要有一个共享文件系统，并且在这个共享文件系统里面所有的节点都去访问同一个文件，会有一个Master会获得这个文件的锁</p><p>在cephfs的使用场景中可以用cephfs的目录作为这个锁文件的路径，这个有个问题就是一旦有一个节点down掉的时候，可能客户端也会卡住目录，这个目录访问会被卡住，文件锁在其他机器无法获取到，需要等到这个锁超时以后，其它节点才能获得到锁，这个切换的周期就会长一点了</p><p>CTDB在最近的版本当中加入了cluster mutex helper using Ceph RADOS的支持，本篇将介绍这个方式锁文件配置方式</p><h2 id="实践过程"><a href="#实践过程" class="headerlink" title="实践过程"></a>实践过程</h2><h3 id="安装CTDB"><a href="#安装CTDB" class="headerlink" title="安装CTDB"></a>安装CTDB</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@customos ~]<span class="hljs-comment"># yum install samba ctdb</span><br></code></pre></td></tr></table></figure><p>检查默认包里面是否有rados的支持</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@customos ~]<span class="hljs-comment"># rpm -qpl samba-4.8.3-4.el7.x86_64.rpm</span><br>…<br>usr/libexec/ctdb<br>/usr/libexec/ctdb/ctdb_event<br>/usr/libexec/ctdb/ctdb_eventd<br>/usr/libexec/ctdb/ctdb_killtcp<br>/usr/libexec/ctdb/ctdb_lock_helper<br>/usr/libexec/ctdb/ctdb_lvs<br>/usr/libexec/ctdb/ctdb_mutex_fcntl_helper<br>/usr/libexec/ctdb/ctdb_natgw<br>/usr/libexec/ctdb/ctdb_recovery_helper<br>/usr/libexec/ctdb/ctdb_takeover_helper<br>/usr/libexec/ctdb/smnotify<br>…<br></code></pre></td></tr></table></figure><p>这个可以看到默认并没有包含这个rados的支持，这个很多通用软件都会这么处理，因为支持第三方插件的时候需要开发库，而开发库又有版本的区别，所以默认并不支持，需要支持就自己编译即可，例如fio支持librbd的接口就是这么处理的，等到插件也通用起来的时候，可能就会默认支持了</p><p>很多软件的编译可以采取源码的编译方式，如果不是有很强的代码合入和patch跟踪能力，直接用发行包的方式是最稳妥的，所以为了不破坏这个稳定性，本篇采用的是基于发行版本，增加模块的方式，这样不会破坏核心组件的稳定性，并且后续升级也是比较简单的，这个也是个人推荐的方式</p><p>查询centos7.6当前使用的samba版本</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@customos ~]<span class="hljs-comment"># rpm -qa|grep samba</span><br>samba-4.8.3-4.el7.x86_64<br></code></pre></td></tr></table></figure><h3 id="打包新的CTDB"><a href="#打包新的CTDB" class="headerlink" title="打包新的CTDB"></a>打包新的CTDB</h3><p>可以查询得到这个的源码包为samba-4.8.3-4.el7.src.rpm,进一步搜索可以查询的到这个src源码rpm包</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">http://vault.centos.org/7.6.1810/os/Source/SPackages/samba-4.8.3-4.el7.src.rpm<br></code></pre></td></tr></table></figure><p>下载这个rpm包</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@customos ~]<span class="hljs-comment"># wget http://vault.centos.org/7.6.1810/os/Source/SPackages/samba-4.8.3-4.el7.src.rpm</span><br></code></pre></td></tr></table></figure><p>如果下载比较慢的话就用迅雷下载，会快很多，国内的源里面把源码包的rpm都删除掉了，上面的是官网会有最全的包</p><p>解压这个rpm包</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@customos ~]<span class="hljs-comment"># rpm2cpio samba-4.8.3-4.el7.src.rpm |cpio -div</span><br></code></pre></td></tr></table></figure><p>检查包的内容</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@customos myctdb]<span class="hljs-comment"># ls</span><br>CVE-2018-10858.patch                                 samba-4.8.3-4.el7.src.rpm<br>CVE-2018-1139.patch                                  samba-4.8.3-fix_krb5_plugins.patch<br>gpgkey-52FBC0B86D954B0843324CDC6F33915B6568B7EA.gpg  samba-4.8.3-fix_winbind_getpwnam_local_user.patch<br>libldb-1.3.4-1.el7.x86_64.rpm                        samba-4.8.3-smbclient_quiet_argument.patch<br>libldb-devel-1.3.4-1.el7.x86_64.rpm                  samba-4.8.3.tar.asc<br>pam_winbind.conf                                     samba-4.8.3.tar.xz<br>pyldb-1.3.4-1.el7.x86_64.rpm                         samba.log<br>pyldb-devel-1.3.4-1.el7.x86_64.rpm                   samba.pamd<br>README.dc                                            samba.spec<br>README.downgrade                                     smb.conf.example<br>samba-4.8.3                                          smb.conf.vendor<br></code></pre></td></tr></table></figure><p>可以看到在源码包基础上还打入了很多的patch，内部的编译采用的是waf编译的方式，内部的过程就不做太多介绍了，这里只去改动我们需要的部分即可，也就是去修改samba.spec文件</p><p>我们先获取相关的编译选项，这个我最开始的时候打算独立编译ctdb的rpm包，发现有依赖关系太多，后来多次验证后，发现直接可以在samba编译里面增加选项的，选项获取方式</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab211 samba-4.6.2]<span class="hljs-comment"># ./configure --help|grep ceph</span><br>  --with-libcephfs=LIBCEPHFS_DIR<br>            Directory under <span class="hljs-built_in">which</span> libcephfs is installed<br>  --enable-cephfs<br>            Build with cephfs support (default=<span class="hljs-built_in">yes</span>)<br>  --enable-ceph-reclock<br></code></pre></td></tr></table></figure><p>这个可以知道需要添加ceph-reclock的支持就添加这个选项，我们把这个选项添加到samba.spec当中<br>修改samba.spec文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><code class="hljs bash">…<br>%configure \<br>        --enable-fhs \<br>        --with-piddir=/run \<br>        --with-sockets-dir=/run/samba \<br>        --with-modulesdir=%&#123;_libdir&#125;/samba \<br>        --with-pammodulesdir=%&#123;_libdir&#125;/security \<br>        --with-lockdir=/var/lib/samba/lock \<br>        --with-statedir=/var/lib/samba \<br>        --with-cachedir=/var/lib/samba \<br>        --disable-rpath-install \<br>        --with-shared-modules=%&#123;_samba4_modules&#125; \<br>        --bundled-libraries=%&#123;_samba4_libraries&#125; \<br>        --with-pam \<br>        --with-pie \<br>        --with-relro \<br>        --enable-ceph-reclock \<br>        --without-fam \<br>…<br>%<span class="hljs-built_in">dir</span> %&#123;_libexecdir&#125;/ctdb<br>%&#123;_libexecdir&#125;/ctdb/ctdb_event<br>%&#123;_libexecdir&#125;/ctdb/ctdb_eventd<br>%&#123;_libexecdir&#125;/ctdb/ctdb_killtcp<br>%&#123;_libexecdir&#125;/ctdb/ctdb_lock_helper<br>%&#123;_libexecdir&#125;/ctdb/ctdb_lvs<br>%&#123;_libexecdir&#125;/ctdb/ctdb_mutex_fcntl_helper<br>%&#123;_libexecdir&#125;/ctdb/ctdb_mutex_ceph_rados_helper<br>…<br>%&#123;_mandir&#125;/man1/ctdb.1.gz<br>%&#123;_mandir&#125;/man1/ctdb_diagnostics.1.gz<br>%&#123;_mandir&#125;/man1/ctdbd.1.gz<br>%&#123;_mandir&#125;/man1/onnode.1.gz<br>%&#123;_mandir&#125;/man1/ltdbtool.1.gz<br>%&#123;_mandir&#125;/man1/ping_pong.1.gz<br>%&#123;_mandir&#125;/man7/ctdb_mutex_ceph_rados_helper.7.gz<br>%&#123;_mandir&#125;/man1/ctdbd_wrapper.1.gz<br>…<br></code></pre></td></tr></table></figure><p>这个文件当中一共添加了三行内容</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">--enable-ceph-reclock \<br>%&#123;_libexecdir&#125;/ctdb/ctdb_mutex_ceph_rados_helper<br>%&#123;_mandir&#125;/man7/ctdb_mutex_ceph_rados_helper.7.gz<br></code></pre></td></tr></table></figure><p>把解压后的目录里面的所有文件都拷贝到源码编译目录,就是上面ls列出的那些文件，以及修改好的samba.spec文件都一起拷贝过去</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@customos myctdb]<span class="hljs-comment"># cp -ra * /root/rpmbuild/SOURCES/</span><br></code></pre></td></tr></table></figure><p>安装librados2的devel包</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@customos myctdb]<span class="hljs-comment"># yum install librados2-devel</span><br></code></pre></td></tr></table></figure><p>如果编译过程缺其他的依赖包就依次安装即可，这个可以通过解压源码先编译一次的方式来把依赖包找全，然后再打rpm包</p><p>开始编译rpm包</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@customos myctdb]<span class="hljs-comment"># rpmbuild -bb samba.spec</span><br></code></pre></td></tr></table></figure><p>这个可以就在当前的目录执行即可</p><p>检查生成的包</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@customos myctdb]<span class="hljs-comment"># rpm -qpl /root/rpmbuild/RPMS/x86_64/ctdb-4.6.2-12.el7.centos.x86_64.rpm|grep rados</span><br>/usr/libexec/ctdb/ctdb_mutex_ceph_rados_helper<br>/usr/share/man/man7/ctdb_mutex_ceph_rados_helper.7.gz<br></code></pre></td></tr></table></figure><p>可以看到已经生成了这个，把这个包拷贝到需要更新的机器上面</p><h3 id="配置ctdb"><a href="#配置ctdb" class="headerlink" title="配置ctdb"></a>配置ctdb</h3><p>首先要升级安装下新的ctdb包，因为名称有改变，会提示依赖问题,这里忽略依赖的问题</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@customos ~]<span class="hljs-comment"># rpm -Uvh ctdb-4.6.2-12.el7.centos.x86_64.rpm --nodeps</span><br></code></pre></td></tr></table></figure><p>添加一个虚拟IP配置</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@customos ~]<span class="hljs-comment"># cat /etc/ctdb/public_addresses </span><br>192.168.0.99/16 ens33<br></code></pre></td></tr></table></figure><p>添加node配置</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@customos ~]<span class="hljs-comment"># cat /etc/ctdb/nodes </span><br>192.168.0.18<br>192.168.0.201<br></code></pre></td></tr></table></figure><p>修改配置文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@customos ~]<span class="hljs-comment"># cat /etc/ctdb/ctdbd.conf|grep -v &quot;#&quot;</span><br> CTDB_RECOVERY_LOCK=<span class="hljs-string">&quot;!/usr/libexec/ctdb/ctdb_mutex_ceph_rados_helper ceph client.admin rbd lockctdb&quot;</span><br> CTDB_NODES=/etc/ctdb/nodes<br> CTDB_PUBLIC_ADDRESSES=/etc/ctdb/public_addresses<br> CTDB_LOGGING=file:/var/log/log.ctdb<br><span class="hljs-comment"># CTDB_DEBUGLEVEL=debug</span><br></code></pre></td></tr></table></figure><p>上面为了调试，我开启了debug来查看重要的信息</p><blockquote><p>CTDB_RECOVERY_LOCK&#x3D;”!&#x2F;usr&#x2F;libexec&#x2F;ctdb&#x2F;ctdb_mutex_ceph_rados_helper ceph client.admin rbd lockctdb”<br><br>最重要的是这行配置文件规则是</p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">CTDB_RECOVERY_LOCK=<span class="hljs-string">&quot;!/usr/libexec/ctdb/ctdb_mutex_ceph_rados_helper [Cluster] [User] [Pool] [Object]&quot;</span><br>Cluster: Ceph cluster name (e.g. ceph)<br>User: Ceph cluster user name (e.g. client.admin)<br>Pool: Ceph RADOS pool name<br>Object: Ceph RADOS object name<br></code></pre></td></tr></table></figure><p>在ctdb的机器上面准备好librados2和ceph配置文件，这个配置的rbd的lockctdb对象会由ctdb去生成</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@customos ~]<span class="hljs-comment"># systemctl restart ctdb</span><br></code></pre></td></tr></table></figure><p>配置好了以后就可以启动进程了，上面的&#x2F;etc&#x2F;ctdb&#x2F;ctdbd.conf配置文件最好是修改好一台机器的，然后scp到其它机器，里面内容有一点点偏差都会判断为异常的，所以最好是相同的配置文件</p><p>查看进程状态</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@customos ceph]<span class="hljs-comment"># ctdb status</span><br>Number of nodes:2<br>pnn:0 192.168.0.18     OK (THIS NODE)<br>pnn:1 192.168.0.201    OK<br>Generation:1662303628<br>Size:2<br><span class="hljs-built_in">hash</span>:0 lmaster:0<br><span class="hljs-built_in">hash</span>:1 lmaster:1<br>Recovery mode:NORMAL (0)<br>Recovery master:1<br></code></pre></td></tr></table></figure><p>查看&#x2F;var&#x2F;log&#x2F;log.ctdb日志</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">2018/01/06 23:18:11.399849 ctdb-recoverd[129134]: Node:1 was <span class="hljs-keyword">in</span> recovery mode. Start recovery process<br>2018/01/06 23:18:11.399879 ctdb-recoverd[129134]: ../ctdb/server/ctdb_recoverd.c:1267 Starting do_recovery<br>2018/01/06 23:18:11.399903 ctdb-recoverd[129134]: Attempting to take recovery lock (!/usr/libexec/ctdb/ctdb_mutex_ceph_rados_helper ceph client.admin rbd lockctdb)<br>2018/01/06 23:18:11.400657 ctdb-recoverd[129134]: ../ctdb/server/ctdb_cluster_mutex.c:251 Created PIPE FD:17<br>2018/01/06 23:18:11.579865 ctdbd[129038]: ../ctdb/server/ctdb_daemon.c:907 client request 40 of <span class="hljs-built_in">type</span> 7 length 72 from node 1 to 4026531841<br></code></pre></td></tr></table></figure><p>日志中可以看到ctdb-recoverd已经是采用的ctdb_mutex_ceph_rados_helper来获取的recovery lock</p><p>停掉ctdb的进程，IP可以正常的切换，到这里，使用对象作为lock文件的功能就实现了，其他更多的ctdb的高级控制就不在这个里作过多的说明</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本篇是基于发行版本的ctdb包进行模块的加入重新发包，并且把配置做了一次实践，这个可以作为一个ctdb的方案之一，具体跟之前的方案相比切换时间可以改善多少，需要通过数据进行对比，这个进行测试即可</p><h2 id="更新"><a href="#更新" class="headerlink" title="更新"></a>更新</h2><p>上面的默认版本的ctdb存在的一个问题是主ctdb故障以后，锁不会释放，这个通过查询命令可以看到</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ~]<span class="hljs-comment"># rados -p rbd lock info lockctdb ctdb_reclock_mutex</span><br>&#123;<span class="hljs-string">&quot;name&quot;</span>:<span class="hljs-string">&quot;ctdb_reclock_mutex&quot;</span>,<span class="hljs-string">&quot;type&quot;</span>:<span class="hljs-string">&quot;exclusive&quot;</span>,<span class="hljs-string">&quot;tag&quot;</span>:<span class="hljs-string">&quot;&quot;</span>,<span class="hljs-string">&quot;lockers&quot;</span>:[&#123;<span class="hljs-string">&quot;name&quot;</span>:<span class="hljs-string">&quot;client.34130&quot;</span>,<span class="hljs-string">&quot;cookie&quot;</span>:<span class="hljs-string">&quot;ctdb_reclock_mutex&quot;</span>,<span class="hljs-string">&quot;description&quot;</span>:<span class="hljs-string">&quot;CTDB recovery lock&quot;</span>,<span class="hljs-string">&quot;expiration&quot;</span>:<span class="hljs-string">&quot;0.000000&quot;</span>,<span class="hljs-string">&quot;addr&quot;</span>:<span class="hljs-string">&quot;192.168.0.101:54263&quot;</span>&#125;]&#125;<br></code></pre></td></tr></table></figure><p>这里的expiration为0，也就是不会释放的</p><p>进行更新以后应该是这样的显示</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ~]<span class="hljs-comment"># rados -p rbd lock info lockctdb ctdb_reclock_mutex</span><br>&#123;<span class="hljs-string">&quot;name&quot;</span>:<span class="hljs-string">&quot;ctdb_reclock_mutex&quot;</span>,<span class="hljs-string">&quot;type&quot;</span>:<span class="hljs-string">&quot;exclusive&quot;</span>,<span class="hljs-string">&quot;tag&quot;</span>:<span class="hljs-string">&quot;&quot;</span>,<span class="hljs-string">&quot;lockers&quot;</span>:[&#123;<span class="hljs-string">&quot;name&quot;</span>:<span class="hljs-string">&quot;client.34239&quot;</span>,<span class="hljs-string">&quot;cookie&quot;</span>:<span class="hljs-string">&quot;ctdb_reclock_mutex&quot;</span>,<span class="hljs-string">&quot;description&quot;</span>:<span class="hljs-string">&quot;CTDB recovery lock&quot;</span>,<span class="hljs-string">&quot;expiration&quot;</span>:<span class="hljs-string">&quot;2019-11-04 16:22:18.119080&quot;</span>,<span class="hljs-string">&quot;addr&quot;</span>:<span class="hljs-string">&quot;192.168.0.102:0/443078450&quot;</span>&#125;]&#125;<br></code></pre></td></tr></table></figure><p>查询后，发现新版本已经对这个问题解决了</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">/*<br> * During failover it may take up to &lt;lock duration&gt; seconds before the<br> * newly elected recovery master can obtain the lock.<br> */<br><span class="hljs-comment">#define CTDB_MUTEX_CEPH_LOCK_DURATION_SECS_DEFAULT10</span><br></code></pre></td></tr></table></figure><p>增加了一个锁超时的参数，默认为10s<br>这个需要自己替换一个文件</p><blockquote><p><a href="https://github.com/samba-team/samba/blob/v4-8-stable/ctdb/utils/ceph/ctdb_mutex_ceph_rados_helper.c">https://github.com/samba-team/samba/blob/v4-8-stable/ctdb/utils/ceph/ctdb_mutex_ceph_rados_helper.c</a></p></blockquote><p>路径为ctdb&#x2F;utils&#x2F;ceph&#x2F;ctdb_mutex_ceph_rados_helper.c<br>这个涉及的东西比较少，修改一个文件即可</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ~]<span class="hljs-comment"># cat /etc/ctdb/ctdbd.conf </span><br>CTDB_RECOVERY_LOCK=<span class="hljs-string">&quot;!/usr/libexec/ctdb/ctdb_mutex_ceph_rados_helper ceph client.admin rbd lockctdb 10&quot;</span><br></code></pre></td></tr></table></figure><p>实际上这里的上面的配置文件的参数也可以新增加一个参数进去，不加参数的就是默认为10s,如果需要调整就自己根据需要调整这个就可以了</p><p>修改好了以后重新进行一次打包即可</p><h2 id="更新历史"><a href="#更新历史" class="headerlink" title="更新历史"></a>更新历史</h2><table><thead><tr><th>why</th><th>when</th></tr></thead><tbody><tr><td>创建</td><td>2018-01-06</td></tr><tr><td>解决无法释放锁的问题</td><td>2019-11-04</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>ceph luminous bluestore热插拔实现</title>
    <link href="/2019/09/02/ceph%20luminous%20bluestore%E7%83%AD%E6%8F%92%E6%8B%94%E5%AE%9E%E7%8E%B0/"/>
    <url>/2019/09/02/ceph%20luminous%20bluestore%E7%83%AD%E6%8F%92%E6%8B%94%E5%AE%9E%E7%8E%B0/</url>
    
    <content type="html"><![CDATA[<h2 id="需求描述"><a href="#需求描述" class="headerlink" title="需求描述"></a>需求描述</h2><p>在某些测试场景下面，需要满足能够拔盘以后在插入的时候能够自动上线磁盘，这个需求实际在生产中是不建议使用的，原因是插入的磁盘如果本身存在问题，那么拉起的操作可能会破坏了本身集群的稳定性，所以这个算是一个测试相关的功能，但是做这个功能的前提是我们不要去影响了正常的环境的逻辑</p><p>类似的功能的开发都是应该去在外面做触发的也就是能够很方便的开启和关闭功能，整个功能也是集成到原来的环境当中的，原来怎么用，现在怎么用</p><h2 id="实现的方式"><a href="#实现的方式" class="headerlink" title="实现的方式"></a>实现的方式</h2><p>本次设计的方式是udev去截取lvm的设备的加载，如果直接截获磁盘的加载，这个在ceph bluestore里面是有问题的，因为bluestore采用的是lvm设备，这个是需要设备插入以后，再用一系列的触发，再去做相关的启动的</p><p>我们的设计方式是在lvm加载的时候去截取判断，是不是我们需要启动的信息</p><p>修改rule文件<br>&#x2F;usr&#x2F;lib&#x2F;udev&#x2F;rules.d&#x2F;69-dm-lvm-metad.rules<br>这个在最后加上，如果不是我们ceph使用的设备，会在脚本里面去排除</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">LABEL=<span class="hljs-string">&quot;lvm_end&quot;</span><br>RUN+=<span class="hljs-string">&quot;/sbin/trigger.sh <span class="hljs-variable">$name</span>&quot;</span><br></code></pre></td></tr></table></figure><p>在最后面添加一个处理的脚本，修改好文件以后我们需要重载一下规则</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">udevadm control --reload<br></code></pre></td></tr></table></figure><h3 id="udev的触发规则"><a href="#udev的触发规则" class="headerlink" title="udev的触发规则"></a>udev的触发规则</h3><p>&#x2F;sbin&#x2F;trigger.sh 文件的内容如下</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-meta">#!/usr/bin/bash</span><br>logfile=/tmp/udev.log<br><span class="hljs-function"><span class="hljs-title">startdisk</span></span>()&#123;<br><span class="hljs-comment">#记录时间信息</span><br><span class="hljs-built_in">echo</span> `<span class="hljs-built_in">date</span>` <br><span class="hljs-comment"># 判断是不是dm设备，不是dm设备的直接忽略即可</span><br><span class="hljs-keyword">if</span> [[ <span class="hljs-variable">$1</span> !=  dm* ]]; <span class="hljs-keyword">then</span><br><span class="hljs-built_in">exit</span> 0<br><span class="hljs-keyword">fi</span><br><span class="hljs-comment"># 记录设备名称，有问题可以排查</span><br><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;<span class="hljs-variable">$1</span>&quot;</span><br><span class="hljs-comment"># 获取lv_name和vgname 这个拔掉盘实际还是能查到</span><br>dev_vg=`udevadm info  --query=property  /dev/<span class="hljs-variable">$1</span>|grep <span class="hljs-string">&quot;DM_LV_NAME=&quot;</span>|awk -F <span class="hljs-string">&#x27;=&#x27;</span> <span class="hljs-string">&#x27;&#123;print $2&#125;&#x27;</span>`<br>dev_vg_name=`udevadm info  --query=property  /dev/<span class="hljs-variable">$1</span>|grep <span class="hljs-string">&quot;DM_VG_NAME=&quot;</span>|awk -F <span class="hljs-string">&#x27;=&#x27;</span> <span class="hljs-string">&#x27;&#123;print $2&#125;&#x27;</span>`<br><span class="hljs-built_in">echo</span> <span class="hljs-variable">$dev_vg_name</span><br><span class="hljs-built_in">echo</span> <span class="hljs-variable">$dev_vg</span><br><span class="hljs-comment"># 为了确认设备是不是存在，进一步判断</span><br>/usr/sbin/lvdisplay <span class="hljs-variable">$dev_vg_name</span> <br><span class="hljs-keyword">if</span> [ $? -ne 0 ]; <span class="hljs-keyword">then</span><br><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;设备不存在&quot;</span><br><span class="hljs-built_in">exit</span> 0<br><span class="hljs-keyword">fi</span><br><span class="hljs-comment"># 判断下是不是ceph用的设备</span><br>osdindex=<span class="hljs-string">&quot;osd-block&quot;</span><br>        <span class="hljs-keyword">if</span> [ ! -n <span class="hljs-string">&quot;<span class="hljs-variable">$dev_vg</span>&quot;</span> ]||[[ <span class="hljs-variable">$dev_vg</span> != *<span class="hljs-variable">$osdindex</span>* ]]; <span class="hljs-keyword">then</span><br>        <span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;not ceph partition&quot;</span><br><span class="hljs-built_in">exit</span> 0<br>        <span class="hljs-keyword">fi</span><br><br>dev_lv_name=<span class="hljs-variable">$dev_vg</span><br>fsid=<span class="hljs-variable">$&#123;dev_lv_name:10&#125;</span><br><span class="hljs-built_in">echo</span> <span class="hljs-variable">$fsid</span><br><span class="hljs-comment"># 去自启动里面拿到这个信息，如果盘本身不想自启动的，我们就不会拉起了,直接跳过</span><br>osdid=`<span class="hljs-built_in">ls</span> /etc/systemd/system/multi-user.target.wants/ceph-volume@*|grep <span class="hljs-variable">$fsid</span> |awk -F <span class="hljs-string">&#x27;/&#x27;</span> <span class="hljs-string">&#x27;&#123;print $6&#125;&#x27;</span>|awk -F <span class="hljs-string">&#x27;@&#x27;</span> <span class="hljs-string">&#x27;&#123;print $2&#125;&#x27;</span> |awk -F <span class="hljs-string">&#x27;-&#x27;</span> <span class="hljs-string">&#x27;&#123;print $2&#125;&#x27;</span>`<br><span class="hljs-built_in">echo</span> <span class="hljs-variable">$osdid</span><br>service=`<span class="hljs-built_in">ls</span> /etc/systemd/system/multi-user.target.wants/ceph-volume@*|grep <span class="hljs-variable">$fsid</span> |awk -F <span class="hljs-string">&#x27;/&#x27;</span> <span class="hljs-string">&#x27;&#123;print $6&#125;&#x27;</span>|awk -F <span class="hljs-string">&#x27;@&#x27;</span> <span class="hljs-string">&#x27;&#123;print $2&#125;&#x27;</span>`<br><br><span class="hljs-keyword">if</span> [ ! -n <span class="hljs-string">&quot;<span class="hljs-variable">$osdid</span>&quot;</span> ]; <span class="hljs-keyword">then</span><br><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;no osd id&quot;</span><br><span class="hljs-built_in">exit</span> 0<br><span class="hljs-keyword">fi</span><br><br><span class="hljs-keyword">if</span> [ ! -n <span class="hljs-string">&quot;<span class="hljs-variable">$service</span>&quot;</span> ]; <span class="hljs-keyword">then</span><br><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;not ceph partition&quot;</span><br><span class="hljs-built_in">exit</span> 0<br><span class="hljs-keyword">fi</span><br><span class="hljs-comment"># 需要判断一次状态，如果设备本身已经是启动的，那么我们就不操作，不判断会无限循环</span><br>osdstatus=`systemctl is-active  ceph-osd@<span class="hljs-variable">$osdid</span>`<br><span class="hljs-keyword">if</span> [[ <span class="hljs-variable">$osdstatus</span> == active ]]; <span class="hljs-keyword">then</span><br><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;osd runing&quot;</span><br><span class="hljs-built_in">exit</span> 0<br><span class="hljs-keyword">fi</span><br><br>dostart=`<span class="hljs-built_in">cat</span> /var/lib/ceph/osd/ceph-<span class="hljs-variable">$osdid</span>/autostart`<br><span class="hljs-comment"># 主动的停止不需要启动拉起</span><br><span class="hljs-keyword">if</span> [[ <span class="hljs-variable">$dostart</span> == 0 ]]; <span class="hljs-keyword">then</span><br>        <span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;not want start osd&quot;</span><br>             <span class="hljs-built_in">exit</span> 0<br>        <span class="hljs-keyword">fi</span><br><br><span class="hljs-comment"># 卸载掉设备，进行一次自启动和挂载</span><br>umount /var/lib/ceph/osd/ceph-<span class="hljs-variable">$osdid</span><br>systemctl start ceph-volume@<span class="hljs-variable">$service</span><br><span class="hljs-comment"># 再判断一次，万一真拉不起来，那就多等待一会，避免反复拉起,后面看下要不要改成拉不起来就不拉了，磁盘应该挂上了只是启动不了</span><br>        osdstatus=`systemctl is-active  ceph-osd@<span class="hljs-variable">$osdid</span>`<br>        <span class="hljs-keyword">if</span> [[ <span class="hljs-variable">$osdstatus</span> != active ]]; <span class="hljs-keyword">then</span><br>        <span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;osd not runing wait 1 min&quot;</span><br><span class="hljs-built_in">sleep</span> 60<br>        <span class="hljs-built_in">exit</span> 0<br>        <span class="hljs-keyword">fi</span><br>&#125;<br><br>startdisk  <span class="hljs-variable">$1</span> &gt;&gt; <span class="hljs-variable">$logfile</span> <br><br></code></pre></td></tr></table></figure><h2 id="测试的时候可以"><a href="#测试的时候可以" class="headerlink" title="测试的时候可以"></a>测试的时候可以</h2><h3 id="删除磁盘"><a href="#删除磁盘" class="headerlink" title="删除磁盘"></a>删除磁盘</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">echo</span> 1 &gt; /sys/block/sdb/device/delete<br></code></pre></td></tr></table></figure><h3 id="扫描磁盘"><a href="#扫描磁盘" class="headerlink" title="扫描磁盘"></a>扫描磁盘</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-keyword">for</span> a <span class="hljs-keyword">in</span> <span class="hljs-built_in">seq</span> 0 2;<span class="hljs-keyword">do</span> <span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;- - -&quot;</span> &gt; /sys/class/scsi_host/host<span class="hljs-variable">$a</span>/scan;<span class="hljs-keyword">done</span>;<br></code></pre></td></tr></table></figure><h2 id="注意"><a href="#注意" class="headerlink" title="注意"></a>注意</h2><p>不管是主动做stop还是对相关的进程做kill，都会触发设备的变化，这个地方就没法判断是真的人为触发的stop，还是挂掉的，还是插入磁盘的时候没有启动，这里就通过打标记去判断下</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">KERNEL[22163.000802] change   /devices/virtual/block/dm-3 (block)<br>UDEV  [22163.060487] change   /devices/virtual/block/dm-3 (block)<br>KERNEL[22183.407454] change   /devices/virtual/block/dm-3 (block)<br>UDEV  [22183.472656] change   /devices/virtual/block/dm-3 (block)<br></code></pre></td></tr></table></figure><p>也就是会再次进入监控的流程里面，所以需要加入判断<br>主动进行的stop是不要去拉起来的，这个会做一个标志位的设置<br>启动的时候，默认会把标志位改成要自启动的，这个可以自己手动修改去控制</p><h3 id="修改启动脚本，加入引用脚本"><a href="#修改启动脚本，加入引用脚本" class="headerlink" title="修改启动脚本，加入引用脚本"></a>修改启动脚本，加入引用脚本</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ~]<span class="hljs-comment"># cat /usr/lib/systemd/system/ceph-osd@.service</span><br>[Unit]<br>Description=Ceph object storage daemon osd.%i<br>After=network-online.target local-fs.target time-sync.target ceph-mon.target<br>Wants=network-online.target local-fs.target time-sync.target<br>PartOf=ceph-osd.target<br><br>[Service]<br>LimitNOFILE=1048576<br>LimitNPROC=1048576<br>EnvironmentFile=-/etc/sysconfig/ceph<br>Environment=CLUSTER=ceph<br>ExecStart=/usr/bin/ceph-osd -f --cluster <span class="hljs-variable">$&#123;CLUSTER&#125;</span> --<span class="hljs-built_in">id</span> %i --setuser ceph --setgroup ceph<br>ExecStartPre=/usr/lib/ceph/ceph-osd-prestart.sh --cluster <span class="hljs-variable">$&#123;CLUSTER&#125;</span> --<span class="hljs-built_in">id</span> %i<br>ExecReload=/bin/kill -HUP <span class="hljs-variable">$MAINPID</span><br>ExecStartPost=-/usr/lib/ceph/ceph-osd-start.sh  %i<br>ExecStop=-/usr/lib/ceph/ceph-osd-stop.sh  %i<br>ProtectHome=<span class="hljs-literal">true</span><br>ProtectSystem=full<br>PrivateTmp=<span class="hljs-literal">true</span><br>TasksMax=infinity<br>Restart=on-failure<br>StartLimitInterval=30min<br>StartLimitBurst=30<br>RestartSec=20s<br><br>[Install]<br>WantedBy=ceph-osd.target<br><br></code></pre></td></tr></table></figure><p>增加两个钩子脚本，不会影响原来的启动</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ~]<span class="hljs-comment"># cat /usr/lib/ceph/ceph-osd-start.sh </span><br><span class="hljs-comment">#! /bin/bash</span><br><span class="hljs-built_in">echo</span> 1 &gt;  /var/lib/ceph/osd/ceph-<span class="hljs-variable">$1</span>/autostart<br>[root@lab101 ~]<span class="hljs-comment"># cat /usr/lib/ceph/ceph-osd-stop.sh </span><br><span class="hljs-comment">#! /bin/bash</span><br><span class="hljs-built_in">echo</span> 0 &gt;  /var/lib/ceph/osd/ceph-<span class="hljs-variable">$1</span>/autostart<br></code></pre></td></tr></table></figure><h2 id="功能基本完成"><a href="#功能基本完成" class="headerlink" title="功能基本完成"></a>功能基本完成</h2><p>需要对一些比较特殊的场景进行下测试了，比如真的起不来的时候要不要做更多的判断控制，目前就这块可能存在一直尝试拉的问题，加个计数器可以解决，这里把主线都基本完成了</p><h2 id="更新历史"><a href="#更新历史" class="headerlink" title="更新历史"></a>更新历史</h2><table><thead><tr><th>why</th><th>when</th></tr></thead><tbody><tr><td>创建</td><td>2019年09月02日</td></tr><tr><td>更新</td><td>2019年12月9日</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>http服务器文件名大小写忽略</title>
    <link href="/2019/09/02/http%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%96%87%E4%BB%B6%E5%90%8D%E5%A4%A7%E5%B0%8F%E5%86%99%E5%BF%BD%E7%95%A5/"/>
    <url>/2019/09/02/http%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%96%87%E4%BB%B6%E5%90%8D%E5%A4%A7%E5%B0%8F%E5%86%99%E5%BF%BD%E7%95%A5/</url>
    
    <content type="html"><![CDATA[<h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>文件从windows里面放到nginx里面去的时候，文件在windows下面是大小写忽略，也就是不论大小写都可以匹配的，而到linux下面的时候，因为linux是区分大小写的，也就是会出现无法忽略大小写的访问</p><h2 id="调研"><a href="#调研" class="headerlink" title="调研"></a>调研</h2><p>通过nginx里面目前还没有找到实现的方式，通过插件的方式也只是能把大小写全部转换成小写，这样要让存储的文件的名称全部改成小写，这个不太适合去改变用户的文件</p><p>而apache里面有模块直接来实现这个，那么考虑有适用场景的时候，可以使用nginx加apache或者直接采用apache的方式，这个最好可以灵活的选择，不限定最好</p><h2 id="实现方式"><a href="#实现方式" class="headerlink" title="实现方式"></a>实现方式</h2><p>修改配置文件&#x2F;etc&#x2F;httpd&#x2F;conf&#x2F;httpd.conf</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs bash">&lt;Directory /&gt;<br>    AllowOverride ALL<br>    CheckSpelling on<br>    CheckCaseOnly on<br>    Require all denied<br>&lt;/Directory&gt;<br><br>LoadModule speling_module modules/mod_speling.so<br></code></pre></td></tr></table></figure><p>经过测试单独开启一个</p><blockquote><p>CheckSpelling on</p></blockquote><p>就可以的</p><p>开启两个</p><blockquote><p>CheckSpelling on<br><br>CheckCaseOnly on</p></blockquote><p>屏蔽无关的更正，这个可以两个都设置或者设置一个也行的，开启两个就是将拼写更正的操作限制为小写&#x2F;大写更改。不执行其他可能的校正</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>这个根据选择进行使用，做成可选的时候就会方便很多，提供一个默认的选项，通过安装里面做控制</p><h2 id="更新历史"><a href="#更新历史" class="headerlink" title="更新历史"></a>更新历史</h2><table><thead><tr><th>why</th><th>when</th></tr></thead><tbody><tr><td>创建</td><td>2019年09月02日</td></tr><tr><td>更新</td><td>2019年12月9日</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Linux操作系统选择</title>
    <link href="/2019/09/01/Linux%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E9%80%89%E6%8B%A9/"/>
    <url>/2019/09/01/Linux%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E9%80%89%E6%8B%A9/</url>
    
    <content type="html"><![CDATA[<h2 id="主流的操作系统"><a href="#主流的操作系统" class="headerlink" title="主流的操作系统"></a>主流的操作系统</h2><ul><li>ubuntu</li><li>centos</li><li>debian</li><li>oracle linux</li></ul><p>主要使用的操作系统就是上面几个，主要是ubuntu和centos，debian是基于ubuntu改的，oracle linux是基于centos改的</p><h2 id="操作系统介绍"><a href="#操作系统介绍" class="headerlink" title="操作系统介绍"></a>操作系统介绍</h2><h3 id="ubuntu操作系统"><a href="#ubuntu操作系统" class="headerlink" title="ubuntu操作系统"></a>ubuntu操作系统</h3><p>ubuntu操作系统是主打桌面的一个操作系统，软件包的更新比较快，内核的版本用的比较新，如果软件系统都是采用的比较新的库和新的内核，那么可以考虑使用这个系统，当然缺点就是系统的维护周期较短，需要去比较频繁的升级来适应新的系统</p><p>Ubuntu每6个月发布一个新版，每个版本都有代号和版本号。版本号源自发布日期，例如第一个版本，4.10，代表是在2004年10月发行的</p><table><thead><tr><th>版本</th><th>开发代号</th><th>发布日期</th><th>支持结束时间</th><th>内核版本</th></tr></thead><tbody><tr><td>4.1</td><td>Warty Warthog</td><td>2004&#x2F;10&#x2F;20</td><td>2006&#x2F;4&#x2F;30</td><td>2.6.8</td></tr><tr><td>5.04</td><td>Hoary Hedgehog</td><td>2005&#x2F;4&#x2F;8</td><td>2006&#x2F;10&#x2F;31</td><td>2.6.10</td></tr><tr><td>5.1</td><td>Breezy Badger</td><td>2005&#x2F;10&#x2F;13</td><td>2007&#x2F;4&#x2F;13</td><td>2.6.12</td></tr><tr><td>6.06 LTS</td><td>Dapper Drake</td><td>2006&#x2F;6&#x2F;1</td><td>2011&#x2F;6&#x2F;1</td><td>2.6.15</td></tr><tr><td>6.1</td><td>Edgy Eft</td><td>2006&#x2F;10&#x2F;26</td><td>2008&#x2F;4&#x2F;25</td><td>2.6.17</td></tr><tr><td>7.04</td><td>Feisty Fawn</td><td>2007&#x2F;4&#x2F;19</td><td>2008&#x2F;10&#x2F;19</td><td>2.6.20</td></tr><tr><td>7.1</td><td>Gutsy Gibbon</td><td>2007&#x2F;10&#x2F;18</td><td>2009&#x2F;4&#x2F;18</td><td>2.6.22</td></tr><tr><td>8.04 LTS</td><td>Hardy Heron</td><td>2008&#x2F;4&#x2F;24</td><td>2013&#x2F;5&#x2F;9</td><td>2.6.24</td></tr><tr><td>8.1</td><td>Intrepid Ibex</td><td>2008&#x2F;10&#x2F;30</td><td>2010&#x2F;4&#x2F;30</td><td>2.6.27</td></tr><tr><td>9.04</td><td>Jaunty Jackalope</td><td>2009&#x2F;4&#x2F;23</td><td>2010&#x2F;10&#x2F;23</td><td>2.6.28</td></tr><tr><td>9.1</td><td>Karmic Koala</td><td>2009&#x2F;10&#x2F;29</td><td>2011&#x2F;4&#x2F;30</td><td>2.6.31</td></tr><tr><td>10.04 LTS</td><td>Lucid Lynx</td><td>2010&#x2F;4&#x2F;29</td><td>2015&#x2F;4&#x2F;30</td><td>2.6.32</td></tr><tr><td>10.1</td><td>Maverick Meerkat</td><td>2010&#x2F;10&#x2F;10</td><td>2012&#x2F;4&#x2F;10</td><td>2.6.35</td></tr><tr><td>11.04</td><td>Natty Narwhal</td><td>2011&#x2F;4&#x2F;28</td><td>2012&#x2F;10&#x2F;28</td><td>2.6.38</td></tr><tr><td>11.1</td><td>Oneiric Ocelot</td><td>2011&#x2F;10&#x2F;13</td><td>2013&#x2F;5&#x2F;9</td><td>3.0</td></tr><tr><td>12.04 LTS</td><td>Precise Pangolin</td><td>2012&#x2F;4&#x2F;26</td><td>2017&#x2F;4&#x2F;28</td><td>3.2</td></tr><tr><td>12.1</td><td>Quantal Quetzal</td><td>2012&#x2F;10&#x2F;18</td><td>2014&#x2F;5&#x2F;16</td><td>3.5</td></tr><tr><td>13.04</td><td>Raring Ringtail</td><td>2013&#x2F;4&#x2F;25</td><td>2014&#x2F;1&#x2F;27</td><td>3.8</td></tr><tr><td>13.1</td><td>Saucy Salamander</td><td>2013&#x2F;10&#x2F;17</td><td>2014&#x2F;7&#x2F;17</td><td>3.11</td></tr><tr><td>14.04 LTS</td><td>Trusty Tahr</td><td>2014&#x2F;4&#x2F;17</td><td>2019&#x2F;4&#x2F;25</td><td>3.13</td></tr><tr><td>14.1</td><td>Utopic Unicorn</td><td>2014&#x2F;10&#x2F;23</td><td>2015&#x2F;7&#x2F;23</td><td>3.16</td></tr><tr><td>15.04</td><td>Vivid Vervet</td><td>2015&#x2F;4&#x2F;23</td><td>2016&#x2F;2&#x2F;4</td><td>3.19</td></tr><tr><td>15.1</td><td>Wily Werewolf</td><td>2015&#x2F;10&#x2F;22</td><td>2016&#x2F;7&#x2F;28</td><td>4.2</td></tr><tr><td>16.04 LTS</td><td>Xenial Xerus</td><td>2016&#x2F;4&#x2F;21</td><td>2021&#x2F;4&#x2F;1</td><td>4.4</td></tr><tr><td>16.1</td><td>Yakkety Yak</td><td>2016&#x2F;10&#x2F;13</td><td>2017&#x2F;7&#x2F;20</td><td>4.8</td></tr><tr><td>17.04</td><td>Zesty Zapus</td><td>2017&#x2F;4&#x2F;13</td><td>2018&#x2F;1&#x2F;13</td><td>4.10</td></tr><tr><td>17.1</td><td>Artful Aardvark</td><td>2017&#x2F;10&#x2F;19</td><td>2018&#x2F;7&#x2F;19</td><td>4.13</td></tr><tr><td>18.04 LTS</td><td>Bionic Beaver[65][66]</td><td>2018&#x2F;4&#x2F;26</td><td>2023&#x2F;4&#x2F;1</td><td>4.15</td></tr><tr><td>18.1</td><td>Cosmic Cuttlefish</td><td>2018&#x2F;10&#x2F;18</td><td>2019&#x2F;7&#x2F;18</td><td>4.18</td></tr><tr><td>19.04</td><td>Disco Dingo</td><td>2019&#x2F;4&#x2F;18</td><td>2020&#x2F;1&#x2F;1</td><td>5.0</td></tr><tr><td>19.1</td><td>Eoan Ermine</td><td>2019&#x2F;10&#x2F;17</td><td>2020&#x2F;7&#x2F;1</td><td>5.3</td></tr><tr><td>20.04 LTS</td><td>Focal Fossa</td><td>2020&#x2F;4&#x2F;23</td><td>2025&#x2F;4&#x2F;1</td><td>5.5</td></tr></tbody></table><h3 id="centos操作系统"><a href="#centos操作系统" class="headerlink" title="centos操作系统"></a>centos操作系统</h3><p>发布历史</p><table><thead><tr><th>版本</th><th>RHEL 版本</th><th>发布日期</th><th>RHEL 发布日期</th></tr></thead><tbody><tr><td>2</td><td>2.1</td><td>2004&#x2F;5&#x2F;14</td><td>2002&#x2F;5&#x2F;17</td></tr><tr><td>3.1</td><td>3</td><td>2004&#x2F;3&#x2F;19</td><td>2003&#x2F;10&#x2F;23</td></tr><tr><td>3.4 - Server CD</td><td>3.4</td><td>2005&#x2F;1&#x2F;23</td><td>-</td></tr><tr><td>3.7</td><td>3.7</td><td>2006&#x2F;4&#x2F;11</td><td>-</td></tr><tr><td>3.8</td><td>3.8</td><td>2006&#x2F;8&#x2F;25</td><td>2006&#x2F;7&#x2F;20</td></tr><tr><td>3.9</td><td>3.9</td><td>2007&#x2F;7&#x2F;26</td><td>2007&#x2F;6&#x2F;15</td></tr><tr><td>4</td><td>4</td><td>2005&#x2F;3&#x2F;9</td><td>2005&#x2F;2&#x2F;14</td></tr><tr><td>4.6</td><td>4.6</td><td>2007&#x2F;12&#x2F;16</td><td>2007&#x2F;11&#x2F;16</td></tr><tr><td>4.7</td><td>4.7</td><td>2008&#x2F;9&#x2F;13</td><td>2008&#x2F;7&#x2F;24</td></tr><tr><td>4.7 - Server CD</td><td>4.7</td><td>2008&#x2F;10&#x2F;17</td><td>-</td></tr><tr><td>4.8</td><td>4.8</td><td>2009&#x2F;8&#x2F;21</td><td>2009&#x2F;5&#x2F;18</td></tr><tr><td>4.9</td><td>4.9</td><td>2011&#x2F;3&#x2F;2</td><td>2011&#x2F;2&#x2F;16</td></tr><tr><td>5</td><td>5</td><td>2007&#x2F;4&#x2F;12</td><td>2007&#x2F;3&#x2F;14</td></tr><tr><td>5.1</td><td>5.1</td><td>2007&#x2F;12&#x2F;2</td><td>2007&#x2F;11&#x2F;7</td></tr><tr><td>5.1 - LiveCD</td><td>5.1</td><td>2008&#x2F;2&#x2F;18</td><td>-</td></tr><tr><td>5.2</td><td>5.2</td><td>2008&#x2F;6&#x2F;24</td><td>2008&#x2F;5&#x2F;21</td></tr><tr><td>5.2 - LiveCD</td><td>5.2</td><td>2008&#x2F;7&#x2F;17</td><td>-</td></tr><tr><td>5.3</td><td>5.3</td><td>2009&#x2F;3&#x2F;31</td><td>2009&#x2F;1&#x2F;20</td></tr><tr><td>5.3 - Live CD</td><td>5.3</td><td>2009&#x2F;5&#x2F;27</td><td>-</td></tr><tr><td>5.4</td><td>5.4</td><td>2009&#x2F;10&#x2F;21</td><td>2009&#x2F;9&#x2F;2</td></tr><tr><td>5.5</td><td>5.5</td><td>2010&#x2F;5&#x2F;14</td><td>2010&#x2F;3&#x2F;31</td></tr><tr><td>5.5 - LiveCD</td><td>5.5</td><td>2010&#x2F;5&#x2F;14</td><td>-</td></tr><tr><td>5.6</td><td>5.6</td><td>2011&#x2F;4&#x2F;8</td><td>2011&#x2F;1&#x2F;13</td></tr><tr><td>5.7</td><td>5.7</td><td>2011&#x2F;9&#x2F;13</td><td>2011&#x2F;7&#x2F;21</td></tr><tr><td>5.8</td><td>5.8</td><td>2012&#x2F;3&#x2F;7</td><td>2012&#x2F;2&#x2F;21</td></tr><tr><td>5.9</td><td>5.9</td><td>2013&#x2F;1&#x2F;17</td><td>2013&#x2F;1&#x2F;7</td></tr><tr><td>5.1</td><td>5.1</td><td>2013&#x2F;10&#x2F;19</td><td>2013&#x2F;10&#x2F;1</td></tr><tr><td>5.11</td><td>5.11</td><td>2014&#x2F;9&#x2F;30</td><td>2014&#x2F;9&#x2F;16</td></tr><tr><td>6</td><td>6</td><td>2011&#x2F;7&#x2F;10</td><td>2010&#x2F;11&#x2F;10</td></tr><tr><td>6.0 - LiveCD</td><td>6</td><td>2011&#x2F;7&#x2F;25</td><td>-</td></tr><tr><td>6.0 - LiveDVD</td><td>6</td><td>2011&#x2F;7&#x2F;27</td><td>-</td></tr><tr><td>6.0 - MinimalCD</td><td>6</td><td>2011&#x2F;7&#x2F;28</td><td>-</td></tr><tr><td>6.1</td><td>6.1</td><td>2011&#x2F;12&#x2F;9</td><td>2011&#x2F;5&#x2F;19</td></tr><tr><td>6.1 - LiveCD</td><td>6.1</td><td>2011&#x2F;12&#x2F;9</td><td>-</td></tr><tr><td>6.1 - LiveDVD</td><td>6.1</td><td>2011&#x2F;12&#x2F;9</td><td>-</td></tr><tr><td>6.1 - MinimalCD</td><td>6.1</td><td>2011&#x2F;12&#x2F;9</td><td>-</td></tr><tr><td>6.2</td><td>6.2</td><td>2011&#x2F;12&#x2F;20</td><td>2011&#x2F;12&#x2F;6</td></tr><tr><td>6.2 - LiveCD</td><td>6.2</td><td>2011&#x2F;12&#x2F;20</td><td>-</td></tr><tr><td>6.2 - LiveDVD</td><td>6.2</td><td>2011&#x2F;12&#x2F;20</td><td>-</td></tr><tr><td>6.2 - MinimalCD</td><td>6.2</td><td>2011&#x2F;12&#x2F;20</td><td>-</td></tr><tr><td>6.3</td><td>6.3</td><td>2012&#x2F;7&#x2F;9</td><td>2012&#x2F;6&#x2F;21</td></tr><tr><td>6.4</td><td>6.4</td><td>2013&#x2F;3&#x2F;9</td><td>2013&#x2F;2&#x2F;21</td></tr><tr><td>6.5</td><td>6.5</td><td>2013&#x2F;12&#x2F;1</td><td>2013&#x2F;11&#x2F;21</td></tr><tr><td>6.6</td><td>6.6</td><td>2014&#x2F;10&#x2F;28</td><td>2014&#x2F;10&#x2F;14</td></tr><tr><td>6.7</td><td>6.7</td><td>2015&#x2F;8&#x2F;7</td><td>2015&#x2F;7&#x2F;22</td></tr><tr><td>6.8</td><td>6.8</td><td>2016&#x2F;5&#x2F;25</td><td>2016&#x2F;5&#x2F;9</td></tr><tr><td>6.9</td><td>6.9</td><td>2017&#x2F;4&#x2F;5</td><td>2017&#x2F;3&#x2F;21</td></tr><tr><td>6.1</td><td>6.1</td><td>2018&#x2F;7&#x2F;3</td><td>2018&#x2F;6&#x2F;19</td></tr><tr><td>7-1406</td><td>7</td><td>2014&#x2F;7&#x2F;7</td><td>2014&#x2F;6&#x2F;10</td></tr><tr><td>7-1503</td><td>7.1</td><td>2015&#x2F;3&#x2F;31</td><td>2015&#x2F;3&#x2F;6</td></tr><tr><td>7-1503-AArch64</td><td>7.1</td><td>2015&#x2F;8&#x2F;4</td><td>2015&#x2F;3&#x2F;6</td></tr><tr><td>7-1503-i386</td><td>7.1</td><td>2015&#x2F;10&#x2F;12</td><td>2015&#x2F;3&#x2F;6</td></tr><tr><td>7-1511</td><td>7.2</td><td>2015&#x2F;12&#x2F;14</td><td>2015&#x2F;11&#x2F;19</td></tr><tr><td>7-1611</td><td>7.3</td><td>2016&#x2F;12&#x2F;12</td><td>2016&#x2F;11&#x2F;3</td></tr><tr><td>7-1708</td><td>7.4</td><td>2017&#x2F;9&#x2F;13</td><td>2017&#x2F;8&#x2F;1</td></tr><tr><td>7-1804</td><td>7.5</td><td>2018&#x2F;5&#x2F;10</td><td>2018&#x2F;4&#x2F;10</td></tr><tr><td>7-1810</td><td>7.6</td><td>2018&#x2F;12&#x2F;3</td><td>2018&#x2F;10&#x2F;30</td></tr><tr><td>7-1908</td><td>7.7</td><td>2019&#x2F;9&#x2F;17</td><td>2019&#x2F;8&#x2F;6</td></tr><tr><td>8-1905</td><td>8</td><td>2019&#x2F;9&#x2F;24</td><td>2019&#x2F;5&#x2F;7</td></tr></tbody></table><p>centos可以理解为redhat的开源版本，现在也是红帽在维护在，红帽会自己合入一些补丁包来完成企业支持，现在centos已经被红帽收购</p><p>维护周期</p><table><thead><tr><th>发行版本</th><th>完整更新</th><th>维护更新</th></tr></thead><tbody><tr><td>3</td><td>2006-07-20</td><td>2010-10-31</td></tr><tr><td>4</td><td>2009-03-31</td><td>2012-02-29</td></tr><tr><td>5</td><td>2014-01-31</td><td>2017-03-31</td></tr><tr><td>6</td><td>2017-05-10</td><td>2020-11-30</td></tr><tr><td>7</td><td>2020-08-06</td><td>2024-06-30</td></tr><tr><td>8</td><td>2024-05-01</td><td>2029-05-01</td></tr></tbody></table><p>从上面的维护周期可以看到，如果还在用centos6那么官方的维护已经只到2020-11-30，并且不会再出小版本了</p><p>而现在的7已经稳定使用了一段时间了，8才出来不久，整体上来说，个人建议直接采用centos7最新版本即可，后面还会出几个完整更新的，整体软件不会打的改动</p><p>尝新的话可以用8，这个就看自身的软件的适应程度了</p><h3 id="debian操作系统"><a href="#debian操作系统" class="headerlink" title="debian操作系统"></a>debian操作系统</h3><p>debian版本和ubuntu版本对应关系</p><table><thead><tr><th>ubuntu版本号</th><th>ubuntu版本名称</th><th>debian名称</th><th>debian版本号</th></tr></thead><tbody><tr><td>19.04</td><td>disco</td><td>buster</td><td>-10</td></tr><tr><td>18.1</td><td>cosmic</td><td>buster</td><td></td></tr><tr><td>18.04</td><td>bionic</td><td>buster</td><td></td></tr><tr><td>17.1</td><td>artful</td><td>stretch</td><td>-9</td></tr><tr><td>17.04</td><td>zesty</td><td>stretch</td><td></td></tr><tr><td>16.1</td><td>yakkety</td><td>stretch</td><td></td></tr><tr><td>16.04</td><td>xenial</td><td>stretch</td><td></td></tr><tr><td>15.1</td><td>wily</td><td>jessie</td><td>-8</td></tr><tr><td>15.04</td><td>vivid</td><td>jessie</td><td></td></tr><tr><td>14.1</td><td>utopic</td><td>jessie</td><td></td></tr><tr><td>14.04</td><td>trusty</td><td>jessie</td><td></td></tr><tr><td>13.1</td><td>saucy</td><td>wheezy</td><td>-7</td></tr><tr><td>13.04</td><td>raring</td><td>wheezy</td><td></td></tr><tr><td>12.1</td><td>quantal</td><td>wheezy</td><td></td></tr><tr><td>12.04</td><td>precise</td><td>wheezy</td><td></td></tr><tr><td>11.1</td><td>oneiric</td><td>wheezy</td><td></td></tr><tr><td>11.04</td><td>natty</td><td>squeeze</td><td>-6</td></tr><tr><td>10.1</td><td>maverick</td><td>squeeze</td><td></td></tr><tr><td>10.04</td><td>lucid</td><td>squeeze</td><td></td></tr></tbody></table><p>从整体上面看debian更新的没ubuntu那么快，会保守很多，一些企业不喜欢频繁变动版本的可以考虑使用这个</p><h3 id="oracle-linux"><a href="#oracle-linux" class="headerlink" title="oracle linux"></a>oracle linux</h3><p>可能很多人并不了解这个版本，但是这个版本你可以理解为免费的企业版本的红帽，这里面肯定有个疑惑是，为什么又是企业版本又是免费，并且这个oracle 不是做数据库的么，这个linux是oracle 来维护的自己的版本，是基于centos版本做的，并且提供了一个更好的内核，可以选择主线版本内核，也提供了一个比较高版本的内核，并且内核名称是The Unbreakable Enterprise Kernel (UEK)，从名称上面看是坚不可摧的企业版内核，这个内核也可以直接安装到centos上的，也就是能够适配centos，这个肯定没有问题的</p><p>在centos还是6版本的时候，由于相互的系统的策略不同，ubuntu下的性能明显要高于centos的，这个你觉得不都是一样的硬件，为什么会有差距，这个是我们之前在相同硬件，大压力下面，ubuntu扛住了，而centos6没有扛住，这个时候，切换了oracle linux，发现基本上是能够跟当时的ubuntu差不多的</p><p>这个问题是发生在centos6的时代，在进入centos 7的时代后，发现相同的压力硬件下，centos7的性能又进一步提升了，再后来来看ubuntu和centos7 差距就没centos6那么明显</p><p>这里为什么推荐oracle linux，如果你的团队没有维护os的能力，没有定制或者调优的能力，那么有这么一款比默认centos的要好一些的，并且提供了企业级的操作系统，这个还是很值得一用的，这个在之前os我自己做维护的时候，就是采用的这个os，但是现在os不是我去控制，各方面的协调的东西太多，就放弃了这个os的选择</p><p>实际情况是如果软件做的足够去耦合，应该是像安装一个samba一样，提供一个核心的软件以后，其它的依赖是可以随便使用哪个平台的，也就不存在一个适配平台的成本了，而实际情况是开发为了方便代码的编写，在做整包系统里面会去导入一些比较特殊的依赖包，并且版本不是去用os依赖的形式维护，就造成了整体搬迁的麻烦，这个部分在未来会更详细的去介绍这块的思路</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本篇章介绍了几个主流的os，关于os的选择，可以根据自己的需要去选择，如果自己的软件比较小，需要的东西新，可以使用ubuntu版本，如果是做企业级比较复杂的软件，可以是用centos系列的操作系统，个人推荐是用centos或者oracle linux的</p><h2 id="更新历史"><a href="#更新历史" class="headerlink" title="更新历史"></a>更新历史</h2><table><thead><tr><th>why</th><th>when</th></tr></thead><tbody><tr><td>创建</td><td>2019年09月01日</td></tr><tr><td>更新</td><td>2019年12月9日</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>iscsi客户端常用操作</title>
    <link href="/2019/09/01/iscsi%E5%AE%A2%E6%88%B7%E7%AB%AF%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C/"/>
    <url>/2019/09/01/iscsi%E5%AE%A2%E6%88%B7%E7%AB%AF%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C/</url>
    
    <content type="html"><![CDATA[<h2 id="说明"><a href="#说明" class="headerlink" title="说明"></a>说明</h2><p>本篇主要记录iscsi的客户端的一些常用的一些操作</p><h2 id="iscsi服务端常用操作"><a href="#iscsi服务端常用操作" class="headerlink" title="iscsi服务端常用操作"></a>iscsi服务端常用操作</h2><p>删除一个lun</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">tgtadm --lld iscsi --mode logicalunit --op delete  --tid 2  --lun 1<br></code></pre></td></tr></table></figure><p>新增一个lun</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">tgtadm --lld iscsi --mode logicalunit --op new --tid 2 --lun 1 --backing-store rbd/test4M --bstype rbd --blocksize 2048<br></code></pre></td></tr></table></figure><p>更新一个lun</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">tgtadm --lld iscsi --mode logicalunit --op update --tid 2 --lun 1   --param <span class="hljs-string">&quot;lbppbe=0&quot;</span><br></code></pre></td></tr></table></figure><h2 id="常用的操作"><a href="#常用的操作" class="headerlink" title="常用的操作"></a>常用的操作</h2><h3 id="安装软件"><a href="#安装软件" class="headerlink" title="安装软件"></a>安装软件</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">yum install iscsi-initiator-utils<br></code></pre></td></tr></table></figure><h3 id="查询远端的iscsi-target"><a href="#查询远端的iscsi-target" class="headerlink" title="查询远端的iscsi target"></a>查询远端的iscsi target</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">iscsiadm -m discovery -t sendtargets -p 66.66.66.189<br></code></pre></td></tr></table></figure><h3 id="登陆指定的target"><a href="#登陆指定的target" class="headerlink" title="登陆指定的target"></a>登陆指定的target</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">iscsiadm -m node -T iqn.vcluster.com:target01 -p 66.66.66.189 -l<br></code></pre></td></tr></table></figure><h3 id="查询当前机器连接"><a href="#查询当前机器连接" class="headerlink" title="查询当前机器连接"></a>查询当前机器连接</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@client181 ~]<span class="hljs-comment"># iscsiadm -m session ls</span><br>tcp: [4] 66.66.66.189:3260,1 iqn.vcluster.com:target01 (non-flash)<br></code></pre></td></tr></table></figure><h3 id="查询当前连接的参数"><a href="#查询当前连接的参数" class="headerlink" title="查询当前连接的参数"></a>查询当前连接的参数</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs bash">iscsiadm -m session -P3|grep -i <span class="hljs-built_in">timeout</span><br>    Timeouts:<br>    Recovery Timeout: 5<br>    Target Reset Timeout: 30<br>    LUN Reset Timeout: 30<br>    Abort Timeout: 15<br></code></pre></td></tr></table></figure><h3 id="断开连接"><a href="#断开连接" class="headerlink" title="断开连接"></a>断开连接</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">iscsiadm -m node -T iqn.vcluster.com:target01 -p 66.66.66.189 -u<br></code></pre></td></tr></table></figure><h3 id="断开搜索"><a href="#断开搜索" class="headerlink" title="断开搜索"></a>断开搜索</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">iscsiadm -m discovery -p 66.66.66.189 -o delete<br></code></pre></td></tr></table></figure><blockquote><p>注意<br><br>iscsi的相关参数是在discovery的时候设置的，所以通过配置文件修改的参数要在下次才能生效，或者通过命令进行在线的更新</p></blockquote><h3 id="服务端在线更新配置"><a href="#服务端在线更新配置" class="headerlink" title="服务端在线更新配置"></a>服务端在线更新配置</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">tgtadm --lld iscsi --op update --mode logicalunit --tid 3  --lun 13 --params lbppbe=0<br></code></pre></td></tr></table></figure><h2 id="更新历史"><a href="#更新历史" class="headerlink" title="更新历史"></a>更新历史</h2><table><thead><tr><th>why</th><th>when</th></tr></thead><tbody><tr><td>创建</td><td>2019年09月01日</td></tr><tr><td>更新</td><td>2019年12月9日</td></tr><tr><td>更新</td><td>2020年9月18日</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>bluestore对象挂载到系统进行提取</title>
    <link href="/2019/07/26/bluestore%E5%AF%B9%E8%B1%A1%E6%8C%82%E8%BD%BD%E5%88%B0%E7%B3%BB%E7%BB%9F%E8%BF%9B%E8%A1%8C%E6%8F%90%E5%8F%96/"/>
    <url>/2019/07/26/bluestore%E5%AF%B9%E8%B1%A1%E6%8C%82%E8%BD%BD%E5%88%B0%E7%B3%BB%E7%BB%9F%E8%BF%9B%E8%A1%8C%E6%8F%90%E5%8F%96/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>之前在filestore里面，pg是直接暴露到文件系统的，也就是可以直接进去查看或者拷贝，在极端情况下，多个osd无法启动，pg无法导出的时候，那么对pg内部对象的操作处理，是可以作为最后恢复数据的一种方式的</p><p>这个在bluestore里面就没那么直接了，之前也碰到过一次，osd无法启动，内部死循环，pg无法export，osd就僵死在那里了，实际上，bluestore也提供了接口把对象能够直接显示出来</p><h2 id="具体操作实践"><a href="#具体操作实践" class="headerlink" title="具体操作实践"></a>具体操作实践</h2><p>我们选择一个pg 1.7</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ceph]<span class="hljs-comment"># ceph pg dump|grep 1.7</span><br>dumped all<br>1.7         128                  0        0         0       0 524353536 1583     1583 active+clean 2019-07-26 10:05:17.715749 14<span class="hljs-string">&#x27;3583  14:3670 [1]          1    [1]              1        0&#x27;</span>0 2019-07-26 10:01:20.337218             0<span class="hljs-string">&#x27;0 2019-07-26 10:01:20.337218</span><br></code></pre></td></tr></table></figure><p>可以看到pg 1.7是有128个对象存储在osd.1上</p><p>检查挂载点</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ceph]<span class="hljs-comment"># df -h|grep ceph-1</span><br>tmpfs                     16G   48K   16G   1% /var/lib/ceph/osd/ceph-1<br></code></pre></td></tr></table></figure><p>可以看到是挂载到tmpfs的，我们先停止掉osd.1</p><p>我们把osd的数据挂载起来</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ceph]<span class="hljs-comment"># ceph-objectstore-tool --op fuse --data-path /var/lib/ceph/osd/ceph-1 --mountpoint /osdmount/</span><br>mounting fuse at /osdmount/ ...<br></code></pre></td></tr></table></figure><p>开另外一个终端</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ~]<span class="hljs-comment"># df -h|grep osdmount</span><br>foo                      3.7T  3.7T  3.7T  51% /osdmount<br></code></pre></td></tr></table></figure><p>可以看到有了新的挂载点，我们看下里面的数据结构</p><p>我们随便选取一个对象</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ~]<span class="hljs-comment"># ll /osdmount/1.7_head/all/#1\:fc00bae4\:\:\:rbd_data.10166b8b4567.00000000000001fc\:head#/data</span><br>-rwx------ 1 root root 4194304 Jan  1  1970 /osdmount/1.7_head/all/<span class="hljs-comment">#1:fc00bae4:::rbd_data.10166b8b4567.00000000000001fc:head#/data</span><br>[root@lab101 ~]<span class="hljs-comment"># md5sum /osdmount/1.7_head/all/#1\:fc00bae4\:\:\:rbd_data.10166b8b4567.00000000000001fc\:head#/data</span><br>7def453c4a818e6cd542bfba4dea9943  /osdmount/1.7_head/all/<span class="hljs-comment">#1:fc00bae4:::rbd_data.10166b8b4567.00000000000001fc:head#/data</span><br></code></pre></td></tr></table></figure><p>这个对象的名称为：rbd_data.10166b8b4567.00000000000001fc</p><p>我们把数据弄到本地</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ~]<span class="hljs-comment"># cp /osdmount/1.7_head/all/#1\:fc00bae4\:\:\:rbd_data.10166b8b4567.00000000000001fc\:head#/data /tmp/rbd_data.10166b8b4567.00000000000001fc-inbluestore</span><br></code></pre></td></tr></table></figure><p>我们通过rados的接口查询获取这个对象</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ceph]<span class="hljs-comment"># rados -p rbd ls|grep 00000000000001fc</span><br>rbd_data.10166b8b4567.00000000000001fc<br>[root@lab101 ceph]<span class="hljs-comment"># rados -p rbd get rbd_data.10166b8b4567.00000000000001fc /tmp/rbd_data.10166b8b4567.00000000000001fc-radosget</span><br></code></pre></td></tr></table></figure><p>现在就有下面两个对象了</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ceph]<span class="hljs-comment"># ls /tmp/rbd_data.10166b8b4567.00000000000001fc-* -hl</span><br>-rwx------ 1 root root 4.0M Jul 26 10:17 /tmp/rbd_data.10166b8b4567.00000000000001fc-inbluestore<br>-rw-r--r-- 1 root root 4.0M Jul 26 10:19 /tmp/rbd_data.10166b8b4567.00000000000001fc-radosget<br></code></pre></td></tr></table></figure><p>这两个对象分别从rados获取的，也就是前端获取的，一个从底层磁盘提取的，也就是模拟的故障提取</p><p>我们来比较一下两个文件的md5值</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bashs">[root@lab101 ceph]# md5sum /tmp/rbd_data.10166b8b4567.00000000000001fc-* <br>7def453c4a818e6cd542bfba4dea9943  /tmp/rbd_data.10166b8b4567.00000000000001fc-inbluestore<br>7def453c4a818e6cd542bfba4dea9943  /tmp/rbd_data.10166b8b4567.00000000000001fc-radosget<br></code></pre></td></tr></table></figure><p>可以看到文件的内容一样的了</p><p>因此通过这个方法在底层获取对象是可以获取到正确的对象的</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>之前对bluestore的感觉一直不太好，是因为一旦出现故障，数据的提取相当困难，之前还有过bluestore内部数据库损坏无法启动osd的，如果用过filestore，应该都做过很多故障的修复，leveldb的数据库的损坏，从其他机器弄回来，bluestore这个在封装以后，一些操作变的困难，虽然也有提供一些repair的工具，但是有时还是无法生效，并不是每个人都能够去做代码级的修复</p><p>随着文件系统对外接口提供的越来越多的时候，修复的方式方法也会增多，相信这个也会越来越稳定的，我们需要做的就是，在任何故障下，做最大的可能去修复，才能更好的面对未来的故障</p><h2 id="变更记录"><a href="#变更记录" class="headerlink" title="变更记录"></a>变更记录</h2><table><thead><tr><th align="center">Why</th><th align="center">Who</th><th align="center">When</th></tr></thead><tbody><tr><td align="center">创建</td><td align="center">武汉-运维-磨渣</td><td align="center">2019-07-26</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>慢话crush-各种crush组合</title>
    <link href="/2019/03/22/%E6%85%A2%E8%AF%9Dcrush-%E5%90%84%E7%A7%8Dcrush%E7%BB%84%E5%90%88/"/>
    <url>/2019/03/22/%E6%85%A2%E8%AF%9Dcrush-%E5%90%84%E7%A7%8Dcrush%E7%BB%84%E5%90%88/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>ceph已经是一个比较成熟的开源的分布式存储了，从功能角度上来说，目前的功能基本能够覆盖大部分场景，而社区的工作基本上是在加入企业级的功能和易用性还有性能等方面在发力在，不管你是新手还是老手，都绕不开的一个问题就是crush，而crush是决定着数据的分布的，很多人并不理解为什么会有这个crush，这个算法到底是怎么去计算的，本篇是从更偏向用户层来对这个分布做一个解释，以及我们该怎么去动这个crush，本篇的内容不需要有代码开发能力，只需要稍加思考，都可以理解，剩下的就是你自己的选择了</p><!--break--><p>所有的存储都离不开分布的问题，存储是不是做副本，副本是如何分布的都是有自己的一套逻辑的</p><p>这里拿gluster做例子，gluster的数据分布就是通过子卷来控制的，副本几，那么数据的子卷就是由几个组成，一个文件是默认落到一个子卷的，如果没做分片的话，然后所有的盘符的子卷组成了一整个的卷，数据是散列到子卷里面去的，这种子卷的组合是固定的，组合是通过命令的先后顺序来控制的，也就是数据的分布组合是固定的</p><p>而ceph的灵活之处在于把这种子卷的逻辑向下走了一层，通过一个pg的概念，以目录为结构做出很多这样的组合来，gluster是以磁盘（或者理解为固定目录）为粒度做brick，而ceph则是以可以飘动的pg来做分布，而pg的分布则可以通过人为的控制来实现我们的需求，好了，准备进入本篇的内容</p><h2 id="探索过程"><a href="#探索过程" class="headerlink" title="探索过程"></a>探索过程</h2><p>本次测试由于需要测试的模式有几种，需要的节点比较多，因此需要多做几个节点的集群，我采用的是虚拟主机的模式，这个跟实际的环境在crush层面是基本一致的，这里是为了说明一下，即使你没有很多机器，一样能够去模拟很多机器的情况，本篇就是在一台机器下完成了整个模拟的</p><p>本次crush的说明都是以副本二进行说明，副本三的情况是同理类比即可</p><p>###模式一：默认模式<br>如果你是新手，或者接触的ceph不久的话，我们来做ceph学习或者验证的时候，应该是准备了几台主机，然后根据官网文档或者各种教程，部署起来ceph，部署完了以后可以通过命令ceph osd tree来看到一个最基本的分布，这个是我们比较常规的模式，在环境小的时候，基本采用的是这个模式，我们来看下这种默认模式的情况</p><p><img src="/images/blog/o_200901094017ceph%E7%9A%84pg%E5%8A%A8%E6%80%81%E5%9B%BE.gif" alt="host分组"></p><p>默认的情况是主机分组的，主机分组的意思是PG的主副本会分布到不同的主机上去，不会出现同一个PG的主副本在一台机器的情况，这样在出现主机级别的故障的时候，集群内还有副本在，这样数据就还是可以保证能够访问得到，主机级别的分组是默认的</p><p>先看下tree的内容：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ~]<span class="hljs-comment"># ceph osd tree</span><br>ID WEIGHT   TYPE NAME       UP/DOWN REWEIGHT PRIMARY-AFFINITY <br>-1 64.00000 root default                                      <br>-2  8.00000     host lab101                                   <br> 0  2.00000         osd.0        up  1.00000          1.00000 <br> 1  2.00000         osd.1        up  1.00000          1.00000 <br> 2  2.00000         osd.2        up  1.00000          1.00000 <br> 3  2.00000         osd.3        up  1.00000          1.00000 <br>-3  8.00000     host lab102                                   <br> 4  2.00000         osd.4        up  1.00000          1.00000 <br> 5  2.00000         osd.5        up  1.00000          1.00000 <br> 6  2.00000         osd.6        up  1.00000          1.00000 <br> 7  2.00000         osd.7        up  1.00000          1.00000 <br>-4  8.00000     host lab103                                   <br> 8  2.00000         osd.8        up  1.00000          1.00000 <br> 9  2.00000         osd.9        up  1.00000          1.00000 <br>10  2.00000         osd.10       up  1.00000          1.00000 <br>11  2.00000         osd.11       up  1.00000          1.00000 <br>-5  8.00000     host lab104                                   <br>12  2.00000         osd.12       up  1.00000          1.00000 <br>13  2.00000         osd.13       up  1.00000          1.00000 <br>14  2.00000         osd.14       up  1.00000          1.00000 <br>15  2.00000         osd.15       up  1.00000          1.00000 <br>-6  8.00000     host lab105                                   <br>16  2.00000         osd.16       up  1.00000          1.00000 <br>17  2.00000         osd.17       up  1.00000          1.00000 <br>18  2.00000         osd.18       up  1.00000          1.00000 <br>19  2.00000         osd.19       up  1.00000          1.00000 <br>-7  8.00000     host lab106                                   <br>20  2.00000         osd.20       up  1.00000          1.00000 <br>21  2.00000         osd.21       up  1.00000          1.00000 <br>22  2.00000         osd.22       up  1.00000          1.00000 <br>23  2.00000         osd.23       up  1.00000          1.00000 <br>-8  8.00000     host lab107                                   <br>24  2.00000         osd.24       up  1.00000          1.00000 <br>25  2.00000         osd.25       up  1.00000          1.00000 <br>26  2.00000         osd.26       up  1.00000          1.00000 <br>27  2.00000         osd.27       up  1.00000          1.00000 <br>-9  8.00000     host lab108                                   <br>28  2.00000         osd.28       up  1.00000          1.00000 <br>29  2.00000         osd.29       up  1.00000          1.00000 <br>30  2.00000         osd.30       up  1.00000          1.00000 <br>31  2.00000         osd.31       up  1.00000          1.00000 <br></code></pre></td></tr></table></figure><p>我们看下crush rule的内容：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 cephsankey]<span class="hljs-comment"># ceph osd crush rule dump replicated_ruleset</span><br>&#123;<br>    <span class="hljs-string">&quot;rule_id&quot;</span>: 0,<br>    <span class="hljs-string">&quot;rule_name&quot;</span>: <span class="hljs-string">&quot;replicated_ruleset&quot;</span>,<br>    <span class="hljs-string">&quot;ruleset&quot;</span>: 0,<br>    <span class="hljs-string">&quot;type&quot;</span>: 1,<br>    <span class="hljs-string">&quot;min_size&quot;</span>: 1,<br>    <span class="hljs-string">&quot;max_size&quot;</span>: 10,<br>    <span class="hljs-string">&quot;steps&quot;</span>: [<br>        &#123;<br>            <span class="hljs-string">&quot;op&quot;</span>: <span class="hljs-string">&quot;take&quot;</span>,<br>            <span class="hljs-string">&quot;item&quot;</span>: -1,<br>            <span class="hljs-string">&quot;item_name&quot;</span>: <span class="hljs-string">&quot;default&quot;</span><br>        &#125;,<br>        &#123;<br>            <span class="hljs-string">&quot;op&quot;</span>: <span class="hljs-string">&quot;chooseleaf_firstn&quot;</span>,<br>            <span class="hljs-string">&quot;num&quot;</span>: 0,<br>            <span class="hljs-string">&quot;type&quot;</span>: <span class="hljs-string">&quot;host&quot;</span><br>        &#125;,<br>        &#123;<br>            <span class="hljs-string">&quot;op&quot;</span>: <span class="hljs-string">&quot;emit&quot;</span><br>        &#125;<br>    ]<br>&#125;<br></code></pre></td></tr></table></figure><p>这个意思是从default为入口，把PG分布到不同的host里面去，默认就是这个，也就不涉及到编辑crush_map的问题，编辑的部分后面会讲</p><p>现在就有一个客户经常会问到的问题了，只有副本2，如果同时关闭两台机器，数据是不是就访问不到了，如果按照目前的默认自由分布来看，散列的分布情况下，必然会有数据的主副本PG是在你关闭的两主机上的，数量不会很多，但是肯定会有，这样基本就是百分百的概率出现有数据访问不到了</p><p>这个怎么解决，还是有办法改善这个情况的，这个就用到了分布式软件里面经常用到的概念rack分组，一般在设计分布式软件的时候，在测试高可用的时候，会去尝试测试关闭整个机架来测试服务的可用性，在ceph里面，如果你的机器并不是很多，只有几个物理机器，那么这里提到的机架是可以是逻辑上的，也就是从概率上面去减小必定损坏前提情况下的数据丢失可能性，这个就是下一个模式要讲到的，rack分组</p><h3 id="模式二：rack分组模式"><a href="#模式二：rack分组模式" class="headerlink" title="模式二：rack分组模式"></a>模式二：rack分组模式</h3><p>rack分组的模式就是在主机分组上面增加一层rack，然后让pg的主副本是分布到不同的rack下面的我们看下具体的情况</p><p>先编辑好tree的，如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ~]<span class="hljs-comment"># ceph osd tree</span><br>ID  WEIGHT   TYPE NAME           UP/DOWN REWEIGHT PRIMARY-AFFINITY <br> -1 64.00000 root default                                          <br>-10 16.00000     rack rack1                                        <br> -2  8.00000         host lab101                                   <br>  0  2.00000             osd.0        up  1.00000          1.00000 <br>  1  2.00000             osd.1        up  1.00000          1.00000 <br>  2  2.00000             osd.2        up  1.00000          1.00000 <br>  3  2.00000             osd.3        up  1.00000          1.00000 <br> -3  8.00000         host lab102                                   <br>  4  2.00000             osd.4        up  1.00000          1.00000 <br>  5  2.00000             osd.5        up  1.00000          1.00000 <br>  6  2.00000             osd.6        up  1.00000          1.00000 <br>  7  2.00000             osd.7        up  1.00000          1.00000 <br>-11 16.00000     rack rack2                                        <br> -4  8.00000         host lab103                                   <br>  8  2.00000             osd.8        up  1.00000          1.00000 <br>  9  2.00000             osd.9        up  1.00000          1.00000 <br> 10  2.00000             osd.10       up  1.00000          1.00000 <br> 11  2.00000             osd.11       up  1.00000          1.00000 <br> -5  8.00000         host lab104                                   <br> 12  2.00000             osd.12       up  1.00000          1.00000 <br> 13  2.00000             osd.13       up  1.00000          1.00000 <br> 14  2.00000             osd.14       up  1.00000          1.00000 <br> 15  2.00000             osd.15       up  1.00000          1.00000 <br>-12 16.00000     rack rack3                                        <br> -6  8.00000         host lab105                                   <br> 16  2.00000             osd.16       up  1.00000          1.00000 <br> 17  2.00000             osd.17       up  1.00000          1.00000 <br> 18  2.00000             osd.18       up  1.00000          1.00000 <br> 19  2.00000             osd.19       up  1.00000          1.00000 <br> -7  8.00000         host lab106                                   <br> 20  2.00000             osd.20       up  1.00000          1.00000 <br> 21  2.00000             osd.21       up  1.00000          1.00000 <br> 22  2.00000             osd.22       up  1.00000          1.00000 <br> 23  2.00000             osd.23       up  1.00000          1.00000 <br>-13 16.00000     rack rack4                                        <br> -8  8.00000         host lab107                                   <br> 24  2.00000             osd.24       up  1.00000          1.00000 <br> 25  2.00000             osd.25       up  1.00000          1.00000 <br> 26  2.00000             osd.26       up  1.00000          1.00000 <br> 27  2.00000             osd.27       up  1.00000          1.00000 <br> -9  8.00000         host lab108                                   <br> 28  2.00000             osd.28       up  1.00000          1.00000 <br> 29  2.00000             osd.29       up  1.00000          1.00000 <br> 30  2.00000             osd.30       up  1.00000          1.00000 <br> 31  2.00000             osd.31       up  1.00000          1.00000 <br></code></pre></td></tr></table></figure><p>创建crush rule</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs bash">创建一个规则<br>[root@lab101 ~]<span class="hljs-comment"># ceph  osd crush rule create-simple replicated_rack default rack  firstn</span><br>查看规则<br>[root@lab101 ~]<span class="hljs-comment"># ceph osd crush rule dump  replicated_rack</span><br>&#123;<br>    <span class="hljs-string">&quot;rule_id&quot;</span>: 1,<br>    <span class="hljs-string">&quot;rule_name&quot;</span>: <span class="hljs-string">&quot;replicated_rack&quot;</span>,<br>    <span class="hljs-string">&quot;ruleset&quot;</span>: 1,<br>    <span class="hljs-string">&quot;type&quot;</span>: 1,<br>    <span class="hljs-string">&quot;min_size&quot;</span>: 1,<br>    <span class="hljs-string">&quot;max_size&quot;</span>: 10,<br>    <span class="hljs-string">&quot;steps&quot;</span>: [<br>        &#123;<br>            <span class="hljs-string">&quot;op&quot;</span>: <span class="hljs-string">&quot;take&quot;</span>,<br>            <span class="hljs-string">&quot;item&quot;</span>: -1,<br>            <span class="hljs-string">&quot;item_name&quot;</span>: <span class="hljs-string">&quot;default&quot;</span><br>        &#125;,<br>        &#123;<br>            <span class="hljs-string">&quot;op&quot;</span>: <span class="hljs-string">&quot;chooseleaf_firstn&quot;</span>,<br>            <span class="hljs-string">&quot;num&quot;</span>: 0,<br>            <span class="hljs-string">&quot;type&quot;</span>: <span class="hljs-string">&quot;rack&quot;</span><br>        &#125;,<br>        &#123;<br>            <span class="hljs-string">&quot;op&quot;</span>: <span class="hljs-string">&quot;emit&quot;</span><br>        &#125;<br>    ]<br>&#125;<br>设置存储池到这个规则<br>[root@lab101 ~]<span class="hljs-comment"># ceph osd pool set rbd  crush_ruleset 1</span><br><span class="hljs-built_in">set</span> pool 1 crush_ruleset to 1<br></code></pre></td></tr></table></figure><p>我们看下pg的分布情况，每个pg的正副本是分布到不同rack的，没有出现主副本在同一个rack下面的两个主机里面的情况<br><img src="/images/blog/o_200901093739ceph%E7%9A%84pgrack4%E5%8A%A8%E6%80%81%E5%9B%BE.gif" alt="ceph的pgrack4动态图.gif-590.7kB"></p><p>可以看到我们这里是8台主机，分了4个rack，这个里面可以比之前主机分组好的情况是，如果down的是同一个rack的两台主机，那么这个情况的数据是不会掉的，因为数据在其他的rack里面肯定会有一份的，可以看到在当前的环境下面，有四种情况的主机组合在down的时候不会掉数据，我们是做了四个rack，这里我们来看下如果只做两个rack的情况，跟做四个rack的情况有什么区别呢？这个也是写本篇文章的时候才发现的一个结论</p><p>看下如果两个rack的情况下的tree</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ~]<span class="hljs-comment"># ceph osd tree</span><br>ID  WEIGHT   TYPE NAME           UP/DOWN REWEIGHT PRIMARY-AFFINITY <br> -1 64.00000 root default                                          <br>-10 32.00000     rack rack1                                        <br> -2  8.00000         host lab101                                   <br>  0  2.00000             osd.0        up  1.00000          1.00000 <br>  1  2.00000             osd.1        up  1.00000          1.00000 <br>  2  2.00000             osd.2        up  1.00000          1.00000 <br>  3  2.00000             osd.3        up  1.00000          1.00000 <br> -3  8.00000         host lab102                                   <br>  4  2.00000             osd.4        up  1.00000          1.00000 <br>  5  2.00000             osd.5        up  1.00000          1.00000 <br>  6  2.00000             osd.6        up  1.00000          1.00000 <br>  7  2.00000             osd.7        up  1.00000          1.00000 <br> -4  8.00000         host lab103                                   <br>  8  2.00000             osd.8        up  1.00000          1.00000 <br>  9  2.00000             osd.9        up  1.00000          1.00000 <br> 10  2.00000             osd.10       up  1.00000          1.00000 <br> 11  2.00000             osd.11       up  1.00000          1.00000 <br> -5  8.00000         host lab104                                   <br> 12  2.00000             osd.12       up  1.00000          1.00000 <br> 13  2.00000             osd.13       up  1.00000          1.00000 <br> 14  2.00000             osd.14       up  1.00000          1.00000 <br> 15  2.00000             osd.15       up  1.00000          1.00000 <br>-11 32.00000     rack rack2                                        <br> -6  8.00000         host lab105                                   <br> 16  2.00000             osd.16       up  1.00000          1.00000 <br> 17  2.00000             osd.17       up  1.00000          1.00000 <br> 18  2.00000             osd.18       up  1.00000          1.00000 <br> 19  2.00000             osd.19       up  1.00000          1.00000 <br> -7  8.00000         host lab106                                   <br> 20  2.00000             osd.20       up  1.00000          1.00000 <br> 21  2.00000             osd.21       up  1.00000          1.00000 <br> 22  2.00000             osd.22       up  1.00000          1.00000 <br> 23  2.00000             osd.23       up  1.00000          1.00000 <br> -8  8.00000         host lab107                                   <br> 24  2.00000             osd.24       up  1.00000          1.00000 <br> 25  2.00000             osd.25       up  1.00000          1.00000 <br> 26  2.00000             osd.26       up  1.00000          1.00000 <br> 27  2.00000             osd.27       up  1.00000          1.00000 <br> -9  8.00000         host lab108                                   <br> 28  2.00000             osd.28       up  1.00000          1.00000 <br> 29  2.00000             osd.29       up  1.00000          1.00000 <br> 30  2.00000             osd.30       up  1.00000          1.00000 <br> 31  2.00000             osd.31       up  1.00000          1.00000<br></code></pre></td></tr></table></figure><p>rule还是不变，不做调整<br><img src="/images/blog/o_200901093748rack%E5%88%86%E7%BB%84rack2p1.png" alt="rack分组rack2p1.png-289kB"><br>可以看到pg是分布到不同的rack的，这个时候down机的组合是有12个的，而上面分4个rack的时候，down机的组合是4个，组合越多，代表着，在一定出现问题的情况下，数据掉的概率越低，组合数如下图所示，可以看的比较清楚</p><p><img src="/images/blog/o_200901093754rack%E5%88%86%E7%BB%84.png" alt="rack分组.png-34.6kB"></p><p>所以可以得出在目前的场景下面的一个结论，副本2的时候，分两个rack能够在只down两台机器的情况下，得到最大的可用性</p><p>我们已经加入了rack的分组形式了，那么还有什么更复杂高级的控制么，那肯定是有的，这个搞法我也是在曾经一个项目上面想到的，并成功的应用到那个生产环境了，我给这个方式起了个通俗名称叫PG分流</p><h3 id="模式三：PG分流"><a href="#模式三：PG分流" class="headerlink" title="模式三：PG分流"></a>模式三：PG分流</h3><p>通过上面的各种图的走向，应该会有一种概念可以形成，就是可以把上面的理解为一个水源往下流去，中间的各种通道就是输水管道，我们做的工作的目的也都是在管道出现阻塞的时候，水源仍然能够往下走去，不会出现断流的情况，为了保证这个的可用性，就有了副本去提高可用性，通过rack，来减小破坏出现的时候的影响，那么我们还有一种做法就是，让整个PG流向一个区域，让另一个PG流向另一个区域，这样做的好处是减小copy set，关于减少copyset的好处，之前有篇文章已经介绍过了，本篇就不去分析了，直接操作</p><p>tree是不用动的，还是上面的rack模式下的tree，需要动的是crush rule的</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs bash">rule replicated_rack &#123;<br>    ruleset 1<br>    <span class="hljs-built_in">type</span> replicated<br>    min_size 1<br>    max_size 10<br>    step take default<br>    step choose firstn 1 <span class="hljs-built_in">type</span> rack<br>    step chooseleaf firstn 0 <span class="hljs-built_in">type</span> host<br>    step emit<br>&#125;<br></code></pre></td></tr></table></figure><p>看下pg分布的图<br><img src="/images/blog/o_200901093801ceph%E7%9A%84pg%E5%88%86%E6%B5%81.gif" alt="ceph的pg分流.gif-262.6kB"></p><p>可以看到PG的主副本是在一个rack里面的，这个时候每个rack实际含有的PG只有集群一半的PG，而且每个主机只跟自己rack内的主机做交互的，这样在海量的机器的时候，能够减少不少的通信交互，也少了IO碰撞的情况出现</p><p>这样做只是减少了主机的交互，并且可用性上面反而提高了，我们看下</p><p><img src="/images/blog/o_200901093808rack%E5%88%86%E6%B5%81.png" alt="rack分流.png-43.9kB"></p><p>可以看到现在有16组的组合可以保证down机不掉数据</p><p>我们看到在rack分流以后，实际上是按的主机分组的，这个时候我们可以进一步的提高这个可用性，在下面再做一层的控制，做一个chassis的分组，这个也是ceph里面含有chassis分组标签，正好也是在rack之下<br>我们先编辑好map如下</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ~]<span class="hljs-comment"># ceph osd tree</span><br>ID  WEIGHT   TYPE NAME                UP/DOWN REWEIGHT PRIMARY-AFFINITY <br> -1 64.00000 root default                                               <br>-10 32.00000     rack rack1                                             <br>-12 16.00000         chassis chassis1                                   <br> -2  8.00000             host lab101                                    <br>  0  2.00000                 osd.0         up  1.00000          1.00000 <br>  1  2.00000                 osd.1         up  1.00000          1.00000 <br>  2  2.00000                 osd.2         up  1.00000          1.00000 <br>  3  2.00000                 osd.3         up  1.00000          1.00000 <br> -3  8.00000             host lab102                                    <br>  4  2.00000                 osd.4         up  1.00000          1.00000 <br>  5  2.00000                 osd.5         up  1.00000          1.00000 <br>  6  2.00000                 osd.6         up  1.00000          1.00000 <br>  7  2.00000                 osd.7         up  1.00000          1.00000 <br>-13 16.00000         chassis chassis2                                   <br> -4  8.00000             host lab103                                    <br>  8  2.00000                 osd.8         up  1.00000          1.00000 <br>  9  2.00000                 osd.9         up  1.00000          1.00000 <br> 10  2.00000                 osd.10        up  1.00000          1.00000 <br> 11  2.00000                 osd.11        up  1.00000          1.00000 <br> -5  8.00000             host lab104                                    <br> 12  2.00000                 osd.12        up  1.00000          1.00000 <br> 13  2.00000                 osd.13        up  1.00000          1.00000 <br> 14  2.00000                 osd.14        up  1.00000          1.00000 <br> 15  2.00000                 osd.15        up  1.00000          1.00000 <br>-11 32.00000     rack rack2                                             <br>-14 16.00000         chassis chassis3                                   <br> -6  8.00000             host lab105                                    <br> 16  2.00000                 osd.16        up  1.00000          1.00000 <br> 17  2.00000                 osd.17        up  1.00000          1.00000 <br> 18  2.00000                 osd.18        up  1.00000          1.00000 <br> 19  2.00000                 osd.19        up  1.00000          1.00000 <br> -7  8.00000             host lab106                                    <br> 20  2.00000                 osd.20        up  1.00000          1.00000 <br> 21  2.00000                 osd.21        up  1.00000          1.00000 <br> 22  2.00000                 osd.22        up  1.00000          1.00000 <br> 23  2.00000                 osd.23        up  1.00000          1.00000 <br>-15 16.00000         chassis chassis4                                   <br> -8  8.00000             host lab107                                    <br> 24  2.00000                 osd.24        up  1.00000          1.00000 <br> 25  2.00000                 osd.25        up  1.00000          1.00000 <br> 26  2.00000                 osd.26        up  1.00000          1.00000 <br> 27  2.00000                 osd.27        up  1.00000          1.00000 <br> -9  8.00000             host lab108                                    <br> 28  2.00000                 osd.28        up  1.00000          1.00000 <br> 29  2.00000                 osd.29        up  1.00000          1.00000 <br> 30  2.00000                 osd.30        up  1.00000          1.00000 <br> 31  2.00000                 osd.31        up  1.00000          1.00000 <br></code></pre></td></tr></table></figure><p>编辑crush_rule</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs bash">rule replicated_rack &#123;<br>    ruleset 1<br>    <span class="hljs-built_in">type</span> replicated<br>    min_size 1<br>    max_size 10<br>    step take default<br>    step choose firstn 1 <span class="hljs-built_in">type</span> rack<br>    step chooseleaf firstn 0 <span class="hljs-built_in">type</span> chassis<br>    step emit<br>&#125;<br></code></pre></td></tr></table></figure><p><img src="/images/blog/o_200901093814ceph%E7%9A%84chassis.gif" alt="ceph的chassis.gif-415.2kB"></p><p>可以看到pg的主副本是在单个rack里面的，而主本副本则是在两个chassis里面的，我们再来看下可用性<br><img src="/images/blog/o_200901093821%E5%85%B3chassis.png" alt="关chassis.png-48.1kB"><br>增加了chassis后，我们关闭chassis也不会出现数据丢失，这样又增加了四个组合，到现在达到了20种组合</p><p>可以看到在并没有改变硬件的情况下，通过逻辑的控制，增加了20种down机模式下不会掉数据的情况，虽然我们无法避免所有的情况的掉数据，但是通过一些改变增加了安全性，这个也是可以的</p><p>有的时候我们为了性能会上ssd，但是全部上ssd某些情况下，又觉得划不来，而读取的时候，数据是在主本上的，就跟副本没什么关系了，然后就会产生一种特殊的省成本的需求了，我想主本存在ssd上面，副本存在sata上面，容量不用担心，以ssd的为准，省了一个ssd的钱，如果是读的场景，这个更好了，那么作为灵活的crush，当然是可以支撑这种比较特殊的需求了，我们看下下面的模式</p><p>###模式四：sata-ssd组合<br>这个模式下的组合是主本存储在ssd上面，副本存储在sata上面，每台机器有sata也有ssd，我们看下我的这个环境的，假设8台机器的，每台4个osd里面较小id的是ssd的磁盘，较大的是id的是sata盘的</p><p>先编辑map，如下</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ~]<span class="hljs-comment"># ceph osd tree</span><br>ID  WEIGHT   TYPE NAME                    UP/DOWN REWEIGHT PRIMARY-AFFINITY <br> -1 64.00000 root default                                                   <br>-10 32.00000     rack rack1                                                 <br>-12  8.00000         chassis chassis1                                       <br>-16  4.00000             host lab101-ssd                                    <br>  0  2.00000                 osd.0             up  1.00000          1.00000 <br>  1  2.00000                 osd.1             up  1.00000          1.00000 <br>-25  4.00000             host lab102-sata                                   <br>  6  2.00000                 osd.6             up  1.00000                0 <br>  7  2.00000                 osd.7             up  1.00000                0 <br>-13  8.00000         chassis chassis2                                       <br>-17  4.00000             host lab102-ssd                                    <br>  4  2.00000                 osd.4             up  1.00000          1.00000 <br>  5  2.00000                 osd.5             up  1.00000          1.00000 <br>-26  4.00000             host lab103-sata                                   <br> 10  2.00000                 osd.10            up  1.00000                0 <br> 11  2.00000                 osd.11            up  1.00000                0 <br>-14  8.00000         chassis chassis3                                       <br>-18  4.00000             host lab103-ssd                                    <br>  8  2.00000                 osd.8             up  1.00000          1.00000 <br>  9  2.00000                 osd.9             up  1.00000          1.00000 <br>-27  4.00000             host lab104-sata                                   <br> 14  2.00000                 osd.14            up  1.00000                0 <br> 15  2.00000                 osd.15            up  1.00000                0 <br>-15  8.00000         chassis chassis4                                       <br>-19  4.00000             host lab104-ssd                                    <br> 12  2.00000                 osd.12            up  1.00000          1.00000 <br> 13  2.00000                 osd.13            up  1.00000          1.00000 <br>-28  4.00000             host lab105-sata                                   <br> 18  2.00000                 osd.18            up  1.00000                0 <br> 19  2.00000                 osd.19            up  1.00000                0 <br>-11 32.00000     rack rack2                                                 <br>-32  8.00000         chassis chassis5                                       <br>-20  4.00000             host lab105-ssd                                    <br> 16  2.00000                 osd.16            up  1.00000          1.00000 <br> 17  2.00000                 osd.17            up  1.00000          1.00000 <br>-29  4.00000             host lab106-sata                                   <br> 22  2.00000                 osd.22            up  1.00000                0 <br> 23  2.00000                 osd.23            up  1.00000                0 <br>-33  8.00000         chassis chassis6                                       <br>-21  4.00000             host lab106-ssd                                    <br> 20  2.00000                 osd.20            up  1.00000          1.00000 <br> 21  2.00000                 osd.21            up  1.00000          1.00000 <br>-30  4.00000             host lab107-sata                                   <br> 26  2.00000                 osd.26            up  1.00000                0 <br> 27  2.00000                 osd.27            up  1.00000                0 <br>-34  8.00000         chassis chassis7                                       <br>-22  4.00000             host lab107-ssd                                    <br> 24  2.00000                 osd.24            up  1.00000          1.00000 <br> 25  2.00000                 osd.25            up  1.00000          1.00000 <br>-31  4.00000             host lab108-sata                                   <br> 30  2.00000                 osd.30            up  1.00000                0 <br> 31  2.00000                 osd.31            up  1.00000                0 <br>-35  8.00000         chassis chassis8                                       <br>-23  4.00000             host lab108-ssd                                    <br> 28  2.00000                 osd.28            up  1.00000          1.00000 <br> 29  2.00000                 osd.29            up  1.00000          1.00000 <br>-24  4.00000             host lab101-sata                                   <br>  2  2.00000                 osd.2             up  1.00000                0 <br>  3  2.00000                 osd.3             up  1.00000                0 <br></code></pre></td></tr></table></figure><p>crushmap编辑到如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs bash">rule replicated_rack &#123;<br>    ruleset 1<br>    <span class="hljs-built_in">type</span> replicated<br>    min_size 1<br>    max_size 10<br>    step take default<br>    step choose firstn 1 <span class="hljs-built_in">type</span> rack<br>    step choose firstn 1 <span class="hljs-built_in">type</span> chassis<br>    step chooseleaf firstn 0 <span class="hljs-built_in">type</span> host<br>    step emit<br>&#125;<br></code></pre></td></tr></table></figure><p>上面的做组合的时候是交叉固定组合的，1机器ssd跟2机器的sata，2机器ssd跟3机器的sata，然后通过ceph osd primary-affinity来控制主本在ssd上面，我们看下我们的动图<br><img src="/images/blog/o_200901093827ssd%E6%B7%B7sata.gif" alt="ssd混sata.gif-651.9kB"></p><p>可以看到数据主本在ssd副本在sata了，但是这里可能会提出一个问题，就是这个组合数目是不是太少了，每个ssd的虚拟主机只跟一个进行组合，我想增加点多样性可以么，这里可以这么操作，rack分流的我们继续保持不变，这里只用增加每个rack里面的组合就行了，我们编辑tree如下</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ~]<span class="hljs-comment"># ceph osd tree</span><br>ID  WEIGHT    TYPE NAME                            UP/DOWN REWEIGHT PRIMARY-AFFINITY <br> -1 192.00000 root default                                                           <br>-10  96.00000     rack rack1                                                         <br> -2   8.00000         chassis chassis-lab101-ssd-1                                   <br>-16   4.00000             host lab101-ssd                                            <br>  0   2.00000                 osd.0                     up  1.00000          1.00000 <br>  1   2.00000                 osd.1                     up  1.00000          1.00000 <br>-25   4.00000             host lab102-sata                                           <br>  6   2.00000                 osd.6                     up  1.00000                0 <br>  7   2.00000                 osd.7                     up  1.00000                0 <br> -3   8.00000         chassis chassis-lab101-ssd-2                                   <br>-16   4.00000             host lab101-ssd                                            <br>  0   2.00000                 osd.0                     up  1.00000          1.00000 <br>  1   2.00000                 osd.1                     up  1.00000          1.00000 <br>-26   4.00000             host lab103-sata                                           <br> 10   2.00000                 osd.10                    up  1.00000                0 <br> 11   2.00000                 osd.11                    up  1.00000                0 <br> -4   8.00000         chassis chassis-lab101-ssd-3                                   <br>-16   4.00000             host lab101-ssd                                            <br>  0   2.00000                 osd.0                     up  1.00000          1.00000 <br>  1   2.00000                 osd.1                     up  1.00000          1.00000 <br>-27   4.00000             host lab104-sata                                           <br> 14   2.00000                 osd.14                    up  1.00000                0 <br> 15   2.00000                 osd.15                    up  1.00000                0 <br> -5   8.00000         chassis chassis-lab102-ssd-1                                   <br>-17   4.00000             host lab102-ssd                                            <br>  4   2.00000                 osd.4                     up  1.00000          1.00000 <br>  5   2.00000                 osd.5                     up  1.00000          1.00000 <br>-26   4.00000             host lab103-sata                                           <br> 10   2.00000                 osd.10                    up  1.00000                0 <br> 11   2.00000                 osd.11                    up  1.00000                0 <br> -8   8.00000         chassis chassis-lab103-ssd-1                                   <br>-18   4.00000             host lab103-ssd                                            <br>  8   2.00000                 osd.8                     up  1.00000          1.00000 <br>  9   2.00000                 osd.9                     up  1.00000          1.00000 <br>-27   4.00000             host lab104-sata                                           <br> 14   2.00000                 osd.14                    up  1.00000                0 <br> 15   2.00000                 osd.15                    up  1.00000                0 <br> -9   8.00000         chassis chassis-lab104-ssd-1                                   <br>-19   4.00000             host lab104-ssd                                            <br> 12   2.00000                 osd.12                    up  1.00000          1.00000 <br> 13   2.00000                 osd.13                    up  1.00000          1.00000 <br>-24   4.00000             host lab101-sata                                           <br>  2   2.00000                 osd.2                     up  1.00000                0 <br>  3   2.00000                 osd.3                     up  1.00000                0 <br> -6   8.00000         chassis chassis-lab102-ssd-2                                   <br>-17   4.00000             host lab102-ssd                                            <br>  4   2.00000                 osd.4                     up  1.00000          1.00000 <br>  5   2.00000                 osd.5                     up  1.00000          1.00000 <br>-27   4.00000             host lab104-sata                                           <br> 14   2.00000                 osd.14                    up  1.00000                0 <br> 15   2.00000                 osd.15                    up  1.00000                0 <br>-40   8.00000         chassis chassis-lab103-ssd-2                                   <br>-18   4.00000             host lab103-ssd                                            <br>  8   2.00000                 osd.8                     up  1.00000          1.00000 <br>  9   2.00000                 osd.9                     up  1.00000          1.00000 <br>-24   4.00000             host lab101-sata                                           <br>  2   2.00000                 osd.2                     up  1.00000                0 <br>  3   2.00000                 osd.3                     up  1.00000                0 <br>-41   8.00000         chassis chassis-lab104-ssd-2                                   <br>-19   4.00000             host lab104-ssd                                            <br> 12   2.00000                 osd.12                    up  1.00000          1.00000 <br> 13   2.00000                 osd.13                    up  1.00000          1.00000 <br>-25   4.00000             host lab102-sata                                           <br>  6   2.00000                 osd.6                     up  1.00000                0 <br>  7   2.00000                 osd.7                     up  1.00000                0 <br> -7   8.00000         chassis chassis-lab102-ssd-3                                   <br>-17   4.00000             host lab102-ssd                                            <br>  4   2.00000                 osd.4                     up  1.00000          1.00000 <br>  5   2.00000                 osd.5                     up  1.00000          1.00000 <br>-24   4.00000             host lab101-sata                                           <br>  2   2.00000                 osd.2                     up  1.00000                0 <br>  3   2.00000                 osd.3                     up  1.00000                0 <br>-46   8.00000         chassis chassis-lab103-ssd-3                                   <br>-18   4.00000             host lab103-ssd                                            <br>  8   2.00000                 osd.8                     up  1.00000          1.00000 <br>  9   2.00000                 osd.9                     up  1.00000          1.00000 <br>-25   4.00000             host lab102-sata                                           <br>  6   2.00000                 osd.6                     up  1.00000                0 <br>  7   2.00000                 osd.7                     up  1.00000                0 <br>-47   8.00000         chassis chassis-lab104-ssd-3                                   <br>-19   4.00000             host lab104-ssd                                            <br> 12   2.00000                 osd.12                    up  1.00000          1.00000 <br> 13   2.00000                 osd.13                    up  1.00000          1.00000 <br>-26   4.00000             host lab103-sata                                           <br> 10   2.00000                 osd.10                    up  1.00000                0 <br> 11   2.00000                 osd.11                    up  1.00000                0 <br>-11  96.00000     rack rack2                                                         <br>-36   8.00000         chassis chassis-lab105-ssd-1                                   <br>-20   4.00000             host lab105-ssd                                            <br> 16   2.00000                 osd.16                    up  1.00000          1.00000 <br> 17   2.00000                 osd.17                    up  1.00000          1.00000 <br>-29   4.00000             host lab106-sata                                           <br> 22   2.00000                 osd.22                    up  1.00000                0 <br> 23   2.00000                 osd.23                    up  1.00000                0 <br>-37   8.00000         chassis chassis-lab106-ssd-1                                   <br>-21   4.00000             host lab106-ssd                                            <br> 20   2.00000                 osd.20                    up  1.00000          1.00000 <br> 21   2.00000                 osd.21                    up  1.00000          1.00000 <br>-30   4.00000             host lab107-sata                                           <br> 26   2.00000                 osd.26                    up  1.00000                0 <br> 27   2.00000                 osd.27                    up  1.00000                0 <br>-38   8.00000         chassis chassis-lab107-ssd-1                                   <br>-22   4.00000             host lab107-ssd                                            <br> 24   2.00000                 osd.24                    up  1.00000          1.00000 <br> 25   2.00000                 osd.25                    up  1.00000          1.00000 <br>-31   4.00000             host lab108-sata                                           <br> 30   2.00000                 osd.30                    up  1.00000                0 <br> 31   2.00000                 osd.31                    up  1.00000                0 <br>-39   8.00000         chassis chassis-lab108-ssd-1                                   <br>-23   4.00000             host lab108-ssd                                            <br> 28   2.00000                 osd.28                    up  1.00000          1.00000 <br> 29   2.00000                 osd.29                    up  1.00000          1.00000 <br>-28   4.00000             host lab105-sata                                           <br> 18   2.00000                 osd.18                    up  1.00000                0 <br> 19   2.00000                 osd.19                    up  1.00000                0 <br>-42   8.00000         chassis chassis-lab105-ssd-2                                   <br>-20   4.00000             host lab105-ssd                                            <br> 16   2.00000                 osd.16                    up  1.00000          1.00000 <br> 17   2.00000                 osd.17                    up  1.00000          1.00000 <br>-30   4.00000             host lab107-sata                                           <br> 26   2.00000                 osd.26                    up  1.00000                0 <br> 27   2.00000                 osd.27                    up  1.00000                0 <br>-43   8.00000         chassis chassis-lab106-ssd-2                                   <br>-21   4.00000             host lab106-ssd                                            <br> 20   2.00000                 osd.20                    up  1.00000          1.00000 <br> 21   2.00000                 osd.21                    up  1.00000          1.00000 <br>-31   4.00000             host lab108-sata                                           <br> 30   2.00000                 osd.30                    up  1.00000                0 <br> 31   2.00000                 osd.31                    up  1.00000                0 <br>-44   8.00000         chassis chassis-lab107-ssd-2                                   <br>-22   4.00000             host lab107-ssd                                            <br> 24   2.00000                 osd.24                    up  1.00000          1.00000 <br> 25   2.00000                 osd.25                    up  1.00000          1.00000 <br>-28   4.00000             host lab105-sata                                           <br> 18   2.00000                 osd.18                    up  1.00000                0 <br> 19   2.00000                 osd.19                    up  1.00000                0 <br>-45   8.00000         chassis chassis-lab108-ssd-2                                   <br>-23   4.00000             host lab108-ssd                                            <br> 28   2.00000                 osd.28                    up  1.00000          1.00000 <br> 29   2.00000                 osd.29                    up  1.00000          1.00000 <br>-29   4.00000             host lab106-sata                                           <br> 22   2.00000                 osd.22                    up  1.00000                0 <br> 23   2.00000                 osd.23                    up  1.00000                0 <br>-48   8.00000         chassis chassis-lab105-ssd-3                                   <br>-20   4.00000             host lab105-ssd                                            <br> 16   2.00000                 osd.16                    up  1.00000          1.00000 <br> 17   2.00000                 osd.17                    up  1.00000          1.00000 <br>-31   4.00000             host lab108-sata                                           <br> 30   2.00000                 osd.30                    up  1.00000                0 <br> 31   2.00000                 osd.31                    up  1.00000                0 <br>-49   8.00000         chassis chassis-lab106-ssd-3                                   <br>-21   4.00000             host lab106-ssd                                            <br> 20   2.00000                 osd.20                    up  1.00000          1.00000 <br> 21   2.00000                 osd.21                    up  1.00000          1.00000 <br>-28   4.00000             host lab105-sata                                           <br> 18   2.00000                 osd.18                    up  1.00000                0 <br> 19   2.00000                 osd.19                    up  1.00000                0 <br>-50   8.00000         chassis chassis-lab107-ssd-3                                   <br>-22   4.00000             host lab107-ssd                                            <br> 24   2.00000                 osd.24                    up  1.00000          1.00000 <br> 25   2.00000                 osd.25                    up  1.00000          1.00000 <br>-29   4.00000             host lab106-sata                                           <br> 22   2.00000                 osd.22                    up  1.00000                0 <br> 23   2.00000                 osd.23                    up  1.00000                0 <br>-51   8.00000         chassis chassis-lab108-ssd-3                                   <br>-23   4.00000             host lab108-ssd                                            <br> 28   2.00000                 osd.28                    up  1.00000          1.00000 <br> 29   2.00000                 osd.29                    up  1.00000          1.00000 <br>-30   4.00000             host lab107-sata                                           <br> 26   2.00000                 osd.26                    up  1.00000                0 <br> 27   2.00000                 osd.27                    up  1.00000                0 <br></code></pre></td></tr></table></figure><p>tree的内容比较多，还是用个图来说明</p><p><img src="/images/blog/o_200901093834ssdsata%E6%B7%B7.png" alt="ssdsata混.png-72.6kB"></p><p>pg的分布是这样的<br><img src="/images/blog/o_200901093841sata%E6%B7%B7ssd%E5%A4%9A%E4%B8%AA.gif" alt="sata混ssd多个.gif-1278kB"><br>可以看到pg是分布在一个ssd上一个sata上，并且分布上面，比上面的那种交叉更多了的</p><h3 id="模式五：写本地"><a href="#模式五：写本地" class="headerlink" title="模式五：写本地"></a>模式五：写本地</h3><p>上面的控制是控制写入到ssd里面，在选择ssd的时候，就让他随机的进行选择的，那么有的时候，我们的环境不大，我只需要尽量写本机，读本地，同时能够有一个副本能够去保证我们的可用性的时候，那么就可以控制一个写入在本地的情况，在一些存储软件里面都有一个设计是local read的设计，如果需要的对象在本地，那么就直接通过程序去从本地的host 本地进行读取即可，这个在ceph里面某些接口里面已经做了，之前有篇文章也写到过，本篇跟那篇是有区别的，这里的设计是通过控制把数据完全写到本地机器去的</p><p>我们来看看怎么弄</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ~]<span class="hljs-comment"># ceph osd tree</span><br>ID  WEIGHT    TYPE NAME                      UP/DOWN REWEIGHT PRIMARY-AFFINITY <br> -1 112.00000 root default                                                     <br>-12  16.00000     host lab101                                                  <br>  0   2.00000         osd.0                       up  1.00000          1.00000 <br>  1   2.00000         osd.1                       up  1.00000          1.00000 <br>  2   2.00000         osd.2                       up  1.00000          1.00000 <br>  3   2.00000         osd.3                       up  1.00000          1.00000 <br>  4   2.00000         osd.4                       up  1.00000          1.00000 <br>  5   2.00000         osd.5                       up  1.00000          1.00000 <br>  6   2.00000         osd.6                       up  1.00000          1.00000 <br>  7   2.00000         osd.7                       up  1.00000          1.00000 <br>-13  16.00000     host lab102                                                  <br>  8   2.00000         osd.8                       up  1.00000          1.00000 <br>  9   2.00000         osd.9                       up  1.00000          1.00000 <br> 10   2.00000         osd.10                      up  1.00000          1.00000 <br> 11   2.00000         osd.11                      up  1.00000          1.00000 <br> 12   2.00000         osd.12                      up  1.00000          1.00000 <br> 13   2.00000         osd.13                      up  1.00000          1.00000 <br> 14   2.00000         osd.14                      up  1.00000          1.00000 <br> 15   2.00000         osd.15                      up  1.00000          1.00000 <br>-14  16.00000     host lab103                                                  <br> 16   2.00000         osd.16                      up  1.00000          1.00000 <br> 17   2.00000         osd.17                      up  1.00000          1.00000 <br> 18   2.00000         osd.18                      up  1.00000          1.00000 <br> 19   2.00000         osd.19                      up  1.00000          1.00000 <br> 20   2.00000         osd.20                      up  1.00000          1.00000 <br> 21   2.00000         osd.21                      up  1.00000          1.00000 <br> 22   2.00000         osd.22                      up  1.00000          1.00000 <br> 23   2.00000         osd.23                      up  1.00000          1.00000 <br>-15  16.00000     host lab104                                                  <br> 24   2.00000         osd.24                      up  1.00000          1.00000 <br> 25   2.00000         osd.25                      up  1.00000          1.00000 <br> 26   2.00000         osd.26                      up  1.00000          1.00000 <br> 27   2.00000         osd.27                      up  1.00000          1.00000 <br> 28   2.00000         osd.28                      up  1.00000          1.00000 <br> 29   2.00000         osd.29                      up  1.00000          1.00000 <br> 30   2.00000         osd.30                      up  1.00000          1.00000 <br> 31   2.00000         osd.31                      up  1.00000          1.00000 <br> -2  48.00000     chassis chassis-for-lab101                                   <br>-13  16.00000         host lab102                                              <br>  8   2.00000             osd.8                   up  1.00000          1.00000 <br>  9   2.00000             osd.9                   up  1.00000          1.00000 <br> 10   2.00000             osd.10                  up  1.00000          1.00000 <br> 11   2.00000             osd.11                  up  1.00000          1.00000 <br> 12   2.00000             osd.12                  up  1.00000          1.00000 <br> 13   2.00000             osd.13                  up  1.00000          1.00000 <br> 14   2.00000             osd.14                  up  1.00000          1.00000 <br> 15   2.00000             osd.15                  up  1.00000          1.00000 <br>-14  16.00000         host lab103                                              <br> 16   2.00000             osd.16                  up  1.00000          1.00000 <br> 17   2.00000             osd.17                  up  1.00000          1.00000 <br> 18   2.00000             osd.18                  up  1.00000          1.00000 <br> 19   2.00000             osd.19                  up  1.00000          1.00000 <br> 20   2.00000             osd.20                  up  1.00000          1.00000 <br> 21   2.00000             osd.21                  up  1.00000          1.00000 <br> 22   2.00000             osd.22                  up  1.00000          1.00000 <br> 23   2.00000             osd.23                  up  1.00000          1.00000 <br>-15  16.00000         host lab104                                              <br> 24   2.00000             osd.24                  up  1.00000          1.00000 <br> 25   2.00000             osd.25                  up  1.00000          1.00000 <br> 26   2.00000             osd.26                  up  1.00000          1.00000 <br> 27   2.00000             osd.27                  up  1.00000          1.00000 <br> 28   2.00000             osd.28                  up  1.00000          1.00000 <br> 29   2.00000             osd.29                  up  1.00000          1.00000 <br> 30   2.00000             osd.30                  up  1.00000          1.00000 <br> 31   2.00000             osd.31                  up  1.00000          1.00000<br></code></pre></td></tr></table></figure><p>tree是这样的，主机的是默认的，在里面增加一个chassis来装除了lab101以外的机器，这样等下在选择的时候，先从lab101里面抽一个osd出来，然后再从剩余的chassis里面抽个主机里面的osd出来就行了</p><p>看下做的rule</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs bash">&#125;<br>rule for-lab101 &#123;<br>    ruleset 2<br>    <span class="hljs-built_in">type</span> replicated<br>    min_size 1<br>    max_size 10<br>    step take lab101<br>    step chooseleaf firstn 1 <span class="hljs-built_in">type</span> osd<br>    step emit<br>    step take chassis-for-lab101<br>    step chooseleaf firstn 1 <span class="hljs-built_in">type</span> host<br>    step emit<br>&#125;<br></code></pre></td></tr></table></figure><p>检查下pg的状态</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ~]<span class="hljs-comment"># ceph pg dump pgs|awk &#x27;&#123;print $1,$15&#125;&#x27;|head -n 15</span><br>dumped pgs <span class="hljs-keyword">in</span> format plain<br>pg_stat up_primary<br>1.ff [5,21]<br>1.fe [6,14]<br>1.fd [5,18]<br>1.<span class="hljs-built_in">fc</span> [6,28]<br>1.fb [7,30]<br>1.fa [0,31]<br>1.f9 [7,21]<br>1.f8 [7,14]<br>1.f7 [6,13]<br>1.f6 [5,16]<br>1.f5 [7,10]<br>1.f4 [3,31]<br>1.f3 [4,29]<br>1.f2 [4,15]<br></code></pre></td></tr></table></figure><p>可以看到主本都在第一个主机的osd上面，如果每个主机都想这样，那就做不同的rule，做不同的存储池就可以了</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>ceph里面的crush比较复杂的一些玩法在本篇中基本都有提及，一般来说，只要你想得到的分布，ceph都是能够去做到的，这个得益于灵活的crush控制，这个也是ceph优于一些其他分布式软件的一点，软件的控制很多，关键是找到适合自己用的，复杂度高的crush意味着后期的维护的时候操作上需要更细粒度的控制，而把这些做到ui层面去，又增加了软件的难度，所以什么是交给客户的什么是专业技术去控制的，这个需要划分好</p><p>自己写的工具自己多用用才能更好的发现问题，比如上面的crush分布的，本篇文章用的过程中就发现了osd的排序问题，以及加入了可以指定随机的pg个数的功能</p><h2 id="变更记录"><a href="#变更记录" class="headerlink" title="变更记录"></a>变更记录</h2><table><thead><tr><th align="center">Why</th><th align="center">Who</th><th align="center">When</th></tr></thead><tbody><tr><td align="center">创建</td><td align="center">武汉-运维-磨渣</td><td align="center">2019-3-22</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>ceph的pg的分布的快速查看</title>
    <link href="/2019/03/08/ceph%E7%9A%84pg%E7%9A%84%E5%88%86%E5%B8%83%E7%9A%84%E5%BF%AB%E9%80%9F%E6%9F%A5%E7%9C%8B/"/>
    <url>/2019/03/08/ceph%E7%9A%84pg%E7%9A%84%E5%88%86%E5%B8%83%E7%9A%84%E5%BF%AB%E9%80%9F%E6%9F%A5%E7%9C%8B/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>本篇的内容实际上是另外一篇文章的字篇章，在另外一篇文章当中，将会对crush的分布的调整的做一次总结，用比较简单的方式来展示各种crush的区别</p><p>在做这个工作过程中，为了更好的能展示出效果，就有了下面的这个小工具的出现</p><h2 id="工具来源"><a href="#工具来源" class="headerlink" title="工具来源"></a>工具来源</h2><p>假如我现在想查看一个存储池内的pg的分布，那么我们需要下面的几个步骤</p><h3 id="1、随机获取一个pg的id"><a href="#1、随机获取一个pg的id" class="headerlink" title="1、随机获取一个pg的id"></a>1、随机获取一个pg的id</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ~]<span class="hljs-comment"># ceph pg dump|grep ^0</span><br></code></pre></td></tr></table></figure><p>后面的是存储池的编号<br>从输出中拿到0.196开头的这个pg</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">0.19600000000active+clean2019-03-07 18:03:28.2627470<span class="hljs-string">&#x27;0259:24[2,30,21]2[2,30,21]0&#x27;</span>02019-03-06 17:30:29.5985340<span class="hljs-string">&#x27;02019-03-06 17:30:29.598534</span><br></code></pre></td></tr></table></figure><p>得到三个osd的id的值，2,30,21</p><h3 id="2、获取ceph-osd-tree拿到osd的分布"><a href="#2、获取ceph-osd-tree拿到osd的分布" class="headerlink" title="2、获取ceph osd tree拿到osd的分布"></a>2、获取ceph osd tree拿到osd的分布</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ~]<span class="hljs-comment"># ceph osd tree</span><br></code></pre></td></tr></table></figure><p><img src="/images/blog/o_200901093450pg-image.png" alt="osd tree"></p><p>的到一个大概这样的图，拿到</p><blockquote><p>osd.2在host lab101上在rack1中，在default下<br>osd.30在host lab108上在rack4中，在default下<br>osd.21在host lab106上在rack3中，在default下</p></blockquote><p>可以看到这个pg是分布在三个rack下的，如果我们需要去看其他的pg的，那么就需要继续重复上面的动作了，这个没什么大的问题，但是为了更好更快的去获取分布，看下我做的小工具的效果</p><h2 id="查询分布的小工具的效果"><a href="#查询分布的小工具的效果" class="headerlink" title="查询分布的小工具的效果"></a>查询分布的小工具的效果</h2><h3 id="横着看的"><a href="#横着看的" class="headerlink" title="横着看的"></a>横着看的</h3><p><img src="/images/blog/o_200901093341pg-image1.png" alt="image.png-365.2kB"></p><h3 id="竖着看的"><a href="#竖着看的" class="headerlink" title="竖着看的"></a>竖着看的</h3><p><img src="/images/blog/o_200901093347pg-image2.png" alt="image.png-246.7kB"></p><h3 id="再来一个"><a href="#再来一个" class="headerlink" title="再来一个"></a>再来一个</h3><p><img src="/images/blog/o_200901093353pg-image3.png" alt="image.png-283.3kB"></p><p>是不是看到虽然分了rack，但是这个时候的rule还是沿用最开始做的主机host的，可以清楚的看到0.2这个pg分布到了rack2下面去了</p><p>这个工具会在后面的做crush的调整后用到，可以清楚的看到pg的路径关系</p><h2 id="变更记录"><a href="#变更记录" class="headerlink" title="变更记录"></a>变更记录</h2><table><thead><tr><th align="center">Why</th><th align="center">Who</th><th align="center">When</th></tr></thead><tbody><tr><td align="center">创建</td><td align="center">武汉-运维-磨渣</td><td align="center">2019-3-8</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>vdbench测试实时可视化显示</title>
    <link href="/2019/01/03/vdbench%E6%B5%8B%E8%AF%95%E5%AE%9E%E6%97%B6%E5%8F%AF%E8%A7%86%E5%8C%96%E6%98%BE%E7%A4%BA/"/>
    <url>/2019/01/03/vdbench%E6%B5%8B%E8%AF%95%E5%AE%9E%E6%97%B6%E5%8F%AF%E8%A7%86%E5%8C%96%E6%98%BE%E7%A4%BA/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>前一段时间碰到一个系统，用rados bench 去跑都还比较正常，但是一跑数据库就非常慢，测试工具会抛出延时过大的提示，经过排查发现，云平台中有一台虚拟机还运行着备份数据库的服务，而这个备份软件是需要反复写一个标记文件的，因为这个标记文件只对应了一个对象，一个对象对应了一个pg，一个pg对应到固定的ssd上面，那个ssd的io几乎被这一个操作给打满了，然后全局的请求到了这个osd上面的时候，都会变得慢和卡顿</p><p>出现这种情况，在业务层面可能需要做好分离，我们在面对这种情况的时候该如何提前就做好测试，对自己的性能的剩余性能做一个更好的评估，什么时候需要分离，什么时候不需要分离，这个都是需要用数据来说话的</p><p>性能测试的时候，经常面临的这些问题，你告诉我这个环境能跑多少iops，带宽能多大，我的数据库能不能跑，这个我也没法回答，一般来说我都是说需要根据环境进行测试，这个测试也只能根据自己设计的模型进行测试，而越接近用户使用场景的业务模型，就越能反应真实的业务能力，最好的测试就是直接拿对接的软件进行测试，接什么业务就用什么业务压</p><p>我们可以自己先问自己几个问题</p><ul><li>1、如果集群里面有一台虚拟机在跑大带宽的业务，你去测试iops，性能能到多少，这个对应的是真实场景里面一个备份业务和一个数据库业务混用的情况</li><li>2、单机iops能到多少，如果几十台服务器都同时在跑的时候，单机的iops还能到多少？</li><li>3、多机并发的时候，单个机器上面的io会不会受到其他的机器的io的影响</li><li>4、性能在遇到scrub的时候，或者迁移的时候，能够还保留多少的性能，这个保留性能是否可控</li><li>5、集群写入到70%的时候，性能是多少，是初始的百分之多少，还够覆盖业务IO不？</li></ul><p>如果你的业务需求是远低于机器能提供性能的时候，上面的这些都不是问题，但是如果跑的业务是敏感型的时候，那么业务很可能收到较大的影响，这个时候我们只有对自己的环境有很精确的掌握才不至于在业务出现性能问题的时候去救火了</p><p>上面的这些是为了引出今天我需要讲的一个测试工具，在之前的文章当中比较多的讲的是故障的处理，后续的文章里面可能会讲一些偏向于控制和监控类的</p><h2 id="性能测试工具"><a href="#性能测试工具" class="headerlink" title="性能测试工具"></a>性能测试工具</h2><p>本篇讲的一个工具是vdbench，这个工具跟fio类似，很多测试里面会用到这个工具，这个比fio强大的是，既能够测试块接口也能测试文件接口，文件接口是去模拟写入文件，这个又和mdtest类似，但是mdtest主要是去测试元数据能力，vdbench则比较综合，这个工具在没有使用之前觉得很复杂，总觉得写个配置文件很麻烦，但是用了几次就会发现其实逻辑上面还是很清楚了，这里给个测试文件的模板，本篇主要写块接口的测试，所以模板也以块接口作为例子</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs bash">*example workload:  Single run, 10 bash disk<br><br>*HD:    HOST Define<br>*SD:    Storage Definition<br>*WD:    Workload Definition<br>*RD:    Run Definition<br>*<br>hd=default,vdbench=/root/vdbench,user=root,shell=ssh<br>hd=hd1,system=192.168.129.40<br>hd=hd2,system=192.168.129.41<br>sd=sd1,lun=/dev/sdb,host=hd1,openflags=o_direct,hitarea=0,range=(0,100),threads=2<br>sd=sd2,lun=/dev/sdb,host=hd2,openflags=o_direct,hitarea=0,range=(0,100),threads=2<br>wd=wd1,sd=(sd1,sd2),xfersize=(4096,100),rdpct=0,seekpct=100<br>rd=run1,wd=wd1,iorate=max,elapsed=600,warmup=300<br>* 2 rbd disks, 100% random, 0% <span class="hljs-built_in">read</span> of 4k blocks at unlimited rate<br></code></pre></td></tr></table></figure><p>上面的例子是测试两台机器的磁盘，每个磁盘两个线程写，4K的块大小，100%的写，100%随机，热身写300s，然后测试600s</p><p>这样一个配置后，就可以同时对两台机器按上面的写入模型写入了，我们看下系统开始测试的时候的显示</p><p><img src="/images/blog/o_200901093123vdbench-image-1.png" alt="testres"><br>就是类似这样的显示，然后最后的测试结果会在output里面生成一些html文件</p><p>这个工具有以下优点：</p><ul><li>1、能够每秒显示整个测试的io叠加，这样测试整个集群的io的时候，可以把所有虚机启动起来，然后进行io的压测，而不是去压单个rbd的iops，那个没有太大的意义，只能是一个数值，真正的环境大多也不是给一个业务使用的，也可以跑起一个业务以后，再看剩余的机器还能跑多少性能</li><li>2、在测试输出报告里面会根据主机统计一次io，这个面向的业务场景就是，比如某台主机上面可能挂载多块云盘，那么可以根据主机进行统计</li><li>3、在报告里面还会根据设备显示io个延时的信息，也就是只要是测试设备，每一个的性能指标都能查到，这个的好处就是检测集群里面的io是不是均匀的，如果做了qos，设备的测试性能值是不是跟设置限制一样</li></ul><p>既然有上面的优点，那有没有缺点呢？这个我个人认为还是有优化的空间的，下面就是我根据自己的需求做的一点点优化工作，并且把工具投入到了自己的测试工作当中去了</p><h2 id="一些需求"><a href="#一些需求" class="headerlink" title="一些需求"></a>一些需求</h2><ul><li>1、比如一个测试在一个小时，测试过程中碰上了scrub对性能的影响，我想知道这个影响到底有多大，如果按现在这个，我得等测试完了，再导出测试结果，再自己用excel图表工具做分析，这样一轮轮的进行测试</li><li>2、如果我需要对某个参数进行调整，进行调优测试，一般来说，都是测试一轮，然后再去调整参数，再重头再来一轮，反复测试，是不是有比较明显的显示让我能够实时的看到这个变化</li><li>3、io的抖动是不是能够比较明显的显示出来</li><li>4、测试的进度是不是能够有比较方便的地方看到</li><li>5、当前测试的是什么测试项目（如果测试是长时间的，可能自己也不清楚了）</li></ul><p>以上这些就是我自己的一些需求，基于这个我写了一个动态显示测试的可视化的工具，我对自己的这个工具有一定的要求</p><ul><li>1、随处可运行，也就是不需要系统上再安装其它软件</li><li>2、随时可运行可中断，不影响测试，即使测试结束后也能运行进行结果的解析</li></ul><p>这个实际上是有个软件HCIBENCH是基于vdbench做的，但这个软件跟vmware结合太紧，测试是包括了创建虚拟机的接口一起的，这个耦合的太紧了，并不符合我自己的测试需求，所以准备自己写一个简单的</p><h2 id="成品"><a href="#成品" class="headerlink" title="成品"></a>成品</h2><p>花了一点时间后，这个工具可以根据我的想法运行了，大概是下面的效果<br><img src="/images/blog/o_200901093131vdbench-image1-1.png" alt="main"><br>显示了进度和配置文件</p><p><img src="/images/blog/o_200901093137vdbench-image2-2.png" alt="iops"><br>显示了iops，这里是测试多少秒就有多少个点，整个测试结果都会显示到页面上面，并且是实时更新的<br>下面还有几张表，分别是响应时间和带宽的，读写响应时间分别显示的（混合读写的时候）</p><p>这个做了可视化以后，基本是能够覆盖我上面的几个需求，最终实现的是对测试结果的一个实时解析，如果需要导出测试结果，在测试完成以后，直接进行几个截图就完成了</p><p>如果是进行调优的测试，测试一小段时间也可以看到变化了</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本篇讲述的是我自己碰到问题，以及自己解决这个问题的方式，如果找不到需要的工具，那就根据自己的需要写一个简单的，这个可能写的过程会花点时间，但是后续用起来确实很节约时间</p><p>后续大概还有个scrub的数据可视化和scrub脚本生成器的工具，这个来可视化scrub的一些东西和我根据自己的分析生成的建议的scrub脚本</p><p>还有一个环境检测器的工具，这个是由于项目中出现了异常的低速的问题，排查起来很麻烦，这个也是准备一个写一个可视化工具来分析异常</p><p>可视化的好处是在大量的数据中间找到你需要的信息，这个在面向客户的系统的时候可能用处不大，或者需要面向客户做系统，但是有一些可视化的调试工具的时候还是很方便的</p><h2 id="变更记录"><a href="#变更记录" class="headerlink" title="变更记录"></a>变更记录</h2><table><thead><tr><th align="center">Why</th><th align="center">Who</th><th align="center">When</th></tr></thead><tbody><tr><td align="center">创建</td><td align="center">武汉-运维-磨渣</td><td align="center">2019-1-3</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>处理ceph incompelete的经验</title>
    <link href="/2018/12/19/%E5%A4%84%E7%90%86ceph%20incompelete%E7%9A%84%E7%BB%8F%E9%AA%8C/"/>
    <url>/2018/12/19/%E5%A4%84%E7%90%86ceph%20incompelete%E7%9A%84%E7%BB%8F%E9%AA%8C/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>最近已经见到几个环境出现过incompelete了，这个在很久以前Jewel正在合入mark-complete工具的时候就有做过类似的处理，但是随着处理的环境越来越多，这个地方还是有些需要注意的，本篇是写一些需要注意的点</p><p>一般来说是环境有多个机器同时坏盘或者掉电，或者掉主机引起的</p><h2 id="处理流程"><a href="#处理流程" class="headerlink" title="处理流程"></a>处理流程</h2><p>拿到环境第一时间是对环境标记noout，这个操作是为了防止集群的环境反复震荡，标记noout没有osd标记为out的情况下，只是pg状态变化，实际数据并不进行迁移</p><p>把能够启动的osd都启动起来，直到没有能启动的osd了，如果有能力处理的话，尽量把osd拉起来，如果是硬盘损坏掉了，确定无法修复了，那么就当这个osd无法救回来了，这个步骤里面是要尽最大努力把osd拉起来</p><p>这里面还有一部分情况是osd启动不起来，但是数据目录是可以访问的，这个地方就把这种盘先保留好，一定不要推掉了，很多运维上去看盘坏了就重新创建osd，这种推掉osd的操作建议只在active+clean的时候才做，否则的话，pg状态不对，又把osd推掉了，数据有比较大的概率丢失</p><p>在以上操作做完以后，开始处理异常的pg，处理的时候，首先把异常的pg的info全部倒好备份一下,还把pg分布保存下</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs raw">ceph pg 1.4 query &gt; 1.4query<br>ceph pg dump|grep incom &gt; pgincom.info<br></code></pre></td></tr></table></figure><p>全部保留一份，通过这个信息能够分析出数据的完整性和数据在哪里，这个一定要保留好原始版本，这个是有可能在后面做一些操作后就变更了，造成你不知道去哪里找数据</p><p>一部分情况下，根据query的信息提示，会告诉你 lost掉某个盘，可能让它恢复，这个操作的时候也是需要检查下，这个pg的数据是不是在当前环境下面有地方有完整的数据，确定有的话再根据提示进行lost的操作，如果还不放心，或者更稳妥的话，这个时候就需要备份pg数据了，这里就有个问题了，在做系统规划的时候，系统盘要尽量大点，这个时候就可以用来保存pg导出的数据了，如果是filestore，容量不够还可以拿osd的目录做临时存储，如果是bluestore，就只能拿本地盘做临时存储了</p><p>做完上面的标记和备份的操作后，有一部分的pg可能恢复正常了，然后还有一部分恢复不了正常，这个时候就需要根据上面保存好的query的信息里面拿到pg的数据在哪个osd上面，注意这个时候当前的query是可能查不到数据在哪里的，这个时候会出现提示数据在osd.1,osd.2,osd.3实际数据在osd.8的情况，并且可能完全没地方知道是在osd.8的，这个信息是存储在最开始版本的query里面的，所以在处理前，一定备份好信息，备份好数据</p><p>这个时候就开始把pg的数据导入到主osd,导入以后就可以标记mark-complete了，然后拉起osd，然后看下处理的这个pg的状态</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>在处理故障过程中，首先要保证能把能够拉起来的osd尽量全部拉起来，这个操作做好了可以省掉很多工作，pg是交叉映射的，有的时候正好交叉的osd全down了，所以能拉起来一个，这个pg也是可以状态恢复的</p><p>在所有操作前都是要进行数据备份的，这样即使出了问题，数据在都可以导入，导出的数据是需要检查下对象数目的，这个在导出前可以用ceph-object-tool做list操作检查pg对象的个数是否跟pg dump里面的一致的，通过大小也可以大致判断，这个在L版本的ceph做rm pg操作的时候，是有一个export-remove的命令的，这个把rm变成了mv操作，安全性比以前要好很多，防止手抖删错了，可以再导入</p><p>总之在数据处理过程中要小心，操作前备份好，操作过程每一步进行命令反馈确认，也就是你执行了命令应该是什么结果，这个要提前有分析，一旦产生偏差的时候，就要去分析了</p><p>本篇是操作上的建议，并没有具体命令，这个需要自己在实际操作过程中体会了，当然生产环境没那么多练手的机会，那么就尝试下对测试环境多进行破坏后进行恢复了，尽量不要直接推掉测试环境，每一次的问题处理都是为生产的处理做好储备工作</p><h2 id="变更记录"><a href="#变更记录" class="headerlink" title="变更记录"></a>变更记录</h2><table><thead><tr><th align="center">Why</th><th align="center">Who</th><th align="center">When</th></tr></thead><tbody><tr><td align="center">创建</td><td align="center">武汉-运维-磨渣</td><td align="center">2018-12-19</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>cephfs根据存储池显示df容量</title>
    <link href="/2018/08/19/cephfs%E6%A0%B9%E6%8D%AE%E5%AD%98%E5%82%A8%E6%B1%A0%E6%98%BE%E7%A4%BAdf%E5%AE%B9%E9%87%8F/"/>
    <url>/2018/08/19/cephfs%E6%A0%B9%E6%8D%AE%E5%AD%98%E5%82%A8%E6%B1%A0%E6%98%BE%E7%A4%BAdf%E5%AE%B9%E9%87%8F/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>如果用cephfs比较多，应该都知道，在cephfs的客户端进行mount以后，看到的容量显示的是集群的总的容量，也就是你的总的磁盘空间是多少这个地方显示的就是多少</p><p>这个一直都是这样显示的，我们之前在hammer版本的时候，阿茂和大黄一起在公司内部实现了这个功能，社区会慢慢的集成一些类似的面向面向商业用户的需求</p><p>社区已经开发了一个版本，接口都做的差不多了，那么稍微改改，就能实现想要的需求的</p><p>本篇内的改动是基于内核客户端代码的改动，改动很小，应该能够看的懂</p><h2 id="改动过程"><a href="#改动过程" class="headerlink" title="改动过程"></a>改动过程</h2><p>首先找到这个补丁</p><blockquote><p>Improve accuracy of statfs reporting for Ceph filesystems comprising exactly one data pool. In this case, the Ceph monitor can now report the space usage for the single data pool instead of the global data for the entire Ceph cluster. Include support for this message in mon_client and leverage it in ceph&#x2F;super.</p></blockquote><p>地址：<a href="https://www.spinics.net/lists/ceph-devel/msg37937.html">https://www.spinics.net/lists/ceph-devel/msg37937.html</a></p><p>这个说的是改善了statfs的显示，这个statfs就是在linux下面的mount的输出的显示的，说是改善了在单存储池下的显示效果，也就是在单存储池下能够显示存储池的容量空间，而不是全局的空间</p><p>这里就有个疑问了，单存储池？那么多存储池呢？我们测试下看看</p><p>这里这个补丁已经打到了centos7.5的默认内核里面去了，也就是内核版本</p><blockquote><p>Linux lab103 3.10.0-862.el7.x86_64</p></blockquote><p>对应的rpm包的版本是</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab103 ceph]<span class="hljs-comment"># rpm -qa|grep  3.10.0-862</span><br>kernel-devel-3.10.0-862.el7.x86_64<br>kernel-3.10.0-862.el7.x86_64<br></code></pre></td></tr></table></figure><p>下载的地址为：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">http://mirrors.163.com/centos/7.5.1804/os/x86_64/Packages/kernel-3.10.0-862.el7.x86_64.rpm<br></code></pre></td></tr></table></figure><p>或者直接安装centos7.5也行，这里只要求是这个内核就可以了</p><p>我们看下默认情况下是怎样的</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab102 ~]<span class="hljs-comment"># ceph -s</span><br>  data:<br>    pools:   3 pools, 72 pgs<br>    objects: 22 objects, 36179 bytes<br>    usage:   5209 MB used, 11645 GB / 11650 GB avail<br>    pgs:     72 active+clean<br> <br>[root@lab102 ~]<span class="hljs-comment"># ceph fs ls</span><br>name: ceph, metadata pool: metadata, data pools: [data ]<br>[root@lab102 ~]<span class="hljs-comment"># ceph df</span><br>GLOBAL:<br>    SIZE       AVAIL      bash USED     %bash USED <br>    11650G     11645G        5209M          0.04 <br>POOLS:<br>    NAME         ID     USED      %USED     MAX AVAIL     OBJECTS <br>    data         9          0         0         3671G           0 <br>    metadata     10     36179         0        11014G          22 <br>    newdata      11         0         0         5507G           0 <br>[root@lab102 ~]<span class="hljs-comment"># ceph osd dump|grep pool</span><br>pool 9 <span class="hljs-string">&#x27;data&#x27;</span> replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 32 pgp_num 32 last_change 136 flags hashpspool stripe_width 0 application cephfs<br>pool 10 <span class="hljs-string">&#x27;metadata&#x27;</span> replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 32 pgp_num 32 last_change 112 flags hashpspool stripe_width 0 application cephfs<br>pool 11 <span class="hljs-string">&#x27;newdata&#x27;</span> replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 8 pgp_num 8 last_change 134 flags hashpspool  stripe_width 0 application cephfs<br></code></pre></td></tr></table></figure><p>从上面可以看到我的硬盘裸空间为12T左右，data存储池副本3那么可用空间为4T左右，文件系统里面只有一个data存储池，看下挂载的情况</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ~]<span class="hljs-comment"># uname -a</span><br>Linux lab101 3.10.0-862.el7.x86_64 <span class="hljs-comment">#1 SMP Fri Apr 20 16:44:24 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux</span><br>[root@lab101 ~]<span class="hljs-comment"># df -Th|grep mnt</span><br>192.168.19.102:/        ceph      3.6T     0  3.6T   0% /mnt<br></code></pre></td></tr></table></figure><p>可以看到显示的容量就是存储池的可用容量为总空间的，现在我们加入一个数据池</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab102 ~]<span class="hljs-comment"># ceph mds add_data_pool newdata</span><br>added data pool 11 to fsmap<br></code></pre></td></tr></table></figure><p>再次查看df的显示</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ~]<span class="hljs-comment"># df -Th|grep mnt</span><br>192.168.19.102:/        ceph       12T  5.1G   12T   1% /mnt<br></code></pre></td></tr></table></figure><p>容量回到了原始的显示的方式，这个跟上面的补丁的预期是一样的，我们看下代码这里怎么控制的</p><h2 id="获取当前内核版本的代码"><a href="#获取当前内核版本的代码" class="headerlink" title="获取当前内核版本的代码"></a>获取当前内核版本的代码</h2><p>首先要找到当前的内核的src.rpm包，这样可以拿到当前内核版本的源码</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">wget http://vault.centos.org/7.5.1804/os/Source/SPackages/kernel-3.10.0-862.el7.src.rpm<br></code></pre></td></tr></table></figure><p>解压源码包</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab103 origin]<span class="hljs-comment"># rpm2cpio kernel-3.10.0-862.el7.src.rpm |cpio -div</span><br>[root@lab103 origin]<span class="hljs-comment"># tar -xvf linux-3.10.0-862.el7.tar.xz</span><br>[root@lab103 origin]<span class="hljs-comment"># cd linux-3.10.0-862.el7/fs/ceph/</span><br></code></pre></td></tr></table></figure><p>上面的操作后我们已经进入了我们想要看的源码目录了<br>我们看下super.c这个文件，这个df的显示的控制是在这个文件里面的</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab103 ceph]<span class="hljs-comment"># cat super.c |less</span><br></code></pre></td></tr></table></figure><p>看下这段代码</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs bash">static int ceph_statfs(struct dentry *dentry, struct kstatfs *buf)<br>&#123;<br>        struct ceph_fs_client *fsc = ceph_inode_to_client(dentry-&gt;d_inode);<br>        struct ceph_monmap *monmap = fsc-&gt;client-&gt;monc.monmap;<br>        struct ceph_statfs st;<br>        u64 fsid;<br>        int err;<br>        u64 data_pool;<br><br>        <span class="hljs-keyword">if</span> (fsc-&gt;mdsc-&gt;mdsmap-&gt;m_num_data_pg_pools == 1) &#123;<br>                data_pool = fsc-&gt;mdsc-&gt;mdsmap-&gt;m_data_pg_pools[0];<br>        &#125; <span class="hljs-keyword">else</span> &#123;<br>                data_pool = CEPH_NOPOOL;<br>        &#125;<br><br>        dout(<span class="hljs-string">&quot;statfs\n&quot;</span>);<br>        err = ceph_monc_do_statfs(&amp;fsc-&gt;client-&gt;monc, data_pool, &amp;st);<br>        <span class="hljs-keyword">if</span> (err &lt; 0)<br>                <span class="hljs-built_in">return</span> err;<br></code></pre></td></tr></table></figure><p>其中的fsc-&gt;mdsc-&gt;mdsmap-&gt;m_num_data_pg_pools &#x3D;&#x3D; 1和data_pool &#x3D; fsc-&gt;mdsc-&gt;mdsmap-&gt;m_data_pg_pools[0];这个地方的意思是如果fs里面包含的存储池的存储池个数为1那么data_pool就取这个存储池的信息，所以上面的我们的实践过程中的就是单个存储池的时候显示存储池的容量，超过一个的时候就显示的全局的容量，这个是跟代码对应的上的</p><p>我们基于上面的已经做好的功能改变一下需求</p><blockquote><p>需要可以根据自己的需要指定存储池的容量来显示，通过挂载内核客户端的时候传递一个参数进去来进行显示</p></blockquote><h2 id="代码改动"><a href="#代码改动" class="headerlink" title="代码改动"></a>代码改动</h2><p>[root@lab103 ceph]# vim super.h<br>在super.h内定义一个默认值</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment">#define ZP_POOL_DEFAULT      0  /* pool id */</span><br><span class="hljs-comment">#define CEPH_CAPS_WANTED_DELAY_MAX_DEFAULT     60  /* cap release delay */</span><br>struct ceph_mount_options &#123;<br>        int flags;<br>        int sb_flags;<br><br>        int wsize;            /* max write size */<br>        int rsize;            /* max <span class="hljs-built_in">read</span> size */<br>        int zp_pool;            /* pool <span class="hljs-built_in">id</span> */<br>        int rasize;           /* max readahead */<br></code></pre></td></tr></table></figure><p>这里增加了两个一个zp_pool和ZP_POOL_DEFAULT<br>这个文件的改动就只有这么多了</p><p>改动super.c的代码<br>在enum里面加上Opt_zp_pool</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs bash">enum &#123;<br>        Opt_wsize,<br>        Opt_rsize,<br>        Opt_rasize,<br>        Opt_caps_wanted_delay_min,<br>        Opt_zp_pool,<br></code></pre></td></tr></table></figure><p>在match_table_t fsopt_tokens里面添加Opt_zp_pool相关的判断，我们自己注意传的是pool在fs里面的id即可</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs bash">static match_table_t fsopt_tokens = &#123;<br>        &#123;Opt_wsize, <span class="hljs-string">&quot;wsize=%d&quot;</span>&#125;,<br>        &#123;Opt_rsize, <span class="hljs-string">&quot;rsize=%d&quot;</span>&#125;,<br>        &#123;Opt_rasize, <span class="hljs-string">&quot;rasize=%d&quot;</span>&#125;,<br>        &#123;Opt_caps_wanted_delay_min, <span class="hljs-string">&quot;caps_wanted_delay_min=%d&quot;</span>&#125;,<br>        &#123;Opt_zp_pool, <span class="hljs-string">&quot;zp_pool=%d&quot;</span>&#125;,<br></code></pre></td></tr></table></figure><p>在static int parse_fsopt_token中添加</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-keyword">case</span> Opt_caps_wanted_delay_max:<br>                <span class="hljs-keyword">if</span> (intval &lt; 1)<br>                        <span class="hljs-built_in">return</span> -EINVAL;<br>                fsopt-&gt;caps_wanted_delay_max = intval;<br>                <span class="hljs-built_in">break</span>;<br>        <span class="hljs-keyword">case</span> Opt_zp_pool:<br>                <span class="hljs-keyword">if</span> (intval &lt; 0)<br>                        <span class="hljs-built_in">return</span> -EINVAL;<br>                fsopt-&gt;zp_pool = intval;<br>                <span class="hljs-built_in">break</span>;<br>        <span class="hljs-keyword">case</span> Opt_readdir_max_entries:<br>                <span class="hljs-keyword">if</span> (intval &lt; 1)<br>                        <span class="hljs-built_in">return</span> -EINVAL;<br>                fsopt-&gt;max_readdir = intval;<br>                <span class="hljs-built_in">break</span>;<br></code></pre></td></tr></table></figure><p>判断如果小于0就抛错，这个id从0开始上升的，所以也不允许小于0</p><p>在static int parse_mount_options中添加</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">fsopt-&gt;caps_wanted_delay_min = CEPH_CAPS_WANTED_DELAY_MIN_DEFAULT;<br>fsopt-&gt;zp_pool = ZP_POOL_DEFAULT;<br>fsopt-&gt;caps_wanted_delay_max = CEPH_CAPS_WANTED_DELAY_MAX_DEFAULT;<br></code></pre></td></tr></table></figure><p>在static int ceph_show_options中添加</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-keyword">if</span> (fsopt-&gt;caps_wanted_delay_min != CEPH_CAPS_WANTED_DELAY_MIN_DEFAULT)<br>        seq_printf(m, <span class="hljs-string">&quot;,caps_wanted_delay_min=%d&quot;</span>,<br>                 fsopt-&gt;caps_wanted_delay_min);<br><span class="hljs-keyword">if</span> (fsopt-&gt;zp_pool)<br>        seq_printf(m, <span class="hljs-string">&quot;,zp_pool=%d&quot;</span>,<br>                 fsopt-&gt;zp_pool);<br><span class="hljs-keyword">if</span> (fsopt-&gt;caps_wanted_delay_max != CEPH_CAPS_WANTED_DELAY_MAX_DEFAULT)<br>        seq_printf(m, <span class="hljs-string">&quot;,caps_wanted_delay_max=%d&quot;</span>,<br>                   fsopt-&gt;caps_wanted_delay_max);<br></code></pre></td></tr></table></figure><p>这个是用来在执行mount命令的时候显示选项的数值的<br>改动到这里我们检查下我们对super.c做过的的改动</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab103 ceph]<span class="hljs-comment"># cat super.c |grep zp_pool</span><br>Opt_zp_pool,<br>&#123;Opt_zp_pool, <span class="hljs-string">&quot;zp_pool=%d&quot;</span>&#125;,<br>    <span class="hljs-keyword">case</span> Opt_zp_pool:<br>        fsopt-&gt;zp_pool = intval;<br>fsopt-&gt;zp_pool = ZP_POOL_DEFAULT;<br>        <span class="hljs-keyword">if</span> (fsopt-&gt;zp_pool)<br>                seq_printf(m, <span class="hljs-string">&quot;,zp_pool=%d&quot;</span>,<br>                         fsopt-&gt;zp_pool);<br></code></pre></td></tr></table></figure><p>做了以上的改动后我们就可以把参数给传进来了，现在我们需要把参数传递到需要用的地方<br>也就是static int ceph_statfs内需要调用这个参数</p><p>在static int ceph_statfs中添加上struct ceph_mount_options *fsopt &#x3D; fsc-&gt;mount_options;</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs bash">static int ceph_statfs(struct dentry *dentry, struct kstatfs *buf)<br>&#123;<br>        struct ceph_fs_client *fsc = ceph_inode_to_client(dentry-&gt;d_inode);<br>        struct ceph_monmap *monmap = fsc-&gt;client-&gt;monc.monmap;<br>        struct ceph_statfs st;<br>        struct ceph_mount_options *fsopt = fsc-&gt;mount_options;<br>        u64 fsid;<br></code></pre></td></tr></table></figure><p>然后改掉这个fsc-&gt;mdsc-&gt;mdsmap-&gt;m_num_data_pg_pools &#x3D;&#x3D; 1的判断，我们判断大于0即可</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-keyword">if</span> (fsc-&gt;mdsc-&gt;mdsmap-&gt;m_num_data_pg_pools &gt; 0) &#123;<br>        data_pool = fsc-&gt;mdsc-&gt;mdsmap-&gt;m_data_pg_pools[fsopt-&gt;zp_pool];<br>&#125; <span class="hljs-keyword">else</span> &#123;<br>        data_pool = CEPH_NOPOOL;<br>&#125;<br></code></pre></td></tr></table></figure><p>并且把写死的0改成我们的变量fsopt-&gt;zp_pool</p><p>到这里改动就完成了，这里还没有完，我们需要编译成我们的需要的模块</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab103 ceph]<span class="hljs-comment"># modinfo ceph</span><br>filename:       /lib/modules/3.10.0-862.el7.x86_64/kernel/fs/ceph/ceph.ko.xz<br></code></pre></td></tr></table></figure><p>可以看到内核在高版本的时候已经改成了xz压缩的模块了,这里等会需要多处理一步<br>我们只需要这一个模块就编译这一个ceph.ko模块就好<br>编译需要装好kernel-devel包kernel-devel-3.10.0-862.el7.x86_64</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab103 ceph]<span class="hljs-comment"># pwd</span><br>/home/origin/linux-3.10.0-862.el7/fs/ceph<br>[root@lab103 ceph]<span class="hljs-comment"># make CONFIG_CEPH_FS=m -C /lib/modules/3.10.0-862.el7.x86_64/build/ M=`pwd` modules</span><br>make: Entering directory `/usr/src/kernels/3.10.0-862.el7.x86_64<span class="hljs-string">&#x27;</span><br><span class="hljs-string">  CC [M]  /home/origin/linux-3.10.0-862.el7/fs/ceph/super.o</span><br><span class="hljs-string">  CC [M]  /home/origin/linux-3.10.0-862.el7/fs/ceph/inode.o</span><br><span class="hljs-string">  CC [M]  /home/origin/linux-3.10.0-862.el7/fs/ceph/dir.o</span><br><span class="hljs-string">  CC [M]  /home/origin/linux-3.10.0-862.el7/fs/ceph/file.o</span><br><span class="hljs-string">  CC [M]  /home/origin/linux-3.10.0-862.el7/fs/ceph/locks.o</span><br><span class="hljs-string">  CC [M]  /home/origin/linux-3.10.0-862.el7/fs/ceph/addr.o</span><br><span class="hljs-string">  CC [M]  /home/origin/linux-3.10.0-862.el7/fs/ceph/ioctl.o</span><br><span class="hljs-string">  CC [M]  /home/origin/linux-3.10.0-862.el7/fs/ceph/export.o</span><br><span class="hljs-string">  CC [M]  /home/origin/linux-3.10.0-862.el7/fs/ceph/caps.o</span><br><span class="hljs-string">  CC [M]  /home/origin/linux-3.10.0-862.el7/fs/ceph/snap.o</span><br><span class="hljs-string">  CC [M]  /home/origin/linux-3.10.0-862.el7/fs/ceph/xattr.o</span><br><span class="hljs-string">  CC [M]  /home/origin/linux-3.10.0-862.el7/fs/ceph/mds_client.o</span><br><span class="hljs-string">  CC [M]  /home/origin/linux-3.10.0-862.el7/fs/ceph/mdsmap.o</span><br><span class="hljs-string">  CC [M]  /home/origin/linux-3.10.0-862.el7/fs/ceph/strings.o</span><br><span class="hljs-string">  CC [M]  /home/origin/linux-3.10.0-862.el7/fs/ceph/ceph_frag.o</span><br><span class="hljs-string">  CC [M]  /home/origin/linux-3.10.0-862.el7/fs/ceph/debugfs.o</span><br><span class="hljs-string">  CC [M]  /home/origin/linux-3.10.0-862.el7/fs/ceph/acl.o</span><br><span class="hljs-string">  LD [M]  /home/origin/linux-3.10.0-862.el7/fs/ceph/ceph.o</span><br><span class="hljs-string">  Building modules, stage 2.</span><br><span class="hljs-string">  MODPOST 1 modules</span><br><span class="hljs-string">  CC      /home/origin/linux-3.10.0-862.el7/fs/ceph/ceph.mod.o</span><br><span class="hljs-string">  LD [M]  /home/origin/linux-3.10.0-862.el7/fs/ceph/ceph.ko</span><br><span class="hljs-string">make: Leaving directory `/usr/src/kernels/3.10.0-862.el7.x86_64&#x27;</span><br></code></pre></td></tr></table></figure><p>正常应该就是上面的没有报错的输出了<br>压缩ko模块</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab103 ceph]<span class="hljs-comment"># find * -name &#x27;*.ko&#x27; | xargs -n 1 xz</span><br>[root@lab103 ceph]<span class="hljs-comment"># rmmod ceph</span><br>[root@lab103 ceph]<span class="hljs-comment"># rm -rf  /lib/modules/3.10.0-862.el7.x86_64/kernel/fs/ceph/ceph.ko.xz</span><br>[root@lab103 ceph]<span class="hljs-comment"># cp -ra ceph.ko.xz /lib/modules/3.10.0-862.el7.x86_64/kernel/fs/ceph/</span><br>[root@lab103 ceph]<span class="hljs-comment"># lsmod |grep ceph</span><br>ceph                  345111  0 <br>libceph               301687  1 ceph<br>dns_resolver           13140  1 libceph<br>libcrc32c              12644  2 xfs,libceph<br></code></pre></td></tr></table></figure><p>现在已经加载好模块了，我们试验下</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab103 ceph]<span class="hljs-comment"># ceph df</span><br>GLOBAL:<br>    SIZE       AVAIL      bash USED     %bash USED <br>    11650G     11645G        5210M          0.04 <br>POOLS:<br>    NAME         ID     USED      %USED     MAX AVAIL     OBJECTS <br>    data         9          0         0         3671G           0 <br>    metadata     10     36391         0        11014G          22 <br>    newdata      11         0         0         5507G           0 <br><br>[root@lab103 ceph]<span class="hljs-comment"># mount -t ceph 192.168.19.102:/ /mnt</span><br>[root@lab103 ceph]<span class="hljs-comment"># df -h|grep mnt</span><br>192.168.19.102:/         3.6T     0  3.6T   0% /mnt<br>[root@lab103 ceph]<span class="hljs-comment"># ceph fs ls</span><br>name: ceph, metadata pool: metadata, data pools: [data newdata ]<br></code></pre></td></tr></table></figure><p>我们给了一个默认存储池的值为0的编号的，现在显示的是data的容量，没有问题，我们想显示newdata存储池的</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab103 ceph]<span class="hljs-comment"># mount -t ceph 192.168.19.102:/ /mnt -o zp_pool=1</span><br>[root@lab103 ceph]<span class="hljs-comment"># df -h|grep mnt</span><br>192.168.19.102:/         5.4T     0  5.4T   0% /mnt<br></code></pre></td></tr></table></figure><p>这里我们显示的要么0，要么1的存储池的那么我如果想显示全局的怎么处理？那就是给个不存在的编号就行了</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab103 ceph]<span class="hljs-comment"># mount -t ceph 192.168.19.102:/ /mnt -o zp_pool=1000</span><br>[root@lab103 ceph]<span class="hljs-comment"># mount|grep ceph|grep zp_pool</span><br>192.168.19.102:/ on /mnt <span class="hljs-built_in">type</span> ceph (rw,relatime,acl,wsize=16777216,zp_pool=1000)<br>[root@lab103 ceph]<span class="hljs-comment"># df -h|grep mnt</span><br>192.168.19.102:/          12T  5.1G   12T   1% /mnt<br></code></pre></td></tr></table></figure><p>也可以自己去改成读取all字段的时候取全局变量，这个是直接用一个不存在的编号去走到全局的容量的逻辑里面去了,这样比较简单</p><p>通过mount命令可以查询到挂载的选项</p><p>到这里就根据需求改完了</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本篇里面涉及的知识点包括了rpm包的源码的获取，解压，以及内核模块的单独编译，改动单个模块进行替换，cephfs客户端的内核参数的自定义传递等等，在本博客的第三篇文章就有一个单独编译一个ext4模块的</p><h2 id="变更记录"><a href="#变更记录" class="headerlink" title="变更记录"></a>变更记录</h2><table><thead><tr><th align="center">Why</th><th align="center">Who</th><th align="center">When</th></tr></thead><tbody><tr><td align="center">创建</td><td align="center">武汉-运维-磨渣</td><td align="center">2018-08-20</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>快速构建ceph可视化监控系统</title>
    <link href="/2018/07/17/%E5%BF%AB%E9%80%9F%E6%9E%84%E5%BB%BAceph%E5%8F%AF%E8%A7%86%E5%8C%96%E7%9B%91%E6%8E%A7%E7%B3%BB%E7%BB%9F/"/>
    <url>/2018/07/17/%E5%BF%AB%E9%80%9F%E6%9E%84%E5%BB%BAceph%E5%8F%AF%E8%A7%86%E5%8C%96%E7%9B%91%E6%8E%A7%E7%B3%BB%E7%BB%9F/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>ceph的可视化方案很多，本篇介绍的是比较简单的一种方式，并且对包都进行了二次封装，所以能够在极短的时间内构建出一个可视化的监控系统</p><p>本系统组件如下：</p><ul><li>ceph-jewel版本</li><li>ceph_exporter的jewel版本</li><li>prometheus的2.3.2版本</li><li>grafana的grafana-5.2.1版本</li><li>Ceph grafana的插件- Clusterby Cristian Calin</li></ul><p>适配的系统为centos7</p><p>资源如下：</p><blockquote><p><a href="http://static.zybuluo.com/zphj1987/jiwx305b8q1hwc5uulo0z7ft/ceph_exporter-2.0.0-1.x86_64.rpm">http://static.zybuluo.com/zphj1987/jiwx305b8q1hwc5uulo0z7ft/ceph_exporter-2.0.0-1.x86_64.rpm</a><br><br><a href="http://static.zybuluo.com/zphj1987/1nu2k4cpcery94q2re3u6s1t/ceph-cluster_rev1.json">http://static.zybuluo.com/zphj1987/1nu2k4cpcery94q2re3u6s1t/ceph-cluster_rev1.json</a><br><br><a href="http://static.zybuluo.com/zphj1987/7ro7up6r03kx52rkwy1qjuwm/prometheus-2.3.2-1.x86_64.rpm">http://static.zybuluo.com/zphj1987/7ro7up6r03kx52rkwy1qjuwm/prometheus-2.3.2-1.x86_64.rpm</a><br><br><a href="http://mysrc.cn-bj.ufileos.com/grafana-5.2.1-1.x86_64.rpm">http://mysrc.cn-bj.ufileos.com/grafana-5.2.1-1.x86_64.rpm</a></p></blockquote><p>以上资源均可以直接用wget进行下载，然后直接安装</p><h2 id="监控的架构介绍"><a href="#监控的架构介绍" class="headerlink" title="监控的架构介绍"></a>监控的架构介绍</h2><p>通过ceph_exporter抓取的ceph相关的数据并且在本地监听端口9128端口</p><p>prometheus抓取ceph_exporter的9128的端口的数据存储在本地的&#x2F;var&#x2F;lib&#x2F;prometheus&#x2F;目录下面</p><p>grafana抓取prometheus的数据进行渲染成web页面</p><p>页面的模板就是使用的grafana的ceph模板插件</p><p>那么我们就根据上面的架构去一步步的把系统配置起来</p><h2 id="配置监控系统"><a href="#配置监控系统" class="headerlink" title="配置监控系统"></a>配置监控系统</h2><h3 id="安装ceph-exporter"><a href="#安装ceph-exporter" class="headerlink" title="安装ceph_exporter"></a>安装ceph_exporter</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 install]<span class="hljs-comment"># wget http://static.zybuluo.com/zphj1987/jiwx305b8q1hwc5uulo0z7ft/ceph_exporter-2.0.0-1.x86_64.rpm</span><br>[root@lab101 install]<span class="hljs-comment"># rpm -qpl ceph_exporter-2.0.0-1.x86_64.rpm </span><br>/usr/bin/ceph_exporter<br>/usr/lib/systemd/system/ceph_exporter.service<br>[root@lab101 install]<span class="hljs-comment"># rpm -ivh ceph_exporter-2.0.0-1.x86_64.rpm </span><br>Preparing...                          <span class="hljs-comment">################################# [100%]</span><br>Updating / installing...<br>   1:ceph_exporter-2:2.0.0-1          <span class="hljs-comment">################################# [100%]</span><br>[root@lab101 install]<span class="hljs-comment"># systemctl start ceph_exporter</span><br>[root@lab101 install]<span class="hljs-comment"># systemctl enable ceph_exporter</span><br>[root@lab101 install]<span class="hljs-comment"># netstat -tunlp|grep 9128</span><br>tcp6       0      0 :::9128                 :::*                    LISTEN      35853/ceph_exporter <br></code></pre></td></tr></table></figure><p>可以看到端口起来了就是安装成功了，这个ceph_exporter建议是安装在管理节点上，也就是能够执行出ceph -s的节点上面的</p><h3 id="安装prometheus"><a href="#安装prometheus" class="headerlink" title="安装prometheus"></a>安装prometheus</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 install]<span class="hljs-comment">#  wget http://static.zybuluo.com/zphj1987/7ro7up6r03kx52rkwy1qjuwm/prometheus-2.3.2-1.x86_64.rpm</span><br>[root@lab101 install]<span class="hljs-comment"># rpm -qpl prometheus-2.3.2-1.x86_64.rpm </span><br>/etc/ceph/prometheus.yml<br>/usr/bin/prometheus<br>/usr/lib/systemd/system/prometheus.service<br>[root@lab101 install]<span class="hljs-comment"># rpm -ivh prometheus-2.3.2-1.x86_64.rpm </span><br>Preparing...                          <span class="hljs-comment">################################# [100%]</span><br>Updating / installing...<br>   1:prometheus-2:2.3.2-1             <span class="hljs-comment">################################# [100%]</span><br>[root@lab101 install]<span class="hljs-comment"># systemctl start prometheus</span><br>[root@lab101 install]<span class="hljs-comment"># netstat -tunlp|grep 9090</span><br>tcp6       0      0 :::9090                 :::*                    LISTEN      36163/prometheus<br></code></pre></td></tr></table></figure><p>这个地方默认是认为prometheus和ceph_exporter在一台机器上面，所以配置文件的&#x2F;etc&#x2F;ceph&#x2F;prometheus.yml里面的targets写的是127.0.0.1，根据需要修改成ceph_exporter的ip地址即可</p><p>prometheus的默认监听端口为9090，到这个时候直接去web 上面就可以看到prometheus的抓取的数据了</p><p><img src="/images/blog/o_200901092628pro-image1-2.png" alt="prometheus"></p><p>到这里是数据到prometheus的已经完成了，下面就去做跟grafana相关的配置了</p><p>###安装grafana</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 install]<span class="hljs-comment"># wget http://mysrc.cn-bj.ufileos.com/grafana-5.2.1-1.x86_64.rpm</span><br>[root@lab101 install]<span class="hljs-comment"># yum localinstall grafana-5.2.1-1.x86_64.rpm</span><br>[root@lab101 install]<span class="hljs-comment"># systemctl start grafana-server.service</span><br>[root@lab101 install]<span class="hljs-comment"># netstat -tunlp|grep gra</span><br>Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name    <br>tcp6       0      0 :::3000                 :::*                    LISTEN      36730/grafana-serve<br></code></pre></td></tr></table></figure><p>grafana默认监听的3000的端口</p><p><img src="/images/blog/o_200901092634pro-image2-1.png" alt="grafanalogin"><br>默认登陆的用户名密码为admin admin,登陆成功后会强制修改密码</p><p>###配置grafana<br><img src="/images/blog/o_200901092641pro-image3-1.png" alt="add sour"><br>首先增加数据源<br><img src="/images/blog/o_200901092648pro-image4.png" alt="配置9090"><br><img src="/images/blog/o_200901092654pro-image5.png" alt="import"></p><p><img src="/images/blog/o_200901092700pro-image6.png" alt="image.png-97.2kB"></p><p><img src="/images/blog/o_200901092707pro-image7.png" alt="image.png-96.2kB"></p><p>这里如果能上网就直接输入id 917 ，如果不能上网就把上面的ceph-cluster_rev1.json文件弄到本地去，导入进去即可</p><p><img src="/images/blog/o_200901092713pro-image8.png" alt="granfa"></p><p>到这里就完成了配置了</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>以上为了方便都把相关的软件做成了rpm包，从安装方便角度来看，grafana，ceph_exporter，还有prometheus都采用的是单二进制文件的方式，稍微组合一下大大的降低了部署难度，比如那个ceph_exporter需要用go进行编译，封好包以后就不需要这个过程，并且接口因为有版本的限制，所以这样直接对应版本安装也避免了出错</p><p>本篇的环境所述均为jewel适配版本</p><h2 id="变更记录"><a href="#变更记录" class="headerlink" title="变更记录"></a>变更记录</h2><table><thead><tr><th align="center">Why</th><th align="center">Who</th><th align="center">When</th></tr></thead><tbody><tr><td align="center">创建</td><td align="center">武汉-运维-磨渣</td><td align="center">2018-07-17</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>利用s3-test进行ceph的接口兼容性测试</title>
    <link href="/2018/06/27/%E5%88%A9%E7%94%A8s3-test%E8%BF%9B%E8%A1%8Cceph%E7%9A%84%E6%8E%A5%E5%8F%A3%E5%85%BC%E5%AE%B9%E6%80%A7%E6%B5%8B%E8%AF%95/"/>
    <url>/2018/06/27/%E5%88%A9%E7%94%A8s3-test%E8%BF%9B%E8%A1%8Cceph%E7%9A%84%E6%8E%A5%E5%8F%A3%E5%85%BC%E5%AE%B9%E6%80%A7%E6%B5%8B%E8%AF%95/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>ceph的rgw能够提供一个兼容性的s3的接口，既然是兼容性，当然不可能是所有接口都会兼容，那么我们需要有一个工具来进行接口的验证以及测试，这个在其他测试工具里面有类似的posix接口验证工具，这类的工具就是跑测试用例，来输出通过或者不通过的列表</p><p>用此类的工具有个好的地方就是，能够对接口进行验证，来避免版本的更新带来的接口破坏</p><h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><p>直接对官方的分支进行clone下来，总文件数不多，下载很快</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 s3]<span class="hljs-comment"># git clone https://github.com/ceph/s3-tests.git</span><br>[root@lab101 s3]<span class="hljs-comment"># cd s3-tests/</span><br></code></pre></td></tr></table></figure><p>这个地方注意下有版本之分，测试的时候需要用对应版本，这里我们测试的jewel版本就切换到jewel的分支(关键步骤)</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 s3-tests]<span class="hljs-comment"># git branch -a</span><br>[root@lab101 s3-tests]<span class="hljs-comment"># git checkout -b jewel remotes/origin/ceph-jewel</span><br>[root@lab101 s3-tests]<span class="hljs-comment"># ./bootstrap</span><br></code></pre></td></tr></table></figure><p>进入到目录当中执行 .&#x2F;bootstrap进行初始化相关的工作，这个是下载一些相关的库和软件包，并且创建了一个python的虚拟环境，如果从其他地方拷贝过来的代码最好是删除掉python虚拟环境，让程序自己去重新创建一套环境</p><p>执行完了以后就是创建测试配置文件test.conf</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><code class="hljs bash">[DEFAULT]<br><span class="hljs-comment">## this section is just used as default for all the &quot;s3 *&quot;</span><br><span class="hljs-comment">## sections, you can place these variables also directly there</span><br><br><span class="hljs-comment">## replace with e.g. &quot;localhost&quot; to run against local software</span><br>host = 192.168.19.101<br><br><span class="hljs-comment">## uncomment the port to use something other than 80</span><br>port = 7481<br><br><span class="hljs-comment">## say &quot;no&quot; to disable TLS</span><br>is_secure = no<br><br>[fixtures]<br><span class="hljs-comment">## all the buckets created will start with this prefix;</span><br><span class="hljs-comment">## &#123;random&#125; will be filled with random characters to pad</span><br><span class="hljs-comment">## the prefix to 30 characters long, and avoid collisions</span><br>bucket prefix = cephtest-&#123;random&#125;-<br><br>[s3 main]<br><span class="hljs-comment">## the tests assume two accounts are defined, &quot;main&quot; and &quot;alt&quot;.</span><br><br><span class="hljs-comment">## user_id is a 64-character hexstring</span><br>user_id = test01<br><br><span class="hljs-comment">## display name typically looks more like a unix login, &quot;jdoe&quot; etc</span><br>display_name = test01<br><br><span class="hljs-comment">## replace these with your access keys</span><br>access_key = test01<br>secret_key = test01<br><br><span class="hljs-comment">## replace with key id obtained when secret is created, or delete if KMS not tested</span><br><span class="hljs-comment">#kms_keyid = 01234567-89ab-cdef-0123-456789abcdef</span><br><br>[s3 alt]<br><span class="hljs-comment">## another user account, used for ACL-related tests</span><br>user_id = test02<br>display_name = test02<br><span class="hljs-comment">## the &quot;alt&quot; user needs to have email set, too</span><br>email = test02@qq.com<br>access_key = test02<br>secret_key = test02<br></code></pre></td></tr></table></figure><p>上面的用户信息是需要提前创建好的，这个用集群内的机器radosgw-admin命令创建即可</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">radosgw-admin user create --uid=test01 --display-name=test01 --access-key=test01 --secret-key=test01 --email=test01@qq.com<br>radosgw-admin user create --uid=test02 --display-name=test02 --access-key=test02 --secret-key=test02 --email=test02@qq.com<br></code></pre></td></tr></table></figure><p>创建好了以后就可以开始测试了</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs rwa">[root@lab101 s3-tests]# S3TEST_CONF=test.conf ./virtualenv/bin/nosetests -a &#x27;!fails_on_rgw&#x27;<br>..................................................SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS.....................................................................................................................SSSS.......................................................................................................................................SSSS.......................................................<br>----------------------------------------------------------------------<br>Ran 408 tests in 122.087s<br><br>OK (SKIP=51)<br></code></pre></td></tr></table></figure><p>正常测试完就应该是上面的ok的状态，也有可能某个版本的测试用例是写的支持，但是rgw也不一定就做好了，这个需要自己判断一下<br>##总结<br>了解软件适配的接口，针对接口进行相关测试即可</p><h2 id="变更记录"><a href="#变更记录" class="headerlink" title="变更记录"></a>变更记录</h2><table><thead><tr><th align="center">Why</th><th align="center">Who</th><th align="center">When</th></tr></thead><tbody><tr><td align="center">创建</td><td align="center">武汉-运维-磨渣</td><td align="center">2018-06-27</td></tr><tr><td align="center">修改配置文件用户错误</td><td align="center">武汉-运维-磨渣</td><td align="center">2018-09-04</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>ceph erasure默认的min_size分析</title>
    <link href="/2018/06/12/ceph%20erasure%E9%BB%98%E8%AE%A4%E7%9A%84min_size%E5%88%86%E6%9E%90/"/>
    <url>/2018/06/12/ceph%20erasure%E9%BB%98%E8%AE%A4%E7%9A%84min_size%E5%88%86%E6%9E%90/</url>
    
    <content type="html"><![CDATA[<h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>最近接触了两个集群都使用到了erasure code,一个集群是hammer版本的，一个环境是luminous版本的，两个环境都出现了incomplete，触发的原因有类似的地方，都是有osd的离线的问题</p><p>准备在本地环境进行复验的时候，发现了一个跟之前接触的erasure不同的地方，这里做个记录，以防后面出现同样的问题</p><h2 id="分析过程"><a href="#分析过程" class="headerlink" title="分析过程"></a>分析过程</h2><p>准备了一个luminous的集群，使用默认的erasure的profile进行了创建存储池的相关工作</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab102 ~]<span class="hljs-comment"># ceph osd erasure-code-profile get default</span><br>k=2<br>m=1<br>plugin=jerasure<br>technique=reed_sol_van<br></code></pre></td></tr></table></figure><p>默认的是2+1的纠删码的配置，创建完了以后存储池的配置是这样的</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab102 ~]<span class="hljs-comment"># ceph osd dump|grep pool</span><br>pool 1 <span class="hljs-string">&#x27;rbd&#x27;</span> erasure size 3 min_size 3 crush_rule 2 object_hash rjenkins pg_num 256 pgp_num 256 last_change 41 flags hashpspool stripe_width 8192 application rbdrc<br></code></pre></td></tr></table></figure><p>然后停止了一个osd以后，状态变成了这样的</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab102 ~]<span class="hljs-comment"># ceph -s</span><br>  cluster:<br>    <span class="hljs-built_in">id</span>:     9ec7768a-5e7c-4f8e-8a85-89895e338cca<br>    health: HEALTH_WARN<br>            1 osds down<br>            Reduced data availability: 42 pgs inactive, 131 pgs incomplete<br> <br>  services:<br>    mon: 1 daemons, quorum lab102<br>    mgr: lab102(active)<br>    osd: 6 osds: 5 up, 6 <span class="hljs-keyword">in</span><br> <br>  data:<br>    pools:   3 pools, 288 pgs<br>    objects: 1666k objects, 13331 MB<br>    usage:   319 GB used, 21659 GB / 21979 GB avail<br>    pgs:     45.486% pgs not active<br>             157 active+clean<br>             131 incomplete<br></code></pre></td></tr></table></figure><p>停止一个osd也会出现incomplete的状态，也就是在默认状态下，是一个osd也不允许down掉的，不然pg就进入了无法使用的状态，这个在我这里感觉无法理解的，开始以为这个是L版本的bug，在查了下资料以后，发现并不是的</p><p>查询到一个这样的patch<a href="https://patchwork.kernel.org/patch/8546771/">：default min_size for erasure pools</a></p><p>这个里面就讨论了min_size的问题，上面的环境我也发现了，默认的配置的2+1,这个在我的理解下，正常应该会配置为min_size 2,在down掉一个的时候还是可写，可读的</p><p>实际上在&#x2F;src&#x2F;mon&#x2F;OSDMonitor.cc 这个里面已经把erasure的min_size的控制改为了</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">*min_size = erasure_code-&gt;get_data_chunk_count();<br>变成<br>*min_size = erasure_code-&gt;get_data_chunk_count() + 1;<br></code></pre></td></tr></table></figure><p>最后面作者提出了自己的担心，假如在K+M的配置下，只有K个的osd允许可以读写的时候，环境是K个OSD是好的，M个OSD挂掉了，这个时候启动一个M中的osd的时候，会进行backfilling，这个时候如果K个osd当中的某个osd挂掉的话，这个时候实际上PG里面的数据就是不完整的，如果是K+1的时候，这个时候做恢复的时候再挂掉一个，实际上还是完整的，也就是开发者考虑的是恢复过程的异常状况还留一个冗余，这个实际我们在日常的维护过程当中也经常遇到恢复过程中确实有osd的挂掉的情况,这个在其他文件系统里面的做法是设计成可读不可写状态</p><p>也就是现在ceph的erasure的min_size设计成了</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">min_size=K+1<br></code></pre></td></tr></table></figure><p>也就是默认的环境下的是min_size是3</p><p>到这里就知道上面为什么会出现上面的状况了，也就是这个编码设置的时候需要自己去控制下，比如4+2的ec，最多能挂掉几个，如果在以前可以很肯定的说是2个，实际在新的情况下是4+1&#x3D;5也就是只允许挂掉一个是可读可写的</p><p>当然真正生产环境出现了4+2挂掉两个变成了incomplete的时候，因为这个时候数据还是完整可拼接的，所以可以强制mark-complete或者自己把代码里面的min_size改掉来触发恢复也是可以的</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>对于ec这块接触的很早，里面还是有很多有意思的可以研究的东西的，ec最适合的场景就是归档，当然在某些配置下面，性能也是很不错的，也能支持一些低延时的任务，这个最大的特点就是一定需要根据实际环境去跑性能测试，拆成几比几性能有多少，这个一般还是不太好预估的，跟写入的文件模型也有关联</p><p>虽然作者的设计初衷是没问题的，但是这个默认配置实际是不符合生产要求的，所以个人觉得这个不是很合理，默认的应该是不需要调整也是可用的，一个osd也不允许down的话，真正也没法用起来，所以不清楚是否有其他可改变的配置来处理这个，自己配置的时候注意下这个min_size，如果未来有控制的参数，会补充进这篇文章</p><h2 id="补充"><a href="#补充" class="headerlink" title="补充"></a>补充</h2><p>通过测试发现，可以通过存储池设置这个min_size来实现继续使用</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">ceph osd pool <span class="hljs-built_in">set</span> rbd min_size 2<br></code></pre></td></tr></table></figure><p>也就是这个地方跟副本池的设计类似，给定一个初始值，然后可以通过设置进行修改</p><h3 id="官方已经更新这里"><a href="#官方已经更新这里" class="headerlink" title="官方已经更新这里"></a>官方已经更新这里</h3><p><a href="https://github.com/ceph/ceph/pull/8008">https://github.com/ceph/ceph/pull/8008</a></p><blockquote><p>Default min_size to k+1<br><br>已经准备改成了<br><br>min_size &#x3D; k + min(1, m - 1)</p></blockquote><h2 id="变更记录"><a href="#变更记录" class="headerlink" title="变更记录"></a>变更记录</h2><table><thead><tr><th align="left">Why</th><th align="center">Who</th><th align="center">When</th></tr></thead><tbody><tr><td align="left">创建</td><td align="center">武汉-运维-磨渣</td><td align="center">2018-06-12</td></tr><tr><td align="left">更新ec的min_size设置</td><td align="center">武汉-运维-磨渣</td><td align="center">2018-06-12</td></tr><tr><td align="left">官方修改ec的min_size设置</td><td align="center">武汉-运维-磨渣</td><td align="center">2019-03-21</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>cephfs元数据池故障的恢复</title>
    <link href="/2018/05/29/cephfs%E5%85%83%E6%95%B0%E6%8D%AE%E6%B1%A0%E6%95%85%E9%9A%9C%E7%9A%84%E6%81%A2%E5%A4%8D/"/>
    <url>/2018/05/29/cephfs%E5%85%83%E6%95%B0%E6%8D%AE%E6%B1%A0%E6%95%85%E9%9A%9C%E7%9A%84%E6%81%A2%E5%A4%8D/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>cephfs 在L版本已经比较稳定了，这个稳定的意义个人觉得是在其故障恢复方面的成熟，一个文件系统可恢复是其稳定必须具备的属性，本篇就是根据官网的文档来实践下这个恢复的过程</p><h2 id="实践过程"><a href="#实践过程" class="headerlink" title="实践过程"></a>实践过程</h2><h3 id="部署一个ceph-Luminous集群"><a href="#部署一个ceph-Luminous集群" class="headerlink" title="部署一个ceph  Luminous集群"></a>部署一个ceph  Luminous集群</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab102 ~]<span class="hljs-comment"># ceph -v</span><br>ceph version 12.2.5 (cad919881333ac92274171586c827e01f554a70a) luminous (stable)<br></code></pre></td></tr></table></figure><p>创建filestore</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">ceph-deploy osd create  lab102  --filestore  --data /dev/sdb1  --journal /dev/sdb2<br></code></pre></td></tr></table></figure><p>这里想用filestore进行测试就按上面的方法去创建osd即可</p><p>传入测试数据</p><ul><li>doc </li><li>pic</li><li>vidio<br>这里提供下载链接</li></ul><blockquote><p>链接：<a href="https://pan.baidu.com/s/19tlFi4butA2WjnPAdNEMwg">https://pan.baidu.com/s/19tlFi4butA2WjnPAdNEMwg</a> 密码：ugjo</p></blockquote><p>这个是网上下载的模板的数据，方便进行真实的文件的模拟，dd产生的是空文件，有的时候会影响到测试</p><p>需要更多的测试文档推荐可以从下面网站下载</p><p>视频下载：</p><blockquote><p><a href="https://videos.pexels.com/popular-videos">https://videos.pexels.com/popular-videos</a></p></blockquote><p>图片下载：</p><blockquote><p><a href="https://www.pexels.com/">https://www.pexels.com/</a></p></blockquote><p>文档下载：</p><blockquote><p><a href="http://office.mmais.com.cn/Template/Home.shtml">http://office.mmais.com.cn/Template/Home.shtml</a></p></blockquote><h3 id="元数据模拟故障"><a href="#元数据模拟故障" class="headerlink" title="元数据模拟故障"></a>元数据模拟故障</h3><p>跟元数据相关的故障无非就是mds无法启动，或者元数据pg损坏了，这里我们模拟的比较极端的情况，把metadata的元数据对象全部清空掉，这个基本能覆盖到最严重的故障了，数据的损坏不在元数据损坏的范畴</p><p>清空元数据存储池</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-keyword">for</span> object <span class="hljs-keyword">in</span> `rados -p metadata <span class="hljs-built_in">ls</span>`;<span class="hljs-keyword">do</span> rados -p metadata <span class="hljs-built_in">rm</span> <span class="hljs-variable">$object</span>;<span class="hljs-keyword">done</span><br></code></pre></td></tr></table></figure><p>重启下mds进程，应该mds是无法恢复正常的</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs bash">cluster:<br>    <span class="hljs-built_in">id</span>:     9ec7768a-5e7c-4f8e-8a85-89895e338cca<br>    health: HEALTH_ERR<br>            1 filesystem is degraded<br>            1 mds daemon damaged<br>            too few PGs per OSD (16 &lt; min 30)<br> <br>  services:<br>    mon: 1 daemons, quorum lab102<br>    mgr: lab102(active)<br>    mds: ceph-0/1/1 up , 1 up:standby, 1 damaged<br>    osd: 1 osds: 1 up, 1 <span class="hljs-keyword">in</span><br></code></pre></td></tr></table></figure><p>准备开始我们的修复过程</p><h3 id="元数据故障恢复"><a href="#元数据故障恢复" class="headerlink" title="元数据故障恢复"></a>元数据故障恢复</h3><p>设置允许多文件系统</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">ceph fs flag <span class="hljs-built_in">set</span> enable_multiple <span class="hljs-literal">true</span> --yes-i-really-mean-it<br></code></pre></td></tr></table></figure><p>创建一个新的元数据池，这里是为了不去动原来的metadata的数据，以免损坏原来的元数据</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">ceph osd pool create recovery 8<br></code></pre></td></tr></table></figure><p>将老的存储池data和新的元数据池recovery关联起来并且创建一个新的recovery-fs</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab102 ~]<span class="hljs-comment"># ceph fs new recovery-fs recovery data --allow-dangerous-metadata-overlay</span><br>new fs with metadata pool 3 and data pool 2<br></code></pre></td></tr></table></figure><p>做下新的文件系统的初始化相关工作</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab102 ~]<span class="hljs-comment">#cephfs-data-scan init --force-init --filesystem recovery-fs --alternate-pool recovery</span><br></code></pre></td></tr></table></figure><p>reset下新的fs</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab102 ~]<span class="hljs-comment">#ceph fs reset recovery-fs --yes-i-really-mean-it</span><br>[root@lab102 ~]<span class="hljs-comment">#cephfs-table-tool recovery-fs:all reset session</span><br>[root@lab102 ~]<span class="hljs-comment">#cephfs-table-tool recovery-fs:all reset snap</span><br>[root@lab102 ~]<span class="hljs-comment">#cephfs-table-tool recovery-fs:all reset inode</span><br></code></pre></td></tr></table></figure><p>做相关的恢复</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab102 ~]<span class="hljs-comment"># cephfs-data-scan scan_extents --force-pool --alternate-pool recovery --filesystem ceph  data</span><br>[root@lab102 ~]<span class="hljs-comment"># cephfs-data-scan scan_inodes --alternate-pool recovery --filesystem ceph --force-corrupt --force-init data</span><br>[root@lab102 ~]<span class="hljs-comment"># cephfs-data-scan scan_links --filesystem recovery-fs</span><br></code></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab102 ~]<span class="hljs-comment"># systemctl start ceph-mds@lab102</span><br>等待mds active 以后再继续下面操作<br>[root@lab102 ~]<span class="hljs-comment"># ceph daemon mds.lab102 scrub_path / recursive repair</span><br></code></pre></td></tr></table></figure><p>设置成默认的fs</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab102 ~]<span class="hljs-comment"># ceph fs set-default recovery-fs</span><br></code></pre></td></tr></table></figure><p>挂载检查数据</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab102 ~]<span class="hljs-comment">#  mount -t ceph 192.168.19.102:/ /mnt</span><br>[root@lab102 ~]<span class="hljs-comment"># ll /mnt</span><br>total 0<br>drwxr-xr-x 1 root root 1 Jan  1  1970 lost+found<br>[root@lab102 ~]<span class="hljs-comment"># ll /mnt/lost+found/</span><br>total 226986<br>-r-x------ 1 root root   569306 May 25 16:16 10000000001<br>-r-x------ 1 root root 16240627 May 25 16:16 10000000002<br>-r-x------ 1 root root  1356367 May 25 16:16 10000000003<br>-r-x------ 1 root root   137729 May 25 16:16 10000000004<br>-r-x------ 1 root root   155163 May 25 16:16 10000000005<br>-r-x------ 1 root root   118909 May 25 16:16 10000000006<br>-r-x------ 1 root root  1587656 May 25 16:16 10000000007<br>-r-x------ 1 root root   252705 May 25 16:16 10000000008<br>-r-x------ 1 root root  1825192 May 25 16:16 10000000009<br>-r-x------ 1 root root   156990 May 25 16:16 1000000000a<br>-r-x------ 1 root root  3493435 May 25 16:16 1000000000b<br>-r-x------ 1 root root   342390 May 25 16:16 1000000000c<br>-r-x------ 1 root root  1172247 May 25 16:16 1000000000d<br>-r-x------ 1 root root  2516169 May 25 16:16 1000000000e<br>-r-x------ 1 root root  3218770 May 25 16:16 1000000000f<br>-r-x------ 1 root root   592729 May 25 16:16 10000000010<br></code></pre></td></tr></table></figure><p>可以看到在lost+found里面就有数据了</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab102 ~]<span class="hljs-comment"># file /mnt/lost+found/10000000010 </span><br>/mnt/lost+found/10000000010: Microsoft PowerPoint 2007+<br>[root@lab102 ~]<span class="hljs-comment"># file /mnt/lost+found/10000000011</span><br>/mnt/lost+found/10000000011: Microsoft Word 2007+<br>[root@lab102 ~]<span class="hljs-comment"># file /mnt/lost+found/10000000012</span><br>/mnt/lost+found/10000000012: Microsoft Word 2007+<br>[root@lab102 ~]<span class="hljs-comment"># file /mnt/lost+found/10000000013</span><br>/mnt/lost+found/10000000013: Microsoft PowerPoint 2007+<br></code></pre></td></tr></table></figure><p>这个生成的文件名称就是实际文件存储的数据的prifix，也就是通过原始inode进行的运算得到的</p><p>如果提前备份好了原始的元数据信息</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab102 ~]<span class="hljs-comment"># ceph daemon mds.lab102 dump cache &gt; /tmp/mdscache</span><br></code></pre></td></tr></table></figure><p>那么可以比较轻松的找到丢失的文件</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>在我另外一篇文章当中已经写过了，通过文件的inode可以把文件跟后台的对象结合起来，在以前我的恢复的思路是，把后台的对象全部抓出来，然后自己手动去对对象进行拼接，实际是数据存在的情况下，反向把文件重新link到一个路径，这个是官方提供的的恢复方法，mds最大的担心就是mds自身的元数据的损坏可能引起整个文件系统的崩溃，而现在，基本上只要data的数据还在的话，就不用担心数据丢掉，即使文件路径信息没有了，但是文件还在</p><p>通过备份mds cache可以把文件名称，路径，大小和inode关联起来，而恢复的数据是对象前缀，也就是备份好了mds cache 就可以把整个文件信息串联起来了</p><p>虽然cephfs的故障不是常发生，但是万一呢</p><p>后续准备带来一篇关于cephfs从挂载点误删除数据后的数据恢复的方案，这个目前已经进行了少量文件的恢复试验了，等后续进行大量文件删除的恢复后，再进行分享</p><h2 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h2><p><a href="http://docs.ceph.com/docs/luminous/cephfs/disaster-recovery/">disaster-recovery</a></p><h2 id="变更记录"><a href="#变更记录" class="headerlink" title="变更记录"></a>变更记录</h2><table><thead><tr><th align="center">Why</th><th align="center">Who</th><th align="center">When</th></tr></thead><tbody><tr><td align="center">创建</td><td align="center">武汉-运维-磨渣</td><td align="center">2018-05-29</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>cosbench使用方法</title>
    <link href="/2018/04/12/cosbench%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95/"/>
    <url>/2018/04/12/cosbench%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<p>##前言<br>cosbench的功能很强大，但是配置起来可能就有点不是太清楚怎么配置了，本篇将梳理一下这个测试的配置过程，以及一些测试注意项目，以免无法完成自己配置模型的情况</p><p>##安装<br>cosbench模式是一个控制端控制几个driver向后端rgw发起请求</p><p>下载最新版本</p><blockquote><p><a href="https://github.com/intel-cloud/cosbench/releases/download/v0.4.2.c4/0.4.2.c4.zip">https://github.com/intel-cloud/cosbench/releases/download/v0.4.2.c4/0.4.2.c4.zip</a></p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab102 cosbench]<span class="hljs-comment">#unzip 0.4.2.zip</span><br>[root@lab102 cosbench]<span class="hljs-comment">#yum install java-1.7.0-openjdk nmap-ncat</span><br></code></pre></td></tr></table></figure><p>访问地址</p><blockquote><p><a href="http://host-ip:19088/controller/index.html">http://HOST-IP:19088/controller/index.html</a></p></blockquote><p>同时可以执行的workloads的个数通过下面的control参数控制</p><blockquote><p>concurrency&#x3D;1</p></blockquote><p>默认是一个，这个为了保证单机的硬件资源足够，保持单机启用一个workload</p><p>创建一个s3用户</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ~]<span class="hljs-comment"># radosgw-admin user create --uid=test1 --display-name=&quot;test1&quot; --access-key=test1  --secret-key=test1</span><br>&#123;<br>    <span class="hljs-string">&quot;user_id&quot;</span>: <span class="hljs-string">&quot;test1&quot;</span>,<br>    <span class="hljs-string">&quot;display_name&quot;</span>: <span class="hljs-string">&quot;test1&quot;</span>,<br>    <span class="hljs-string">&quot;email&quot;</span>: <span class="hljs-string">&quot;&quot;</span>,<br>    <span class="hljs-string">&quot;suspended&quot;</span>: 0,<br>    <span class="hljs-string">&quot;max_buckets&quot;</span>: 1000,<br>    <span class="hljs-string">&quot;auid&quot;</span>: 0,<br>    <span class="hljs-string">&quot;subusers&quot;</span>: [],<br>    <span class="hljs-string">&quot;keys&quot;</span>: [<br>        &#123;<br>            <span class="hljs-string">&quot;user&quot;</span>: <span class="hljs-string">&quot;test1&quot;</span>,<br>            <span class="hljs-string">&quot;access_key&quot;</span>: <span class="hljs-string">&quot;test1&quot;</span>,<br>            <span class="hljs-string">&quot;secret_key&quot;</span>: <span class="hljs-string">&quot;test1&quot;</span><br>        &#125;<br>    ],<br>    <span class="hljs-string">&quot;swift_keys&quot;</span>: [],<br>    <span class="hljs-string">&quot;caps&quot;</span>: [],<br>    <span class="hljs-string">&quot;op_mask&quot;</span>: <span class="hljs-string">&quot;read, write, delete&quot;</span>,<br>    <span class="hljs-string">&quot;default_placement&quot;</span>: <span class="hljs-string">&quot;&quot;</span>,<br>    <span class="hljs-string">&quot;placement_tags&quot;</span>: [],<br>    <span class="hljs-string">&quot;bucket_quota&quot;</span>: &#123;<br>        <span class="hljs-string">&quot;enabled&quot;</span>: <span class="hljs-literal">false</span>,<br>        <span class="hljs-string">&quot;max_size_kb&quot;</span>: -1,<br>        <span class="hljs-string">&quot;max_objects&quot;</span>: -1<br>    &#125;,<br>    <span class="hljs-string">&quot;user_quota&quot;</span>: &#123;<br>        <span class="hljs-string">&quot;enabled&quot;</span>: <span class="hljs-literal">false</span>,<br>        <span class="hljs-string">&quot;max_size_kb&quot;</span>: -1,<br>        <span class="hljs-string">&quot;max_objects&quot;</span>: -1<br>    &#125;,<br>    <span class="hljs-string">&quot;temp_url_keys&quot;</span>: []<br>&#125;<br><br><br></code></pre></td></tr></table></figure><p>##配置相关</p><p>cosbench的配置文件结构<br><img src="/images/blog/o_200925030632cosbench2.png" alt="image.png-47.7kB"></p><ul><li>一个workload 可以定义一个或者多个work stages</li><li>执行多个work stages是顺序的，执行同一个work stage里面的work是可以并行执行的</li><li>每个work里面，worker是来调整负载的</li><li>认证可以多个级别的定义，低级别的认证会覆盖高级别的配置</li></ul><p>可以通过配置多个work的方式来实现并发，而在work内通过增加worker的方式增加并发，从而实现多对多的访问，worker的分摊是分到了driver上面，注意多work的时候的containers不要重名，划分好bucker的空间</p><p><img src="/images/blog/o_200925030638cosbench3.png" alt="image.png-144.5kB"></p><p>work相关的说明</p><ul><li>可以通过写入时间，写入容量，写入iops来控制什么时候结束</li><li>interval默认是5s是用来对性能快照的间隔，可以理解为采样点</li><li>division 控制workers之间的分配工作的方式是bucket还是对象还是none</li><li>默认全部的driver参与工作，也可以通过参数控制部分driver参与</li><li>时间会控制执行，如果时间没到，但是指定的对象已经写完了的话就会去进行复写的操作，这里要注意是进行对象的控制还是时间的控制进行的测试</li></ul><p>如果读取测试的时候，如果没有那个对象，会中断的提示，所以测试读之前需要把测试的对象都填充完毕（最好检查下先）</p><p>##单项的配置文件</p><p>###通过单网关创建bucket</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs bash">&lt;?xml version=<span class="hljs-string">&quot;1.0&quot;</span> encoding=<span class="hljs-string">&quot;UTF-8&quot;</span>?&gt;<br>&lt;workload name=<span class="hljs-string">&quot;create-bucket&quot;</span> description=<span class="hljs-string">&quot;create s3 bucket&quot;</span> config=<span class="hljs-string">&quot;&quot;</span>&gt;<br>    &lt;auth <span class="hljs-built_in">type</span>=<span class="hljs-string">&quot;none&quot;</span> config=<span class="hljs-string">&quot;&quot;</span>/&gt;<br>    &lt;workflow config=<span class="hljs-string">&quot;&quot;</span>&gt;<br>        &lt;workstage name=<span class="hljs-string">&quot;create bucket&quot;</span> closuredelay=<span class="hljs-string">&quot;0&quot;</span> config=<span class="hljs-string">&quot;&quot;</span>&gt;<br>            &lt;auth <span class="hljs-built_in">type</span>=<span class="hljs-string">&quot;none&quot;</span> config=<span class="hljs-string">&quot;&quot;</span>/&gt;<br>            &lt;work name=<span class="hljs-string">&quot;rgw1&quot;</span> <span class="hljs-built_in">type</span>=<span class="hljs-string">&quot;init&quot;</span> workers=<span class="hljs-string">&quot;2&quot;</span> interval=<span class="hljs-string">&quot;5&quot;</span><br>                division=<span class="hljs-string">&quot;container&quot;</span> runtime=<span class="hljs-string">&quot;0&quot;</span> rampup=<span class="hljs-string">&quot;0&quot;</span> rampdown=<span class="hljs-string">&quot;0&quot;</span><br>                afr=<span class="hljs-string">&quot;0&quot;</span> totalOps=<span class="hljs-string">&quot;1&quot;</span> totalBytes=<span class="hljs-string">&quot;0&quot;</span> config=<span class="hljs-string">&quot;cprefix=zp;containers=r(1,8)&quot;</span>&gt;<br>                &lt;auth <span class="hljs-built_in">type</span>=<span class="hljs-string">&quot;none&quot;</span> config=<span class="hljs-string">&quot;&quot;</span>/&gt;<br>                &lt;storage <span class="hljs-built_in">type</span>=<span class="hljs-string">&quot;s3&quot;</span> config=<span class="hljs-string">&quot;accesskey=test1;secretkey=test1;endpoint=http://66.66.66.63:7481;path_style_access=true&quot;</span>/&gt;<br>            &lt;/work&gt;<br>        &lt;/workstage&gt;<br>    &lt;/workflow&gt;<br>&lt;/workload&gt;<br></code></pre></td></tr></table></figure><p>执行完建议检查下是不是创建了这么多的bucket，之前测试的时候发现即使没有创建成功bucket，在put的时候也不会报错，bucket并没有创建成功，读取的时候才抛出的错，总之每做一步确认下上一步的结果</p><p>执行检查</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ~]<span class="hljs-comment"># radosgw-admin bucket list</span><br>[<br>    <span class="hljs-string">&quot;zp2&quot;</span>,<br>    <span class="hljs-string">&quot;zp8&quot;</span>,<br>    <span class="hljs-string">&quot;zp7&quot;</span>,<br>    <span class="hljs-string">&quot;zp3&quot;</span>,<br>    <span class="hljs-string">&quot;zp4&quot;</span>,<br>    <span class="hljs-string">&quot;zp6&quot;</span>,<br>    <span class="hljs-string">&quot;zp5&quot;</span>,<br>    <span class="hljs-string">&quot;zp1&quot;</span><br>]<br></code></pre></td></tr></table></figure><p>如上配置的时候，如果设置的是workers&#x3D;1,那么就会从当前的driver中挑选一个driver出来，然后选择配置storage进行bucket的创建，如果设置的是workers&#x3D;2，那么就会挑选两个driver出来进行创建，一个driver负责一半的工作，相当于两个客户端同时向一个网关发起创建的操作,cprefix参数是控制创建的bucket的前缀的</p><p>rgw的网关是对等的关系，那么这里肯定就有另外一种配置，我想通过不只一个网关进行创建的操作，那么这个地方是通过增加work的配置来实现的，我们看下配置</p><p>###通过多网关创建bucket</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs bash">&lt;?xml version=<span class="hljs-string">&quot;1.0&quot;</span> encoding=<span class="hljs-string">&quot;UTF-8&quot;</span>?&gt;<br>&lt;workload name=<span class="hljs-string">&quot;create-bucket&quot;</span> description=<span class="hljs-string">&quot;create s3 bucket&quot;</span> config=<span class="hljs-string">&quot;&quot;</span>&gt;<br>    &lt;auth <span class="hljs-built_in">type</span>=<span class="hljs-string">&quot;none&quot;</span> config=<span class="hljs-string">&quot;&quot;</span>/&gt;<br>    &lt;workflow config=<span class="hljs-string">&quot;&quot;</span>&gt;<br>        &lt;workstage name=<span class="hljs-string">&quot;create bucket&quot;</span> closuredelay=<span class="hljs-string">&quot;0&quot;</span> config=<span class="hljs-string">&quot;&quot;</span>&gt;<br>            &lt;auth <span class="hljs-built_in">type</span>=<span class="hljs-string">&quot;none&quot;</span> config=<span class="hljs-string">&quot;&quot;</span>/&gt;<br>            &lt;work name=<span class="hljs-string">&quot;rgw1&quot;</span> <span class="hljs-built_in">type</span>=<span class="hljs-string">&quot;init&quot;</span> workers=<span class="hljs-string">&quot;2&quot;</span> interval=<span class="hljs-string">&quot;5&quot;</span><br>                division=<span class="hljs-string">&quot;container&quot;</span> runtime=<span class="hljs-string">&quot;0&quot;</span> rampup=<span class="hljs-string">&quot;0&quot;</span> rampdown=<span class="hljs-string">&quot;0&quot;</span><br>                afr=<span class="hljs-string">&quot;0&quot;</span> totalOps=<span class="hljs-string">&quot;1&quot;</span> totalBytes=<span class="hljs-string">&quot;0&quot;</span> config=<span class="hljs-string">&quot;cprefix=zp;containers=r(1,4)&quot;</span>&gt;<br>                &lt;auth <span class="hljs-built_in">type</span>=<span class="hljs-string">&quot;none&quot;</span> config=<span class="hljs-string">&quot;&quot;</span>/&gt;<br>                &lt;storage <span class="hljs-built_in">type</span>=<span class="hljs-string">&quot;s3&quot;</span> config=<span class="hljs-string">&quot;accesskey=test1;secretkey=test1;endpoint=http://66.66.66.63:7481;path_style_access=true&quot;</span>/&gt;<br>            &lt;/work&gt;<br>            &lt;work name=<span class="hljs-string">&quot;rgw2&quot;</span> <span class="hljs-built_in">type</span>=<span class="hljs-string">&quot;init&quot;</span> workers=<span class="hljs-string">&quot;2&quot;</span> interval=<span class="hljs-string">&quot;5&quot;</span><br>                division=<span class="hljs-string">&quot;container&quot;</span> runtime=<span class="hljs-string">&quot;0&quot;</span> rampup=<span class="hljs-string">&quot;0&quot;</span> rampdown=<span class="hljs-string">&quot;0&quot;</span><br>                afr=<span class="hljs-string">&quot;0&quot;</span> totalOps=<span class="hljs-string">&quot;1&quot;</span> totalBytes=<span class="hljs-string">&quot;0&quot;</span> config=<span class="hljs-string">&quot;cprefix=zp;containers=r(4,8)&quot;</span>&gt;<br>                &lt;auth <span class="hljs-built_in">type</span>=<span class="hljs-string">&quot;none&quot;</span> config=<span class="hljs-string">&quot;&quot;</span>/&gt;<br>                &lt;storage <span class="hljs-built_in">type</span>=<span class="hljs-string">&quot;s3&quot;</span> config=<span class="hljs-string">&quot;accesskey=test1;secretkey=test1;endpoint=http://66.66.66.63:7482;path_style_access=true&quot;</span>/&gt;<br>            &lt;/work&gt;<br>        &lt;/workstage&gt;<br>    &lt;/workflow&gt;<br>&lt;/workload&gt;<br></code></pre></td></tr></table></figure><p>以上配置就实现了通过两个网关进行创建bucket的配置了，下面是做prepare的相关配置，在cosbench里面有两个部分可以进行写操作，在prepare stage里面和 main stage里面<br>这个地方这样设置的理由是：<br>如果有读和写混合测试的时候，那么就需要提前进行读数据的准备，然后再开始进行读写并发的测试，所以会有一个prepare的阶段，这个在配置文件里面只是type设置的不同，其他没区别，我们可以看下这里web界面里面提供的配置项目，下面其他项目默认都是采取双并发的模式</p><p><img src="/images/blog/o_200925030645cosbench4.png" alt="prepare"></p><p>在写的部分是一样的</p><p>###通过多网关写数据</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs bash">&lt;?xml version=<span class="hljs-string">&quot;1.0&quot;</span> encoding=<span class="hljs-string">&quot;UTF-8&quot;</span>?&gt;<br>&lt;workload name=<span class="hljs-string">&quot;create-object&quot;</span> description=<span class="hljs-string">&quot;create object&quot;</span> config=<span class="hljs-string">&quot;&quot;</span>&gt;<br>    &lt;auth <span class="hljs-built_in">type</span>=<span class="hljs-string">&quot;none&quot;</span> config=<span class="hljs-string">&quot;&quot;</span>/&gt; <br>&lt;workflow config=<span class="hljs-string">&quot;&quot;</span>&gt;<br>&lt;workstage name=<span class="hljs-string">&quot;putobject&quot;</span> closuredelay=<span class="hljs-string">&quot;0&quot;</span> config=<span class="hljs-string">&quot;&quot;</span>&gt;<br>            &lt;auth <span class="hljs-built_in">type</span>=<span class="hljs-string">&quot;none&quot;</span> config=<span class="hljs-string">&quot;&quot;</span>/&gt;<br>            &lt;work name=<span class="hljs-string">&quot;rgw1-put-4M&quot;</span> <span class="hljs-built_in">type</span>=<span class="hljs-string">&quot;normal&quot;</span> workers=<span class="hljs-string">&quot;2&quot;</span> interval=<span class="hljs-string">&quot;5&quot;</span><br>                division=<span class="hljs-string">&quot;container&quot;</span> runtime=<span class="hljs-string">&quot;0&quot;</span> rampup=<span class="hljs-string">&quot;0&quot;</span> rampdown=<span class="hljs-string">&quot;0&quot;</span><br>                afr=<span class="hljs-string">&quot;200000&quot;</span> totalOps=<span class="hljs-string">&quot;240&quot;</span> totalBytes=<span class="hljs-string">&quot;0&quot;</span> config=<span class="hljs-string">&quot;&quot;</span>&gt;<br>                &lt;auth <span class="hljs-built_in">type</span>=<span class="hljs-string">&quot;none&quot;</span> config=<span class="hljs-string">&quot;&quot;</span>/&gt;<br>                &lt;storage <span class="hljs-built_in">type</span>=<span class="hljs-string">&quot;s3&quot;</span> config=<span class="hljs-string">&quot;timeout=300000;accesskey=test1;secretkey=test1;endpoint=http://66.66.66.63:7481;path_style_access=true&quot;</span>/&gt;<br>                &lt;operation <span class="hljs-built_in">type</span>=<span class="hljs-string">&quot;write&quot;</span> ratio=<span class="hljs-string">&quot;100&quot;</span> division=<span class="hljs-string">&quot;none&quot;</span><br>                    config=<span class="hljs-string">&quot;cprefix=zp;containers=r(1,4);oprefix=hj;objects=r(1,240);sizes=c(64)KB&quot;</span> <span class="hljs-built_in">id</span>=<span class="hljs-string">&quot;op1&quot;</span>/&gt;<br>            &lt;/work&gt;<br><br>            &lt;work name=<span class="hljs-string">&quot;rgw2-put-4M&quot;</span> <span class="hljs-built_in">type</span>=<span class="hljs-string">&quot;normal&quot;</span> workers=<span class="hljs-string">&quot;2&quot;</span> interval=<span class="hljs-string">&quot;5&quot;</span><br>                division=<span class="hljs-string">&quot;container&quot;</span> runtime=<span class="hljs-string">&quot;0&quot;</span> rampup=<span class="hljs-string">&quot;0&quot;</span> rampdown=<span class="hljs-string">&quot;0&quot;</span><br>                afr=<span class="hljs-string">&quot;200000&quot;</span> totalOps=<span class="hljs-string">&quot;240&quot;</span> totalBytes=<span class="hljs-string">&quot;0&quot;</span> config=<span class="hljs-string">&quot;&quot;</span>&gt;<br>                &lt;auth <span class="hljs-built_in">type</span>=<span class="hljs-string">&quot;none&quot;</span> config=<span class="hljs-string">&quot;&quot;</span>/&gt;<br>                &lt;storage <span class="hljs-built_in">type</span>=<span class="hljs-string">&quot;s3&quot;</span> config=<span class="hljs-string">&quot;timeout=300000;accesskey=test1;secretkey=test1;endpoint=http://66.66.66.63:7482;path_style_access=true&quot;</span>/&gt;<br>                &lt;operation <span class="hljs-built_in">type</span>=<span class="hljs-string">&quot;write&quot;</span> ratio=<span class="hljs-string">&quot;100&quot;</span> division=<span class="hljs-string">&quot;none&quot;</span><br>                    config=<span class="hljs-string">&quot;cprefix=zp;containers=r(5,8);oprefix=hj;objects=r(1,240);sizes=c(64)KB&quot;</span> <span class="hljs-built_in">id</span>=<span class="hljs-string">&quot;op1&quot;</span>/&gt;<br>            &lt;/work&gt;<br><br>        &lt;/workstage&gt;<br>    &lt;/workflow&gt;<br>&lt;/workload&gt;<br></code></pre></td></tr></table></figure><p>这里有几个参数可以注意一下：</p><blockquote><p>cprefix&#x3D;zp;containers&#x3D;r(1,4);oprefix&#x3D;hj;objects&#x3D;r(1,240);sizes&#x3D;c(64)KB” id&#x3D;”op1”</p></blockquote><p>控制写入的bucket的名称的，是全部散列还是把负载均分可以自己去控制，objects是指定写入bucke里面的对象的名称的，sizes是指定大小的，如果两个值不同，就是设置的范围，相同就是设置的指定大小的</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">runtime=<span class="hljs-string">&quot;0&quot;</span> rampup=<span class="hljs-string">&quot;0&quot;</span> rampdown=<span class="hljs-string">&quot;0&quot;</span> afr=<span class="hljs-string">&quot;200000&quot;</span> totalOps=<span class="hljs-string">&quot;240&quot;</span> totalBytes=<span class="hljs-string">&quot;0&quot;</span><br></code></pre></td></tr></table></figure><p>这个是控制写入什么时候中止的，可以通过时间，也可以通过总的ops，或者总的大小来控制，这个需求可以自己定，afr是控制允许的失效率的，单位为1百万分之</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">interval=<span class="hljs-string">&quot;5&quot;</span><br></code></pre></td></tr></table></figure><p>这个是控制抓取性能数据的周期的</p><p>写的部分还可以通过prepare控制，因为读和写需要对应上，不然读取会报错，所以这里还有一种方法写数据</p><p>我们在测试的时候，有时候有两种需求，一种是我只关注读取，那么就需要准备好测试数据，不用关心写入的性能，一种是我想看下复写的性能，那么也是需要把数据先填充完，那么这种情况的填充就不用iops控制，也不用时间控制了，我需要填充满，那么cosbench里面就提供了这种写法，就是prepare这个写类型做了的，我们看下配置文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs bash">&lt;?xml version=<span class="hljs-string">&quot;1.0&quot;</span> encoding=<span class="hljs-string">&quot;UTF-8&quot;</span>?&gt;<br>&lt;workload name=<span class="hljs-string">&quot;create-object&quot;</span> description=<span class="hljs-string">&quot;create object&quot;</span> config=<span class="hljs-string">&quot;&quot;</span>&gt;<br>    &lt;auth <span class="hljs-built_in">type</span>=<span class="hljs-string">&quot;none&quot;</span> config=<span class="hljs-string">&quot;&quot;</span>/&gt; <br><br>&lt;workflow config=<span class="hljs-string">&quot;&quot;</span>&gt;<br>&lt;workstage name=<span class="hljs-string">&quot;prepare-rgw1&quot;</span> closuredelay=<span class="hljs-string">&quot;0&quot;</span> config=<span class="hljs-string">&quot;&quot;</span>&gt;<br>            &lt;auth <span class="hljs-built_in">type</span>=<span class="hljs-string">&quot;none&quot;</span> config=<span class="hljs-string">&quot;&quot;</span>/&gt;<br>            &lt;work name=<span class="hljs-string">&quot;prepare&quot;</span> <span class="hljs-built_in">type</span>=<span class="hljs-string">&quot;prepare&quot;</span> workers=<span class="hljs-string">&quot;2&quot;</span> interval=<span class="hljs-string">&quot;5&quot;</span><br>                division=<span class="hljs-string">&quot;object&quot;</span> runtime=<span class="hljs-string">&quot;0&quot;</span> rampup=<span class="hljs-string">&quot;0&quot;</span> rampdown=<span class="hljs-string">&quot;0&quot;</span><br>                afr=<span class="hljs-string">&quot;0&quot;</span> totalOps=<span class="hljs-string">&quot;1&quot;</span> totalBytes=<span class="hljs-string">&quot;0&quot;</span> config=<span class="hljs-string">&quot;cprefix=zp;oprefix=hj;containers=r(1,4);objects=r(1,240);sizes=u(64,64)KB&quot;</span>&gt;<br>                &lt;storage <span class="hljs-built_in">type</span>=<span class="hljs-string">&quot;s3&quot;</span> config=<span class="hljs-string">&quot;timeout=300000;accesskey=test1;secretkey=test1;endpoint=http://66.66.66.63:7481;path_style_access=true&quot;</span>/&gt;<br>            &lt;/work&gt;<br><br>&lt;work name=<span class="hljs-string">&quot;prepare-rgw2&quot;</span> <span class="hljs-built_in">type</span>=<span class="hljs-string">&quot;prepare&quot;</span> workers=<span class="hljs-string">&quot;2&quot;</span> interval=<span class="hljs-string">&quot;5&quot;</span><br>                division=<span class="hljs-string">&quot;object&quot;</span> runtime=<span class="hljs-string">&quot;0&quot;</span> rampup=<span class="hljs-string">&quot;0&quot;</span> rampdown=<span class="hljs-string">&quot;0&quot;</span><br>                afr=<span class="hljs-string">&quot;0&quot;</span> totalOps=<span class="hljs-string">&quot;1&quot;</span> totalBytes=<span class="hljs-string">&quot;0&quot;</span> config=<span class="hljs-string">&quot;cprefix=zp;oprefix=hj;containers=r(5,8);objects=r(1,240);sizes=u(64,64)KB&quot;</span>&gt;<br>                &lt;storage <span class="hljs-built_in">type</span>=<span class="hljs-string">&quot;s3&quot;</span> config=<span class="hljs-string">&quot;timeout=300000;accesskey=test1;secretkey=test1;endpoint=http://66.66.66.63:7482;path_style_access=true&quot;</span>/&gt;<br>            &lt;/work&gt;<br>        &lt;/workstage&gt;<br>    &lt;/workflow&gt;<br>&lt;/workload&gt;<br></code></pre></td></tr></table></figure><p>division这个参数用下面的这张图比较好解释</p><p><img src="http://static.zybuluo.com/zphj1987/isguvl8srcixm3tppgreloaa/image.png" alt="division"></p><p>写入的配置就完了</p><p>###并发读取的配置</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs bash">&lt;?xml version=<span class="hljs-string">&quot;1.0&quot;</span> encoding=<span class="hljs-string">&quot;UTF-8&quot;</span>?&gt;<br>&lt;workload name=<span class="hljs-string">&quot;read-object&quot;</span> description=<span class="hljs-string">&quot;create object&quot;</span> config=<span class="hljs-string">&quot;&quot;</span>&gt;<br>    &lt;auth <span class="hljs-built_in">type</span>=<span class="hljs-string">&quot;none&quot;</span> config=<span class="hljs-string">&quot;&quot;</span>/&gt; <br>&lt;workflow config=<span class="hljs-string">&quot;&quot;</span>&gt;<br>&lt;workstage name=<span class="hljs-string">&quot;readobject&quot;</span> closuredelay=<span class="hljs-string">&quot;0&quot;</span> config=<span class="hljs-string">&quot;&quot;</span>&gt;<br>            &lt;auth <span class="hljs-built_in">type</span>=<span class="hljs-string">&quot;none&quot;</span> config=<span class="hljs-string">&quot;&quot;</span>/&gt;<br>            &lt;work name=<span class="hljs-string">&quot;rgw1-put-4M&quot;</span> <span class="hljs-built_in">type</span>=<span class="hljs-string">&quot;normal&quot;</span> workers=<span class="hljs-string">&quot;2&quot;</span> interval=<span class="hljs-string">&quot;5&quot;</span><br>                division=<span class="hljs-string">&quot;container&quot;</span> runtime=<span class="hljs-string">&quot;0&quot;</span> rampup=<span class="hljs-string">&quot;0&quot;</span> rampdown=<span class="hljs-string">&quot;0&quot;</span><br>                afr=<span class="hljs-string">&quot;200000&quot;</span> totalOps=<span class="hljs-string">&quot;240&quot;</span> totalBytes=<span class="hljs-string">&quot;0&quot;</span> config=<span class="hljs-string">&quot;&quot;</span>&gt;<br>                &lt;auth <span class="hljs-built_in">type</span>=<span class="hljs-string">&quot;none&quot;</span> config=<span class="hljs-string">&quot;&quot;</span>/&gt;<br>                &lt;storage <span class="hljs-built_in">type</span>=<span class="hljs-string">&quot;s3&quot;</span> config=<span class="hljs-string">&quot;timeout=300000;accesskey=test1;secretkey=test1;endpoint=http://66.66.66.63:7481;path_style_access=true&quot;</span>/&gt;<br>                &lt;operation <span class="hljs-built_in">type</span>=<span class="hljs-string">&quot;read&quot;</span> ratio=<span class="hljs-string">&quot;100&quot;</span> division=<span class="hljs-string">&quot;none&quot;</span><br>                    config=<span class="hljs-string">&quot;cprefix=zp;containers=r(1,4);oprefix=hj;objects=r(1,240);sizes=c(64)KB&quot;</span> <span class="hljs-built_in">id</span>=<span class="hljs-string">&quot;op1&quot;</span>/&gt;<br>            &lt;/work&gt;<br><br>            &lt;work name=<span class="hljs-string">&quot;rgw2-read-4M&quot;</span> <span class="hljs-built_in">type</span>=<span class="hljs-string">&quot;normal&quot;</span> workers=<span class="hljs-string">&quot;2&quot;</span> interval=<span class="hljs-string">&quot;5&quot;</span><br>                division=<span class="hljs-string">&quot;container&quot;</span> runtime=<span class="hljs-string">&quot;0&quot;</span> rampup=<span class="hljs-string">&quot;0&quot;</span> rampdown=<span class="hljs-string">&quot;0&quot;</span><br>                afr=<span class="hljs-string">&quot;200000&quot;</span> totalOps=<span class="hljs-string">&quot;240&quot;</span> totalBytes=<span class="hljs-string">&quot;0&quot;</span> config=<span class="hljs-string">&quot;&quot;</span>&gt;<br>                &lt;auth <span class="hljs-built_in">type</span>=<span class="hljs-string">&quot;none&quot;</span> config=<span class="hljs-string">&quot;&quot;</span>/&gt;<br>                &lt;storage <span class="hljs-built_in">type</span>=<span class="hljs-string">&quot;s3&quot;</span> config=<span class="hljs-string">&quot;timeout=300000;accesskey=test1;secretkey=test1;endpoint=http://66.66.66.63:7482;path_style_access=true&quot;</span>/&gt;<br>                &lt;operation <span class="hljs-built_in">type</span>=<span class="hljs-string">&quot;read&quot;</span> ratio=<span class="hljs-string">&quot;100&quot;</span> division=<span class="hljs-string">&quot;none&quot;</span><br>                    config=<span class="hljs-string">&quot;cprefix=zp;containers=r(5,8);oprefix=hj;objects=r(1,240);sizes=c(64)KB&quot;</span> <span class="hljs-built_in">id</span>=<span class="hljs-string">&quot;op1&quot;</span>/&gt;<br>            &lt;/work&gt;<br>        &lt;/workstage&gt;<br>    &lt;/workflow&gt;<br>&lt;/workload&gt;<br></code></pre></td></tr></table></figure><p>###删除对象的配置</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs bash">&lt;?xml version=<span class="hljs-string">&quot;1.0&quot;</span> encoding=<span class="hljs-string">&quot;UTF-8&quot;</span>?&gt;<br>&lt;workload name=<span class="hljs-string">&quot;read-object&quot;</span> description=<span class="hljs-string">&quot;create object&quot;</span> config=<span class="hljs-string">&quot;&quot;</span>&gt;<br>    &lt;auth <span class="hljs-built_in">type</span>=<span class="hljs-string">&quot;none&quot;</span> config=<span class="hljs-string">&quot;&quot;</span>/&gt; <br>&lt;workflow config=<span class="hljs-string">&quot;&quot;</span>&gt;<br>&lt;workstage name=<span class="hljs-string">&quot;rgw1-cleanup&quot;</span> closuredelay=<span class="hljs-string">&quot;0&quot;</span> config=<span class="hljs-string">&quot;&quot;</span>&gt;<br>            &lt;auth <span class="hljs-built_in">type</span>=<span class="hljs-string">&quot;none&quot;</span> config=<span class="hljs-string">&quot;&quot;</span>/&gt;<br>            &lt;work name=<span class="hljs-string">&quot;rgw1-cleanup&quot;</span> <span class="hljs-built_in">type</span>=<span class="hljs-string">&quot;cleanup&quot;</span> workers=<span class="hljs-string">&quot;2&quot;</span> interval=<span class="hljs-string">&quot;5&quot;</span><br>                division=<span class="hljs-string">&quot;object&quot;</span> runtime=<span class="hljs-string">&quot;0&quot;</span> rampup=<span class="hljs-string">&quot;0&quot;</span> rampdown=<span class="hljs-string">&quot;0&quot;</span><br>                afr=<span class="hljs-string">&quot;0&quot;</span> totalOps=<span class="hljs-string">&quot;1&quot;</span> totalBytes=<span class="hljs-string">&quot;0&quot;</span> config=<span class="hljs-string">&quot;cprefix=zp;containers=r(1,4);oprefix=hj;objects=r(1,240);deleteContainer=false;&quot;</span>&gt;<br>                &lt;auth <span class="hljs-built_in">type</span>=<span class="hljs-string">&quot;none&quot;</span> config=<span class="hljs-string">&quot;&quot;</span>/&gt;<br>                &lt;storage <span class="hljs-built_in">type</span>=<span class="hljs-string">&quot;s3&quot;</span> config=<span class="hljs-string">&quot;timeout=300000;accesskey=test1;secretkey=test1;endpoint=http://66.66.66.63:7481;path_style_access=true&quot;</span>/&gt;<br>            &lt;/work&gt;<br><br>            &lt;work name=<span class="hljs-string">&quot;rgw2-cleanup&quot;</span> <span class="hljs-built_in">type</span>=<span class="hljs-string">&quot;cleanup&quot;</span> workers=<span class="hljs-string">&quot;2&quot;</span> interval=<span class="hljs-string">&quot;5&quot;</span><br>                division=<span class="hljs-string">&quot;object&quot;</span> runtime=<span class="hljs-string">&quot;0&quot;</span> rampup=<span class="hljs-string">&quot;0&quot;</span> rampdown=<span class="hljs-string">&quot;0&quot;</span><br>                afr=<span class="hljs-string">&quot;0&quot;</span> totalOps=<span class="hljs-string">&quot;1&quot;</span> totalBytes=<span class="hljs-string">&quot;0&quot;</span> config=<span class="hljs-string">&quot;cprefix=zp;containers=r(5,8);oprefix=hj;objects=r(1,240);deleteContainer=false;&quot;</span>&gt;<br>                &lt;auth <span class="hljs-built_in">type</span>=<span class="hljs-string">&quot;none&quot;</span> config=<span class="hljs-string">&quot;&quot;</span>/&gt;<br>                &lt;storage <span class="hljs-built_in">type</span>=<span class="hljs-string">&quot;s3&quot;</span> config=<span class="hljs-string">&quot;timeout=300000;accesskey=test1;secretkey=test1;endpoint=http://66.66.66.63:7482;path_style_access=true&quot;</span>/&gt;<br>            &lt;/work&gt;<br>        &lt;/workstage&gt;<br>    &lt;/workflow&gt;<br>&lt;/workload&gt;<br></code></pre></td></tr></table></figure><p>###删除bucket的配置</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs bash">&lt;?xml version=<span class="hljs-string">&quot;1.0&quot;</span> encoding=<span class="hljs-string">&quot;UTF-8&quot;</span>?&gt;<br>&lt;workload name=<span class="hljs-string">&quot;read-object&quot;</span> description=<span class="hljs-string">&quot;create object&quot;</span> config=<span class="hljs-string">&quot;&quot;</span>&gt;<br>    &lt;auth <span class="hljs-built_in">type</span>=<span class="hljs-string">&quot;none&quot;</span> config=<span class="hljs-string">&quot;&quot;</span>/&gt; <br>&lt;workflow config=<span class="hljs-string">&quot;&quot;</span>&gt;<br>&lt;workstage name=<span class="hljs-string">&quot;dispose&quot;</span> closuredelay=<span class="hljs-string">&quot;0&quot;</span> config=<span class="hljs-string">&quot;&quot;</span>&gt;<br>            &lt;auth <span class="hljs-built_in">type</span>=<span class="hljs-string">&quot;none&quot;</span> config=<span class="hljs-string">&quot;&quot;</span>/&gt;<br>            &lt;work name=<span class="hljs-string">&quot;rgw1-dispose&quot;</span> <span class="hljs-built_in">type</span>=<span class="hljs-string">&quot;dispose&quot;</span> workers=<span class="hljs-string">&quot;2&quot;</span> interval=<span class="hljs-string">&quot;5&quot;</span><br>                division=<span class="hljs-string">&quot;container&quot;</span> runtime=<span class="hljs-string">&quot;0&quot;</span> rampup=<span class="hljs-string">&quot;0&quot;</span> rampdown=<span class="hljs-string">&quot;0&quot;</span><br>                afr=<span class="hljs-string">&quot;0&quot;</span> totalOps=<span class="hljs-string">&quot;1&quot;</span> totalBytes=<span class="hljs-string">&quot;0&quot;</span> config=<span class="hljs-string">&quot;cprefix=zp;containers=r(1,4);&quot;</span>&gt;<br>                &lt;auth <span class="hljs-built_in">type</span>=<span class="hljs-string">&quot;none&quot;</span> config=<span class="hljs-string">&quot;&quot;</span>/&gt;<br>                &lt;storage <span class="hljs-built_in">type</span>=<span class="hljs-string">&quot;s3&quot;</span> config=<span class="hljs-string">&quot;timeout=300000;accesskey=test1;secretkey=test1;endpoint=http://66.66.66.63:7481;path_style_access=true&quot;</span>/&gt;<br>            &lt;/work&gt;<br>            &lt;work name=<span class="hljs-string">&quot;rgw2-dispose&quot;</span> <span class="hljs-built_in">type</span>=<span class="hljs-string">&quot;dispose&quot;</span> workers=<span class="hljs-string">&quot;2&quot;</span> interval=<span class="hljs-string">&quot;5&quot;</span><br>                division=<span class="hljs-string">&quot;container&quot;</span> runtime=<span class="hljs-string">&quot;0&quot;</span> rampup=<span class="hljs-string">&quot;0&quot;</span> rampdown=<span class="hljs-string">&quot;0&quot;</span><br>                afr=<span class="hljs-string">&quot;0&quot;</span> totalOps=<span class="hljs-string">&quot;1&quot;</span> totalBytes=<span class="hljs-string">&quot;0&quot;</span> config=<span class="hljs-string">&quot;cprefix=zp;containers=r(5,8);&quot;</span>&gt;<br>                &lt;auth <span class="hljs-built_in">type</span>=<span class="hljs-string">&quot;none&quot;</span> config=<span class="hljs-string">&quot;&quot;</span>/&gt;<br>                &lt;storage <span class="hljs-built_in">type</span>=<span class="hljs-string">&quot;s3&quot;</span> config=<span class="hljs-string">&quot;timeout=300000;accesskey=test1;secretkey=test1;endpoint=http://66.66.66.63:7482;path_style_access=true&quot;</span>/&gt;<br>            &lt;/work&gt;<br>        &lt;/workstage&gt;<br>    &lt;/workflow&gt;<br>&lt;/workload&gt;<br></code></pre></td></tr></table></figure><p>上面的workstage一共包括下面几种</p><ul><li>init 创建bucket</li><li>normal write 写入对象</li><li>normal read  读取对象</li><li>cleanup  清理对象</li><li>dispose  清理bucket</li></ul><p>division是控制多个worker之间的操作怎么去分的控制，最好在operation那层进行控制</p><p>##测试前自我提问</p><ul><li>单机用了几个workload（默认一般一个，保证单个测试资源的独占）</li><li>采用了几个driver（决定了客户端的发起是有几个客户端，单机一个就可以）</li><li>测试了哪几个项目（init,prepare or normal,remove），单独测试还是混合测试</li><li>单个项目的workstage里面启动了几个work（work可以控制请求发向哪里）</li><li>单个work里面采用了几个workers(这个是控制几个driver进行并发的)</li><li>测试的ceph集群有多少个rgw网关，创建了多少个bucket测试</li><li>设置的写入每个bucket的对象为多少？对象大小为多少？测试时间为多久？</li></ul><p>测试很多文件的时候，可以用ops控制，并且将ops设置大于想测试的文件数目，保证能写入那么多的数据，或者比较确定性能，也可以通过时间控制</p><p>那么我来根据自己的需求来进行一个测试模型说明，然后根据说明进行配置</p><ul><li>采用两个客户端测试，那么准备两个driver</li><li>准备配置两个rgw的网关，那么在配置workstage的时候配置两个work对应到两个storage</li><li>测试创建，写入，读取，删除对象，删除bucket一套完整测试</li><li>wokers设置为2的倍数，初始值为2，让每个driver分得一半的负载，在进行一轮测试后，成倍的增加driver的数目，来增大并发，在性能基本不增加，时延在增加的时候，记录性能值和参数值，这个为本环境的最大性能</li><li>创建8个bucket，每个bucket写入30个64K的对象，测试文件数目为240个</li></ul><p>简单框架图<br><img src="/images/blog/o_200925030652cosbench5.png" alt="cosbench"></p><p>配置文件如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br></pre></td><td class="code"><pre><code class="hljs bash">&lt;?xml version=<span class="hljs-string">&quot;1.0&quot;</span> encoding=<span class="hljs-string">&quot;UTF-8&quot;</span>?&gt;<br>&lt;workload name=<span class="hljs-string">&quot;create-bucket&quot;</span> description=<span class="hljs-string">&quot;create s3 bucket&quot;</span> config=<span class="hljs-string">&quot;&quot;</span>&gt;<br>    &lt;auth <span class="hljs-built_in">type</span>=<span class="hljs-string">&quot;none&quot;</span> config=<span class="hljs-string">&quot;&quot;</span>/&gt;<br>    &lt;workflow config=<span class="hljs-string">&quot;&quot;</span>&gt;<br>        &lt;workstage name=<span class="hljs-string">&quot;create bucket&quot;</span> closuredelay=<span class="hljs-string">&quot;0&quot;</span> config=<span class="hljs-string">&quot;&quot;</span>&gt;<br>            &lt;auth <span class="hljs-built_in">type</span>=<span class="hljs-string">&quot;none&quot;</span> config=<span class="hljs-string">&quot;&quot;</span>/&gt;<br>            &lt;work name=<span class="hljs-string">&quot;rgw1&quot;</span> <span class="hljs-built_in">type</span>=<span class="hljs-string">&quot;init&quot;</span> workers=<span class="hljs-string">&quot;2&quot;</span> interval=<span class="hljs-string">&quot;5&quot;</span><br>                division=<span class="hljs-string">&quot;container&quot;</span> runtime=<span class="hljs-string">&quot;0&quot;</span> rampup=<span class="hljs-string">&quot;0&quot;</span> rampdown=<span class="hljs-string">&quot;0&quot;</span><br>                afr=<span class="hljs-string">&quot;0&quot;</span> totalOps=<span class="hljs-string">&quot;1&quot;</span> totalBytes=<span class="hljs-string">&quot;0&quot;</span> config=<span class="hljs-string">&quot;cprefix=zp;containers=r(1,4)&quot;</span>&gt;<br>                &lt;auth <span class="hljs-built_in">type</span>=<span class="hljs-string">&quot;none&quot;</span> config=<span class="hljs-string">&quot;&quot;</span>/&gt;<br>                &lt;storage <span class="hljs-built_in">type</span>=<span class="hljs-string">&quot;s3&quot;</span> config=<span class="hljs-string">&quot;accesskey=test1;secretkey=test1;endpoint=http://66.66.66.63:7481;path_style_access=true&quot;</span>/&gt;<br>            &lt;/work&gt;<br>            &lt;work name=<span class="hljs-string">&quot;rgw2&quot;</span> <span class="hljs-built_in">type</span>=<span class="hljs-string">&quot;init&quot;</span> workers=<span class="hljs-string">&quot;2&quot;</span> interval=<span class="hljs-string">&quot;5&quot;</span><br>                division=<span class="hljs-string">&quot;container&quot;</span> runtime=<span class="hljs-string">&quot;0&quot;</span> rampup=<span class="hljs-string">&quot;0&quot;</span> rampdown=<span class="hljs-string">&quot;0&quot;</span><br>                afr=<span class="hljs-string">&quot;0&quot;</span> totalOps=<span class="hljs-string">&quot;1&quot;</span> totalBytes=<span class="hljs-string">&quot;0&quot;</span> config=<span class="hljs-string">&quot;cprefix=zp;containers=r(4,8)&quot;</span>&gt;<br>                &lt;auth <span class="hljs-built_in">type</span>=<span class="hljs-string">&quot;none&quot;</span> config=<span class="hljs-string">&quot;&quot;</span>/&gt;<br>                &lt;storage <span class="hljs-built_in">type</span>=<span class="hljs-string">&quot;s3&quot;</span> config=<span class="hljs-string">&quot;accesskey=test1;secretkey=test1;endpoint=http://66.66.66.63:7482;path_style_access=true&quot;</span>/&gt;<br>            &lt;/work&gt;<br>        &lt;/workstage&gt;<br><br><br>&lt;workstage name=<span class="hljs-string">&quot;putobject&quot;</span> closuredelay=<span class="hljs-string">&quot;0&quot;</span> config=<span class="hljs-string">&quot;&quot;</span>&gt;<br>            &lt;auth <span class="hljs-built_in">type</span>=<span class="hljs-string">&quot;none&quot;</span> config=<span class="hljs-string">&quot;&quot;</span>/&gt;<br>            &lt;work name=<span class="hljs-string">&quot;rgw1-put-4M&quot;</span> <span class="hljs-built_in">type</span>=<span class="hljs-string">&quot;normal&quot;</span> workers=<span class="hljs-string">&quot;2&quot;</span> interval=<span class="hljs-string">&quot;5&quot;</span><br>                division=<span class="hljs-string">&quot;container&quot;</span> runtime=<span class="hljs-string">&quot;0&quot;</span> rampup=<span class="hljs-string">&quot;0&quot;</span> rampdown=<span class="hljs-string">&quot;0&quot;</span><br>                afr=<span class="hljs-string">&quot;200000&quot;</span> totalOps=<span class="hljs-string">&quot;240&quot;</span> totalBytes=<span class="hljs-string">&quot;0&quot;</span> config=<span class="hljs-string">&quot;&quot;</span>&gt;<br>                &lt;auth <span class="hljs-built_in">type</span>=<span class="hljs-string">&quot;none&quot;</span> config=<span class="hljs-string">&quot;&quot;</span>/&gt;<br>                &lt;storage <span class="hljs-built_in">type</span>=<span class="hljs-string">&quot;s3&quot;</span> config=<span class="hljs-string">&quot;timeout=300000;accesskey=test1;secretkey=test1;endpoint=http://66.66.66.63:7481;path_style_access=true&quot;</span>/&gt;<br>                &lt;operation <span class="hljs-built_in">type</span>=<span class="hljs-string">&quot;write&quot;</span> ratio=<span class="hljs-string">&quot;100&quot;</span> division=<span class="hljs-string">&quot;none&quot;</span><br>                    config=<span class="hljs-string">&quot;cprefix=zp;containers=r(1,4);oprefix=hj;objects=r(1,240);sizes=c(64)KB&quot;</span> <span class="hljs-built_in">id</span>=<span class="hljs-string">&quot;op1&quot;</span>/&gt;<br>            &lt;/work&gt;<br><br>            &lt;work name=<span class="hljs-string">&quot;rgw2-put-4M&quot;</span> <span class="hljs-built_in">type</span>=<span class="hljs-string">&quot;normal&quot;</span> workers=<span class="hljs-string">&quot;2&quot;</span> interval=<span class="hljs-string">&quot;5&quot;</span><br>                division=<span class="hljs-string">&quot;container&quot;</span> runtime=<span class="hljs-string">&quot;0&quot;</span> rampup=<span class="hljs-string">&quot;0&quot;</span> rampdown=<span class="hljs-string">&quot;0&quot;</span><br>                afr=<span class="hljs-string">&quot;200000&quot;</span> totalOps=<span class="hljs-string">&quot;240&quot;</span> totalBytes=<span class="hljs-string">&quot;0&quot;</span> config=<span class="hljs-string">&quot;&quot;</span>&gt;<br>                &lt;auth <span class="hljs-built_in">type</span>=<span class="hljs-string">&quot;none&quot;</span> config=<span class="hljs-string">&quot;&quot;</span>/&gt;<br>                &lt;storage <span class="hljs-built_in">type</span>=<span class="hljs-string">&quot;s3&quot;</span> config=<span class="hljs-string">&quot;timeout=300000;accesskey=test1;secretkey=test1;endpoint=http://66.66.66.63:7482;path_style_access=true&quot;</span>/&gt;<br>                &lt;operation <span class="hljs-built_in">type</span>=<span class="hljs-string">&quot;write&quot;</span> ratio=<span class="hljs-string">&quot;100&quot;</span> division=<span class="hljs-string">&quot;none&quot;</span><br>                    config=<span class="hljs-string">&quot;cprefix=zp;containers=r(5,8);oprefix=hj;objects=r(1,240);sizes=c(64)KB&quot;</span> <span class="hljs-built_in">id</span>=<span class="hljs-string">&quot;op1&quot;</span>/&gt;<br>            &lt;/work&gt;<br>        &lt;/workstage&gt;<br><br><br>&lt;workstage name=<span class="hljs-string">&quot;readobject&quot;</span> closuredelay=<span class="hljs-string">&quot;0&quot;</span> config=<span class="hljs-string">&quot;&quot;</span>&gt;<br>            &lt;auth <span class="hljs-built_in">type</span>=<span class="hljs-string">&quot;none&quot;</span> config=<span class="hljs-string">&quot;&quot;</span>/&gt;<br>            &lt;work name=<span class="hljs-string">&quot;rgw1-put-4M&quot;</span> <span class="hljs-built_in">type</span>=<span class="hljs-string">&quot;normal&quot;</span> workers=<span class="hljs-string">&quot;2&quot;</span> interval=<span class="hljs-string">&quot;5&quot;</span><br>                division=<span class="hljs-string">&quot;container&quot;</span> runtime=<span class="hljs-string">&quot;0&quot;</span> rampup=<span class="hljs-string">&quot;0&quot;</span> rampdown=<span class="hljs-string">&quot;0&quot;</span><br>                afr=<span class="hljs-string">&quot;200000&quot;</span> totalOps=<span class="hljs-string">&quot;240&quot;</span> totalBytes=<span class="hljs-string">&quot;0&quot;</span> config=<span class="hljs-string">&quot;&quot;</span>&gt;<br>                &lt;auth <span class="hljs-built_in">type</span>=<span class="hljs-string">&quot;none&quot;</span> config=<span class="hljs-string">&quot;&quot;</span>/&gt;<br>                &lt;storage <span class="hljs-built_in">type</span>=<span class="hljs-string">&quot;s3&quot;</span> config=<span class="hljs-string">&quot;timeout=300000;accesskey=test1;secretkey=test1;endpoint=http://66.66.66.63:7481;path_style_access=true&quot;</span>/&gt;<br>                &lt;operation <span class="hljs-built_in">type</span>=<span class="hljs-string">&quot;read&quot;</span> ratio=<span class="hljs-string">&quot;100&quot;</span> division=<span class="hljs-string">&quot;none&quot;</span><br>                    config=<span class="hljs-string">&quot;cprefix=zp;containers=r(1,4);oprefix=hj;objects=r(1,240);sizes=c(64)KB&quot;</span> <span class="hljs-built_in">id</span>=<span class="hljs-string">&quot;op1&quot;</span>/&gt;<br>            &lt;/work&gt;<br><br>            &lt;work name=<span class="hljs-string">&quot;rgw2-read-4M&quot;</span> <span class="hljs-built_in">type</span>=<span class="hljs-string">&quot;normal&quot;</span> workers=<span class="hljs-string">&quot;2&quot;</span> interval=<span class="hljs-string">&quot;5&quot;</span><br>                division=<span class="hljs-string">&quot;container&quot;</span> runtime=<span class="hljs-string">&quot;0&quot;</span> rampup=<span class="hljs-string">&quot;0&quot;</span> rampdown=<span class="hljs-string">&quot;0&quot;</span><br>                afr=<span class="hljs-string">&quot;200000&quot;</span> totalOps=<span class="hljs-string">&quot;240&quot;</span> totalBytes=<span class="hljs-string">&quot;0&quot;</span> config=<span class="hljs-string">&quot;&quot;</span>&gt;<br>                &lt;auth <span class="hljs-built_in">type</span>=<span class="hljs-string">&quot;none&quot;</span> config=<span class="hljs-string">&quot;&quot;</span>/&gt;<br>                &lt;storage <span class="hljs-built_in">type</span>=<span class="hljs-string">&quot;s3&quot;</span> config=<span class="hljs-string">&quot;timeout=300000;accesskey=test1;secretkey=test1;endpoint=http://66.66.66.63:7482;path_style_access=true&quot;</span>/&gt;<br>                &lt;operation <span class="hljs-built_in">type</span>=<span class="hljs-string">&quot;read&quot;</span> ratio=<span class="hljs-string">&quot;100&quot;</span> division=<span class="hljs-string">&quot;none&quot;</span><br>                    config=<span class="hljs-string">&quot;cprefix=zp;containers=r(5,8);oprefix=hj;objects=r(1,240);sizes=c(64)KB&quot;</span> <span class="hljs-built_in">id</span>=<span class="hljs-string">&quot;op1&quot;</span>/&gt;<br>            &lt;/work&gt;<br>        &lt;/workstage&gt;<br><br>&lt;workstage name=<span class="hljs-string">&quot;rgw1-cleanup&quot;</span> closuredelay=<span class="hljs-string">&quot;0&quot;</span> config=<span class="hljs-string">&quot;&quot;</span>&gt;<br>            &lt;auth <span class="hljs-built_in">type</span>=<span class="hljs-string">&quot;none&quot;</span> config=<span class="hljs-string">&quot;&quot;</span>/&gt;<br>            &lt;work name=<span class="hljs-string">&quot;rgw1-cleanup&quot;</span> <span class="hljs-built_in">type</span>=<span class="hljs-string">&quot;cleanup&quot;</span> workers=<span class="hljs-string">&quot;2&quot;</span> interval=<span class="hljs-string">&quot;5&quot;</span><br>                division=<span class="hljs-string">&quot;object&quot;</span> runtime=<span class="hljs-string">&quot;0&quot;</span> rampup=<span class="hljs-string">&quot;0&quot;</span> rampdown=<span class="hljs-string">&quot;0&quot;</span><br>                afr=<span class="hljs-string">&quot;0&quot;</span> totalOps=<span class="hljs-string">&quot;1&quot;</span> totalBytes=<span class="hljs-string">&quot;0&quot;</span> config=<span class="hljs-string">&quot;cprefix=zp;containers=r(1,4);oprefix=hj;objects=r(1,240);deleteContainer=false;&quot;</span>&gt;<br>                &lt;auth <span class="hljs-built_in">type</span>=<span class="hljs-string">&quot;none&quot;</span> config=<span class="hljs-string">&quot;&quot;</span>/&gt;<br>                &lt;storage <span class="hljs-built_in">type</span>=<span class="hljs-string">&quot;s3&quot;</span> config=<span class="hljs-string">&quot;timeout=300000;accesskey=test1;secretkey=test1;endpoint=http://66.66.66.63:7481;path_style_access=true&quot;</span>/&gt;<br>            &lt;/work&gt;<br><br>            &lt;work name=<span class="hljs-string">&quot;rgw2-cleanup&quot;</span> <span class="hljs-built_in">type</span>=<span class="hljs-string">&quot;cleanup&quot;</span> workers=<span class="hljs-string">&quot;2&quot;</span> interval=<span class="hljs-string">&quot;5&quot;</span><br>                division=<span class="hljs-string">&quot;object&quot;</span> runtime=<span class="hljs-string">&quot;0&quot;</span> rampup=<span class="hljs-string">&quot;0&quot;</span> rampdown=<span class="hljs-string">&quot;0&quot;</span><br>                afr=<span class="hljs-string">&quot;0&quot;</span> totalOps=<span class="hljs-string">&quot;1&quot;</span> totalBytes=<span class="hljs-string">&quot;0&quot;</span> config=<span class="hljs-string">&quot;cprefix=zp;containers=r(5,8);oprefix=hj;objects=r(1,240);deleteContainer=false;&quot;</span>&gt;<br>                &lt;auth <span class="hljs-built_in">type</span>=<span class="hljs-string">&quot;none&quot;</span> config=<span class="hljs-string">&quot;&quot;</span>/&gt;<br>                &lt;storage <span class="hljs-built_in">type</span>=<span class="hljs-string">&quot;s3&quot;</span> config=<span class="hljs-string">&quot;timeout=300000;accesskey=test1;secretkey=test1;endpoint=http://66.66.66.63:7482;path_style_access=true&quot;</span>/&gt;<br>            &lt;/work&gt;<br>        &lt;/workstage&gt;<br><br>&lt;workstage name=<span class="hljs-string">&quot;dispose&quot;</span> closuredelay=<span class="hljs-string">&quot;0&quot;</span> config=<span class="hljs-string">&quot;&quot;</span>&gt;<br>            &lt;auth <span class="hljs-built_in">type</span>=<span class="hljs-string">&quot;none&quot;</span> config=<span class="hljs-string">&quot;&quot;</span>/&gt;<br>            &lt;work name=<span class="hljs-string">&quot;rgw1-dispose&quot;</span> <span class="hljs-built_in">type</span>=<span class="hljs-string">&quot;dispose&quot;</span> workers=<span class="hljs-string">&quot;2&quot;</span> interval=<span class="hljs-string">&quot;5&quot;</span><br>                division=<span class="hljs-string">&quot;container&quot;</span> runtime=<span class="hljs-string">&quot;0&quot;</span> rampup=<span class="hljs-string">&quot;0&quot;</span> rampdown=<span class="hljs-string">&quot;0&quot;</span><br>                afr=<span class="hljs-string">&quot;0&quot;</span> totalOps=<span class="hljs-string">&quot;1&quot;</span> totalBytes=<span class="hljs-string">&quot;0&quot;</span> config=<span class="hljs-string">&quot;cprefix=zp;containers=r(1,4);&quot;</span>&gt;<br>                &lt;auth <span class="hljs-built_in">type</span>=<span class="hljs-string">&quot;none&quot;</span> config=<span class="hljs-string">&quot;&quot;</span>/&gt;<br>                &lt;storage <span class="hljs-built_in">type</span>=<span class="hljs-string">&quot;s3&quot;</span> config=<span class="hljs-string">&quot;timeout=300000;accesskey=test1;secretkey=test1;endpoint=http://66.66.66.63:7481;path_style_access=true&quot;</span>/&gt;<br>            &lt;/work&gt;<br>            &lt;work name=<span class="hljs-string">&quot;rgw2-dispose&quot;</span> <span class="hljs-built_in">type</span>=<span class="hljs-string">&quot;dispose&quot;</span> workers=<span class="hljs-string">&quot;2&quot;</span> interval=<span class="hljs-string">&quot;5&quot;</span><br>                division=<span class="hljs-string">&quot;container&quot;</span> runtime=<span class="hljs-string">&quot;0&quot;</span> rampup=<span class="hljs-string">&quot;0&quot;</span> rampdown=<span class="hljs-string">&quot;0&quot;</span><br>                afr=<span class="hljs-string">&quot;0&quot;</span> totalOps=<span class="hljs-string">&quot;1&quot;</span> totalBytes=<span class="hljs-string">&quot;0&quot;</span> config=<span class="hljs-string">&quot;cprefix=zp;containers=r(5,8);&quot;</span>&gt;<br>                &lt;auth <span class="hljs-built_in">type</span>=<span class="hljs-string">&quot;none&quot;</span> config=<span class="hljs-string">&quot;&quot;</span>/&gt;<br>                &lt;storage <span class="hljs-built_in">type</span>=<span class="hljs-string">&quot;s3&quot;</span> config=<span class="hljs-string">&quot;timeout=300000;accesskey=test1;secretkey=test1;endpoint=http://66.66.66.63:7482;path_style_access=true&quot;</span>/&gt;<br>            &lt;/work&gt;<br>        &lt;/workstage&gt;<br><br>    &lt;/workflow&gt;<br>&lt;/workload&gt;<br></code></pre></td></tr></table></figure><p>上面的测试是为了做测试模板，所以采用了比较小的对象数目和比较小的测试时间</p><p>可以根据自己的硬件环境或者客户的要求来设计测试模型，环境够大的时候，提供足够的rgw和足够的客户端才能测出比较大的性能值</p><p>测试的时候，尽量把写入和读取的测试分开，也就是分两次测试，避免第一次的写入没写足够对象，读取的时候读不到中断了，对于长达数小时的测试的时候，中断是很令人头疼的，分段可以减少这种中断后的继续测试的时间</p><p>写入的测试在允许的范围内，尽量写入多点对象，尽量避免复写，也能够在读取的时候尽量能够足够散列</p><p>测试时间能够长尽量长</p><p>##测试结果<br><img src="/images/blog/o_200925030659cosbench6.png" alt="result"><br><img src="/images/blog/o_200925030704cosbench7.png" alt="graph"></p><p>可以通过线图来看指定测试项目的中间情况，一般是去关注是否出现比较大的抖动，相同性能下，抖动越小越好</p><p>##其他调优<br>在硬件环境一定的情况下，可以通过增加nginx负载均衡，或者lvs负载均衡来尝试增加性能值，这个不在本篇的讨论范围内</p><p>##补充一个BUG<br>如果读取的时候，写入的文件数目是正确的，还是get失败，或者前面成功后面失败，那么可能是软件的bug</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">about setting the property:<br>stop all cosbench processes (controller, drivers)<br>edit cosbench-start.sh, locate the java launching line, and add <span class="hljs-string">&quot;-Dcom.amazonaws.services.s3.disableGetObjectMD5Validation=true&quot;</span><br>restart cosbench processes<br></code></pre></td></tr></table></figure><p>来源：<br><a href="https://github.com/intel-cloud/cosbench/issues/320">https://github.com/intel-cloud/cosbench/issues/320</a></p><h2 id="变更记录"><a href="#变更记录" class="headerlink" title="变更记录"></a>变更记录</h2><table><thead><tr><th align="center">Why</th><th align="center">Who</th><th align="center">When</th></tr></thead><tbody><tr><td align="center">创建</td><td align="center">武汉-运维-磨渣</td><td align="center">2018-04-12</td></tr><tr><td align="center">修改测试模式时间为文件数目控制</td><td align="center">武汉-运维-磨渣</td><td align="center">2019-03-15</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>ceph的ISCSI GATEWAY</title>
    <link href="/2018/04/11/ceph%E7%9A%84ISCSI%20GATEWAY/"/>
    <url>/2018/04/11/ceph%E7%9A%84ISCSI%20GATEWAY/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>最开始接触这个是在L版本的监控平台里面看到的，有个iscsi网关，但是没看到有类似的介绍，然后通过接口查询到了一些资料，当时由于有比较多的东西需要新内核，新版本的支持，所以并没有配置出来，由于内核已经更新迭代了几个小版本了，经过测试验证可以跑起来了，这里只是把东西跑起来，性能相关的对比需要根据去做</p><h2 id="实践过程"><a href="#实践过程" class="headerlink" title="实践过程"></a>实践过程</h2><h3 id="架构图"><a href="#架构图" class="headerlink" title="架构图"></a>架构图</h3><p><img src="/images/blog/o_200901091503Ceph_iSCSI_HA_424879_1116_ECE-01.png" alt="Ceph_iSCSI_HA_424879_1116_ECE-01.png-79.4kB"></p><p>这个图是引用的红帽的架构图，可以理解为一个多路径的实现方式，那么这个跟之前的有什么不同</p><p>主要是有个新的tcmu-runner来处理LIO TCM后端存储的用户空间端的守护进程，这个是在内核之上多了一个用户态的驱动层，这样只需要根据tcmu的标准来对接接口就可以了，而不用去直接跟内核进行交互</p><h3 id="需要的软件"><a href="#需要的软件" class="headerlink" title="需要的软件"></a>需要的软件</h3><p>Ceph Luminous 版本的集群或者更新的版本<br>RHEL&#x2F;CentOS 7.5或者Linux kernel v4.16或者更新版本的内核<br>其他控制软件</p><blockquote><p>targetcli-2.1.fb47 or newer package<br><br> python-rtslib-2.1.fb64 or newer package<br><br> tcmu-runner-1.3.0 or newer package<br><br> ceph-iscsi-config-2.4 or newer package<br><br> ceph-iscsi-cli-2.5 or newer package</p></blockquote><p>以上为配置这个环境需要的软件，下面为我使用的版本的软件，统一打包放在一个下载路径<br>我安装的版本如下：</p><blockquote><p>kernel-4.16.0-0.rc5.git0.1<br><br>targetcli-fb-2.1.fb48<br><br>python-rtslib-2.1.67<br><br>tcmu-runner-1.3.0-rc4<br><br>ceph-iscsi-config-2.5<br><br>ceph-iscsi-cli-2.6</p></blockquote><p>下载链接：</p><blockquote><p>链接:<a href="https://pan.baidu.com/s/12OwR5ZNtWFW13feLXy3Ezg">https://pan.baidu.com/s/12OwR5ZNtWFW13feLXy3Ezg</a> 密码:m09k</p></blockquote><p>如果环境之前有安装过其他版本，需要先卸载掉，并且需要提前部署好一个Luminous 最新版本的集群<br>官方建议调整的参数</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># ceph tell osd.* injectargs &#x27;--osd_client_watch_timeout 15&#x27;</span><br><span class="hljs-comment"># ceph tell osd.* injectargs &#x27;--osd_heartbeat_grace 20&#x27;</span><br><span class="hljs-comment"># ceph tell osd.* injectargs &#x27;--osd_heartbeat_interval 5&#x27;</span><br></code></pre></td></tr></table></figure><h3 id="配置过程"><a href="#配置过程" class="headerlink" title="配置过程"></a>配置过程</h3><p>创建一个存储池<br>需要用到rbd存储池，用来存储iscsi的配置文件，提前创建好一个名字是rbd的存储池</p><p>创建iscsi-gateway配置文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">touch</span> /etc/ceph/iscsi-gateway.cfg<br></code></pre></td></tr></table></figure><p>修改iscsi-gateway.cfg配置文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs bash">[config]<br><span class="hljs-comment"># Name of the Ceph storage cluster. A suitable Ceph configuration file allowing</span><br><span class="hljs-comment"># access to the Ceph storage cluster from the gateway node is required, if not</span><br><span class="hljs-comment"># colocated on an OSD node.</span><br>cluster_name = ceph<br><br><span class="hljs-comment"># Place a copy of the ceph cluster&#x27;s admin keyring in the gateway&#x27;s /etc/ceph</span><br><span class="hljs-comment"># drectory and reference the filename here</span><br>gateway_keyring = ceph.client.admin.keyring<br><br><br><span class="hljs-comment"># API settings.</span><br><span class="hljs-comment"># The API supports a number of options that allow you to tailor it to your</span><br><span class="hljs-comment"># local environment. If you want to run the API under https, you will need to</span><br><span class="hljs-comment"># create cert/key files that are compatible for each iSCSI gateway node, that is</span><br><span class="hljs-comment"># not locked to a specific node. SSL cert and key files *must* be called</span><br><span class="hljs-comment"># &#x27;iscsi-gateway.crt&#x27; and &#x27;iscsi-gateway.key&#x27; and placed in the &#x27;/etc/ceph/&#x27; directory</span><br><span class="hljs-comment"># on *each* gateway node. With the SSL files in place, you can use &#x27;api_secure = true&#x27;</span><br><span class="hljs-comment"># to switch to https mode.</span><br><br><span class="hljs-comment"># To support the API, the bear minimum settings are:</span><br>api_secure = <span class="hljs-literal">false</span><br><br><span class="hljs-comment"># Additional API configuration options are as follows, defaults shown.</span><br><span class="hljs-comment"># api_user = admin</span><br><span class="hljs-comment"># api_password = admin</span><br><span class="hljs-comment"># api_port = 5001</span><br><span class="hljs-comment"># trusted_ip_list = 192.168.0.10,192.168.0.11</span><br></code></pre></td></tr></table></figure><p>最后一行的trusted_ip_list修改为用来配置网关的主机IP，我的环境为</p><blockquote><p>trusted_ip_list &#x3D;192.168.219.128,192.168.219.129</p></blockquote><p>所有网关节点的这个配置文件的内容需要一致，修改好一台直接scp到每个网关节点上</p><p>启动API服务</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 install]<span class="hljs-comment"># systemctl daemon-reload</span><br>[root@lab101 install]<span class="hljs-comment"># systemctl enable rbd-target-api</span><br>[root@lab101 install]<span class="hljs-comment"># systemctl start rbd-target-api</span><br>[root@lab101 install]<span class="hljs-comment"># systemctl status rbd-target-api</span><br>● rbd-target-api.service - Ceph iscsi target configuration API<br>   Loaded: loaded (/usr/lib/systemd/system/rbd-target-api.service; enabled; vendor preset: disabled)<br>   Active: active (running) since Thu 2018-03-15 09:44:34 CST; 18min ago<br> Main PID: 1493 (rbd-target-api)<br>   CGroup: /system.slice/rbd-target-api.service<br>           └─1493 /usr/bin/python /usr/bin/rbd-target-api<br><br>Mar 15 09:44:34 lab101 systemd[1]: Started Ceph iscsi target configuration API.<br>Mar 15 09:44:34 lab101 systemd[1]: Starting Ceph iscsi target configuration API...<br>Mar 15 09:44:58 lab101 rbd-target-api[1493]: Started the configuration object watcher<br>Mar 15 09:44:58 lab101 rbd-target-api[1493]: Checking <span class="hljs-keyword">for</span> config object changes every 1s<br>Mar 15 09:44:58 lab101 rbd-target-api[1493]:  * Running on http://0.0.0.0:5000/<br></code></pre></td></tr></table></figure><p>配置iscsi<br>执行gwcli命令<br><img src="/images/blog/o_200901091511iscsi-image2.png" alt="image.png-23kB"></p><p>默认是这样的</p><p>进入icsi-target创建一个target</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">/&gt; <span class="hljs-built_in">cd</span> iscsi-target <br>/iscsi-target&gt; create iqn.2003-01.com.redhat.iscsi-gw:iscsi-igw<br>ok<br></code></pre></td></tr></table></figure><p>创建iSCSI网关。以下使用的IP是用于iSCSI数据传输的IP,它们可以与trusted_ip_list中列出的用于管理操作的IP相同，也可以不同，看有没有做多网卡分离</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs bash">/iscsi-target&gt; <span class="hljs-built_in">cd</span> iqn.2003-01.com.redhat.iscsi-gw:iscsi-igw/<br>/iscsi-target...-gw:iscsi-igw&gt; <span class="hljs-built_in">cd</span> gateways <br>/iscsi-target...-igw/gateways&gt; create lab101 192.168.219.128 skipchecks=<span class="hljs-literal">true</span><br>OS version/package checks have been bypassed<br>Adding gateway, syncing 0 disk(s) and 0 client(s)<br>  /iscsi-target...-igw/gateways&gt; create lab102 192.168.219.129 skipchecks=<span class="hljs-literal">true</span><br>OS version/package checks have been bypassed<br>Adding gateway, <span class="hljs-built_in">sync</span><span class="hljs-string">&#x27;ing 0 disk(s) and 0 client(s)</span><br><span class="hljs-string">ok</span><br><span class="hljs-string">/iscsi-target...-igw/gateways&gt; ls</span><br><span class="hljs-string">o- gateways ............. [Up: 2/2, Portals: 2]</span><br><span class="hljs-string">  o- lab101 ............. [192.168.219.128 (UP)]</span><br><span class="hljs-string">  o- lab102 ............. [192.168.219.129 (UP)]</span><br></code></pre></td></tr></table></figure><p>创建一个rbd设备disk_1</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">/iscsi-target...-igw/gateways&gt; <span class="hljs-built_in">cd</span> /disks <br>/disks&gt; create pool=rbd image=disk_1 size=100G<br>ok<br></code></pre></td></tr></table></figure><p>创建一个客户端名称iqn.1994-05.com.redhat:75c3d5efde0</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">/disks&gt; <span class="hljs-built_in">cd</span> /iscsi-target/iqn.2003-01.com.redhat.iscsi-gw:iscsi-igw/hosts <br>/iscsi-target...csi-igw/hosts&gt; create iqn.1994-05.com.redhat:75c3d5efde0<br>ok<br></code></pre></td></tr></table></figure><p>创建chap的用户名密码，由于用户名密码都有特殊要求，如果你不确定，就按我给的去设置，并且chap必须设置，否则服务端是禁止连接的</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">/iscsi-target...t:75c3d5efde0&gt; auth chap=iqn.1994-05.com.redhat:75c3d5efde0/admin@a_12a-bb<br>ok<br></code></pre></td></tr></table></figure><p>chap的命名规则可以这样查询</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><code class="hljs bash">/iscsi-target...t:75c3d5efde0&gt; <span class="hljs-built_in">help</span> auth<br><br>SYNTAX<br>======<br>auth [chap] <br><br><br>DESCRIPTION<br>===========<br><br>Client authentication can be <span class="hljs-built_in">set</span> to use CHAP by supplying the<br>a string of the form &lt;username&gt;/&lt;password&gt;<br><br>e.g.<br>auth chap=username/password | nochap<br><br>username ... the username is 8-64 character string. Each character<br>             may either be an alphanumeric or use one of the following<br>             special characters .,:,-,@.<br>             Consider using the hosts <span class="hljs-string">&#x27;shortname&#x27;</span> or the initiators IQN<br>             value as the username<br><br>password ... the password must be between 12-16 chars <span class="hljs-keyword">in</span> length<br>             containing alphanumeric characters, plus the following<br>             special characters @,_,-<br><br>WARNING: Using unsupported special characters may result <span class="hljs-keyword">in</span> truncation,<br>         resulting <span class="hljs-keyword">in</span> failed logins.<br><br><br>Specifying <span class="hljs-string">&#x27;nochap&#x27;</span> will remove chap authentication <span class="hljs-keyword">for</span> the client<br>across all gateways.<br></code></pre></td></tr></table></figure><p>增加磁盘到客户端</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">/iscsi-target...t:75c3d5efde0&gt; disk add rbd.disk_1<br>ok<br></code></pre></td></tr></table></figure><p>到这里就配置完成了，我们看下最终应该是怎么样的<br><img src="/images/blog/o_200901091524iscsi-image3.png" alt="image.png-38.5kB"></p><h2 id="windows客户端配置"><a href="#windows客户端配置" class="headerlink" title="windows客户端配置"></a>windows客户端配置</h2><p>这个地方我配置的时候用的win10配置的时候出现了无法连接的情况，可能是windows10自身的认证要求跟服务端冲突了，这里用windows server 2016 进行连接测试</p><p>windows server开启下Multipath IO</p><p>修改windows iscsi客户端的名称<br><img src="/images/blog/o_200901091532iscsi-image4.png" alt="image.png-47.5kB"><br>修改为上面创建的客户端名称</p><p>发现门户<br><img src="/images/blog/o_200901091541iscsi-image5.png" alt="image.png-37.7kB"><br>点击发现门户，填写好服务端的IP后直接点确定，这里先不用高级里面的配置</p><p><img src="/images/blog/o_200901091547iscsi-image6.png" alt="image.png-35.1kB"></p><p>这个时候目标里面已经有一个发现的目标了，显示状态是不活动的，准备点击连接</p><p><img src="/images/blog/o_200901091554iscsi-image7.png" alt="image.png-80.7kB"><br>点击高级，选择门户IP，填写chap登陆信息，然后chap名称就是上面设置的用户名称，因为跟客户端名称设置的一致，也就是客户端的名称，密码就是上面设置的admin@a_12a-bb</p><p><img src="/images/blog/o_200901091559iscsi-image8.png" alt="image.png-21.9kB"></p><p>切换到卷和设备，点击自动配置<br><img src="/images/blog/o_200901091607iscsi-image9.png" alt="image.png-47.4kB"></p><p>可以看到已经装载设备了</p><p>在服务管理器，文件存储服务，卷，磁盘里面查看设备<br><img src="/images/blog/o_200901091613iscsi-image10.png" alt="image.png-92.8kB"></p><p>可以看到是配置的LIO-ORG TCMU设备，对设备进行格式化即可</p><p><img src="/images/blog/o_200901091619iscsi-image11.png" alt="image.png-42.6kB"></p><p>完成了连接了</p><h2 id="Linux的客户端连接"><a href="#Linux的客户端连接" class="headerlink" title="Linux的客户端连接"></a>Linux的客户端连接</h2><p>Linux客户端选择建议就选择3.10默认内核，选择高版本的内核的时候在配置多路径的时候碰到内核崩溃的问题</p><p>安装连接软件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab103 ~]<span class="hljs-comment"># yum install iscsi-initiator-utils</span><br>[root@lab103 ~]<span class="hljs-comment"># yum install device-mapper-multipath</span><br></code></pre></td></tr></table></figure><p>配置多路径</p><p>开启服务</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab103 ~]<span class="hljs-comment"># mpathconf --enable --with_multipathd y</span><br></code></pre></td></tr></table></figure><p>修改配置文件&#x2F;etc&#x2F;multipath.conf</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs bash">devices &#123;<br>        device &#123;<br>                vendor                 <span class="hljs-string">&quot;LIO-ORG&quot;</span><br>                hardware_handler       <span class="hljs-string">&quot;1 alua&quot;</span><br>                path_grouping_policy   <span class="hljs-string">&quot;failover&quot;</span><br>                path_selector          <span class="hljs-string">&quot;queue-length 0&quot;</span><br>                failback               60<br>                path_checker           tur<br>                prio                   alua<br>                prio_args              exclusive_pref_bit<br>                fast_io_fail_tmo       25<br>                no_path_retry          queue<br>        &#125;<br>&#125;<br></code></pre></td></tr></table></figure><p>重启多路径服务</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab103 ~]<span class="hljs-comment"># systemctl reload multipathd</span><br></code></pre></td></tr></table></figure><p>配置chap的认证</p><p>修改配置客户端的名称为上面设置的名称</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab103 ~]<span class="hljs-comment"># cat /etc/iscsi/initiatorname.iscsi </span><br>InitiatorName=iqn.1994-05.com.redhat:75c3d5efde0<br></code></pre></td></tr></table></figure><p>修改认证的配置文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab103 ~]<span class="hljs-comment"># cat /etc/iscsi/iscsid.conf |grep &quot;node.session.auth.username\|node.session.auth.password\|node.session.auth.authmethod&quot;</span><br>node.session.auth.authmethod = CHAP<br>node.session.auth.username = iqn.1994-05.com.redhat:75c3d5efde0<br>node.session.auth.password = admin@a_12a-bb<br></code></pre></td></tr></table></figure><p>查询iscsi target</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab103 ~]<span class="hljs-comment"># iscsiadm -m discovery -t st -p 192.168.219.128</span><br>192.168.219.128:3260,1 iqn.2003-01.com.redhat.iscsi-gw:iscsi-igw<br>192.168.219.129:3260,2 iqn.2003-01.com.redhat.iscsi-gw:iscsi-igw<br></code></pre></td></tr></table></figure><p>连接target</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab103 ~]<span class="hljs-comment"># iscsiadm -m node -T iqn.2003-01.com.redhat.iscsi-gw:iscsi-igw -l</span><br>Logging <span class="hljs-keyword">in</span> to [iface: default, target: iqn.2003-01.com.redhat.iscsi-gw:iscsi-igw, portal: 192.168.219.129,3260] (multiple)<br>Logging <span class="hljs-keyword">in</span> to [iface: default, target: iqn.2003-01.com.redhat.iscsi-gw:iscsi-igw, portal: 192.168.219.129,3260] (multiple)<br>Login to [iface: default, target: iqn.2003-01.com.redhat.iscsi-gw:iscsi-igw, portal: 192.168.219.129,3260] successful.<br>Login to [iface: default, target: iqn.2003-01.com.redhat.iscsi-gw:iscsi-igw, portal: 192.168.219.129,3260] successful.<br></code></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ~]<span class="hljs-comment"># multipath -ll</span><br>mpathb (360014052fc39ba627874fdba9aefcf6c) dm-4 LIO-ORG ,TCMU device     <br>size=100G features=<span class="hljs-string">&#x27;1 queue_if_no_path&#x27;</span> hwhandler=<span class="hljs-string">&#x27;1 alua&#x27;</span> wp=rw<br>|-+- policy=<span class="hljs-string">&#x27;queue-length 0&#x27;</span> prio=10 status=active<br>| `- 5:0:0:0 sdc 8:32 active ready running<br>`-+- policy=<span class="hljs-string">&#x27;queue-length 0&#x27;</span> prio=10 status=enabled<br>  `- 6:0:0:0 sdd 8:48 active ready running<br></code></pre></td></tr></table></figure><p>查看盘符</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ~]<span class="hljs-comment"># parted -s /dev/mapper/mpathb print</span><br>Model: Linux device-mapper (multipath) (dm)<br>Disk /dev/mapper/mpathb: 107GB<br>Sector size (logical/physical): 512B/512B<br>Partition Table: gpt<br>Disk Flags: <br><br>Number  Start   End    Size   File system  Name                          Flags<br> 1      17.4kB  134MB  134MB               Microsoft reserved partition  msftres<br> 2      135MB   107GB  107GB  ntfs         Basic data partition<br></code></pre></td></tr></table></figure><p>直接使用这个&#x2F;dev&#x2F;mapper&#x2F;mpathb设备即可</p><h2 id="变更记录"><a href="#变更记录" class="headerlink" title="变更记录"></a>变更记录</h2><table><thead><tr><th align="center">Why</th><th align="center">Who</th><th align="center">When</th></tr></thead><tbody><tr><td align="center">创建</td><td align="center">武汉-运维-磨渣</td><td align="center">2018-04-11</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>parted会启动你的ceph osd，意外不？</title>
    <link href="/2018/03/23/parted%E4%BC%9A%E5%90%AF%E5%8A%A8%E4%BD%A0%E7%9A%84ceph%20osd%EF%BC%8C%E6%84%8F%E5%A4%96%E4%B8%8D%EF%BC%9F/"/>
    <url>/2018/03/23/parted%E4%BC%9A%E5%90%AF%E5%8A%A8%E4%BD%A0%E7%9A%84ceph%20osd%EF%BC%8C%E6%84%8F%E5%A4%96%E4%B8%8D%EF%BC%9F/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>如果看到标题，你是不是第一眼觉得写错了，这个怎么可能，完全就是两个不相关的东西，最开始我也是这么想的，直到我发现真的是这样的时候，也是很意外，还是弄清楚下比较好，不然在某个操作下，也许就会出现意想不到的情况</p><h2 id="定位"><a href="#定位" class="headerlink" title="定位"></a>定位</h2><p>如果你看过我的博客，正好看过这篇 &lt;&lt;ceph在centos7下一个不容易发现的改变&gt;&gt; ，那么应该还记得这个讲的是centos 7 下面通过udev来实现了osd的自动挂载，这个自动挂载就是本篇需要了解的前提</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ~]<span class="hljs-comment"># df -h|grep ceph</span><br>/dev/sdf1                233G   34M  233G   1% /var/lib/ceph/osd/ceph-1<br>[root@lab101 ~]<span class="hljs-comment"># systemctl stop ceph-osd@1</span><br>[root@lab101 ~]<span class="hljs-comment"># umount /dev/sdf1 </span><br>[root@lab101 ~]<span class="hljs-comment"># parted -l &amp;&gt;/dev/null</span><br>[root@lab101 ~]<span class="hljs-comment"># df -h|grep ceph</span><br>/dev/sdf1                233G   34M  233G   1% /var/lib/ceph/osd/ceph-1<br>[root@lab101 ~]<span class="hljs-comment"># ps -ef|grep osd</span><br>ceph      62701      1  1 23:25 ?        00:00:00 /usr/bin/ceph-osd -f --cluster ceph --<span class="hljs-built_in">id</span> 1 --setuser ceph --setgroup ceph<br>root      62843  35114  0 23:25 pts/0    00:00:00 grep --color=auto osd<br></code></pre></td></tr></table></figure><p>看这个操作过程，是不是很神奇，是不是很意外，不管怎么说，parted -l的一个操作把我们的osd给自动mount 起来了，也自动给启动了</p><p>出现这个以后，我们先看下日志怎么出的，大概看起来的是这样的<br><img src="/images/blog/o_200901090946parted-min.gif" alt="parted.gif-4083.1kB"></p><p>可以看到确实是实时去触发的</p><p>服务器上面是有一个这个服务的 </p><blockquote><p>systemd-udevd.service<br>看到在做parted -l 后就会起一个这个子进程的<br><img src="/images/blog/o_200901091015parted-image.png" alt="image.png-146.3kB"></p></blockquote><p>在尝试关闭这个服务后，再做parted -l操作就不会出现自动启动进程</p><h2 id="原因"><a href="#原因" class="headerlink" title="原因"></a>原因</h2><p>执行parted -l 对指定设备发起parted命令的时候，就会对内核做一个trigger，而我们的</p><blockquote><p>&#x2F;lib&#x2F;udev&#x2F;rules.d&#x2F;95-ceph-osd.rules<br>这个文件一旦触发是会去调用<br>&#x2F;usr&#x2F;sbin&#x2F;ceph-disk –log-stdout -v trigger &#x2F;dev&#x2F;$name</p></blockquote><p>也就是自动挂载加上启动osd的的操作了</p><h3 id="可能带来什么困扰"><a href="#可能带来什么困扰" class="headerlink" title="可能带来什么困扰"></a>可能带来什么困扰</h3><p>其实这个我也不知道算不算bug，至少在正常使用的时候是没有问题的，以至于这个功能已经有了这么久，而我并没有察觉到，也没有感觉到它给我带来的干扰，那么作为一名测试人员，现在来构思一种可能出现的破坏场景，只要按照正常操作去做的，还会出现的，就是有可能发生的事情</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">cd</span> /var/lib/ceph/osd/<br>[root@lab101 osd]<span class="hljs-comment"># df -h|grep osd</span><br>/dev/sdf1                233G   34M  233G   1% /var/lib/ceph/osd/ceph-1<br>[root@lab101 osd]<span class="hljs-comment"># systemctl stop ceph-osd@1</span><br>[root@lab101 osd]<span class="hljs-comment"># umount /dev/sdf1</span><br>[root@lab101 osd]<span class="hljs-comment"># parted -l  &amp;&gt;/dev/null</span><br>[root@lab101 osd]<span class="hljs-comment"># rm -rf ceph-1/</span><br><span class="hljs-built_in">rm</span>: cannot remove ‘ceph-1/’: Device or resource busy<br>[root@lab101 osd]<span class="hljs-comment"># ll ceph-1/</span><br>total 0<br>[root@lab101 osd]<span class="hljs-comment"># df -h|grep ceph</span><br>/dev/sdf1                233G   33M  233G   1% /var/lib/ceph/osd/ceph-1<br></code></pre></td></tr></table></figure><p>可以看到除了上面的parted -l以外，其他操作都是一个正常的操作，umount掉挂载点，然后清理掉这个目录，然后数据就被删了，当然正常情况下也许没人在正好那个点来了一个parted,但是不是完全没有可能</p><p>还有种情况就是我是要做维护，我想umount掉挂载点，不想进程起来，执行parted是很常规的操作了，结果自己给我拉起来了，这个操作应该比较常见的</p><h3 id="如何解决这个情况"><a href="#如何解决这个情况" class="headerlink" title="如何解决这个情况"></a>如何解决这个情况</h3><p>第一种方法<br>什么都不动，你知道这个事情就行，执行过parted后再加上个df多检查下</p><p>第二种方法</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">systemctl stop systemd-udevd<br></code></pre></td></tr></table></figure><p>这个会带来其他什么影响，暂时不好判断，还没深入研究，影响应该也只会在硬件变动和一些udev触发的需求，不确定的情况可以不改，不推荐此方法</p><p>第三种方法<br>不用这个&#x2F;lib&#x2F;udev&#x2F;rules.d&#x2F;95-ceph-osd.rules做控制了，自己去写配置文件，或者写fstab，都可以，保证启动后能够自动mount，服务能够正常启动就可以了，个人从维护角度还是偏向于第三种方法，记录的信息越多，维护的时候越方便，这个是逼着记录了一些信息，虽然可以什么信息也不记</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>其实这个问题梳理清楚了也还好，最可怕的也许就是不知道为什么，特别是觉得完全不搭边的东西相互起了关联，至少在我们的研发跟我描述这个问题的时候，我想的是，还有这种神操作，是不是哪里加入了钩子程序什么的，花了点时间查到了原因，也方便在日后碰到不那么惊讶了</p><p>ceph北京大会已经顺利开完了，等PPT出来以后再学习一下新的东西，内容应该还是很多的，其实干货不干货，都在于你发现了什么，如果有一个PPT里面你提取到了一个知识点，你都是赚到了，何况分享的人并没有告知的义务的，所以每次看到有分享都是很感谢分享者的</p><h2 id="变更记录"><a href="#变更记录" class="headerlink" title="变更记录"></a>变更记录</h2><table><thead><tr><th align="center">Why</th><th align="center">Who</th><th align="center">When</th></tr></thead><tbody><tr><td align="center">创建</td><td align="center">武汉-运维-磨渣</td><td align="center">2018-03-23</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>REDHAT 7.5beta 新推出的VDO功能</title>
    <link href="/2018/02/10/REDHAT%207.5beta%20%E6%96%B0%E6%8E%A8%E5%87%BA%E7%9A%84VDO%E5%8A%9F%E8%83%BD/"/>
    <url>/2018/02/10/REDHAT%207.5beta%20%E6%96%B0%E6%8E%A8%E5%87%BA%E7%9A%84VDO%E5%8A%9F%E8%83%BD/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><h3 id="关于VDO"><a href="#关于VDO" class="headerlink" title="关于VDO"></a>关于VDO</h3><p>VDO的技术来源于收购的Permabit公司，一个专门从事重删技术的公司，所以技术可靠性是没有问题的</p><p>VDO是一个内核模块，目的是通过重删减少磁盘的空间占用，以及减少复制带宽，VDO是基于块设备层之上的，也就是在原设备基础上映射出mapper虚拟设备，然后直接使用即可，功能的实现主要基于以下技术：</p><ul><li><p>零区块的排除：</p><p>在初始化阶段，整块为0的会被元数据记录下来，这个可以用水杯里面的水和沙子混合的例子来解释，使用滤纸（零块排除），把沙子（非零空间）给过滤出来，然后就是下一个阶段的处理</p></li><li><p>重复数据删除：</p><p>在第二阶段，输入的数据会判断是不是冗余数据（在写入之前就判断），这个部分的数据通过UDS内核模块来判断（U niversal D eduplication S ervice），被判断为重复数据的部分不会被写入，然后对元数据进行更新，直接指向原始已经存储的数据块即可</p></li><li><p>压缩：</p><p>一旦消零和重删完成，LZ4压缩会对每个单独的数据块进行处理，然后压缩好的数据块会以固定大小4KB的数据块存储在介质上，由于一个物理块可以包含很多的压缩块，这个也可以加速读取的性能</p></li></ul><p>上面的技术看起来很容易理解，但是实际做成产品还是相当大的难度的，技术设想和实际输出还是有很大距离，不然redhat也不会通过收购来获取技术，而不是自己去重新写一套了</p><h3 id="如何获取VDO"><a href="#如何获取VDO" class="headerlink" title="如何获取VDO"></a>如何获取VDO</h3><p>主要有两种方式，一种是通过申请测试版的方式申请redhat 7.5的ISO，这个可以进行一个月的测试</p><p>另外一种方式是申请测试版本，然后通过源码在你正在使用的ISO上面进行相关的测试，从适配方面在自己的ISO上面进行测试能够更好的对比，由于基于redhat的源码做分发会涉及法律问题，这里就不做过多讲解，也不提供rpm包，自行申请测试即可</p><h2 id="实践过程"><a href="#实践过程" class="headerlink" title="实践过程"></a>实践过程</h2><h3 id="安装VDO"><a href="#安装VDO" class="headerlink" title="安装VDO"></a>安装VDO</h3><p>安装的操作系统为CentOS Linux release 7.4.1708</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ~]<span class="hljs-comment"># lsb_release -a</span><br>LSB Version::core-4.1-amd64:core-4.1-noarch<br>Distributor ID:CentOS<br>Description:CentOS Linux release 7.4.1708 (Core) <br>Release:7.4.1708<br>Codename:Core<br></code></pre></td></tr></table></figure><p>内核版本如下</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ~]<span class="hljs-comment"># uname -a</span><br>Linux lab101 3.10.0-693.el7.x86_64 <span class="hljs-comment">#1 SMP Tue Aug 22 21:09:27 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux</span><br></code></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ~]<span class="hljs-comment"># rpm -qa|grep kernel</span><br>kernel-tools-libs-3.10.0-693.el7.x86_64<br>abrt-addon-kerneloops-2.1.11-48.el7.centos.x86_64<br>kernel-3.10.0-693.el7.x86_64<br></code></pre></td></tr></table></figure><p>我们把内核升级一下，因为这个模块比较新，所以选择目前updates里面最新的</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">wget http://mirror.centos.org/centos/7/updates/x86_64/Packages/kernel-3.10.0-693.17.1.el7.x86_64.rpm<br></code></pre></td></tr></table></figure><p>大版本一致，小版本不同，直接安装即可</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ~]<span class="hljs-comment"># rpm -ivh kernel-3.10.0-693.17.1.el7.x86_64.rpm </span><br>Preparing...                          <span class="hljs-comment">################################# [100%]</span><br>Updating / installing...<br>   1:kernel-3.10.0-693.17.1.el7       <span class="hljs-comment">################################# [100%]</span><br>[root@lab101 ~]<span class="hljs-comment"># grub2-set-default &#x27;CentOS Linux (3.10.0-693.17.1.el7.x86_64) 7 (Core)&#x27;</span><br><br></code></pre></td></tr></table></figure><p>重启服务器<br>安装</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ~]<span class="hljs-comment"># rpm -ivh kmod-kvdo-6.1.0.98-11.el7.centos.x86_64.rpm </span><br>Preparing...                          <span class="hljs-comment">################################# [100%]</span><br>Updating / installing...<br>   1:kmod-kvdo-6.1.0.98-11.el7.centos <span class="hljs-comment">################################# [100%]</span><br>[root@lab101 ~]<span class="hljs-comment"># yum install PyYAML   </span><br>[root@lab101 ~]<span class="hljs-comment"># rpm -ivh vdo-6.1.0.98-13.x86_64.rpm </span><br>Preparing...                          <span class="hljs-comment">################################# [100%]</span><br>Updating / installing...<br>   1:vdo-6.1.0.98-13                  <span class="hljs-comment">################################# [100%]</span><br></code></pre></td></tr></table></figure><p>到这里安装就完成了</p><h3 id="配置VDO"><a href="#配置VDO" class="headerlink" title="配置VDO"></a>配置VDO</h3><p>创建一个vdo卷</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ~]<span class="hljs-comment"># vdo create --name=my_vdo  --device=/dev/sdb1   --vdoLogicalSize=80G --writePolicy=sync</span><br>Creating VDO my_vdo<br>Starting VDO my_vdo<br>Starting compression on VDO my_vdo<br>VDO instance 0 volume is ready at /dev/mapper/my_vdo<br></code></pre></td></tr></table></figure><p>参数解释：<br>name是创建的vdo名称，也就是生成的新设备的名称，device是指定的设备，vdoLogicalSize是指定新生成的设备的大小，因为vdo是支持精简配置的，也就是你原来1T的物理空间，这里可以创建出超过1T的逻辑空间，因为内部支持重删，可以根据数据类型进行放大，writePolicy是指定写入的模式的</p><p>如果磁盘设备是write back模式的可以设置为aysnc，如果没有的话就设置为sync模式</p><p>如果磁盘没有写缓存或者有write throuth cache的时候设置为sync模式<br>如果磁盘有write back cache的时候就必须设置成async模式</p><p>默认是sync模式的，这里的同步异步实际上是告诉vdo，我们的底层存储是不是有写缓存，有缓存的话就要告诉vdo我们底层是async的，没有缓存的时候就是sync</p><p>检查我们的磁盘的写入方式</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ~]<span class="hljs-comment"># cat /sys/block/sdb/device/scsi_disk/0\:0\:1\:0/cache_type </span><br>write through<br></code></pre></td></tr></table></figure><p>这个输出的根据上面的规则，我们设置为sync模式</p><p>修改缓存模式的命令</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">vdo changeWritePolicy --writePolicy=sync_or_async --name=vdo_name<br></code></pre></td></tr></table></figure><p>格式化硬盘</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ~]<span class="hljs-comment"># mkfs.xfs -K /dev/mapper/my_vdo </span><br>meta-data=/dev/mapper/my_vdo     isize=512    agcount=4, agsize=5242880 blks<br>         =                       sectsz=4096  attr=2, projid32bit=1<br>         =                       crc=1        finobt=0, sparse=0<br>data     =                       bsize=4096   blocks=20971520, imaxpct=25<br>         =                       sunit=0      swidth=0 blks<br>naming   =version 2              bsize=4096   ascii-ci=0 ftype=1<br><span class="hljs-built_in">log</span>      =internal <span class="hljs-built_in">log</span>           bsize=4096   blocks=10240, version=2<br>         =                       sectsz=4096  sunit=1 blks, lazy-count=1<br>realtime =none                   extsz=4096   blocks=0, rtextents=0<br></code></pre></td></tr></table></figure><p>使用-K参数是加速了格式化的操作，也就是不发送丢弃的请求，因为之前创建了vdo，已经将其初始化为0了，所以可以采用这个操作</p><p>我们挂载的时候最好能加上discard的选项，精简配置的设备需要对之前的空间进行回收，一般来说有在线的和离线的回收，离线的就通过fstrim来进行回收即可</p><p>挂载设备</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ~]<span class="hljs-comment"># mount -o discard /dev/mapper/my_vdo /myvod/</span><br></code></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ~]<span class="hljs-comment"># vdostats --human-readable </span><br>Device                    Size      Used Available Use% Space saving%<br>/dev/mapper/my_vdo       50.0G      4.0G     46.0G   8%           99%<br></code></pre></td></tr></table></figure><p>默认创建完vdo设备就会占用4G左右的空间，这个用来存储UDS和VDO的元数据</p><p>检查重删和压缩是否开启</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ~]<span class="hljs-comment"># vdo status -n my_vdo|grep Deduplication</span><br>    Deduplication: enabled<br>[root@lab101 ~]<span class="hljs-comment"># vdo status -n my_vdo|grep Compress</span><br>    Compression: enabled<br></code></pre></td></tr></table></figure><p>如果没有开启，可以通过下面的命令开启</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">vdo enableCompression -n &lt;vdo_vol_name&gt;<br>vdo enableDeduplication -n &lt;vdo_vol_name&gt;<br></code></pre></td></tr></table></figure><h3 id="验证重删功能"><a href="#验证重删功能" class="headerlink" title="验证重删功能"></a>验证重删功能</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ~]<span class="hljs-comment"># df -h|grep vdo</span><br>/dev/mapper/my_vdo   80G   33M   80G   1% /myvod<br>[root@lab101 ~]<span class="hljs-comment"># vdostats --hu</span><br>Device                    Size      Used Available Use% Space saving%<br>/dev/mapper/my_vdo       50.0G      4.0G     46.0G   8%           99%<br></code></pre></td></tr></table></figure><p>传入一个ISO文件CentOS-7-x86_64-NetInstall-1708.iso 422M的</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ~]<span class="hljs-comment"># df -h|grep vdo</span><br>/dev/mapper/my_vdo   80G  455M   80G   1% /myvod<br>[root@lab101 ~]<span class="hljs-comment"># vdostats --hu</span><br>Device                    Size      Used Available Use% Space saving%<br>/dev/mapper/my_vdo       50.0G      4.4G     45.6G   8%            9%<br></code></pre></td></tr></table></figure><p>然后重复传入3个相同文件，一共四个文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ~]<span class="hljs-comment"># df -h|grep vdo</span><br>/dev/mapper/my_vdo   80G  1.7G   79G   3% /myvod<br>[root@lab101 ~]<span class="hljs-comment"># vdostats --hu</span><br>Device                    Size      Used Available Use% Space saving%<br>/dev/mapper/my_vdo       50.0G      4.4G     45.6G   8%           73%<br></code></pre></td></tr></table></figure><p>可以看到后面传入的文件，并没有占用底层存储的实际空间</p><h3 id="验证压缩功能"><a href="#验证压缩功能" class="headerlink" title="验证压缩功能"></a>验证压缩功能</h3><p>测试数据来源 silesia的资料库</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">http://sun.aei.polsl.pl/~sdeor/corpus/silesia.zip<br></code></pre></td></tr></table></figure><p>通过资料库里面的文件来看下对不同类型的数据的压缩情况</p><table><thead><tr><th align="center">Filename</th><th align="center">描述</th><th align="center">类型</th><th align="center">原始空间（KB）</th><th align="center">实际占用空间（KB）</th></tr></thead><tbody><tr><td align="center">dickens</td><td align="center">狄更斯文集</td><td align="center">英文原文</td><td align="center">9953</td><td align="center">9948</td></tr><tr><td align="center">mozilla</td><td align="center">Mozilla的1.0可执行文件</td><td align="center">执行程序</td><td align="center">50020</td><td align="center">33228</td></tr><tr><td align="center">mr</td><td align="center">医用resonanse图像</td><td align="center">图片</td><td align="center">9736</td><td align="center">9272</td></tr><tr><td align="center">nci</td><td align="center">结构化的化学数据库</td><td align="center">数据库</td><td align="center">32767</td><td align="center">10168</td></tr><tr><td align="center">ooffice</td><td align="center">Open Office.org 1.01 DLL</td><td align="center">可执行程序</td><td align="center">6008</td><td align="center">5640</td></tr><tr><td align="center">osdb</td><td align="center">基准测试用的MySQL格式示例数据库</td><td align="center">数据库</td><td align="center">9849</td><td align="center">9824</td></tr><tr><td align="center">reymont</td><td align="center">瓦迪斯瓦夫·雷蒙特的书</td><td align="center">PDF</td><td align="center">6471</td><td align="center">6312</td></tr><tr><td align="center">samba</td><td align="center">samba源代码</td><td align="center">src源码</td><td align="center">21100</td><td align="center">11768</td></tr><tr><td align="center">sao</td><td align="center">星空数据</td><td align="center">天文格式的bin文件</td><td align="center">7081</td><td align="center">7036</td></tr><tr><td align="center">webster</td><td align="center">辞海</td><td align="center">HTML</td><td align="center">40487</td><td align="center">40144</td></tr><tr><td align="center">xml</td><td align="center">XML文件</td><td align="center">HTML</td><td align="center">5220</td><td align="center">2180</td></tr><tr><td align="center">x-ray</td><td align="center">透视医学图片</td><td align="center">医院数据</td><td align="center">8275</td><td align="center">8260</td></tr></tbody></table><p>可以看到都有不同程度的压缩，某些类型的数据压缩能达到50%的比例</p><p>停止vdo操作</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ~]<span class="hljs-comment"># vdo stop  -n my_vdo</span><br></code></pre></td></tr></table></figure><p>启动vdo操作</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ~]<span class="hljs-comment"># vdo start  -n my_vdo</span><br></code></pre></td></tr></table></figure><p>删除vdo操作</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ~]<span class="hljs-comment"># vdo remove -n my_vdo</span><br></code></pre></td></tr></table></figure><h2 id="VDO和CEPH能产生什么火花？"><a href="#VDO和CEPH能产生什么火花？" class="headerlink" title="VDO和CEPH能产生什么火花？"></a>VDO和CEPH能产生什么火花？</h2><p>在ceph里面可以用到vdo的地方有两个，一个是作为Kernel rbd的前端，在块设备的上层，另外一个是作为OSD的底层，也就是把VDO当OSD来使用，我们看下怎么使用</p><h3 id="作为rbd的上层"><a href="#作为rbd的上层" class="headerlink" title="作为rbd的上层"></a>作为rbd的上层</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ceph]<span class="hljs-comment"># rbd create testvdorbd --size 20G</span><br>[root@lab101 ceph]<span class="hljs-comment"># rbd map testvdorbd</span><br></code></pre></td></tr></table></figure><p>创建rbd的vdo</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ceph]<span class="hljs-comment"># vdo create --name=rbd_vdo  --device=/dev/rbd/rbd/testvdorbd  --vdoLogicalSize  200G</span><br>Creating VDO rbd_vdo<br>vdo: ERROR -   Device /dev/rbd/rbd/testvdorbd not found (or ignored by filtering).<br></code></pre></td></tr></table></figure><p>被默认排除掉了，这个以前正好见过类似的问题，比较好处理</p><p>这个地方因为vdo添加存储的时候内部调用了lvm相关的配置，然后lvm默认会排除掉rbd，这里修改下lvm的配置文件即可<br>在&#x2F;etc&#x2F;lvm&#x2F;lvm.conf的修改如下</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">types = [ <span class="hljs-string">&quot;fd&quot;</span>, 16 ,<span class="hljs-string">&quot;rbd&quot;</span>, 64 ]<br></code></pre></td></tr></table></figure><p>把types里面增加下rbd 的文件类型即可</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ceph]<span class="hljs-comment"># vdo create --name=rbd_vdo  --device=/dev/rbd/rbd/testvdorbd --vdoLogicalSize  200G</span><br>Creating VDO rbd_vdo<br>Starting VDO rbd_vdo<br>Starting compression on VDO rbd_vdo<br>VDO instance 2 volume is ready at /dev/mapper/rbd_vdo<br></code></pre></td></tr></table></figure><p>这里注意下，设置了逻辑卷空间需要比rbd的设备大，不然无法看到重删的效果，因为如果rbd为100G，vdo之后的卷也为100G，那么因为对外暴露的接口并不会减少数据，所以最多只能存100G的数据，跟rbd大小就一致了，这个就无法看到重删的效果了</p><p>挂载</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">mount -o discard /dev/mapper/rbd_vdo /mnt<br></code></pre></td></tr></table></figure><p>查看容量</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 mnt]<span class="hljs-comment"># vdostats --human-readable</span><br>Device                    Size      Used Available Use% Space saving%<br>/dev/mapper/rbd_vdo      20.0G      4.4G     15.6G  22%            3%<br></code></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 mnt]<span class="hljs-comment"># ceph df</span><br>GLOBAL:<br>    SIZE       AVAIL      bash USED     %bash USED <br>    57316M     49409M        7906M         13.79 <br>POOLS:<br>    NAME     ID     USED     %USED     MAX AVAIL     OBJECTS <br>    rbd      0      566M      1.20        46543M         148 <br></code></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 mnt]<span class="hljs-comment"># ceph df</span><br>GLOBAL:<br>    SIZE       AVAIL      bash USED     %bash USED <br>    57316M     48699M        8616M         15.03 <br>POOLS:<br>    NAME     ID     USED      %USED     MAX AVAIL     OBJECTS <br>    rbd      0      1393M      2.95        45833M         355 <br></code></pre></td></tr></table></figure><p>多次传入相同的时候可以看到对于ceph内部来说还是会产生对象的，只是这个在vdo的文件系统来看是不占用物理空间的</p><p>对镜像做下copy</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ~]<span class="hljs-comment"># rbd cp testvdorbd testvdorbdclone</span><br></code></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ~]<span class="hljs-comment">#rbd map  testvdorbdclone</span><br></code></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ~]<span class="hljs-comment"># cat /etc/vdoconf.yml |grep device</span><br>      device: /dev/rbd/rbd/testvdorbdclone<br></code></pre></td></tr></table></figure><p>修改配置文件为对应的设备，就可以启动了,这个操作说明vdo设备是不绑定硬件的，只需要有相关的配置文件，即可对文件系统进行启动</p><p>那么这个在一个数据转移用途下，就可以利用vdo对数据进行重删压缩，然后把整个img转移到远端去，这个也符合现在的私有云和公有云之间的数据传输量的问题，会节省不少空间</p><h3 id="vdo作为ceph的osd"><a href="#vdo作为ceph的osd" class="headerlink" title="vdo作为ceph的osd"></a>vdo作为ceph的osd</h3><p>ceph对设备的属性有要求，这里直接采用目录部署的方式</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ceph]<span class="hljs-comment"># vdo create --name sdb1 --device=/dev/sdb1</span><br>[root@lab101 ceph]<span class="hljs-comment"># vdo create --name sdb2 --device=/dev/sdb2</span><br>[root@lab101 ceph]<span class="hljs-comment"># mkfs.xfs -K -f /dev/mapper/sdb1</span><br>[root@lab101 ceph]<span class="hljs-comment"># mkfs.xfs -K -f /dev/mapper/sdb2</span><br>[root@lab101 ceph]<span class="hljs-comment"># mkdir /osd1</span><br>[root@lab101 ceph]<span class="hljs-comment"># mkdir /osd2</span><br>[root@lab101 ceph]<span class="hljs-comment"># mount /dev/mapper/sdb1 /osd1/</span><br>[root@lab101 ceph]<span class="hljs-comment"># mount /dev/mapper/sdb2 /osd2/</span><br>[root@lab101 ceph]<span class="hljs-comment"># chown ceph:ceph /osd1</span><br>[root@lab101 ceph]<span class="hljs-comment"># chown ceph:ceph /osd2</span><br>[root@lab101 ceph]<span class="hljs-comment"># ceph-deploy osd prepare lab101:/osd1/</span><br>[root@lab101 ceph]<span class="hljs-comment"># ceph-deploy osd prepare lab101:/osd2/</span><br>[root@lab101 ceph]<span class="hljs-comment"># ceph-deploy osd activate lab101:/osd1/</span><br>[root@lab101 ceph]<span class="hljs-comment"># ceph-deploy osd activate lab101:/osd2/</span><br></code></pre></td></tr></table></figure><p>写入测试数据</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ceph]<span class="hljs-comment"># rados  -p rbd bench 60 write --no-cleanup</span><br></code></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ceph]<span class="hljs-comment"># df -h</span><br>Filesystem        Size  Used Avail Use% Mounted on<br>/dev/sda2          56G  2.0G   54G   4% /<br>devtmpfs          983M     0  983M   0% /dev<br>tmpfs             992M     0  992M   0% /dev/shm<br>tmpfs             992M  8.8M  983M   1% /run<br>tmpfs             992M     0  992M   0% /sys/fs/cgroup<br>/dev/sda1        1014M  151M  864M  15% /boot<br>tmpfs             199M     0  199M   0% /run/user/0<br>/dev/mapper/sdb1   22G  6.5G   16G  30% /osd1<br>/dev/mapper/sdb2   22G  6.5G   16G  30% /osd2<br>[root@lab101 ceph]<span class="hljs-comment"># vdostats --human-readable </span><br>Device                    Size      Used Available Use% Space saving%<br>/dev/mapper/sdb2         25.0G      3.0G     22.0G  12%           99%<br>/dev/mapper/sdb1         25.0G      3.0G     22.0G  12%           99%<br></code></pre></td></tr></table></figure><p>可以看到虽然在df看到了空间的占用，实际上因为rados bench写入的是填充的空洞数据，vdo作为osd对数据直接进行了重删了，测试可以看到vdo是可以作为ceph osd的，由于我的测试环境是在vmware虚拟机里面的，所以并不能做性能测试，有硬件环境的情况下可以对比下开启vdo和不开启的情况的性能区别</p><h2 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h2><p><a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/storage_administration_guide/vdo-qs-creating-a-volume">vdo-qs-creating-a-volume</a><br><a href="https://rhelblog.redhat.com/tag/vdo/">Determining the space savings of virtual data optimizer (VDO) in RHEL 7.5 Beta</a></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本篇从配置和部署以及适配方面对vdo进行一次比较完整的实践，从目前的测试情况来看，配置简单，对环境友好，基本是可以作为一个驱动层嵌入到任何块设备之上的，未来应该有广泛的用途，目前还不清楚红帽是否会把这个属性放到centos下面去，目前可以通过在<a href="https://access.redhat.com/downloads/">https://access.redhat.com/downloads/</a> 申请测试版本的ISO进行功能的测试</p><p>应该是农历年前的最后一篇文章了，祝新春快乐！</p><h2 id="变更记录"><a href="#变更记录" class="headerlink" title="变更记录"></a>变更记录</h2><table><thead><tr><th align="center">Why</th><th align="center">Who</th><th align="center">When</th></tr></thead><tbody><tr><td align="center">创建</td><td align="center">武汉-运维-磨渣</td><td align="center">2018-02-10</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>定位一个网络问题引起的ceph异常</title>
    <link href="/2018/01/16/%E5%AE%9A%E4%BD%8D%E4%B8%80%E4%B8%AA%E7%BD%91%E7%BB%9C%E9%97%AE%E9%A2%98%E5%BC%95%E8%B5%B7%E7%9A%84ceph%E5%BC%82%E5%B8%B8/"/>
    <url>/2018/01/16/%E5%AE%9A%E4%BD%8D%E4%B8%80%E4%B8%AA%E7%BD%91%E7%BB%9C%E9%97%AE%E9%A2%98%E5%BC%95%E8%B5%B7%E7%9A%84ceph%E5%BC%82%E5%B8%B8/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>有一个ceph环境出现了异常，状态就是恢复异常的慢，但是所有数据又都在走，只是非常的慢，本篇将记录探测出问题的过程，以便以后处理类似的问题有个思路</p><h2 id="处理过程"><a href="#处理过程" class="headerlink" title="处理过程"></a>处理过程</h2><p>问题的现象是恢复的很慢，但是除此以外并没有其它的异常，通过iostat监控磁盘，也没有出现异常的100%的情况，暂时排除了是osd底层慢的问题</p><h3 id="检测整体写入的速度"><a href="#检测整体写入的速度" class="headerlink" title="检测整体写入的速度"></a>检测整体写入的速度</h3><p>通过rados bench写入</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">rados -p rbd bench 5 write<br></code></pre></td></tr></table></figure><p>刚开始写入的时候没问题，但是写入了以后不久就会出现一只是0的情况，可以判断在写入某些对象的时候出现了异常</p><p>本地生成一些文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">seq</span> 0 30|xargs -i <span class="hljs-built_in">dd</span> <span class="hljs-keyword">if</span>=/dev/zero of=benchmarkzp&#123;&#125; bs=4M count=2<br></code></pre></td></tr></table></figure><p>通过rados put 命令把对象put进去</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-keyword">for</span> a <span class="hljs-keyword">in</span> `<span class="hljs-built_in">ls</span> ./`;<span class="hljs-keyword">do</span> time rados -p rbd put <span class="hljs-variable">$a</span> <span class="hljs-variable">$a</span>;<span class="hljs-built_in">echo</span> <span class="hljs-variable">$a</span>;ceph osd map rbd <span class="hljs-variable">$a</span>;<span class="hljs-keyword">done</span><br></code></pre></td></tr></table></figure><p>得到的结果里面会有部分是好的，部分是非常长的时间，对结果进行过滤，分为bad 和good</p><p>开始怀疑会不会是固定的盘符出了问题，首先把磁盘组合分出来，完全没问题的磁盘全部排除，结果最后都排除完了，所以磁盘本省是没问题的</p><h3 id="根据pg的osd组合进行主机分类"><a href="#根据pg的osd组合进行主机分类" class="headerlink" title="根据pg的osd组合进行主机分类"></a>根据pg的osd组合进行主机分类</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs bash">1  2  4  ok<br>3  1   2  bad<br>2  4   1 ok<br>3  1 2   bad<br>3  4  2  bad<br>……<br></code></pre></td></tr></table></figure><p>上面的编号是写入对象所在的pg对应的osd所在的主机，严格按照顺序写入，第一个主机为发送数据方，第二个和第三个为接收数据方，并且使用了cluster network</p><p>通过上面的结果发现了从3往2进行发送副本数据的时候出现了问题，然后去主机上排查网络</p><p>在主机2上面做iperf -s<br>在主机3上面做iperf -c host2然后就发现了网络异常了</p><p>最终还是定位在了网络上面</p><p>已经在好几个环境上面发现没装可以监控实时网络流量dstat工具或者ifstat的动态监控，做操作的时候监控下网络，可以发现一些异常</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>这个环境在最开始的时候就怀疑是网络可能有问题，但是没有去进行全部服务器的网络的检测，这个在出现一些奇奇怪怪的异常的时候，还是可能出现在网络上面，特别是这种坏掉又不是完全坏掉，只是掉速的情况，通过集群的一些内部告警还没法完全体现出来，而主机很多的时候，又没有多少人愿意一个个的去检测，就容易出现这种疏漏了</p><p>在做一个ceph的管理平台的时候，对整个集群做全员对等网络带宽测试还是很有必要的，如果有一天我来设计管理平台，一定会加入这个功能进去</p><h2 id="变更记录"><a href="#变更记录" class="headerlink" title="变更记录"></a>变更记录</h2><table><thead><tr><th align="center">Why</th><th align="center">Who</th><th align="center">When</th></tr></thead><tbody><tr><td align="center">创建</td><td align="center">武汉-运维-磨渣</td><td align="center">2018-01-16</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Kernel RBD的QOS配置方案</title>
    <link href="/2018/01/05/Kernel%20RBD%E7%9A%84QOS%E9%85%8D%E7%BD%AE%E6%96%B9%E6%A1%88/"/>
    <url>/2018/01/05/Kernel%20RBD%E7%9A%84QOS%E9%85%8D%E7%BD%AE%E6%96%B9%E6%A1%88/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>关于qos的讨论有很多，ceph内部也正在实现着一整套的基于dmclock的qos的方案，这个不是本篇的内容，之前在社区的邮件列表看过有研发在聊qos的相关的实现的，当时一个研发就提出了在使用kernel rbd的时候，可以直接使用linux的操作系统qos来实现，也就是cgroup来控制读取写入</p><p>cgroup之前也有接触过，主要测试了限制cpu和内存相关的，没有做io相关的测试，这个当然可以通过ceph内部来实现qos，但是有现成的解决方案的时候，可以减少很多开发周期，以及测试的成本</p><p>本篇将介绍的是kernel rbd的qos方案</p><h2 id="时间过长"><a href="#时间过长" class="headerlink" title="时间过长"></a>时间过长</h2><p>首先介绍下几个测试qos相关的命令，用来比较设置前后的效果<br>验证写入IOPS命令</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">fio -filename=/dev/rbd0 -direct=1 -iodepth 1 -thread -rw=write -ioengine=libaio -bs=4K -size=1G -numjobs=1 -runtime=60 -group_reporting -name=mytest<br></code></pre></td></tr></table></figure><p>验证写入带宽的命令</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">fio -filename=/dev/rbd0 -direct=1 -iodepth 1 -thread -rw=write -ioengine=libaio -bs=4M -size=1G -numjobs=1 -runtime=60 -group_reporting -name=mytest<br></code></pre></td></tr></table></figure><p>验证读取IOPS命令</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">fio -filename=/dev/rbd0 -direct=1 -iodepth 1 -thread -rw=<span class="hljs-built_in">read</span> -ioengine=libaio -bs=4K -size=1G -numjobs=1 -runtime=60 -group_reporting -name=mytest<br></code></pre></td></tr></table></figure><p>验证读取带宽命令</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">fio -filename=/dev/rbd0 -direct=1 -iodepth 1 -thread -rw=<span class="hljs-built_in">read</span> -ioengine=libaio -bs=4M -size=1G -numjobs=1 -runtime=60 -group_reporting -name=mytest<br></code></pre></td></tr></table></figure><p>上面为什么会设置不同的块大小，这个是因为测试的存储是会受到带宽和iops的共同制约的，当测试小io的时候，这个时候的峰值是受到iops的限制的，测试大io的时候，受到的是带宽限制，所以在做测试的时候，需要测试iops是否被限制住的时候就使用小的bs&#x3D;4K，需要测试大的带宽的限制的时候就采用bs&#x3D;4M来测试</p><p>测试的时候都是，开始不用做qos来进行测试得到一个当前不配置qos的性能数值，然后根据需要进行qos设置后通过上面的fio去测试是否能限制住</p><p>启用cgroup的blkio模块</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">mkdir</span> -p  /cgroup/blkio/<br>mount -t cgroup -o blkio blkio /cgroup/blkio/<br></code></pre></td></tr></table></figure><p>获取rbd磁盘的major&#x2F;minor numbers</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab211 ~]<span class="hljs-comment"># lsblk </span><br>NAME   MAJ:MIN RM   SIZE RO TYPE MOUNTPOINT<br>rbd0   252:0    0  19.5G  0 disk <br>sda      8:0    1 238.4G  0 disk <br>├─sda4   8:4    1     1K  0 part <br>├─sda2   8:2    1  99.9G  0 part <br>├─sda5   8:5    1     8G  0 part [SWAP]<br>├─sda3   8:3    1     1G  0 part /boot<br>├─sda1   8:1    1   100M  0 part <br>└─sda6   8:6    1 129.4G  0 part /<br></code></pre></td></tr></table></figure><p>通过lsblk命令可以获取到磁盘对应的major number和minor number,这里可以看到rbd0对应的编号为252:0</p><p>设置rbd0的iops的qos为10</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;252:0 10&quot;</span> &gt; /cgroup/blkio/blkio.throttle.write_iops_device<br></code></pre></td></tr></table></figure><p>如果想清理这个规则,把后面的数值设置为0就清理了，后面几个配置也是相同的方法</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;252:0 0&quot;</span> &gt; /cgroup/blkio/blkio.throttle.write_iops_device<br></code></pre></td></tr></table></figure><p>限制写入的带宽为10MB&#x2F;s</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;252:0 10485760&quot;</span> &gt; /cgroup/blkio/blkio.throttle.write_bps_device<br></code></pre></td></tr></table></figure><p>限制读取的qos为10</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;252:0 10&quot;</span> &gt; /cgroup/blkio/blkio.throttle.read_iops_device<br></code></pre></td></tr></table></figure><p>限制读取的带宽为10MB&#x2F;s</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;252:0 10485760&quot;</span> &gt; /cgroup/blkio/blkio.throttle.read_bps_device<br></code></pre></td></tr></table></figure><p>以上简单的设置就完成了kernel rbd的qos设置了，我测试了下，确实是生效了的</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>这个知识点很久前就看到了，一直没总结，现在记录下，个人观点是能快速，有效，稳定的实现功能是最好的，所以使用这个在kernel rbd方式下可以不用再进行qos的开发了</p><h2 id="变更记录"><a href="#变更记录" class="headerlink" title="变更记录"></a>变更记录</h2><table><thead><tr><th align="center">Why</th><th align="center">Who</th><th align="center">When</th></tr></thead><tbody><tr><td align="center">创建</td><td align="center">武汉-运维-磨渣</td><td align="center">2018-01-05</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Ceph对象主本损坏的修复方法</title>
    <link href="/2018/01/02/Ceph%E5%AF%B9%E8%B1%A1%E4%B8%BB%E6%9C%AC%E6%8D%9F%E5%9D%8F%E7%9A%84%E4%BF%AE%E5%A4%8D%E6%96%B9%E6%B3%95/"/>
    <url>/2018/01/02/Ceph%E5%AF%B9%E8%B1%A1%E4%B8%BB%E6%9C%AC%E6%8D%9F%E5%9D%8F%E7%9A%84%E4%BF%AE%E5%A4%8D%E6%96%B9%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>问题的触发是在进行一个目录的查询的时候，osd就会挂掉，开始以为是osd操作超时了，后来发现每次访问这个对象都有问题</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">log</span> [WRN] ： slow request 60.793196 seconds old, received at osd_op(mds.0.188:728345234100006c6ddc.00000000 [o map-get-header 0-0,omap-get-vals 0~16,getxattr parent] snapc 0=[] ack+<span class="hljs-built_in">read</span>+known_if_redirected+full_force e218901) currently started<br>heartbeat_map is_healthy  ··· osd_op_tp thread ··· had timed out after 60<br></code></pre></td></tr></table></figure><p>这个对象是元数据的一个空对象，保留数据在扩展属性当中</p><p>然后做了一个操作判断是对象损坏了:</p><p>直接列取omapkeys</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">rados -p metadata listomapvals 100006c6ddc.00000000<br></code></pre></td></tr></table></figure><p>发现会卡住，然后关闭这个osd再次做操作，就可以了，启动后还是不行，这里可以判断是主本的对象已经有问题了，本篇将讲述多种方法来解决这个问题</p><h2 id="处理办法"><a href="#处理办法" class="headerlink" title="处理办法"></a>处理办法</h2><p>本章将会根据操作粒度的不同来讲述三种方法的恢复，根据自己的实际情况，和风险的判断来选择自己的操作</p><h3 id="方法一：通过repair修复"><a href="#方法一：通过repair修复" class="headerlink" title="方法一：通过repair修复"></a>方法一：通过repair修复</h3><p>首先能确定是主本损坏了，那么先把主本的对象进行一个备份，然后移除</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab71 2.0_head]<span class="hljs-comment"># systemctl stop ceph-osd@0</span><br>[root@lab71 2.0_head]<span class="hljs-comment"># cp -ra 100.00000000__head_C5265AB3__2 ../../</span><br></code></pre></td></tr></table></figure><p>通过ceph-object-tool进行移除的时候有bug,无法移除metadata的对象，已经提了一个<a href="http://tracker.ceph.com/issues/22553">bug</a></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab71 2.0_head]<span class="hljs-comment"># mv 100.00000000__head_C5265AB3__2 ../</span><br></code></pre></td></tr></table></figure><p>注意一下在老版本的时候，对对象进行删除以后，可能元数据里面记录了对象信息，而对象又不在的时候可能会引起osd无法启动，这个在10.2.10是没有这个问题</p><p>重启osd</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab71 2.0_head]<span class="hljs-comment"># systemctl restart ceph-osd@0</span><br></code></pre></td></tr></table></figure><p>对pg做scrub</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab71 2.0_head]<span class="hljs-comment"># ceph pg scrub 2.0</span><br>instructing pg 2.0 on osd.0 to scrub<br></code></pre></td></tr></table></figure><p>这种方法就是需要做scrub的操作，如果对象特别多，并且是线上环境，可能不太好去做scrub的操作<br>检查状态</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab71 2.0_head]<span class="hljs-comment"># ceph -s</span><br>    cluster 03580f14-9906-4257-9182-65c886e7f5a7<br>     health HEALTH_ERR<br>            1 pgs inconsistent<br>            1 scrub errors<br>            too few PGs per OSD (3 &lt; min 30)<br>     monmap e1: 1 mons at &#123;lab71=20.20.20.71:6789/0&#125;<br>            election epoch 4, quorum 0 lab71<br>      fsmap e30: 1/1/1 up &#123;0=lab71=up:active&#125;<br>     osdmap e101: 2 osds: 2 up, 2 <span class="hljs-keyword">in</span><br>            flags sortbitwise,require_jewel_osds<br>      pgmap v377: 3 pgs, 3 pools, 100814 bytes data, 41 objects<br>            70196 kB used, 189 GB / 189 GB avail<br>                   2 active+clean<br>                   1 active+clean+inconsistent<br></code></pre></td></tr></table></figure><p>发起修复请求</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab71 2.0_head]<span class="hljs-comment"># ceph pg repair 2.0</span><br>instructing pg 2.0 on osd.0 to repair<br></code></pre></td></tr></table></figure><p>修复完成后检查集群状态和对象，到这里可以恢复正常了</p><h3 id="方法二：通过rsync拷贝数据方式恢复"><a href="#方法二：通过rsync拷贝数据方式恢复" class="headerlink" title="方法二：通过rsync拷贝数据方式恢复"></a>方法二：通过rsync拷贝数据方式恢复</h3><p>跟上面一样这里首先能确定是主本损坏了，那么先把主本的对象进行一个备份，然后移除</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab71 2.0_head]<span class="hljs-comment"># systemctl stop ceph-osd@0</span><br>[root@lab71 2.0_head]<span class="hljs-comment"># cp -ra 100.00000000__head_C5265AB3__2 ../../</span><br></code></pre></td></tr></table></figure><p>移除对象</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab71 2.0_head]<span class="hljs-comment"># mv 100.00000000__head_C5265AB3__2 ../</span><br></code></pre></td></tr></table></figure><p>在副本的机器上执行rsync命令，这里我们直接从副本拷贝对象过来，注意下不能直接使用scp会掉扩展属性</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab72 2.0_head]<span class="hljs-comment"># rsync  -avXH  /var/lib/ceph/osd/ceph-1/current/2.0_head/100.00000000__head_C5265AB3__2 20.20.20.71:/var/lib/ceph/osd/ceph-0/current/2.0_head/100.00000000__head_C5265AB3__2</span><br></code></pre></td></tr></table></figure><p>在主本机器检查扩展属性</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab71 2.0_head]<span class="hljs-comment"># getfattr 100.00000000__head_C5265AB3__2 </span><br><span class="hljs-comment"># file: 100.00000000__head_C5265AB3__2</span><br>user.ceph._<br>user.ceph._@1<br>user.ceph.snapset<br>user.cephos.spill_out<br></code></pre></td></tr></table></figure><p>重启osd</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab71 2.0_head]<span class="hljs-comment"># systemctl restart ceph-osd@0</span><br></code></pre></td></tr></table></figure><p>检查对象的扩展属性</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab71 2.0_head]<span class="hljs-comment"># rados -p metadata listomapvals 100.00000000</span><br></code></pre></td></tr></table></figure><h3 id="方法三：通过删除PG的方式恢复"><a href="#方法三：通过删除PG的方式恢复" class="headerlink" title="方法三：通过删除PG的方式恢复"></a>方法三：通过删除PG的方式恢复</h3><p>这个方式是删除PG，然后重新启动的方式<br>这种方式操作比较危险，所以提前备份好pg的数据，最好主备pg都备份下，万一出了问题或者数据不对，可以根据需要再导入<br>备份PG</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">ceph-objectstore-tool --pgid 2.0 --op <span class="hljs-built_in">export</span> --data-path /var/lib/ceph/osd/ceph-0/ --journal-path   /var/lib/ceph/osd/ceph-0/journal --file /root/2.0<br></code></pre></td></tr></table></figure><p>删除PG的操作</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab71 current]<span class="hljs-comment"># ceph-objectstore-tool --pgid 2.0  --op remove --data-path /var/lib/ceph/osd/ceph-0/ --journal-path /var/lib/ceph/osd/ceph-0/journal</span><br>SG_IO: bad/missing sense data, sb[]:  70 00 05 00 00 00 00 0a 00 00 00 00 20 00 00 c0 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00<br>SG_IO: bad/missing sense data, sb[]:  70 00 05 00 00 00 00 0a 00 00 00 00 20 00 00 c0 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00<br> marking collection <span class="hljs-keyword">for</span> removal<br>setting <span class="hljs-string">&#x27;_remove&#x27;</span> omap key<br>finish_remove_pgs 2.0_head removing 2.0<br>Remove successful<br></code></pre></td></tr></table></figure><p>重启osd</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab71 current]<span class="hljs-comment"># systemctl restart ceph-osd@0</span><br></code></pre></td></tr></table></figure><p>等待回复即可</p><p>本方法里面还可以衍生一种就是，通过导出的副本的PG数据,在主本删除了相应的PG以后,进行导入的方法，这样就不会产生迁移</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab71 current]<span class="hljs-comment">#  ceph-objectstore-tool --pgid 2.0  --op import --data-path /var/lib/ceph/osd/ceph-0/ --journal-path /var/lib/ceph/osd/ceph-0/journal --file /root/2.0</span><br></code></pre></td></tr></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>上面用三种方法来实现了副本向主本同步的操作，判断主本是否有问题的方法就是主动的把主本所在的OSD停掉，然后检查请求是否可达，在确定主本已经坏掉的情况下，就可以做将副本同步到主本的操作，可以根据PG的对象的多少来选择需要做哪种操作</p><h2 id="变更记录"><a href="#变更记录" class="headerlink" title="变更记录"></a>变更记录</h2><table><thead><tr><th align="center">Why</th><th align="center">Who</th><th align="center">When</th></tr></thead><tbody><tr><td align="center">创建</td><td align="center">武汉-运维-磨渣</td><td align="center">2018-01-02</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>mds的cpu占用问题分析以及解决办法</title>
    <link href="/2017/12/04/mds%E7%9A%84cpu%E5%8D%A0%E7%94%A8%E9%97%AE%E9%A2%98%E5%88%86%E6%9E%90%E4%BB%A5%E5%8F%8A%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95/"/>
    <url>/2017/12/04/mds%E7%9A%84cpu%E5%8D%A0%E7%94%A8%E9%97%AE%E9%A2%98%E5%88%86%E6%9E%90%E4%BB%A5%E5%8F%8A%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>mds是ceph里面处理文件接口的组件，一旦使用文件系统，不可避免的会出现一种场景就是目录很多，目录里面的文件很多，而mds是一个单进程的组件，现在虽然有了muti mds，但稳定的使用的大部分场景还是单acitve mds的</p><p>这就会出现一种情况，一旦一个目录里面有很多文件的时候，去查询这个目录里的文件就会在当前目录做一次遍历，这个需要一个比较长的时间，如果能比较好的缓存文件信息，也能避免一些过载情况，本篇讲述的是内核客户端正常，而export nfs后mds的负载长时间过高的情况</p><h2 id="问题复现"><a href="#问题复现" class="headerlink" title="问题复现"></a>问题复现</h2><h3 id="准备测试数据-准备好监控环境"><a href="#准备测试数据-准备好监控环境" class="headerlink" title="准备测试数据,准备好监控环境"></a>准备测试数据,准备好监控环境</h3><p>监控mds cpu占用</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">pidstat -u  1 -p 27076 &gt; /tmp/mds.cpu.log<br>UserParameter=mds.cpu,<span class="hljs-built_in">cat</span> /tmp/mds.cpu.log|<span class="hljs-built_in">tail</span> -n 1|grep -v Average| awk <span class="hljs-string">&#x27;&#123;print $8&#125;&#x27;</span><br></code></pre></td></tr></table></figure><p>整个测试避免屏幕的打印影响时间统计,把输出需要重定向<br>测试一：<br>内核客户端写入10000文件查看时间以及cpu占用</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@nfsserver kc10000]<span class="hljs-comment"># time seq 10000|xargs -i dd if=/dev/zero of=a&#123;&#125; bs=1K count=1  2&gt;/dev/null</span><br>real0m30.121s<br>user0m1.901s<br>sys0m10.420s<br></code></pre></td></tr></table></figure><p><img src="/images/blog/o_200901085303aa.png" alt="aa.png-32.5kB"></p><p>测试二：<br>内核客户端写入20000文件查看时间以及cpu占用</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@nfsserver kc20000]<span class="hljs-comment"># time seq 20000|xargs -i dd if=/dev/zero of=a&#123;&#125; bs=1K count=1  2&gt;/dev/null</span><br>real1m38.233s<br>user0m3.761s<br>sys0m21.510s<br></code></pre></td></tr></table></figure><p><img src="/images/blog/o_200901085311bbb.png" alt="bbb.png-39kB"><br>测试三：<br>内核客户端写入40000文件查看时间以及cpu占用</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@nfsserver kc40000]<span class="hljs-comment">#  time seq 40000|xargs -i dd if=/dev/zero of=a&#123;&#125; bs=1K count=1  2&gt;/dev/null</span><br><br>real2m55.261s<br>user0m7.699s<br>sys0m42.410s<br></code></pre></td></tr></table></figure><p><img src="/images/blog/o_200901085317cccc.png" alt="cccc.png-57.3kB"></p><p>测试4：<br>内核客户端列目录10000文件，第一次写完有缓存情况</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@nfsserver kc10000]<span class="hljs-comment"># time ll 2&gt;&amp;1 &gt; /dev/null</span><br><br>real0m0.228s<br>user0m0.063s<br>sys0m0.048s<br></code></pre></td></tr></table></figure><p>内核客户端列目录20000文件，第一次写完有缓存情况</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@nfsserver kc20000]<span class="hljs-comment"># time ll 2&gt;&amp;1 &gt; /dev/null</span><br><br>real0m0.737s<br>user0m0.141s<br>sys0m0.092s<br></code></pre></td></tr></table></figure><p>内核客户端列目录40000文件，第一次写完有缓存情况</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@nfsserver kc40000]<span class="hljs-comment"># time ll 2&gt;&amp;1 &gt; /dev/null</span><br><br>real0m1.658s<br>user0m0.286s<br>sys0m0.196s<br></code></pre></td></tr></table></figure><p>都是比较快的返回，CPU可以忽略不计</p><p>现在重启mds后再次列目录<br>客户端如果不umount,直接重启mds的话,还是会缓存在<br>新版本这个地方好像已经改了（重启了mds 显示inode还在，但是随着时间的增长inode会减少，说明还是有周期，会释放，这个还不知道哪个地方控制，用什么参数控制，这个不是本篇着重关注的地方，后续再看下,jewel版本已经比hammer版本的元数据时间快了很多了）</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@nfsserver kc10000]<span class="hljs-comment"># time ll 2&gt;&amp;1 &gt; /dev/null</span><br><br>real0m0.380s<br>user0m0.065s<br>sys0m0.041s<br>[root@nfsserver kc10000]<span class="hljs-comment"># cd ../kc20000/</span><br>[root@nfsserver kc20000]<span class="hljs-comment"># time ll 2&gt;&amp;1 &gt; /dev/null</span><br><br>real0m0.868s<br>user0m0.154s<br>sys0m0.074s<br>[root@nfsserver kc20000]<span class="hljs-comment"># cd ../kc40000/</span><br>[root@nfsserver kc40000]<span class="hljs-comment"># time ll 2&gt;&amp;1 &gt; /dev/null</span><br><br>real0m1.947s<br>user0m0.300s<br>sys0m0.166s<br></code></pre></td></tr></table></figure><p>测试都是看到很快的返回，以上都是正常的，下面开始将这个目录exportnfs出去，看下是个什么情况</p><h3 id="负载问题复现"><a href="#负载问题复现" class="headerlink" title="负载问题复现"></a>负载问题复现</h3><p>从nfs客户端第一次列10000个小文件的目录</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@nfsclient kc10000]<span class="hljs-comment"># time ll 2&gt;&amp;1 &gt; /dev/null</span><br><br>real0m4.038s<br>user0m0.095s<br>sys0m0.069s<br></code></pre></td></tr></table></figure><p><img src="/images/blog/o_200901085323nfs10000.png" alt="nfs10000.png-36.7kB"></p><p>从nfs客户端第一次列20000个小文件的目录</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@nfsclient kc20000]<span class="hljs-comment"># time ll 2&gt;&amp;1 &gt; /dev/null</span><br><br>real0m17.446s<br>user0m0.175s<br>sys0m0.141s<br></code></pre></td></tr></table></figure><p><img src="/images/blog/o_200901085331nfs20000.png" alt="nfs20000.png-43.2kB"><br>从nfs客户端第二次列20000个小文件目录</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@nfsclient kc20000]<span class="hljs-comment"># time ll 2&gt;&amp;1 &gt; /dev/null</span><br><br>real0m21.215s<br>user0m0.182s<br>sys0m0.151s<br></code></pre></td></tr></table></figure><p><img src="/images/blog/o_200901085338nfs200002.png" alt="nfs200002.png-56.7kB"></p><p>从nfs客户端第三次列20000个小文件目录</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@nfsclient kc20000]<span class="hljs-comment"># time ll 2&gt;&amp;1 &gt; /dev/null</span><br><br>real0m16.222s<br>user0m0.189s<br>sys0m0.143s<br></code></pre></td></tr></table></figure><p>可以看到在20000量级的时候列目录维持在20000左右，CPU维持一个高位</p><p>从nfs客户端列40000个小文件的目录</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@nfsclient kc40000]<span class="hljs-comment"># time ll 2&gt;&amp;1 &gt; /dev/null</span><br><br>real7m15.663s<br>user0m0.319s<br>sys0m0.581s<br>[root@nfsclient kc40000]<span class="hljs-comment"># </span><br></code></pre></td></tr></table></figure><p><img src="/images/blog/o_200901085344nfs40000.png" alt="nfs40000.png-77.2kB"><br>第一次列完，马上第二次列看下情况</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@nfsclient kc40000]<span class="hljs-comment"># time ll 2&gt;&amp;1 &gt; /dev/null</span><br><br>real1m12.816s<br>user0m0.163s<br>sys0m0.142s<br></code></pre></td></tr></table></figure><p>可以看到第二次列的时间已经缩短了，再来第三次</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@nfsclient kc40000]<span class="hljs-comment"># time ll 2&gt;&amp;1 &gt; /dev/null</span><br><br>real1m33.549s<br>user0m0.162s<br>sys0m0.183s<br></code></pre></td></tr></table></figure><p><img src="/images/blog/o_200901085351nfs400003.png" alt="nfs400003.png-61.3kB"><br>可以看到在后面列的时候时间确实缩短了，但是还是维持一个非常高CPU的占用，以及比较长的一个时间，这个很容易造成过载</p><p>这个地方目前看应该是内核客户端与内核NFS的结合的问题</p><h2 id="解决办法-用ganesha的ceph用户态接口替代kernel-nfs"><a href="#解决办法-用ganesha的ceph用户态接口替代kernel-nfs" class="headerlink" title="解决办法:用ganesha的ceph用户态接口替代kernel nfs"></a>解决办法:用ganesha的ceph用户态接口替代kernel nfs</h2><p>我们看下另外一种方案用户态的NFS+ceph同样的环境下测试结果：</p><p>从nfs客户端第一次列40000个小文件的目录</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@nfsclient kc40000]<span class="hljs-comment"># time ll 2&gt;&amp;1 &gt; /dev/null</span><br><br>real0m3.289s<br>user0m0.335s<br>sys0m0.386s<br></code></pre></td></tr></table></figure><p>从nfs客户端第二次列40000个小文件的目录</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@nfsclient kc40000]<span class="hljs-comment"># time ll 2&gt;&amp;1 &gt; /dev/null</span><br><br>real0m1.686s<br>user0m0.351s<br>sys0m0.389s<br></code></pre></td></tr></table></figure><p>从nfs客户端第三次列40000个小文件的目录</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@nfsclient kc40000]<span class="hljs-comment"># time ll 2&gt;&amp;1 &gt; /dev/null</span><br><br>real0m1.675s<br>user0m0.320s<br>sys0m0.391s<br></code></pre></td></tr></table></figure><p><img src="/images/blog/o_200901085357ganesha.png" alt="ganesha.png-51.5kB"><br>基本mds无多余的负载，非常快的返回</p><p>可以从上面的测试看到差别是非常的大的，这个地方应该是内核模块与内核之间的问题，而采用用户态的以后解决了列目录慢以及卡顿的问题</p><h2 id="如何配置ganesha支持ceph的nfs接口"><a href="#如何配置ganesha支持ceph的nfs接口" class="headerlink" title="如何配置ganesha支持ceph的nfs接口"></a>如何配置ganesha支持ceph的nfs接口</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs bash">git <span class="hljs-built_in">clone</span> -b V2.3-stable https://github.com/nfs-ganesha/nfs-ganesha.git<br><span class="hljs-built_in">cd</span> nfs-ganesha/<br>git submodule update --init --recursive<br><span class="hljs-built_in">cd</span> ..<br><span class="hljs-built_in">cd</span> nfs-ganesha/<br>ll src/FSAL/FSAL_CEPH/<br><span class="hljs-built_in">cd</span> ..<br><span class="hljs-built_in">mkdir</span> mybuild<br><span class="hljs-built_in">cd</span> mybuild/<br>cmake -DUSE_FSAL_CEPH=ON ../nfs-ganesha/src/<br>ll FSAL/FSAL_CEPH/<br>make<br>make -j 12<br>make install<br></code></pre></td></tr></table></figure><p>vim &#x2F;etc&#x2F;ganesha&#x2F;ganesha.conf<br>修改配置文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs bash">EXPORT<br>&#123;<br>    Export_ID=1;<br><br>    Path = <span class="hljs-string">&quot;/&quot;</span>;<br><br>    Pseudo = <span class="hljs-string">&quot;/&quot;</span>;<br><br>    Access_Type = RW;<br><br>    NFS_Protocols = 4;<br><br>    Transport_Protocols = TCP;<br><br>    FSAL &#123;<br>        Name = CEPH;<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure><p>停止掉原生的nfs</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">systemctl stop nfs<br></code></pre></td></tr></table></figure><p>启用ganesha nfs</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">systemctl start  nfs-ganesha.service<br></code></pre></td></tr></table></figure><p>然后在客户端进行nfs的挂载即可</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>ganesha在需要用到cephfs又正好是要用到nfs接口的时候，可以考虑这个方案，至少在缓存文件，降低负载上面能够比kernel client有更好的效果，这个可以根据测试情况用数据来做比较</p><h2 id="变更记录"><a href="#变更记录" class="headerlink" title="变更记录"></a>变更记录</h2><table><thead><tr><th align="center">Why</th><th align="center">Who</th><th align="center">When</th></tr></thead><tbody><tr><td align="center">创建</td><td align="center">武汉-运维-磨渣</td><td align="center">2017-12-04</td></tr></tbody></table><p>&#x2F;assets&#x2F;images&#x2F;blogimg&#x2F;mds-use-too-more-cpu&#x2F;<br><img src="http://cloudshouji.com/wp-content/uploads/2020/03/aa.png"></p>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>CentOS GRUB损坏修复方法</title>
    <link href="/2017/11/30/CentOS%20GRUB%E6%8D%9F%E5%9D%8F%E4%BF%AE%E5%A4%8D%E6%96%B9%E6%B3%95/"/>
    <url>/2017/11/30/CentOS%20GRUB%E6%8D%9F%E5%9D%8F%E4%BF%AE%E5%A4%8D%E6%96%B9%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>博客很久没有更新了，一个原因就是原来存放部署博客的环境坏了，硬盘使用的是SSD，只要读取到某个文件，整个磁盘就直接识别不到了，还好博客环境之前有做备份，最近一直没有把部署环境做下恢复，今天抽空把环境做下恢复并且记录一篇基础的GRUB的处理文档</p><p>这两天正好碰到GRUB损坏的事，很久前处理过，但是没留下文档，正好现在把流程梳理一下，来解决grub.cfg损坏的情况,或者无法启动的情况</p><h2 id="实践步骤"><a href="#实践步骤" class="headerlink" title="实践步骤"></a>实践步骤</h2><p>安装操作系统的时候会有多种可能分区的方法，一个直接的分区，一个是用了lvm,本篇将几种分区的情况分别写出来</p><h3 id="lvm分区的情况"><a href="#lvm分区的情况" class="headerlink" title="lvm分区的情况"></a>lvm分区的情况</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@localhost ~]<span class="hljs-comment"># df -h</span><br>Filesystem               Size  Used Avail Use% Mounted on<br>/dev/mapper/centos-root   17G  927M   17G   6% /<br>devtmpfs                 901M     0  901M   0% /dev<br>tmpfs                    912M     0  912M   0% /dev/shm<br>tmpfs                    912M  8.6M  904M   1% /run<br>tmpfs                    912M     0  912M   0% /sys/fs/cgroup<br>/dev/sda1               1014M  143M  872M  15% /boot<br>tmpfs                    183M     0  183M   0% /run/user/0<br></code></pre></td></tr></table></figure><p>模拟&#x2F;boot&#x2F;grub2&#x2F;grub.cfg的破坏</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@localhost ~]<span class="hljs-comment"># mv /boot/grub2/grub.cfg /boot/grub2/grub.cfgbk</span><br></code></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@localhost ~]<span class="hljs-comment"># reboot</span><br></code></pre></td></tr></table></figure><p>重启后就会出现这个</p><p><img src="/images/blog/o_200901084926grub-image1-3.png" alt="image.png-13.4kB"></p><p>使用ls查询当前的分区情况</p><p><img src="/images/blog/o_200901084934grub-image2-3.png" alt="image.png-7.7kB"><br>查询分区情况<br><img src="/images/blog/o_200901084942grub-image3-2.png" alt="image.png-29.1kB"></p><p>可以看到(hd0,msdos1)可以列出&#x2F;boot里面的内容，可以确定这个就是启动分区</p><p>设置root</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">grub&gt; <span class="hljs-built_in">set</span> root=(hd0,msdos1)<br></code></pre></td></tr></table></figure><p>命令后面的路径可以用tab键补全,&#x2F;dev&#x2F;mapper&#x2F;centos-root为根分区，因为当前的分区模式是lvm的</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">grub&gt; linux16 /vmlinuz-3.10.0-693.el7.x86_64 root=/dev/mapper/centos-root<br></code></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">grub&gt; initrd16 /initramfs-3.10.0-693.el7.x86_64.img<br></code></pre></td></tr></table></figure><p>启动</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">grub&gt; boot<br></code></pre></td></tr></table></figure><p>进入系统后重新生成grub.cfg</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">grub2-mkconfig -o /boot/grub2/grub.cfg<br></code></pre></td></tr></table></figure><p>然后重启下系统验证是否好了</p><p>###一个完整&#x2F;分区形式<br>这种情况，整个安装的系统就一个分区，boot是作为&#x2F;分区的一个子目录的情况<br>ls 查询分区<br><img src="/images/blog/o_200901084948grub-image4-1.png" alt="image.png-4.6kB"></p><p>设置根分区</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">grub&gt; <span class="hljs-built_in">set</span> root=(hd0,msdos3)<br></code></pre></td></tr></table></figure><p>可以看到上面是msdos3分区对应的就是root&#x3D;&#x2F;dev&#x2F;sda3,下面就设置这个root</p><p>设置linux16</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">grub&gt; linux16 /root/vmlinuz-3.10.0-693.el7.x86_64 root=/dev/sda3<br></code></pre></td></tr></table></figure><p>设置initrd16</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">grub&gt; initrd16 /root/initramfs-3.10.0-693.el7.x86_64.img<br></code></pre></td></tr></table></figure><p>启动</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">grub&gt; boot<br></code></pre></td></tr></table></figure><p>进入系统后重新生成grub.cfg</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">grub2-mkconfig -o /boot/grub2/grub.cfg<br></code></pre></td></tr></table></figure><p>然后重启下系统验证是否好了</p><h3 id="分区和-boot分区独立分区的情况"><a href="#分区和-boot分区独立分区的情况" class="headerlink" title="&#x2F;分区和&#x2F;boot分区独立分区的情况"></a>&#x2F;分区和&#x2F;boot分区独立分区的情况</h3><p><img src="/images/blog/o_200901084953grub-image5-1.png" alt="image.png-16.3kB"></p><p>设置根分区</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">grub&gt; <span class="hljs-built_in">set</span> root=(hd0,msdos1)<br></code></pre></td></tr></table></figure><p>根据&#x2F;分区为msdos2可以知道root分区为&#x2F;dev&#x2F;sda2</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">grub&gt; linux16 /vmlinuz-3.10.0-693.el7.x86_64 root=/dev/sda2<br></code></pre></td></tr></table></figure><p>设置initrd16</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">grub&gt; initrd16 /initramfs-3.10.0-693.el7.x86_64.img<br></code></pre></td></tr></table></figure><p>启动</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">grub&gt; boot<br></code></pre></td></tr></table></figure><p>进入系统后重新生成grub.cfg</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">grub2-mkconfig -o /boot/grub2/grub.cfg<br></code></pre></td></tr></table></figure><p>然后重启下系统验证是否好了</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>主要的处理流程如下：</p><ul><li>首先通过<code>ls</code>得到分区的情况</li><li>通过<code>set</code>设置&#x2F;boot所在的分区为root</li><li>分别设置linux16，initrd16并且指定root分区为&#x2F;分区所在的目录</li><li>重启后重新生成grub即可</li></ul><p>本篇作为一个总结以备不时之需</p><h2 id="变更记录"><a href="#变更记录" class="headerlink" title="变更记录"></a>变更记录</h2><table><thead><tr><th align="center">Why</th><th align="center">Who</th><th align="center">When</th></tr></thead><tbody><tr><td align="center">创建</td><td align="center">武汉-运维-磨渣</td><td align="center">2017-11-30</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>掉电后osdmap丢失无法启动osd的解决方案</title>
    <link href="/2017/09/27/%E6%8E%89%E7%94%B5%E5%90%8Eosdmap%E4%B8%A2%E5%A4%B1%E6%97%A0%E6%B3%95%E5%90%AF%E5%8A%A8osd%E7%9A%84%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/"/>
    <url>/2017/09/27/%E6%8E%89%E7%94%B5%E5%90%8Eosdmap%E4%B8%A2%E5%A4%B1%E6%97%A0%E6%B3%95%E5%90%AF%E5%8A%A8osd%E7%9A%84%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>本篇讲述的是一个比较极端的故障的恢复场景，在整个集群全部服务器突然掉电的时候，osd里面的osdmap可能会出现没刷到磁盘上的情况，这个时候osdmap的最新版本为空或者为没有这个文件</p><p>还有一种情况就是机器宕机了，没有马上处理，等了一段时间以后，服务器机器启动了起来，而这个时候osdmap已经更新了，全局找不到需要的旧版本的osdmap和incmap，osd无法启动</p><p>一般情况下能找到的就直接从其他osd上面拷贝过来，然后就可以启动了，本篇讲述的是无法启动的情况</p><h2 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h2><h3 id="获取运行的ceph集群当前版本"><a href="#获取运行的ceph集群当前版本" class="headerlink" title="获取运行的ceph集群当前版本"></a>获取运行的ceph集群当前版本</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8107 ~]<span class="hljs-comment"># ceph -v</span><br>ceph version 10.2.9 (2ee413f77150c0f375ff6f10edd6c8f9c7d060d0)<br></code></pre></td></tr></table></figure><p>获取最新的osdmap</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8107 ~]<span class="hljs-comment"># ceph osd getmap -o /tmp/productosdmap</span><br>got osdmap epoch 142<br></code></pre></td></tr></table></figure><p>通过osdmap可以得到crushmap，fsid，osd，存储池，pg等信息</p><p>提取crushmap</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8107 tmp]<span class="hljs-comment"># osdmaptool /tmp/productosdmap --export-crush /tmp/productcrushmap</span><br>osdmaptool: osdmap file <span class="hljs-string">&#x27;/tmp/productosdmap&#x27;</span><br>osdmaptool: exported crush map to /tmp/productcrushmap<br></code></pre></td></tr></table></figure><p>拷贝到开发环境的机器上面</p><p>通过osdmap获取集群的fsid</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8107 tmp]<span class="hljs-comment"># osdmaptool --print productosdmap |grep fsid</span><br>osdmaptool: osdmap file <span class="hljs-string">&#x27;productosdmap&#x27;</span><br>fsid d153844c-16f5-4f48-829d-87fb49120bbe<br></code></pre></td></tr></table></figure><p>获取存储池相关的信息</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8107 tmp]<span class="hljs-comment"># osdmaptool --print productosdmap |grep  pool</span><br>osdmaptool: osdmap file <span class="hljs-string">&#x27;productosdmap&#x27;</span><br>pool 0 <span class="hljs-string">&#x27;rbd&#x27;</span> replicated size 2 min_size 1 crush_ruleset 0 object_hash rjenkins pg_num 64 pgp_num 64 last_change 1 flags hashpspool stripe_width 0<br></code></pre></td></tr></table></figure><p>获取osd相关的信息</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8107 tmp]<span class="hljs-comment"># osdmaptool --print productosdmap |grep  osd</span><br>osdmaptool: osdmap file <span class="hljs-string">&#x27;productosdmap&#x27;</span><br>flags sortbitwise,require_jewel_osds<br>max_osd 3<br>osd.0 up   <span class="hljs-keyword">in</span>  weight 1 up_from 135 up_thru 141 down_at 127 last_clean_interval [23,24) 192.168.8.107:6800/28245 192.168.8.107:6801/28245 192.168.8.107:6802/28245 192.168.8.107:6803/28245 exists,up d8040272-7afb-49c0-bb78-9ff13cf7d31b<br>osd.1 up   <span class="hljs-keyword">in</span>  weight 1 up_from 140 up_thru 141 down_at 131 last_clean_interval [33,130) 192.168.8.107:6808/28698 192.168.8.107:6809/28698 192.168.8.107:6810/28698 192.168.8.107:6811/28698 exists,up c6ac4c7a-0227-4af4-ac3f-bd844b2480f8<br>osd.2 up   <span class="hljs-keyword">in</span>  weight 1 up_from 137 up_thru 141 down_at 133 last_clean_interval [29,132) 192.168.8.107:6804/28549 192.168.8.107:6805/28549 192.168.8.107:6806/28549 192.168.8.107:6807/28549 exists,up 2170260b-bb05-4965-baf2-12d1c41b3ba0<br></code></pre></td></tr></table></figure><h3 id="构建新集群"><a href="#构建新集群" class="headerlink" title="构建新集群"></a>构建新集群</h3><p>下载这个版本的源码</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">http://mirrors.aliyun.com/ceph/rpm-jewel/el7/SRPMS/ceph-10.2.9-0.el7.src.rpm<br></code></pre></td></tr></table></figure><p>放到一台独立的机器上面</p><p>解压rpm包</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 bianyi]<span class="hljs-comment"># rpm2cpio ceph-10.2.9-0.el7.src.rpm |cpio -div</span><br>[root@lab8106 bianyi]<span class="hljs-comment"># tar -xvf ceph-10.2.9.tar.bz2</span><br></code></pre></td></tr></table></figure><p>编译环境</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">cd</span> ceph<br>./install-deps.sh<br>./autogen.sh<br>./configure<br>make -j 12<br><span class="hljs-built_in">cd</span> src<br></code></pre></td></tr></table></figure><p>修改vstart.sh里面的fsid<br>启动集群</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">./vstart.sh -n  --mon_num 1 --osd_num 3 --mds_num 0  --short  -d<br></code></pre></td></tr></table></figure><p>检查集群状态：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 src]<span class="hljs-comment"># ./ceph -c ceph.conf -s</span><br>    cluster d153844c-16f5-4f48-829d-87fb49120bbe<br>     health HEALTH_OK<br>     monmap e1: 1 mons at &#123;a=192.168.8.106:6789/0&#125;<br>            election epoch 3, quorum 0 a<br>     osdmap e12: 3 osds: 3 up, 3 <span class="hljs-keyword">in</span><br>            flags sortbitwise,require_jewel_osds<br>      pgmap v16: 8 pgs, 1 pools, 0 bytes data, 0 objects<br>            115 GB used, 1082 GB / 1197 GB avail<br>                   8 active+clean<br></code></pre></td></tr></table></figure><p>导入crushmap</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 src]<span class="hljs-comment"># ./ceph -c ceph.conf osd setcrushmap -i /root/rpmbuild/bianyi/productcrushmap </span><br><span class="hljs-built_in">set</span> crush map<br>2017-09-26 14:13:29.052246 7f19fd01d700  0 lockdep stop<br></code></pre></td></tr></table></figure><p>设置PG</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">./ceph -c ceph.conf osd pool <span class="hljs-built_in">set</span> rbd pg_num 64<br>./ceph -c ceph.conf osd pool <span class="hljs-built_in">set</span> rbd pgp_num 64<br></code></pre></td></tr></table></figure><p>模拟正式集群上的故障</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8107 meta]<span class="hljs-comment"># systemctl stop ceph-osd@0</span><br>[root@lab8107 meta]<span class="hljs-comment"># mv /var/lib/ceph/osd/ceph-0/current/meta/osdmap.153__0_AC977A95__none  /tmp/</span><br>[root@lab8107 meta]<span class="hljs-comment"># mv /var/lib/ceph/osd/ceph-0/current/meta/inc\\uosdmap.153__0_C67D77C2__none  /tmp/</span><br></code></pre></td></tr></table></figure><p>相当于无法读取这个osdmap和incmap了</p><p>尝试启动osd<br>设置debug_osd&#x3D;20后</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">systemctl restart ceph-osd@0<br></code></pre></td></tr></table></figure><p>检查日志</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">/var/log/ceph/ceph-osd.0.<span class="hljs-built_in">log</span><br></code></pre></td></tr></table></figure><p><img src="/images/blog/o_200901084757osdmap-image-3.png" alt="image.png-56.9kB"></p><p>可以看到153 epoch的osdmap是有问题的，那么我们需要的就是这个版本的osdmap</p><p>检查当前开发集群的osdmap的版本</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">osdmap e18: 3 osds: 3 up, 3 <span class="hljs-keyword">in</span><br></code></pre></td></tr></table></figure><p>那么先快速把osdmap版本提高到153附近，这里我选择120</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 src]<span class="hljs-comment"># ./ceph -c ceph.conf osd thrash 120</span><br>will thrash map <span class="hljs-keyword">for</span> 120 epochs<br></code></pre></td></tr></table></figure><p>检查快速变化后的osdmap epoch</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">osdmap e138: 3 osds: 2 up, 1 <span class="hljs-keyword">in</span>; 64 remapped pgs<br></code></pre></td></tr></table></figure><p>做了上面的thrash后，集群的osd会是比较乱的，比如我的</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 src]<span class="hljs-comment"># ./ceph -c ceph.conf osd tree</span><br>ID WEIGHT  TYPE NAME        UP/DOWN REWEIGHT PRIMARY-AFFINITY <br>-1 0.80338 root default                                       <br>-2 0.80338     host lab8107                                   <br> 0 0.26779         osd.0         up        0          1.00000 <br> 1 0.26779         osd.1       down        0          1.00000 <br> 2 0.26779         osd.2         up  1.00000          1.00000 <br>2017-09-27 09:43:24.817177 7fbcc7cdb700  0 lockdep stop<br></code></pre></td></tr></table></figure><p>做下恢复，启动下相关osd</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 src]<span class="hljs-comment"># ./ceph -c ceph.conf osd reweight 0 1</span><br>reweighted osd.0 to 1 (10000)<br>2017-09-27 09:45:01.439009 7f56c147b700  0 lockdep stop<br>[root@lab8106 src]<span class="hljs-comment"># ./ceph -c ceph.conf osd reweight 1 1</span><br>reweighted osd.1 to 1 (10000)<br>2017-09-27 09:45:04.020686 7fea3345c700  0 lockdep stop<br></code></pre></td></tr></table></figure><p>注意提取下开发集群上面新生成的osdmap的文件（多次执行以免刷掉了）</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 src]<span class="hljs-comment">#rsync -qvzrtopg   dev/osd0/current/meta/ /root/meta/</span><br></code></pre></td></tr></table></figure><p>重启一遍开发集群</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 src]<span class="hljs-comment"># ./vstart.sh   --mon_num 1 --osd_num 3 --mds_num 0  --short  -d</span><br></code></pre></td></tr></table></figure><p>注意这里少了一个参数 -n,n是重建集群，这里我们只需要重启即可<br>再次检查</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">osdmap e145: 3 osds: 3 up, 3 <span class="hljs-keyword">in</span><br></code></pre></td></tr></table></figure><p>还是不够，不够的时候就执行上面的这个多次即可，一直到epoch到满足即可</p><p>将得到的osdmap拷贝到无法启动的osd的主机上面</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 src]<span class="hljs-comment"># scp /root/meta/osdmap.153__0_AC977A95__none 192.168.8.107:/root</span><br>osdmap.153__0_AC977A95__none                            100% 2824     2.8KB/s   00:00    <br>[root@lab8106 src]<span class="hljs-comment"># scp /root/meta/inc\\uosdmap.153__0_C67D77C2__none 192.168.8.107:/root</span><br>inc\uosdmap.153__0_C67D77C2__none                       100%  198     0.2KB/s   00:00<br></code></pre></td></tr></table></figure><p>拷贝到osdmap的路径下面</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8107 meta]<span class="hljs-comment"># cp /root/osdmap.153__0_AC977A95__none ./</span><br>[root@lab8107 meta]<span class="hljs-comment"># cp /root/inc\\uosdmap.153__0_C67D77C2__none ./</span><br>[root@lab8107 meta]<span class="hljs-comment"># chown ceph:ceph osdmap.153__0_AC977A95__none </span><br>[root@lab8107 meta]<span class="hljs-comment"># chown ceph:ceph inc\\uosdmap.153__0_C67D77C2__none</span><br></code></pre></td></tr></table></figure><p>启动并且观测</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8107 meta]<span class="hljs-comment"># systemctl start ceph-osd@0</span><br>[root@lab8107 meta]<span class="hljs-comment">#tailf /var/log/ceph/ceph-osd.0.log</span><br></code></pre></td></tr></table></figure><p>检查集群状态，可以看到已经可以启动了</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>一般来说，出问题的时候都会说一句，如果备份了，就没那多事情，在一套生产环境当中，可以考虑下，什么是可以备份的，备份对环境的影响大不大，这种关键数据，并且可以全局共用，数据量也不大的数据，就需要备份好，比如上面的osdmap就可以在一个osd节点上面做一个实时的备份，或者短延时备份</p><p>本篇讲的是已经没有备份的情况下的做的一个恢复，掉电不是没有可能发生，至少解决了一个在osdmap无法找回的情况下的恢复办法</p><p>当然这里如果能够通过直接基于最新的osdmap和incmap做一定的解码，修改，编码，这样的方式应该也是可行的，这个就需要有一定的开发基础了，如果后面有找到这个方法会补充进本篇文章</p><p>你备份osdmap了么？</p><h2 id="变更记录"><a href="#变更记录" class="headerlink" title="变更记录"></a>变更记录</h2><table><thead><tr><th align="center">Why</th><th align="center">Who</th><th align="center">When</th></tr></thead><tbody><tr><td align="center">创建</td><td align="center">武汉-运维-磨渣</td><td align="center">2017-09-27</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>怎样禁止Ceph OSD的自动挂载</title>
    <link href="/2017/09/07/%E6%80%8E%E6%A0%B7%E7%A6%81%E6%AD%A2Ceph%20OSD%E7%9A%84%E8%87%AA%E5%8A%A8%E6%8C%82%E8%BD%BD/"/>
    <url>/2017/09/07/%E6%80%8E%E6%A0%B7%E7%A6%81%E6%AD%A2Ceph%20OSD%E7%9A%84%E8%87%AA%E5%8A%A8%E6%8C%82%E8%BD%BD/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>本篇来源于群里一个人的问题，有没有办法让ceph的磁盘不自动挂载，一般人的问题都是怎样让ceph能够自动挂载，在centos 7 平台下 ceph jewel版本以后都是有自动挂载的处理的，这个我之前也写过两篇文章《ceph在centos7下一个不容易发现的改变》和《Ceph数据盘怎样实现自动挂载》，来讲述这个自动挂载的</p><p>这里讲下流程：</p><blockquote><p>开机后 udev 匹配 95-ceph-osd.rules 规则，触发 ceph-disk  trigger，遍历磁盘，匹配到磁盘的标记后就触发了自动挂载</p></blockquote><p>为什么要取消挂载？<br>也许一般都会想：不就是停掉osd，然后umount掉，检查磁盘吗<br>这个想法如果放在一般情况下都没有问题，但是为什么有这个需求就是有不一般的情况，这个我在很久前遇到过，所以对这个需求的场景比较清楚</p><p>在很久以前碰到过一次，机器启动都是正常的，但是只要某个磁盘一挂载，机器就直接挂掉了，所以这个是不能让它重启机器自动挂载的，也许还有其他的情况，这里总结成一个简单的需求就是不想它自动挂载</p><h2 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h2><p>从上面的自启动后的自动挂载流程里面，我们可以知道这里可以有两个方案去解决这个问题，第一种是改变磁盘的标记，第二种就是改变udev的rule的规则匹配，这里两个方法都行，一个是完全不动磁盘，一个是动了磁盘的标记</p><h3 id="修改udev规则的方式"><a href="#修改udev规则的方式" class="headerlink" title="修改udev规则的方式"></a>修改udev规则的方式</h3><p>这个因为曾经有一段时间看过udev相关的一些东西，所以处理起来还是比较简单的，这里顺便把调试过程也记录下来<br>&#x2F;lib&#x2F;udev&#x2F;rules.d&#x2F;95-ceph-osd.rules这个文件里面就是集群自动挂载的触发规则，所以在这里我们在最开始匹配上我们需要屏蔽的盘，然后绕过内部的所有匹配规则，具体办法就是<br>在这个文件里面第一行加上</p><blockquote><p>KERNEL&#x3D;&#x3D;”sdb1|sdb2”, GOTO&#x3D;”not_auto_mount”</p></blockquote><p>在最后一行加上</p><blockquote><p>LABEL&#x3D;”not_auto_mount”</p></blockquote><p>验证规则是否正确</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">udevadm <span class="hljs-built_in">test</span> /sys/block/sdb/sdb1<br></code></pre></td></tr></table></figure><p>我们先看下正常的可以挂载的盘符的触发测试显示<br><img src="/images/blog/o_200901084543image4-2.png" alt="image.png-17.2kB"><br>再看下屏蔽了后的规则是怎样的<br><img src="/images/blog/o_200901084550image5-2.png" alt="image.png-16kB"><br>可以看到在加入屏蔽条件以后，就没有触发挂载了，这里要注意，做屏蔽规则的时候需要把这个osd相关的盘都屏蔽，不然在触发相关分区的时候可能顺带挂载起来了，上面的sdb1就是数据盘，sdb2就是bluestore的block盘</p><p>测试没问题后就执行下</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">udevadm control --reload-rules<br></code></pre></td></tr></table></figure><p>重启后验证是否自动挂载了</p><h3 id="修改磁盘标记的方式"><a href="#修改磁盘标记的方式" class="headerlink" title="修改磁盘标记的方式"></a>修改磁盘标记的方式</h3><p>查询磁盘的标记typecode,也就是ID_PART_ENTRY_TYPE这个属性</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ~]<span class="hljs-comment"># blkid -o udev -p /dev/sdb1</span><br>ID_FS_UUID=7a852eec-b32d-4c0a-8b8e-1e056a67ee35<br>ID_FS_UUID_ENC=7a852eec-b32d-4c0a-8b8e-1e056a67ee35<br>ID_FS_TYPE=xfs<br>ID_FS_USAGE=filesystem<br>ID_PART_ENTRY_SCHEME=gpt<br>ID_PART_ENTRY_NAME=ceph\x20data<br>ID_PART_ENTRY_UUID=7b321ca3-402c-4557-b121-887266a1e1b8<br>ID_PART_ENTRY_TYPE=4fbd7e29-9d25-41b8-afd0-062c0ceff05d<br>ID_PART_ENTRY_NUMBER=1<br>ID_PART_ENTRY_OFFSET=2048<br>ID_PART_ENTRY_SIZE=204800<br>ID_PART_ENTRY_DISK=8:16<br></code></pre></td></tr></table></figure><p>匹配到这个属性就认为是集群的节点，可以挂载的，那么我们先改变这个</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ~]<span class="hljs-comment"># /usr/sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff0f9 -- /dev/sdb</span><br>[root@lab8106 ~]<span class="hljs-comment"># blkid -o udev -p /dev/sdb1</span><br>ID_FS_UUID=7a852eec-b32d-4c0a-8b8e-1e056a67ee35<br>ID_FS_UUID_ENC=7a852eec-b32d-4c0a-8b8e-1e056a67ee35<br>ID_FS_TYPE=xfs<br>ID_FS_USAGE=filesystem<br>ID_PART_ENTRY_SCHEME=gpt<br>ID_PART_ENTRY_NAME=ceph\x20data<br>ID_PART_ENTRY_UUID=7b321ca3-402c-4557-b121-887266a1e1b8<br>ID_PART_ENTRY_TYPE=4fbd7e29-9d25-41b8-afd0-062c0ceff0f9<br>ID_PART_ENTRY_NUMBER=1<br>ID_PART_ENTRY_OFFSET=2048<br>ID_PART_ENTRY_SIZE=204800<br>ID_PART_ENTRY_DISK=8:16<br></code></pre></td></tr></table></figure><p>可以看到type的属性已经被修改了<br>再次测试，可以看到已经不匹配了<br><img src="/images/blog/o_200901084556image6-1.png" alt="image.png-14.1kB"></p><p>如果需要恢复就执行</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ~]<span class="hljs-comment"># /usr/sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sdb</span><br></code></pre></td></tr></table></figure><p>这里同样需要改掉相关的block盘的标记，否则一样被关联的挂载起来了</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本篇用两种方法来实现了ceph osd的盘符的不自动挂载，这个一般情况下都不会用到，比较特殊的情况遇到了再这么处理就可以了，或者比较暴力的方法就是直接把挂载的匹配的规则全部取消掉，使用手动触发挂载的方式也行，这个方法很多，能够快速，简单的满足需求即可</p><p>此mount非彼mount，题图无关</p><h2 id="变更记录"><a href="#变更记录" class="headerlink" title="变更记录"></a>变更记录</h2><table><thead><tr><th align="center">Why</th><th align="center">Who</th><th align="center">When</th></tr></thead><tbody><tr><td align="center">创建</td><td align="center">武汉-运维-磨渣</td><td align="center">2017-09-07</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Ceph OSD服务失效自动启动控制</title>
    <link href="/2017/09/06/Ceph%20OSD%E6%9C%8D%E5%8A%A1%E5%A4%B1%E6%95%88%E8%87%AA%E5%8A%A8%E5%90%AF%E5%8A%A8%E6%8E%A7%E5%88%B6/"/>
    <url>/2017/09/06/Ceph%20OSD%E6%9C%8D%E5%8A%A1%E5%A4%B1%E6%95%88%E8%87%AA%E5%8A%A8%E5%90%AF%E5%8A%A8%E6%8E%A7%E5%88%B6/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>服务器上面的服务会因为各种各样的原因失败，磁盘故障，权限问题，或者是服务过载引起超时，这些都可能引起</p><p>这个在ceph里面systemctl unit 默认有个on-fail restart,默认的可能并不适合所有的场景，所以自动化的服务应该是尽量去适配你手动处理的过程，手动怎么处理的，就怎么去设置</p><h2 id="启动分析"><a href="#启动分析" class="headerlink" title="启动分析"></a>启动分析</h2><p>如果有osd失败了，一般上去会先启动一次，尽快让服务启动，然后去检查是否有故障，如果失败了，就开启调试日志，再次重启，在问题解决之前，是不会再启动了，所以这里我们的自动启动设置也这么设置</p><h2 id="参数配置"><a href="#参数配置" class="headerlink" title="参数配置"></a>参数配置</h2><p>ceph的osd的启动配置在这个配置文件</p><blockquote><p>&#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;system&#x2F;ceph-osd@.service</p></blockquote><p>默认参数：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">Restart=on-failure<br>StartLimitInterval=30min<br>StartLimitBurst=30<br>RestartSec=20s<br></code></pre></td></tr></table></figure><p>默认的参数意思是<br>在30min的周期内，如果没启动成功，那么在失败后20s进行启动，这样的启动尝试30次</p><p>这个在启动机器的时候，是尽量在osd启动失败的情况下，能够在30min分钟内尽量把服务都启动起来，这个对于关机启动后的控制是没问题的</p><p>参数解释：<br>StartLimitInterval不能设置太小，在osd崩溃的情况里面有一种是对象异常了，这个在启动了后，内部会加载一段时间的数据以后才会崩溃，所以RestartSec*StartLimitBurst 必须小于StartLimitInterval，否则可能出现无限重启的情况</p><p>restart的触发条件</p><table><thead><tr><th align="left">Restart settings&#x2F;Exit causes</th><th align="center">always</th><th align="center">on-success</th><th align="center">on-failure</th><th align="center">on-abnormal</th><th align="center">on-abort</th><th align="center">on-watchdog</th></tr></thead><tbody><tr><td align="left">Clean exit code or signal</td><td align="center">X</td><td align="center">X</td><td align="center"></td><td align="center"></td><td align="center"></td><td align="center"></td></tr><tr><td align="left">Unclean exit code</td><td align="center">X</td><td align="center"></td><td align="center">X</td><td align="center"></td><td align="center"></td><td align="center"></td></tr><tr><td align="left">Unclean signal</td><td align="center">X</td><td align="center"></td><td align="center">X</td><td align="center">X</td><td align="center">X</td><td align="center"></td></tr><tr><td align="left">Timeout</td><td align="center">X</td><td align="center"></td><td align="center">X</td><td align="center">X</td><td align="center"></td><td align="center"></td></tr><tr><td align="left">Watchdog</td><td align="center">X</td><td align="center"></td><td align="center">X</td><td align="center">X</td><td align="center"></td><td align="center">X</td></tr></tbody></table><p>可调整项目<br>Restart&#x3D;always就是只要非正常的退出了，就满足重启的条件，kill -9 进程也能够自动启动</p><p>在osd崩溃的情况里面有一种情况是对象异常了，这个在启动了后，内部会加载一段时间的数据以后才会崩溃，这种崩溃的情况我们不需要尝试多次重启,所以适当降低重启频率</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">StartLimitBurst=3<br>RestartSec=10s<br></code></pre></td></tr></table></figure><p>这个设置后能够在运行的集群当中比较好的处理异常退出的情况，但是设置后就要注意关机osd osd启动的问题，一般关机的时候肯定是有人在维护的，所以这个问题不大，人为处理下就行</p><p>所以建议的参数是</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">Restart=always<br>StartLimitInterval=30min<br>StartLimitBurst=3<br>RestartSec=10s<br></code></pre></td></tr></table></figure><p>可以根据自己的需要进行设置，这个设置下，停止osd就用systemctl 命令去 stop，然后其他的任何异常退出情况都会把osd给拉起来</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>systemctl在服务控制方面有着很丰富的功能，可以根据自己的需求进行调整，特别是对启动条件有约束的场景，这个是最适合的</p><h2 id="变更记录"><a href="#变更记录" class="headerlink" title="变更记录"></a>变更记录</h2><table><thead><tr><th align="center">Why</th><th align="center">Who</th><th align="center">When</th></tr></thead><tbody><tr><td align="center">创建</td><td align="center">武汉-运维-磨渣</td><td align="center">2017-09-06</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>osd磁盘空间足够无法写入数据的分析与解决</title>
    <link href="/2017/09/04/osd%E7%A3%81%E7%9B%98%E7%A9%BA%E9%97%B4%E8%B6%B3%E5%A4%9F%E6%97%A0%E6%B3%95%E5%86%99%E5%85%A5%E6%95%B0%E6%8D%AE%E7%9A%84%E5%88%86%E6%9E%90%E4%B8%8E%E8%A7%A3%E5%86%B3/"/>
    <url>/2017/09/04/osd%E7%A3%81%E7%9B%98%E7%A9%BA%E9%97%B4%E8%B6%B3%E5%A4%9F%E6%97%A0%E6%B3%95%E5%86%99%E5%85%A5%E6%95%B0%E6%8D%AE%E7%9A%84%E5%88%86%E6%9E%90%E4%B8%8E%E8%A7%A3%E5%86%B3/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>这个问题的来源是ceph社区里面一个群友的环境出现在85%左右的时候，启动osd报错，然后在本地文件系统当中进行touch文件的时候也是报错，df -i查询inode也是没用多少，使用的也是inode64挂载的，开始的时候排除了配置原因引起的，在ceph的邮件列表里面有一个相同<a href="http://lists.ceph.com/pipermail/ceph-users-ceph.com/2016-October/013929.html">问题</a>，也是没有得到解决</p><p>看到这个问题比较感兴趣，就花了点时间来解决来定位和解决这个问题，现在分享出来，如果有类似的生产环境，可以提前做好检查预防工作</p><p>##现象描述<br>ceph版本</p><blockquote><p>[root@lab8107 mnt]# ceph -v<br>ceph version 10.2.9 (2ee413f77150c0f375ff6f10edd6c8f9c7d060d0)<br>我复现的环境为这个版本</p></blockquote><p>查询使用空间</p><p><img src="/images/blog/o_200901082808space-image2.png" alt="image.png-19.8kB"><br>可以看到空间才使用了54%<br><img src="/images/blog/o_200901082815space-image3.png" alt="image.png-28kB"><br>可以看到，inode剩余比例很多，而文件确实无法创建</p><p>这个时候把一个文件mv出来，然后又可以创建了，并且可以写入比mv出来的文件更大的文件，写完一个无法再写入更多文件了</p><p>这里有个初步判断，不是容量写完了，而是文件的个数限制住了</p><p>那么来查询下文件系统的inode还剩余多少，xfs文件系统的inode是动态分配的，我们先检查无法写入的文件系统的</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">xfs_db -r -c <span class="hljs-string">&quot;sb 0&quot;</span> -c <span class="hljs-string">&quot;p&quot;</span> -c <span class="hljs-string">&quot;freesp -s&quot;</span> /dev/sdb1|grep ifree<br></code></pre></td></tr></table></figure><p><img src="/images/blog/o_200901082823space-image4.png" alt="image.png-5.1kB"><br>可以看到剩余的inode确实为0，这里确实是没有剩余inode了，所以通过df -i来判断inode是否用完并不准确，那个是已经使用值与理论值的相除的结果</p><p>查询xfs碎片，也是比例很低</p><h2 id="定位问题"><a href="#定位问题" class="headerlink" title="定位问题"></a>定位问题</h2><p>首先查看xfs上面的数据结构</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">xfs_db -r -c <span class="hljs-string">&quot;sb 0&quot;</span> -c <span class="hljs-string">&quot;p&quot;</span> -c <span class="hljs-string">&quot;freesp -s &quot;</span> /dev/sdb1<br></code></pre></td></tr></table></figure><p><img src="/images/blog/o_200901082832space-image5.png" alt="image.png-13.7kB"></p><p>上面的输出结果这里简单解释一下，这里我也是反复比对和查看资料才理解这里的意思，这里有篇<a href="https://www.novell.com/support/kb/doc.php?id=7014320">novell</a>的资料有提到这个，这里我再拿一个刚刚格式化完的分区结果来看下<br><img src="/images/blog/o_200901082837space-image6.png" alt="image.png-14.3kB"></p><p>这里用我自己的理解来描述下，这个extents的剩余数目是动态变化的，刚分完区的那个，有4个1048576-1220608左右的逻辑区间，而上面的无法写入数据的数据结构，剩下的extent的平均大小为22个block，而这样的blocks总数有1138886个，占总体的99.85，也就是剩余的空间的的extents所覆盖的区域全部是16个block到31个block的这种空洞，相当于蛋糕被切成很多小块了，大的都拿走了，剩下的总量还很多，但是都是很小的碎蛋糕，所以也没法取了</p><p>默认来说inode chunk 为64 ，也就是需要64*inodesize的存储空间来存储inode，这个剩下的空间已经不够分配了</p><h2 id="解决办法"><a href="#解决办法" class="headerlink" title="解决办法"></a>解决办法</h2><p>下个段落会讲下为什么会出现上面的情况，现在先说解决办法，把文件mv出来，然后mv进去，这个是在其他场景下的一个解决方法，这个操作要小心，因为有扩展属性，操作不小心会弄掉了，这里建议用另外一个办法xfs_dump的方法</p><p>我的环境比较小，20G的盘，如果盘大就准备大盘,这里是验证是否可行</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">xfsdump -L osd0 -M osd0 -f /mnt/osd0 /var/lib/ceph/osd/ceph-0<br></code></pre></td></tr></table></figure><p>还原回去</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8107 ceph-0]<span class="hljs-comment"># xfsrestore -f /mnt/osd0 /var/lib/ceph/osd/ceph-0</span><br>xfsrestore: using file dump (drive_simple) strategy<br>xfsrestore: version 3.1.4 (dump format 3.0) - <span class="hljs-built_in">type</span> ^C <span class="hljs-keyword">for</span> status and control<br>xfsrestore: ERROR: unable to create /var/lib/ceph/osd/ceph-0/xfsrestorehousekeepingdir: No space left on device<br>xfsrestore: Restore Status: ERROR<br></code></pre></td></tr></table></figure><p>直接还原还是会有问题,没有可以写的地方了，这里因为已经dump了一份，这里就mv pg的数据目录出去</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">mv</span> /var/lib/ceph/osd/ceph-0/current/ /mnt<br></code></pre></td></tr></table></figure><p>开始还原</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">xfsrestore  -o -f /mnt/osd0 /var/lib/ceph/osd/ceph-0<br></code></pre></td></tr></table></figure><p>还原以后如果有权限需要处理的就处理下权限，先检查下文件系统的数据结构<br><img src="/images/blog/o_200901082843space-image7.png" alt="image.png-19.6kB"><br>可以看到数据结构已经很理想了<br>然后启动osd</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">systemctl restart ceph-osd@0<br></code></pre></td></tr></table></figure><p>然后检查下数据是不是都可以正常写进去了</p><ul><li>如果出现了上面的空间已经满了的情况，处理的时候需要注意</li><li>备份好数据</li><li>单个盘进行处理</li><li>备份的数据先保留好以防万一</li><li>启动好了后，验证下集群的状态后再继续，可以尝试get下数据检查数据</li></ul><h2 id="为什么会出现这样"><a href="#为什么会出现这样" class="headerlink" title="为什么会出现这样"></a>为什么会出现这样</h2><p>我们在本地文件系统里面连续写100个文件<br>准备一个a文件里面有每行150个a字符，700行，这个文件大小就是100K</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8107 <span class="hljs-built_in">test</span>]<span class="hljs-comment"># seq 100|xargs -i dd if=a of=a&#123;&#125; bs=100K count=1</span><br></code></pre></td></tr></table></figure><p>检查文件的分布</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8107 <span class="hljs-built_in">test</span>]<span class="hljs-comment"># seq 100|xargs -i xfs_bmap -v a&#123;&#125; |less</span><br></code></pre></td></tr></table></figure><p><img src="/images/blog/o_200901082850space-image8.png" alt="image.png-47.1kB"></p><p>大部分情况下这个block的分配是连续的</p><p>先检查下当前的数据结构<br><img src="/images/blog/o_200901082856space-image9.png" alt="image.png-30.8kB"></p><p>我们把刚刚的100个对象put到集群里面去，监控下集群的数据目录的写入情况</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">inotifywait -m --timefmt <span class="hljs-string">&#x27;%Y %B %d %H:%M:%S&#x27;</span> --format <span class="hljs-string">&#x27;%T %w %e %f&#x27;</span> -r -m /var/lib/ceph/osd/ceph-0/<br></code></pre></td></tr></table></figure><p>put数据进去</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-keyword">for</span> a <span class="hljs-keyword">in</span> `<span class="hljs-built_in">ls</span> ./`;<span class="hljs-keyword">do</span> rados -p rbd put <span class="hljs-variable">$a</span> <span class="hljs-variable">$a</span>;<span class="hljs-keyword">done</span><br></code></pre></td></tr></table></figure><p><img src="/images/blog/o_200901082902space-image10.png" alt="image.png-53.7kB"><br><img src="/images/blog/o_200901082920space-image11.png" alt="image.png-64.2kB"><br>查看对象的数据，里面并没有连续起来，并且写入的数据的方式是:<br>打开文件，设置扩展属性，填充内容，设置属性，关闭，很多并发在一起做</p><p>写完的数据结构<br><img src="/images/blog/o_200901082926space-image12.png" alt="image.png-30.9kB"></p><p>结果就是在100K这个数据模型下，会产生很多小的block空隙，最后就是无法写完文件的情况，这里产生空隙并不是很大的问题，问题是这里剩下的空隙无法完成inode的动态分配的工作，这里跟一个格式化选项的变化有关</p><p>准备一个集群<br>然后写入(一直写)</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">rados -p rbd bench -b 100K 6000 write --no-cleanup<br></code></pre></td></tr></table></figure><p>就可以必现这个问题，可以看到上面的从16-31 block的区间从 12 extents涨到了111 extents</p><h2 id="解决办法-1"><a href="#解决办法-1" class="headerlink" title="解决办法"></a>解决办法</h2><p>用deploy在部署的时候默认的格式化参数为</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">command_check_call: Running <span class="hljs-built_in">command</span>: /usr/sbin/mkfs -t xfs -f -i size=2048 -- /dev/sdb1<br></code></pre></td></tr></table></figure><p>这个isize设置的是2048，这个在后面剩余的空洞比较小的时候就无法写入新的数据了，所以在ceph里面存储100K这种小文件的场景的时候，把mkfs.xfs的isize改成默认的256就可以提前避免这个问题<br>修改 &#x2F;usr&#x2F;lib&#x2F;python2.7&#x2F;site-packages&#x2F;ceph_disk&#x2F;main.py的256行</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs bash">xfs=[<br>    <span class="hljs-comment"># xfs insists on not overwriting previous fs; even if we wipe</span><br>    <span class="hljs-comment"># partition table, we often recreate it exactly the same way,</span><br>    <span class="hljs-comment"># so we&#x27;ll see ghosts of filesystems past</span><br>    <span class="hljs-string">&#x27;-f&#x27;</span>,<br>    <span class="hljs-string">&#x27;-i&#x27;</span>, <span class="hljs-string">&#x27;size=2048&#x27;</span>,<br>],<br></code></pre></td></tr></table></figure><p>改成</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-string">&#x27;-i&#x27;</span>, <span class="hljs-string">&#x27;size=256&#x27;</span>,<br></code></pre></td></tr></table></figure><p><img src="/images/blog/o_200901082932space-image13.png" alt="image.png-24.4kB"><br>这个地方检查下是不是对的，然后就可以避免这个问题了，可以测试下是不是一直可以写到很多，我的这个测试环境写到91%还没问题</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>在特定的数据写入模型下，可能出现一些可能无法预料的问题，而参数的改变可能也没法覆盖所有场景，本篇就是其中的一个比较特殊的问题，定位好问题，在遇到的时候能够解决，或者提前避免掉</p><h2 id="后续"><a href="#后续" class="headerlink" title="后续"></a>后续</h2><p>在升级了内核到</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8107 ~]<span class="hljs-comment"># uname  -a</span><br>Linux lab8107 4.13.0-1.el7.elrepo.x86_64 <span class="hljs-comment">#1 SMP Sun Sep 3 19:07:24 EDT 2017 x86_64 x86_64 x86_64 GNU/Linux</span><br></code></pre></td></tr></table></figure><p>升级xfsprogs到</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8107 ~]<span class="hljs-comment"># rpm -qa|grep xfsprogs</span><br>xfsprogs-4.12.0-4.el7.centos.x86_64<br></code></pre></td></tr></table></figure><p>重新部署osd，还是一样的isize&#x3D;2048，一样的写入模型</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8107 ~]<span class="hljs-comment"># df -h /var/lib/ceph/osd/ceph-0</span><br>Filesystem      Size  Used Avail Use% Mounted on<br>/dev/sdb1       9.4G  9.0G  395M  96% /var/lib/ceph/osd/ceph-0<br></code></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs bash">meta_uuid = 00000000-0000-0000-0000-000000000000<br>   from      to extents  blocks    pct<br>      1       1     545     545   0.50<br>      2       3     665    1666   1.52<br>      4       7    1624    8927   8.12<br>      8      15    1853   19063  17.34<br>     16      31      19     352   0.32<br>   4096    8191       1    7694   7.00<br>  16384   32767       3   71659  65.20<br>total free extents 4710<br>total free blocks 109906<br>average free extent size 23.3346<br>[root@lab8107 ~]<span class="hljs-comment"># xfs_db -r -c &quot;sb 0&quot; -c &quot;p&quot; -c &quot;freesp -s &quot; /dev/sdb1</span><br></code></pre></td></tr></table></figure><p>可以看到已经很少的稀疏空间了，留下比较大的空间，这个地方应该是优化了底层数据存储的算法</p><p>另外，xfs的inode是动态分配的,xfs官方也考虑到了这个可能空洞太多无法分配inode问题，这个是最新的mkfs.xfs的man page</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">sparse[=value]<br>  Enable sparse inode chunk allocation. The value is either 0 or 1, with 1 signifying that sparse allocation is enabled.  If  the value  is omitted, 1 is assumed. Sparse inode allocation is disabled by default. This feature is only available <span class="hljs-keyword">for</span> filesystems formatted with -m crc=1.<br>  <br>   When enabled, sparse inode allocation allows the filesystem to allocate smaller than the  standard  64-inode  chunk  when  free space  is  severely  limited. This feature is useful <span class="hljs-keyword">for</span> filesystems that might fragment free space over time such that no free extents are large enough to accommodate a chunk of 64 inodes. Without this feature enabled, inode allocations can fail with out of space errors under severe fragmented free space conditions.<br></code></pre></td></tr></table></figure><p>是以64个inode为chunk来进行动态分配的，应该是有两个chunk，也就是动态查询看到的是128个inode以下，在更新到最新的版本以后，因为已经没有那么多空洞了，所以即使在没开这个稀疏inode的情况下，ceph的小文件也能够把磁盘写满</p><h2 id="变更记录"><a href="#变更记录" class="headerlink" title="变更记录"></a>变更记录</h2><table><thead><tr><th align="center">Why</th><th align="center">Who</th><th align="center">When</th></tr></thead><tbody><tr><td align="center">创建</td><td align="center">武汉-运维-磨渣</td><td align="center">2017-09-04</td></tr><tr><td align="center">增加更新内核和xfsprogs的验证</td><td align="center">武汉-运维-磨渣</td><td align="center">2017-09-05</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>为什么关不掉所有的OSD</title>
    <link href="/2017/08/21/%E4%B8%BA%E4%BB%80%E4%B9%88%E5%85%B3%E4%B8%8D%E6%8E%89%E6%89%80%E6%9C%89%E7%9A%84OSD/"/>
    <url>/2017/08/21/%E4%B8%BA%E4%BB%80%E4%B9%88%E5%85%B3%E4%B8%8D%E6%8E%89%E6%89%80%E6%9C%89%E7%9A%84OSD/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>碰到一个cepher问了一个问题：</p><blockquote><p>为什么我的OSD关闭到最后有92个OSD无法关闭,总共的OSD有300个左右</p></blockquote><p>想起来在很久以前帮人处理过一次问题，当时环境是遇上了一个BUG，需要升级到新版本进行解决，然后当时我来做操作，升级以后，发现osd无法启动，进程在，状态无法更新，当时又回滚回去，就可以了，当时好像是K版本升级到J版本，想起来之前看过这个版本里面有数据结构的变化，需要把osd全部停掉以后才能升级，然后就stop掉所有osd，当时发现有的osd还是无法stop，然后就手动去标记了，然后顺利升级</p><p>今天这个现象应该跟当时是一个问题，然后搜索了一番参数以后，最后定位在确实是参数进行了控制</p><h2 id="实践"><a href="#实践" class="headerlink" title="实践"></a>实践</h2><p>我的一个8个osd的单机环境，对所有OSD进行stop以后就是这个状态，还有2个状态无法改变</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ~]<span class="hljs-comment"># ceph -s</span><br>    cluster 49ee8a7f-fb7c-4239-a4b7-acf0bc37430d<br>     health HEALTH_ERR<br>            295 pgs are stuck inactive <span class="hljs-keyword">for</span> more than 300 seconds<br>            295 pgs stale<br>            295 pgs stuck stale<br>            too many PGs per OSD (400 &gt; max 300)<br>     monmap e1: 1 mons at &#123;lab8106=192.168.8.106:6789/0&#125;<br>            election epoch 3, quorum 0 lab8106<br>     osdmap e77: 8 osds: 2 up, 2 <span class="hljs-keyword">in</span>; 178 remapped pgs<br>            flags sortbitwise,require_jewel_osds<br>      pgmap v296: 400 pgs, 1 pools, 0 bytes data, 0 objects<br>            76440 kB used, 548 GB / 548 GB avail<br>                 295 stale+active+clean<br>                 105 active+clean<br></code></pre></td></tr></table></figure><p>看下这组参数：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">mon_osd_min_up_ratio = 0.3<br>mon_osd_min_in_ratio = 0.3<br></code></pre></td></tr></table></figure><p>我们修改成0 后再测试</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">mon_osd_min_up_ratio = 0<br>mon_osd_min_in_ratio = 0<br></code></pre></td></tr></table></figure><p>停止进程</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">systemctl stop ceph-osd.target<br></code></pre></td></tr></table></figure><p>查看状态</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ~]<span class="hljs-comment"># ceph -s</span><br>    cluster 49ee8a7f-fb7c-4239-a4b7-acf0bc37430d<br>     health HEALTH_ERR<br>            48 pgs are stuck inactive <span class="hljs-keyword">for</span> more than 300 seconds<br>            85 pgs degraded<br>            15 pgs peering<br>            400 pgs stale<br>            48 pgs stuck inactive<br>            48 pgs stuck unclean<br>            85 pgs undersized<br>            8/8 <span class="hljs-keyword">in</span> osds are down<br>     monmap e1: 1 mons at &#123;lab8106=192.168.8.106:6789/0&#125;<br>            election epoch 4, quorum 0 lab8106<br>     osdmap e86: 8 osds: 0 up, 8 <span class="hljs-keyword">in</span><br>            flags sortbitwise,require_jewel_osds<br>      pgmap v310: 400 pgs, 1 pools, 0 bytes data, 0 objects<br>            286 MB used, 2193 GB / 2194 GB avail<br>                 300 stale+active+clean<br>                  85 stale+undersized+degraded+peered<br>                  15 stale+peering<br></code></pre></td></tr></table></figure><p>可以看到状态已经可以正常全部关闭了</p><h2 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h2><p>这里不清楚官方做这个的理由，个人推断是这样的，默认的副本为3，那么在集群有三分之二的OSD都挂掉了以后，再出现OSD挂掉的情况下，这个集群其实就是一个废掉的状态的集群，而这个时候，还去触发down和out，对于环境来说已经是无效的操作了，触发的迁移也属于无效的迁移了，这个时候保持一个最终的可用的osdmap状态，对于整个环境的恢复也有一个基准点</p><p>在Luminous版本中已经把这个参数改成</p><blockquote><p>mon_osd min_up_ratio &#x3D; 0.3<br><br>mon_osd_min_in_ratio &#x3D; 0.75</p></blockquote><p>来降低其他异常情况引起的down，来避免过量的迁移</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本篇就是一个参数的实践</p><h2 id="变更记录"><a href="#变更记录" class="headerlink" title="变更记录"></a>变更记录</h2><table><thead><tr><th align="center">Why</th><th align="center">Who</th><th align="center">When</th></tr></thead><tbody><tr><td align="center">创建</td><td align="center">武汉-运维-磨渣</td><td align="center">2017-08-21</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>关于scrub的详细分析和建议</title>
    <link href="/2017/08/19/%E5%85%B3%E4%BA%8Escrub%E7%9A%84%E8%AF%A6%E7%BB%86%E5%88%86%E6%9E%90%E5%92%8C%E5%BB%BA%E8%AE%AE/"/>
    <url>/2017/08/19/%E5%85%B3%E4%BA%8Escrub%E7%9A%84%E8%AF%A6%E7%BB%86%E5%88%86%E6%9E%90%E5%92%8C%E5%BB%BA%E8%AE%AE/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>关于scrub这块一直想写一篇文章的，这个在很久前，就做过一次测试，当时是看这个scrub到底有多大的影响，当时看到的是磁盘读占很高，启动deep-scrub后会有大量的读,前端可能会出现 slow request,这个是当时测试看到的现象，一个比较简单的处理办法就是直接给scrub关掉了，当然关掉了就无法检测底层到底有没有对象不一致的问题<br>关于这个scrub生产上是否开启，仁者见仁，智者见智，就是选择的问题了，这里不做讨论，个人觉得开和关都有各自的道理，本篇是讲述的如果想开启的情况下如何把scrub给控制住</p><p>最近在ceph群里看到一段大致这样的讨论：</p><blockquote><p>scrub是个坑</p></blockquote><p>小文件多的场景一定要把scrub关掉<br>单pg的文件量达到一定规模，scrub一开就会有slow request<br>这个问题解决不了</p><p>上面的说法有没有问题呢？在一般情况下来看，确实如此，但是我们是否能尝试去解决下这个问题，或者缓解下呢？那么我们就来尝试下</p><h2 id="scrub的一些追踪"><a href="#scrub的一些追踪" class="headerlink" title="scrub的一些追踪"></a>scrub的一些追踪</h2><p>下面的一些追踪并不涉及代码，仅仅从配置和日志的观测来看看scrub到底干了什么</p><h3 id="环境准备"><a href="#环境准备" class="headerlink" title="环境准备"></a>环境准备</h3><p>我的环境为了便于观测，配置的是一个pg的存储池，然后往这个pg里面put了100个对象，然后对这个pg做deep-scrub，deep-scrub比scrub对磁盘的压力要大些，所以本篇主要是去观测的deep-scrub</p><h4 id="开启对pg目录的访问的监控"><a href="#开启对pg目录的访问的监控" class="headerlink" title="开启对pg目录的访问的监控"></a>开启对pg目录的访问的监控</h4><p>使用的是inotifywait，我想看下deep-scrub的时候，pg里面的对象到底接收了哪些请求</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br></pre></td><td class="code"><pre><code class="hljs raw">inotifywait -m 1.0_head<br>1.0_head/ OPEN,ISDIR <br>1.0_head/ ACCESS,ISDIR <br>1.0_head/ ACCESS,ISDIR <br>1.0_head/ CLOSE_NOWRITE,CLOSE,ISDIR <br>1.0_head/ OPEN,ISDIR <br>1.0_head/ ACCESS,ISDIR <br>1.0_head/ ACCESS,ISDIR <br>1.0_head/ CLOSE_NOWRITE,CLOSE,ISDIR <br>1.0_head/ OPEN a16__head_8FA46F40__1<br>1.0_head/ ACCESS a16__head_8FA46F40__1<br>1.0_head/ OPEN a39__head_621FD720__1<br>1.0_head/ ACCESS a39__head_621FD720__1<br>1.0_head/ OPEN a30__head_655287E0__1<br>1.0_head/ ACCESS a30__head_655287E0__1<br>1.0_head/ OPEN a91__head_B02EE3D0__1<br>1.0_head/ ACCESS a91__head_B02EE3D0__1<br>1.0_head/ OPEN a33__head_9E9E3E30__1<br>1.0_head/ ACCESS a33__head_9E9E3E30__1<br>1.0_head/ OPEN a92__head_6AFC6B30__1<br>1.0_head/ ACCESS a92__head_6AFC6B30__1<br>1.0_head/ OPEN a22__head_AC48AAB0__1<br>1.0_head/ ACCESS a22__head_AC48AAB0__1<br>1.0_head/ OPEN a42__head_76B90AC8__1<br>1.0_head/ ACCESS a42__head_76B90AC8__1<br>1.0_head/ OPEN a5__head_E5A1A728__1<br>1.0_head/ ACCESS a5__head_E5A1A728__1<br>1.0_head/ OPEN a34__head_4D9ABA68__1<br>1.0_head/ ACCESS a34__head_4D9ABA68__1<br>1.0_head/ OPEN a69__head_7AF2B6E8__1<br>1.0_head/ ACCESS a69__head_7AF2B6E8__1<br>1.0_head/ OPEN a95__head_BD3695B8__1<br>1.0_head/ ACCESS a95__head_BD3695B8__1<br>1.0_head/ OPEN a67__head_6BCD37B8__1<br>1.0_head/ ACCESS a67__head_6BCD37B8__1<br>1.0_head/ OPEN a10__head_F0F08AF8__1<br>1.0_head/ ACCESS a10__head_F0F08AF8__1<br>1.0_head/ OPEN a3__head_88EF0BF8__1<br>1.0_head/ ACCESS a3__head_88EF0BF8__1<br>1.0_head/ OPEN a82__head_721BC094__1<br>1.0_head/ ACCESS a82__head_721BC094__1<br>1.0_head/ OPEN a48__head_27A729D4__1<br>1.0_head/ ACCESS a48__head_27A729D4__1<br>1.0_head/ OPEN a36__head_F63E6AF4__1<br>1.0_head/ ACCESS a36__head_F63E6AF4__1<br>1.0_head/ OPEN a29__head_F06D540C__1<br>1.0_head/ ACCESS a29__head_F06D540C__1<br>1.0_head/ OPEN a31__head_AC83164C__1<br>1.0_head/ ACCESS a31__head_AC83164C__1<br>1.0_head/ OPEN a59__head_884F9B6C__1<br>1.0_head/ ACCESS a59__head_884F9B6C__1<br>1.0_head/ OPEN a58__head_06954F6C__1<br>1.0_head/ ACCESS a58__head_06954F6C__1<br>1.0_head/ OPEN a55__head_2A42E61C__1<br>1.0_head/ ACCESS a55__head_2A42E61C__1<br>1.0_head/ OPEN a90__head_1B88FEDC__1<br>1.0_head/ ACCESS a90__head_1B88FEDC__1<br>1.0_head/ OPEN,ISDIR <br>1.0_head/ ACCESS,ISDIR <br>1.0_head/ ACCESS,ISDIR <br>1.0_head/ CLOSE_NOWRITE,CLOSE,ISDIR <br>1.0_head/ OPEN,ISDIR <br>1.0_head/ ACCESS,ISDIR <br>1.0_head/ ACCESS,ISDIR <br>1.0_head/ CLOSE_NOWRITE,CLOSE,ISDIR <br>1.0_head/ OPEN a100__head_C29E0C42__1<br>1.0_head/ ACCESS a100__head_C29E0C42__1<br>1.0_head/ OPEN a15__head_87123BE2__1<br>1.0_head/ ACCESS a15__head_87123BE2__1<br>1.0_head/ OPEN a23__head_AABFFB92__1<br>1.0_head/ ACCESS a23__head_AABFFB92__1<br>1.0_head/ OPEN a41__head_4EA9A5D2__1<br>1.0_head/ ACCESS a41__head_4EA9A5D2__1<br>1.0_head/ OPEN a85__head_83760D72__1<br>1.0_head/ ACCESS a85__head_83760D72__1<br>1.0_head/ OPEN a72__head_8A105D72__1<br>1.0_head/ ACCESS a72__head_8A105D72__1<br>1.0_head/ OPEN a60__head_5536480A__1<br>1.0_head/ ACCESS a60__head_5536480A__1<br>1.0_head/ OPEN a73__head_F1819D0A__1<br>1.0_head/ ACCESS a73__head_F1819D0A__1<br>1.0_head/ OPEN a78__head_6929D12A__1<br>1.0_head/ ACCESS a78__head_6929D12A__1<br>1.0_head/ OPEN a57__head_2C43153A__1<br>1.0_head/ ACCESS a57__head_2C43153A__1<br>1.0_head/ OPEN a1__head_51903B7A__1<br>1.0_head/ ACCESS a1__head_51903B7A__1<br>1.0_head/ OPEN a12__head_14D7ABC6__1<br>1.0_head/ ACCESS a12__head_14D7ABC6__1<br>1.0_head/ OPEN a63__head_9490B166__1<br>1.0_head/ ACCESS a63__head_9490B166__1<br>1.0_head/ OPEN a53__head_DF95B716__1<br>1.0_head/ ACCESS a53__head_DF95B716__1<br>1.0_head/ OPEN a13__head_E09E0896__1<br>1.0_head/ ACCESS a13__head_E09E0896__1<br>1.0_head/ OPEN a27__head_7ED31896__1<br>1.0_head/ ACCESS a27__head_7ED31896__1<br>1.0_head/ OPEN a43__head_7052A656__1<br>1.0_head/ ACCESS a43__head_7052A656__1<br>1.0_head/ OPEN a28__head_E6257CD6__1<br>1.0_head/ ACCESS a28__head_E6257CD6__1<br>1.0_head/ OPEN a35__head_ACABD736__1<br>1.0_head/ ACCESS a35__head_ACABD736__1<br>1.0_head/ OPEN a54__head_B9482876__1<br>1.0_head/ CLOSE_WRITE,CLOSE a12__head_14D7ABC6__1<br>1.0_head/ ACCESS a54__head_B9482876__1<br>1.0_head/ OPEN a4__head_F12ACA76__1<br>1.0_head/ CLOSE_WRITE,CLOSE a63__head_9490B166__1<br>1.0_head/ ACCESS a4__head_F12ACA76__1<br>1.0_head/ OPEN a84__head_B033038E__1<br>1.0_head/ ACCESS a84__head_B033038E__1<br>1.0_head/ OPEN a19__head_D6A64F9E__1<br>1.0_head/ ACCESS a19__head_D6A64F9E__1<br>1.0_head/ OPEN a93__head_F54E757E__1<br>1.0_head/ ACCESS a93__head_F54E757E__1<br>1.0_head/ OPEN a7__head_1F08F77E__1<br>1.0_head/ ACCESS a7__head_1F08F77E__1<br>1.0_head/ OPEN,ISDIR <br>1.0_head/ ACCESS,ISDIR <br>1.0_head/ ACCESS,ISDIR <br>1.0_head/ CLOSE_NOWRITE,CLOSE,ISDIR <br>1.0_head/ OPEN,ISDIR <br>1.0_head/ ACCESS,ISDIR <br>1.0_head/ ACCESS,ISDIR <br>1.0_head/ CLOSE_NOWRITE,CLOSE,ISDIR <br>1.0_head/ OPEN a9__head_635C6201__1<br>1.0_head/ ACCESS a9__head_635C6201__1<br>1.0_head/ OPEN a11__head_12780121__1<br>1.0_head/ ACCESS a11__head_12780121__1<br>1.0_head/ OPEN a50__head_5E524321__1<br>1.0_head/ ACCESS a50__head_5E524321__1<br>1.0_head/ OPEN a75__head_27E1CB21__1<br>1.0_head/ ACCESS a75__head_27E1CB21__1<br>1.0_head/ OPEN a21__head_69ACD1A1__1<br>1.0_head/ ACCESS a21__head_69ACD1A1__1<br>1.0_head/ OPEN a25__head_698E7751__1<br>1.0_head/ ACCESS a25__head_698E7751__1<br>1.0_head/ OPEN a44__head_57E29949__1<br>1.0_head/ ACCESS a44__head_57E29949__1<br>1.0_head/ OPEN a66__head_944E79C9__1<br>1.0_head/ ACCESS a66__head_944E79C9__1<br>1.0_head/ OPEN a52__head_DAC6BF29__1<br>1.0_head/ ACCESS a52__head_DAC6BF29__1<br>1.0_head/ OPEN a14__head_295EA1A9__1<br>1.0_head/ ACCESS a14__head_295EA1A9__1<br>1.0_head/ OPEN a70__head_62941259__1<br>1.0_head/ ACCESS a70__head_62941259__1<br>1.0_head/ OPEN a18__head_53B48959__1<br>1.0_head/ ACCESS a18__head_53B48959__1<br>1.0_head/ OPEN a17__head_7D103759__1<br>1.0_head/ ACCESS a17__head_7D103759__1<br>1.0_head/ OPEN a6__head_9505BEF9__1<br>1.0_head/ ACCESS a6__head_9505BEF9__1<br>1.0_head/ OPEN a77__head_88A7CC25__1<br>1.0_head/ ACCESS a77__head_88A7CC25__1<br>1.0_head/ OPEN a37__head_141AFE65__1<br>1.0_head/ ACCESS a37__head_141AFE65__1<br>1.0_head/ OPEN a74__head_90DAAD15__1<br>1.0_head/ ACCESS a74__head_90DAAD15__1<br>1.0_head/ OPEN a32__head_B7957195__1<br>1.0_head/ ACCESS a32__head_B7957195__1<br>1.0_head/ OPEN a45__head_CCCFB5D5__1<br>1.0_head/ ACCESS a45__head_CCCFB5D5__1<br>1.0_head/ OPEN a24__head_3B937275__1<br>1.0_head/ ACCESS a24__head_3B937275__1<br>1.0_head/ OPEN a26__head_2AB240F5__1<br>1.0_head/ ACCESS a26__head_2AB240F5__1<br>1.0_head/ OPEN a89__head_8E387EF5__1<br>1.0_head/ ACCESS a89__head_8E387EF5__1<br>1.0_head/ OPEN a80__head_6FEFE78D__1<br>1.0_head/ ACCESS a80__head_6FEFE78D__1<br>1.0_head/ OPEN a51__head_0BCC72CD__1<br>1.0_head/ ACCESS a51__head_0BCC72CD__1<br>1.0_head/ OPEN a71__head_88F4796D__1<br>1.0_head/ ACCESS a71__head_88F4796D__1<br>1.0_head/ OPEN,ISDIR <br>1.0_head/ ACCESS,ISDIR <br>1.0_head/ ACCESS,ISDIR <br>1.0_head/ CLOSE_NOWRITE,CLOSE,ISDIR <br>1.0_head/ OPEN,ISDIR <br>1.0_head/ ACCESS,ISDIR <br>1.0_head/ ACCESS,ISDIR <br>1.0_head/ CLOSE_NOWRITE,CLOSE,ISDIR <br>1.0_head/ OPEN a88__head_B0A64FED__1<br>1.0_head/ ACCESS a88__head_B0A64FED__1<br>1.0_head/ OPEN a8__head_F885EA9D__1<br>1.0_head/ ACCESS a8__head_F885EA9D__1<br>1.0_head/ OPEN a83__head_1322679D__1<br>1.0_head/ ACCESS a83__head_1322679D__1<br>1.0_head/ OPEN a76__head_B8285A7D__1<br>1.0_head/ ACCESS a76__head_B8285A7D__1<br>1.0_head/ OPEN a94__head_D3BBB683__1<br>1.0_head/ ACCESS a94__head_D3BBB683__1<br>1.0_head/ OPEN a46__head_E2C6C983__1<br>1.0_head/ ACCESS a46__head_E2C6C983__1<br>1.0_head/ OPEN a56__head_A1E888C3__1<br>1.0_head/ ACCESS a56__head_A1E888C3__1<br>1.0_head/ OPEN a99__head_DD3B45C3__1<br>1.0_head/ ACCESS a99__head_DD3B45C3__1<br>1.0_head/ OPEN a79__head_AC19FC13__1<br>1.0_head/ ACCESS a79__head_AC19FC13__1<br>1.0_head/ OPEN a81__head_BC0AFFF3__1<br>1.0_head/ ACCESS a81__head_BC0AFFF3__1<br>1.0_head/ OPEN a64__head_C042B84B__1<br>1.0_head/ ACCESS a64__head_C042B84B__1<br>1.0_head/ OPEN a97__head_29054B4B__1<br>1.0_head/ ACCESS a97__head_29054B4B__1<br>1.0_head/ OPEN a96__head_BAAC0DCB__1<br>1.0_head/ ACCESS a96__head_BAAC0DCB__1<br>1.0_head/ OPEN a62__head_84A40AAB__1<br>1.0_head/ ACCESS a62__head_84A40AAB__1<br>1.0_head/ OPEN a98__head_C15FD53B__1<br>1.0_head/ ACCESS a98__head_C15FD53B__1<br>1.0_head/ OPEN a87__head_12F9237B__1<br>1.0_head/ ACCESS a87__head_12F9237B__1<br>1.0_head/ OPEN a2__head_E2983C17__1<br>1.0_head/ ACCESS a2__head_E2983C17__1<br>1.0_head/ OPEN a20__head_7E477A77__1<br>1.0_head/ ACCESS a20__head_7E477A77__1<br>1.0_head/ OPEN a49__head_3ADEC577__1<br>1.0_head/ ACCESS a49__head_3ADEC577__1<br>1.0_head/ OPEN a61__head_C860ABF7__1<br>1.0_head/ ACCESS a61__head_C860ABF7__1<br>1.0_head/ OPEN a68__head_BC5C8F8F__1<br>1.0_head/ ACCESS a68__head_BC5C8F8F__1<br>1.0_head/ OPEN a38__head_78AE322F__1<br>1.0_head/ ACCESS a38__head_78AE322F__1<br>1.0_head/ OPEN a65__head_7EE57AEF__1<br>1.0_head/ ACCESS a65__head_7EE57AEF__1<br>1.0_head/ OPEN a47__head_B6C48D1F__1<br>1.0_head/ ACCESS a47__head_B6C48D1F__1<br>1.0_head/ OPEN a86__head_7FB2C85F__1<br>1.0_head/ ACCESS a86__head_7FB2C85F__1<br>1.0_head/ OPEN,ISDIR <br>1.0_head/ ACCESS,ISDIR <br>1.0_head/ ACCESS,ISDIR <br>1.0_head/ CLOSE_NOWRITE,CLOSE,ISDIR <br>1.0_head/ OPEN,ISDIR <br>1.0_head/ ACCESS,ISDIR <br>1.0_head/ ACCESS,ISDIR <br>1.0_head/ CLOSE_NOWRITE,CLOSE,ISDIR <br>1.0_head/ OPEN a40__head_5F0404DF__1<br>1.0_head/ ACCESS a40__head_5F0404DF__1<br></code></pre></td></tr></table></figure><p>在给osd.0开启debug_osd&#x3D;20后观测chunky相关的日志</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ceph]# cat ceph-osd.0.log |grep chunky:1|grep handle_replica_op<br>2017-08-18 23:50:40.262448 7f2ac583c700 10 osd.0 26 handle_replica_op replica scrub(pg: 1.0,from:0&#x27;0,to:22&#x27;2696,epoch:26,start:1:00000000::::head,end:1:42307943:::a100:0,chunky:1,deep:1,seed:4294967295,version:6) v6 epoch 26<br>2017-08-18 23:50:40.294637 7f2ac583c700 10 osd.0 26 handle_replica_op replica scrub(pg: 1.0,from:0&#x27;0,to:22&#x27;2694,epoch:26,start:1:42307943:::a100:0,end:1:80463ac6:::a9:0,chunky:1,deep:1,seed:4294967295,version:6) v6 epoch 26<br>2017-08-18 23:50:40.320986 7f2ac583c700 10 osd.0 26 handle_replica_op replica scrub(pg: 1.0,from:0&#x27;0,to:22&#x27;2690,epoch:26,start:1:80463ac6:::a9:0,end:1:b7f2650d:::a88:0,chunky:1,deep:1,seed:4294967295,version:6) v6 epoch 26<br>2017-08-18 23:50:40.337646 7f2ac583c700 10 osd.0 26 handle_replica_op replica scrub(pg: 1.0,from:0&#x27;0,to:22&#x27;2700,epoch:26,start:1:b7f2650d:::a88:0,end:1:fb2020fa:::a40:0,chunky:1,deep:1,seed:4294967295,version:6) v6 epoch 26<br>2017-08-18 23:50:40.373227 7f2ac583c700 10 osd.0 26 handle_replica_op replica scrub(pg: 1.0,from:0&#x27;0,to:22&#x27;2636,epoch:26,start:1:fb2020fa:::a40:0,end:MAX,chunky:1,deep:1,seed:4294967295,version:6) v6 epoch 26<br></code></pre></td></tr></table></figure><p>截取关键部分看下，如图<br><img src="/images/blog/o_200901082444image-4.png" alt="a100"><br>我们看下上面的文件访问监控里面这些对象在什么位置</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs raw">25:1.0_head/ ACCESS a100__head_C29E0C42__1<br>50:1.0_head/ ACCESS a9__head_635C6201__1<br>75:1.0_head/ ACCESS a88__head_B0A64FED__1<br>100:1.0_head/ ACCESS a40__head_5F0404DF__1<br></code></pre></td></tr></table></figure><p>看上去是不是很有规律，这个地方在ceph里面会有个chunk的概念，在做scrub的时候，ceph会对这个chunk进行加锁，这个可以在很多地方看到这个，这个也就是为什么有slow request，并不一定是你的磁盘慢了，而是加了锁，就没法读的</p><blockquote><p>osd scrub chunk min</p><p>Description:The minimal number of object store chunks to scrub during single operation. Ceph blocks writes to single chunk during scrub.<br>Type:32-bit Integer<br>Default:5</p></blockquote><p>从配置文件上面看说是会锁住写，没有提及读的锁定的问题，那么我们下面验证下这个问题，到底deep-scrub，是不是会引起读的slow request</p><p>上面的环境100个对象，现在把100个对象的大小调整为100M一个，并且chunk设置为100个对象的，也就是我把我这个环境所有的对象认为是一个大的chunk，然后去用rados读取这个对象，来看下会发生什么</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs raw">osd_scrub_chunk_min = 100<br>osd_scrub_chunk_max = 100<br></code></pre></td></tr></table></figure><p>使用ceph -w监控</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs raw">2017-08-19 00:19:26.045032 mon.0 [INF] pgmap v377: 1 pgs: 1 active+clean+scrubbing+deep; 10000 MB data, 30103 MB used, 793 GB / 822 GB avail<br>2017-08-19 00:19:17.540413 osd.0 [WRN] 1 slow requests, 1 included below; oldest blocked for &gt; 30.398705 secs<br>2017-08-19 00:19:17.540456 osd.0 [WRN] slow request 30.398705 seconds old, received at 2017-08-19 00:18:47.141483: replica scrub(pg: 1.0,from:0&#x27;0,to:26&#x27;5200,epoch:32,start:1:00000000::::head,end:MAX,chunky:1,deep:1,seed:4294967295,version:6) currently reached_pg<br></code></pre></td></tr></table></figure><p>我从deep scrub 一开始就进行a40对象的get rados -p rbd get a40 a40，直接就卡着不返回，在pg内对象不变的情况下，对pg做scrub的顺序是不变的，我专门挑了我这个scrub顺序下最后一个scrub的对象来做get，还是出现了slow request ，这个可以证明上面的推断，也就是在做scrub的时候，对scub的chunk的对象的读取请求也会卡死，现在我把我的scrub的chunk弄成1看下会发生什么</p><p>配置参数改成</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs raw">osd_scrub_chunk_min = 1<br>osd_scrub_chunk_max = 1<br></code></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs raw">watch -n 1 &#x27;rados -p rbd get a9 a1&#x27;<br>watch -n 1 &#x27;rados -p rbd get a9 a2&#x27;<br>watch -n 1 &#x27;rados -p rbd get a9 a3&#x27;<br>watch -n 1 &#x27;rados -p rbd get a9 a4&#x27;<br>watch -n 1 &#x27;rados -p rbd get a9 a5&#x27;<br></code></pre></td></tr></table></figure><p>使用五个请求同时去get a9,循环的去做</p><p>然后做deep scrub，这一次并没有出现slow  request 的情况</p><p>###另外一个重要参数<br>再看看这个参数osd_scrub_sleep &#x3D; 0</p><blockquote><p>osd scrub sleep</p><p>Description:Time to sleep before scrubbing next group of chunks. Increasing this value will slow down whole scrub operation while client operations will be less impacted.<br>Type:Float<br>Default:0</p></blockquote><p>可以看到还有scrub group这个概念，从数据上分析这个group 是3，也就是3个chunks<br>我们来设置下</p><blockquote><p>osd_scrub_sleep &#x3D; 5</p></blockquote><p>然后再次做deep-scrub,然后看下日志的内容</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs raw">cat /var/log/ceph/ceph-osd.0.log |grep be_deep_scrub|awk &#x27;&#123;print $1,$2,$28&#125;&#x27;|less<br>2017-08-19 00:48:37.930455 1:02f625f1:::a16:head<br>2017-08-19 00:48:38.477271 1:02f625f1:::a16:head<br>2017-08-19 00:48:38.477367 1:04ebf846:::a39:head<br>2017-08-19 00:48:39.023952 1:04ebf846:::a39:head<br>2017-08-19 00:48:39.024084 1:07e14aa6:::a30:head<br>2017-08-19 00:48:39.572683 1:07e14aa6:::a30:head<br>2017-08-19 00:48:44.989551 1:0bc7740d:::a91:head<br>2017-08-19 00:48:45.556758 1:0bc7740d:::a91:head<br>2017-08-19 00:48:45.556857 1:0c7c7979:::a33:head<br>2017-08-19 00:48:46.109657 1:0c7c7979:::a33:head<br>2017-08-19 00:48:46.109768 1:0cd63f56:::a92:head<br>2017-08-19 00:48:46.657849 1:0cd63f56:::a92:head<br>2017-08-19 00:48:52.084712 1:0d551235:::a22:head<br>2017-08-19 00:48:52.614345 1:0d551235:::a22:head<br>2017-08-19 00:48:52.614458 1:13509d6e:::a42:head<br>2017-08-19 00:48:53.158826 1:13509d6e:::a42:head<br>2017-08-19 00:48:53.158916 1:14e585a7:::a5:head<br></code></pre></td></tr></table></figure><p>可以看到1s做一个对象的deep-scrub，然后在做了3个对象后就停止了5s</p><h3 id="默认情况下的scrub和修改后的对比"><a href="#默认情况下的scrub和修改后的对比" class="headerlink" title="默认情况下的scrub和修改后的对比"></a>默认情况下的scrub和修改后的对比</h3><p>我们来计算下在修改前后的情况对比，我们来模拟pg里面有10000个对象的情况小文件 测试的文件都是1K的，这个可以根据自己的文件模型进行测试</p><p>假设是海量对象的场景，那么算下来单pg 1w左右对象左右也算比较多了，我们就模拟10000个对象的场景的deep-scrub</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">cat /var/log/ceph/ceph-osd.0.log |grep be_deep_scrub|awk &#x27;&#123;print $1,$2,$28&#125;&#x27;|awk &#x27;&#123;sub(/.*/,substr($2,1,8),$2); print $0&#125;&#x27;|uniq|awk &#x27;&#123;a[$1,&quot; &quot;,$2]++&#125;END&#123;for (j in a) print j,a[j]|&quot;sort -k 1&quot;&#125;&#x27;<br></code></pre></td></tr></table></figure><p>使用上面的脚本统计每秒scrub的对象数目</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><code class="hljs raw">2017-08-19 01:23:33 184<br>2017-08-19 01:23:34 236<br>2017-08-19 01:23:35 261<br>2017-08-19 01:23:36 263<br>2017-08-19 01:23:37 229<br>2017-08-19 01:23:38 289<br>2017-08-19 01:23:39 236<br>2017-08-19 01:23:40 258<br>2017-08-19 01:23:41 276<br>2017-08-19 01:23:42 238<br>2017-08-19 01:23:43 224<br>2017-08-19 01:23:44 282<br>2017-08-19 01:23:45 254<br>2017-08-19 01:23:46 258<br>2017-08-19 01:23:47 261<br>2017-08-19 01:23:48 233<br>2017-08-19 01:23:49 300<br>2017-08-19 01:23:50 243<br>2017-08-19 01:23:51 257<br>2017-08-19 01:23:52 252<br>2017-08-19 01:23:53 246<br>2017-08-19 01:23:54 313<br>2017-08-19 01:23:55 252<br>2017-08-19 01:23:56 276<br>2017-08-19 01:23:57 245<br>2017-08-19 01:23:58 256<br>2017-08-19 01:23:59 307<br>2017-08-19 01:24:00 276<br>2017-08-19 01:24:01 310<br>2017-08-19 01:24:02 220<br>2017-08-19 01:24:03 250<br>2017-08-19 01:24:04 313<br>2017-08-19 01:24:05 265<br>2017-08-19 01:24:06 304<br>2017-08-19 01:24:07 262<br>2017-08-19 01:24:08 308<br>2017-08-19 01:24:09 263<br>2017-08-19 01:24:10 293<br>2017-08-19 01:24:11 42<br></code></pre></td></tr></table></figure><p>可以看到1s 会扫300个对象左右，差不多40s钟就扫完了一个pg，默认25个对象一个trunk</p><p>这里可以打个比喻，在一条长为40m的马路上，一个汽车以1m&#x2F;s速度前进，中间会有人来回穿，如果穿梭的人只有一两个可能没什么问题，但是一旦有40个人在这个区间进行穿梭的时候，可想而知碰撞的概率会有多大了</p><p>或者同一个文件被连续请求40次，那么对应到这里就是40个人在同一个位置不停的穿马路，这样撞上的概率是不是非常的大了？</p><p>上面说了这么多，那么我想如果整个看下来，应该知道怎么处理了<br>我们看下这样的全部为1的情况下，会出现什么情况</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs raw">osd_scrub_chunk_min = 1<br>osd_scrub_chunk_max = 1<br>osd_scrub_sleep = 3<br></code></pre></td></tr></table></figure><p>这里减少chunk大小，相当于减少上面例子当中汽车的长度，原来25米的大卡车，变成1米的自行车了</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ceph]# cat /var/log/ceph/ceph-osd.0.log |grep be_deep_scrub|awk &#x27;&#123;print $1,$2,$28&#125;&#x27;<br>2017-08-19 16:12:21.927440 1:0000b488:::a5471:head<br>2017-08-19 16:12:21.931914 1:0000b488:::a5471:head<br>2017-08-19 16:12:21.932039 1:000fbbcb:::a5667:head<br>2017-08-19 16:12:21.933568 1:000fbbcb:::a5667:head<br>2017-08-19 16:12:21.933646 1:00134ebd:::a1903:head<br>2017-08-19 16:12:21.934972 1:00134ebd:::a1903:head<br>2017-08-19 16:12:24.960697 1:0018f641:::a2028:head<br>2017-08-19 16:12:24.966653 1:0018f641:::a2028:head<br>2017-08-19 16:12:24.966733 1:00197a21:::a1463:head<br>2017-08-19 16:12:24.967085 1:00197a21:::a1463:head<br>2017-08-19 16:12:24.967162 1:001cb17d:::a1703:head<br>2017-08-19 16:12:24.967492 1:001cb17d:::a1703:head<br>2017-08-19 16:12:27.972252 1:002d911c:::a1585:head<br>2017-08-19 16:12:27.976621 1:002d911c:::a1585:head<br>2017-08-19 16:12:27.976740 1:00301acf:::a6131:head<br>2017-08-19 16:12:27.977097 1:00301acf:::a6131:head<br>2017-08-19 16:12:27.977181 1:0039a0a8:::a1840:head<br>2017-08-19 16:12:27.979053 1:0039a0a8:::a1840:head<br>2017-08-19 16:12:30.983556 1:00484881:::a8781:head<br>2017-08-19 16:12:30.989098 1:00484881:::a8781:head<br>2017-08-19 16:12:30.989181 1:004f234f:::a4402:head<br>2017-08-19 16:12:30.989531 1:004f234f:::a4402:head<br>2017-08-19 16:12:30.989626 1:00531b36:::a5251:head<br>2017-08-19 16:12:30.989954 1:00531b36:::a5251:head<br>2017-08-19 16:12:33.994419 1:00584c30:::a3374:head<br>2017-08-19 16:12:34.001296 1:00584c30:::a3374:head<br>2017-08-19 16:12:34.001378 1:005d6aa5:::a2115:head<br>2017-08-19 16:12:34.002174 1:005d6aa5:::a2115:head<br>2017-08-19 16:12:34.002287 1:005e0dfd:::a9945:head<br>2017-08-19 16:12:34.002686 1:005e0dfd:::a9945:head<br>2017-08-19 16:12:37.005645 1:006320f9:::a5207:head<br>2017-08-19 16:12:37.011498 1:006320f9:::a5207:head<br>2017-08-19 16:12:37.011655 1:006d32b4:::a7517:head<br>2017-08-19 16:12:37.011998 1:006d32b4:::a7517:head<br>2017-08-19 16:12:37.012111 1:006dae55:::a4702:head<br>2017-08-19 16:12:37.012442 1:006dae55:::a4702:head<br></code></pre></td></tr></table></figure><p>上面从日志里面截取部分的日志，这个是什么意思呢，是每秒钟扫描3个对象，然后休息3s再进行下一个，这个是不是已经把速度压到非常低了？还有上面做测试scrub sleep例子里面好像是1s 会scrub 1个对象，这里怎么就成了1s会scrub 3 个对象了，这个跟scrub的对象大小有关，对象越大，scrub的时间就相对长一点，这个测试里面的对象是1K的，基本算非常小了，也就是1s会扫描3个对象，然后根据你的设置的sleep值等待进入下一组的scrub</p><p>在上面的环境下默认每秒钟会对300左右的对象进行scrub，以25个对象的锁定窗口移动，无法写入和读取，而参数修改后每秒有3个对象被scrub，以1个对象的锁定窗口移动，这个单位时间锁定的对象的数目已经降低到一个非常低的程度了，如果你有生产环境又想去开scrub，不妨尝试下降低chunk，增加sleep</p><p>这个的影响就是扫描的速度而已，而如果你想加快扫描速度，就去调整sleep参数来控制这个扫描的速度了，这个就不在这里赘述了</p><p>本篇讲述的是一个PG上开启deep-scrub以后的影响，默认的是到了最大的intelval以后就会开启自动开启scrub了，所以我建议的是不用系统自带的时间控制，而是自己去分析的scrub的时间戳和对象数目，然后计算好以后，可以是每天晚上，扫描指定个数的PG，然后等一轮全做完以后，中间就是自定义的一段时间的不扫描期，这个可以自己定义，是一个月或者两个月扫一轮都行，这个会在后面单独写一篇文章来讲述这个</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>关于scrub，你需要了解，scrub什么时候会发生，发生以后会对你的osd产生多少的负载，每秒钟会扫描多少对象，如何去降低这个影响，这些问题就是本篇的来源了，很多问题是能从参数上进行解决的，关键是你要知道它们到底在干嘛</p><h2 id="变更记录"><a href="#变更记录" class="headerlink" title="变更记录"></a>变更记录</h2><table><thead><tr><th align="center">Why</th><th align="center">Who</th><th align="center">When</th></tr></thead><tbody><tr><td align="center">创建</td><td align="center">武汉-运维-磨渣</td><td align="center">2017-08-19</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>如何测量Ceph OSD内存占用</title>
    <link href="/2017/08/10/%E5%A6%82%E4%BD%95%E6%B5%8B%E9%87%8FCeph%20OSD%E5%86%85%E5%AD%98%E5%8D%A0%E7%94%A8/"/>
    <url>/2017/08/10/%E5%A6%82%E4%BD%95%E6%B5%8B%E9%87%8FCeph%20OSD%E5%86%85%E5%AD%98%E5%8D%A0%E7%94%A8/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>这个工具我第一次看到是在填坑群里面看到，是由研发-北京-蓝星同学分享的，看到比较有趣，就写一篇相关的记录下用法</p><p>火焰图里面也可以定位内存方面的问题，那个是通过一段时间的统计，以一个汇总的方式来查看内存在哪个地方可能出了问题</p><!--break--><p>本篇是另外一个工具，这个工具的好处是有很清晰的图表操作，以及基于时间线的统计，下面来看下这个工具怎么使用的</p><p>本篇对具体的内存函数的调用占用不会做更具体的分析，这里是提供一个工具的使用方法供感兴趣的研发同学来使用</p><h2 id="环境准备"><a href="#环境准备" class="headerlink" title="环境准备"></a>环境准备</h2><p>目前大多数的ceph运行在centos7系列上面，笔者的环境也是在centos7上面，所以以这个举例，其他平台同样可以</p><p>需要用到的工具</p><ul><li>valgrind</li><li>massif-visualizer</li></ul><p>安装valgrind</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">yum install valgrind<br></code></pre></td></tr></table></figure><p>massif-visualizer是数据可视化的工具，由于并没有centos的发行版本，但是有fedora的版本，从网上看到资料说这个可以直接安装忽略掉需要的依赖即可，我自己跑了下，确实可行</p><p>下载massif-visualizer</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">wget ftp://ftp.pbone.net/mirror/download.fedora.redhat.com/pub/fedora/linux/releases/23/Everything/x86_64/os/Packages/m/massif-visualizer-0.4.0-6.fc23.x86_64.rpm<br></code></pre></td></tr></table></figure><p>安装massif-visualizer</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">rpm -ivh massif-visualizer-0.4.0-6.fc23.x86_64.rpm  --nodeps<br></code></pre></td></tr></table></figure><p>不要漏了后面的nodeps</p><h2 id="抓取ceph-osd运行时内存数据"><a href="#抓取ceph-osd运行时内存数据" class="headerlink" title="抓取ceph osd运行时内存数据"></a>抓取ceph osd运行时内存数据</h2><p>停掉需要监控的osd（例如我的是osd.4）</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ~]<span class="hljs-comment"># systemctl stop ceph-osd@4</span><br></code></pre></td></tr></table></figure><p>开始运行监控</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ~]<span class="hljs-comment"># valgrind --tool=massif /usr/bin/ceph-osd -f --cluster ceph --id 4 --setuser ceph --setgroup ceph</span><br>==21522== Massif, a heap profiler<br>==21522== Copyright (C) 2003-2015, and GNU GPL<span class="hljs-string">&#x27;d, by Nicholas Nethercote</span><br><span class="hljs-string">==21522== Using Valgrind-3.11.0 and LibVEX; rerun with -h for copyright info</span><br><span class="hljs-string">==21522== Command: /usr/bin/ceph-osd -f --cluster ceph --id 4 --setuser ceph --setgroup ceph</span><br><span class="hljs-string">==21522== </span><br><span class="hljs-string">==21522== </span><br><span class="hljs-string">starting osd.4 at :/0 osd_data /var/lib/ceph/osd/ceph-4 /var/lib/ceph/osd/ceph-4/journal</span><br><span class="hljs-string">2017-08-10 16:36:42.395682 a14d680 -1 osd.4 522 log_to_monitors &#123;default=true&#125;</span><br></code></pre></td></tr></table></figure><p>监控已经开始了,在top下可以看到有这个进程运行，占用cpu还是比较高的，可能是要抓取很多数据的原因<br><img src="/images/blog/o_200901082210mem-image1.png" alt="valtop"></p><p>等待一段时间后，就可以把之前运行的命令ctrl+C掉</p><p>在当前目录下面就会生成一个【massif.out.进程号】的文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ~]<span class="hljs-comment"># ll massif.out.21522 </span><br>-rw------- 1 root root 142682 Aug 10 16:39 massif.out.21522<br></code></pre></td></tr></table></figure><h2 id="查看截取的数据"><a href="#查看截取的数据" class="headerlink" title="查看截取的数据"></a>查看截取的数据</h2><h3 id="命令行下的查看"><a href="#命令行下的查看" class="headerlink" title="命令行下的查看"></a>命令行下的查看</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ~]<span class="hljs-comment"># ms_print massif.out.21522 |less</span><br></code></pre></td></tr></table></figure><p>这个方式是文本方式的查看，也比较方便，自带的文本分析工具，效果如下：<br><img src="/images/blog/o_200901082218mem-image2.png" alt="image.png-38kB"><br><img src="/images/blog/o_200901082226mem-image3.png" alt="image.png-94.6kB"></p><h3 id="图形界面的查看"><a href="#图形界面的查看" class="headerlink" title="图形界面的查看"></a>图形界面的查看</h3><p>首先在windows上面运行好xmanager-Passive，这个走的x11转发的（也可以用另外一个工具MobaXterm）<br><img src="/images/blog/o_200901082233mem-image4.png" alt="image.png-4.4kB"><br>运行好了后，直接在xshell命令行运行</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ~]<span class="hljs-comment"># massif-visualizer massif.out.21522 </span><br>massif-visualizer(22494)/kdeui (kdelibs): Attempt to use QAction <span class="hljs-string">&quot;toggleDataTree&quot;</span> with KXMLGUIFactory! <br>massif-visualizer(22494)/kdeui (kdelibs): Attempt to use QAction <span class="hljs-string">&quot;toggleAllocators&quot;</span> with KXMLGUIFactory! <br>description: <span class="hljs-string">&quot;(none)&quot;</span> <br><span class="hljs-built_in">command</span>: <span class="hljs-string">&quot;/usr/bin/ceph-osd -f --cluster ceph --id 4&quot;</span> <br>time unit: <span class="hljs-string">&quot;i&quot;</span> <br>snapshots: 56 <br>peak: snapshot <span class="hljs-comment"># 52 after &quot;2.3138e+09i&quot; </span><br>peak cost: <span class="hljs-string">&quot;16.2 MiB&quot;</span>  heap <span class="hljs-string">&quot;749.0 KiB&quot;</span>  heap extra <span class="hljs-string">&quot;0 B&quot;</span>  stacks <br></code></pre></td></tr></table></figure><p>然后在windows上面就会弹出下面的<br><img src="/images/blog/o_200901082239osdmem.png" alt="osdmem.png-282kB"><br>就可以交互式的查看快照点的内存占用了，然后根据这个就可以进行内存分析了，剩下的工作就留给研发去做了</p><h2 id="相关链接"><a href="#相关链接" class="headerlink" title="相关链接"></a>相关链接</h2><p><a href="https://codeday.me/bug/20170415/1699.html">linux – 如何测量应用程序或进程的实际内存使用情况？</a></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>只有分析落地到数据层面，这样的分析才是比较精准的</p><h2 id="变更记录"><a href="#变更记录" class="headerlink" title="变更记录"></a>变更记录</h2><table><thead><tr><th align="center">Why</th><th align="center">Who</th><th align="center">When</th></tr></thead><tbody><tr><td align="center">创建</td><td align="center">武汉-运维-磨渣</td><td align="center">2017-08-10</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Ceph recover的速度控制</title>
    <link href="/2017/08/10/Ceph%20recover%E7%9A%84%E9%80%9F%E5%BA%A6%E6%8E%A7%E5%88%B6/"/>
    <url>/2017/08/10/Ceph%20recover%E7%9A%84%E9%80%9F%E5%BA%A6%E6%8E%A7%E5%88%B6/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>磁盘损坏对于一个大集群来说，可以说是必然发生的事情，即使再小的概率，磁盘量上去，总会坏那么几块盘，这个时候就会触发内部的修复过程，修复就是让不满足副本要求的PG，恢复到满足的情况</p><p>一般是踢掉坏盘和增加新盘会触发这个修复过程，或者对磁盘的权重做了修改，也会触发这个迁移的过程，本篇是用剔除OSD的方式来对这个修复的控制做一个探索</p><p>大部分场景下要求的是不能影响前端的业务，而加速迁移，忽略迁移影响不在本篇的讨论范围内，本篇将用数据来说明迁移的控制</p><p>本次测试在无读写情况下进程的</p><h2 id="几个需要用到脚本和命令"><a href="#几个需要用到脚本和命令" class="headerlink" title="几个需要用到脚本和命令"></a>几个需要用到脚本和命令</h2><h3 id="磁盘本身的大概速度"><a href="#磁盘本身的大概速度" class="headerlink" title="磁盘本身的大概速度"></a>磁盘本身的大概速度</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]# ceph tell osd.0 bench<br>&#123;<br>    &quot;bytes_written&quot;: 1073741824,<br>    &quot;blocksize&quot;: 4194304,<br>    &quot;bytes_per_sec&quot;: 102781897<br>&#125;<br></code></pre></td></tr></table></figure><p>得到的结果为102MB&#x2F;s</p><h3 id="获取osd上pg迁移的对象的脚本"><a href="#获取osd上pg迁移的对象的脚本" class="headerlink" title="获取osd上pg迁移的对象的脚本"></a>获取osd上pg迁移的对象的脚本</h3><p>OSD的日志需要开启到10，这里采取动态开启的方式</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">ceph daemon osd.0 config set debug_osd 10<br></code></pre></td></tr></table></figure><p>日志解析的脚本</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">cat  /var/log/ceph/ceph-osd.0.log | awk  &#x27;$7==&quot;finish_recovery_op&quot;&amp;&amp;$8==&quot;pg[0.15(&quot; &#123;sub(/.*/,substr($2,1,8),$2); print $0&#125;&#x27;|awk &#x27;&#123;a[$1,&quot; &quot;,$2]++&#125;END&#123;for (j in a) print j,a[j]|&quot;sort -k 1&quot;&#125;&#x27;<br></code></pre></td></tr></table></figure><p>获取osd.0上的pg0.15的迁移速度<br>运行后的效果如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs raw">2017-08-08 17:14:33 1<br>2017-08-08 17:14:34 2<br>2017-08-08 17:14:35 2<br>2017-08-08 17:14:36 1<br>2017-08-08 17:14:37 2<br>2017-08-08 17:14:38 2<br>2017-08-08 17:14:39 1<br>2017-08-08 17:14:40 2<br>2017-08-08 17:14:41 1<br>2017-08-08 17:14:42 2<br>2017-08-08 17:14:43 2<br></code></pre></td></tr></table></figure><h3 id="设置不迁移和恢复迁移"><a href="#设置不迁移和恢复迁移" class="headerlink" title="设置不迁移和恢复迁移"></a>设置不迁移和恢复迁移</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs raw">ceph osd set nobackfill;ceph osd set norecover<br>ceph osd unset nobackfill;ceph osd unset norecover<br></code></pre></td></tr></table></figure><p>获取当前的正在迁移的PG</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]# ceph pg dump|grep recovering<br>dumped all<br>3.e         513                  0      978         0       0 2151677952 513      513    active+recovering+degraded 2017-08-07 16:40:44.840780 118&#x27;513  332:7367 [2,3]          2  [2,3]              2        0&#x27;0 2017-07-28 14:28:53.351664             0&#x27;0 2017-07-28 14:28:53.351664 <br>3.2c        522                  0      996         0       0 2189426688 522      522    active+recovering+degraded 2017-08-07 16:40:44.882450 118&#x27;522  332:1177 [3,2]          3  [3,2]              3    118&#x27;522 2017-07-29 16:21:56.398682             0&#x27;0 2017-07-28 14:28:53.351664 <br></code></pre></td></tr></table></figure><p>过滤下输出结果</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]# ceph pg dump|grep recovering|awk &#x27;&#123;print $1,$2,$4,$10,$15,$16,$17,$18&#125;&#x27;<br>dumped all in format plain<br>0.1d 636 1272 active+recovering+degraded [5,3] 5 [5,3] 5<br>0.14 618 1236 active+recovering+degraded [1,0] 1 [1,0] 1<br>0.15 682 1364 active+recovering+degraded [0,5] 0 [0,5] 0<br>0.35 661 1322 active+recovering+degraded [2,1] 2 [2,1] 2<br></code></pre></td></tr></table></figure><p>动态监控PG的迁移</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">watch -n 1 -d &quot;ceph pg dump|grep recovering|awk &#x27;&#123;print \$1,\$2,\$4,\$10,\$15,\$16,\$17,\$18&#125;&#x27;&quot;<br></code></pre></td></tr></table></figure><p>我们要看PG 0.15的</p><h3 id="防止缓存影响"><a href="#防止缓存影响" class="headerlink" title="防止缓存影响"></a>防止缓存影响</h3><p>同步数据然后清空缓存</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs raw">sync<br>echo 3 &gt; /proc/sys/vm/drop_caches<br></code></pre></td></tr></table></figure><p>重启OSD进程</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">systemctl restart ceph-osd.target<br></code></pre></td></tr></table></figure><h3 id="磁盘的读写速度"><a href="#磁盘的读写速度" class="headerlink" title="磁盘的读写速度"></a>磁盘的读写速度</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">dstat -td -D /dev/sdb -o disk.csv<br></code></pre></td></tr></table></figure><p>sdb为需要监控的盘</p><h2 id="测试的步骤与流程"><a href="#测试的步骤与流程" class="headerlink" title="测试的步骤与流程"></a>测试的步骤与流程</h2><p>整个测试需要保证每一次获取数据的过程都近似，这样才能最大程度减少环境对数据的影响</p><p>开始需要写入一些测试数据，这个可以用</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">rados -p rbd bench 3600 --no-cleanup<br></code></pre></td></tr></table></figure><p>这个让每个PG上面大概有600-700个object，写入这个数据后就不再写入数据了</p><p>每一轮测试步骤如下：</p><ol><li>恢复集群状态为active+clean</li><li>设置nobackfill，norecover</li><li>清空缓存</li><li>设置需要调整的参数</li><li>重启osd进程</li><li>停止osd，out osd</li><li>观察需要迁移的数据（尽量每次监测同一个PG）</li><li>清空日志，设置OSD debug 10</li><li>开启监控磁盘脚本</li><li>解除设置nobackfill，norecover</li><li>动态监控迁移状态，等待指定PG迁移完毕</li><li>停止磁盘监控脚本</li><li>获取PG迁移的情况，获取磁盘的读写情况</li><li>数据处理</li></ol><p>每一轮测试需要按上面的步骤进行处理</p><h2 id="测试分析"><a href="#测试分析" class="headerlink" title="测试分析"></a>测试分析</h2><p>我的测试选择的是osd.4,按上面的步骤进行处理后，到了观察PG的步骤，此时因为做了不迁移的标记，只会状态改变，不会真正的迁移 我们来观察下需要迁移的pg<br>默认情况下的</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]# ceph pg dump|grep recovering|awk &#x27;&#123;print $1,$2,$10,$15,$16,$17,$18&#125;&#x27;<br>dumped all in format plain<br>0.15 682 active+recovering+degraded [0,5] 0 [0,5] 0<br>0.24 674 active+recovering+degraded [5,2] 5 [5,2] 5<br>0.35 661 active+recovering+degraded [2,1] 2 [2,1] 2<br>0.37 654 active+recovering+degraded [1,0] 1 [1,0] 1<br></code></pre></td></tr></table></figure><p>可以看到这个环境下，每个OSD上面基本上是一个PG的写入，和一个PG的读取，实际上是读写同时在进行的</p><p>默认的</p><blockquote><p>osd_max_backfills &#x3D; 1<br><br>osd_recovery_max_active &#x3D; 3</p></blockquote><p>两个参数是一个是每个OSD上面启动的恢复的PG数目，下面一个是控制同时恢复的请求数目</p><p>默认的参数的情况<br><img src="/images/blog/o_200901081609pg.png" alt="pg.png-37.1kB"><br>上图为迁移的对象数目<br><img src="/images/blog/o_200901081617diskspeed.png" alt="diskspeed.png-63.7kB"><br>上图为OSD的磁盘读取写入的情况</p><p>可以看到迁移的对象每秒在6-15之间<br>磁盘上的读取为20-60MB&#x2F;s，写入为80MB左右</p><p>这个只是默认的情况下的,占用了磁盘带宽的80%左右，在真正有写入的时候，因为有优先级的控制，占的带宽可能没那么多，本篇目的是在静态的时候就把磁盘占用给控制下来，那么即使有读写，恢复的磁盘占用只会更低</p><h3 id="调整一个参数"><a href="#调整一个参数" class="headerlink" title="调整一个参数"></a>调整一个参数</h3><blockquote><p>osd_recovery_max_active &#x3D; 3</p></blockquote><p>调整如下</p><blockquote><p>osd_recovery_max_active &#x3D; 1</p></blockquote><p><img src="/images/blog/o_200901081624pgactive1.png" alt="pgactive1.png-30.9kB"></p><p><img src="/images/blog/o_200901081632diskactive1.png" alt="diskactive1.png-66.4kB"></p><p>从磁盘占用上和迁移上面可以看到，磁盘的负载确实降低了一些，峰值从16降低到了11左右</p><h2 id="sleep-参数的控制"><a href="#sleep-参数的控制" class="headerlink" title="sleep 参数的控制"></a>sleep 参数的控制</h2><p>下面是一个关键的参数了</p><blockquote><p>osd_recovery_sleep &#x3D; 0</p></blockquote><p>这个在jewel最新版本下还是0，在luminous版本已经设置成ssd是0，sata变成0.1，相当于增加了一个延时的过程，本篇主要就是对这个参数进行研究，看下能控制最低到一个什么程度</p><p>下面的测试的数据就统计到一个图当中去了，这样也便于对比的</p><p><img src="/images/blog/o_200901081640sleeppg.png" alt="sleeppg.png-76.6kB"></p><p><img src="/images/blog/o_200901081648sleepdiskread.png" alt="sleepdiskread.png-86.7kB"></p><p><img src="/images/blog/o_200901081654sleepdiskwrite.png" alt="sleepdiskwrite.png-130.8kB"></p><p>上面测试了几组参数:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">sleep=0;sleep=0.1;sleep=0.2;sleep=0.5<br></code></pre></td></tr></table></figure><p>从上面的图中可以看到：<br>迁移速度从12降低到1-2个<br>磁盘读取占用从40Mb&#x2F;s降到 8Mb&#x2F;s左右<br>磁盘写入的占用从60MB&#x2F;s-80MB&#x2F;s降低到8MB&#x2F;s-40MB&#x2F;s</p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>通过sleep的控制可以大大的降低迁移磁盘的占用，对于本身磁盘性能不太好的硬件环境下，可以用这个参数进行一下控制，能够缓解磁盘压力过大引起的osd崩溃的情况</p><h2 id="变更记录"><a href="#变更记录" class="headerlink" title="变更记录"></a>变更记录</h2><table><thead><tr><th align="center">Why</th><th align="center">Who</th><th align="center">When</th></tr></thead><tbody><tr><td align="center">创建</td><td align="center">武汉-运维-磨渣</td><td align="center">2017-08-10</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Ceph S3 基于NGINX的集群复制方案</title>
    <link href="/2017/08/10/Ceph%20S3%20%E5%9F%BA%E4%BA%8ENGINX%E7%9A%84%E9%9B%86%E7%BE%A4%E5%A4%8D%E5%88%B6%E6%96%B9%E6%A1%88/"/>
    <url>/2017/08/10/Ceph%20S3%20%E5%9F%BA%E4%BA%8ENGINX%E7%9A%84%E9%9B%86%E7%BE%A4%E5%A4%8D%E5%88%B6%E6%96%B9%E6%A1%88/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>ceph的s3数据的同步可以通过radosgw-agent进行同步，同region可以同步data和metadata，不同region只能同步metadata，这个地方可以参考下秦牧羊梳理的 <a href="https://my.oschina.net/diluga/blog/391928">ceph radosgw 多集群同步部署流程</a>，本篇讲述的方案与radosgw-agent的复制方案不同在于,这个属于前端复制，后端相当于透明的两个相同集群，在入口层面就将数据进行了复制分流</p><p>在某些场景下，需求可能比较简单：</p><ul><li>需要数据能同时存储在两个集群当中</li><li>数据写一次，读多次</li><li>两个集群都能写</li></ul><p>一方面两个集群可以增加数据的可靠性，另一方面可以提高读带宽，两个集群同时可以提供读的服务</p><p>radosgw-agent是从底层做的同步，正好看到秦牧羊有提到nginx新加入了ngx_http_mirror_module 这个模块，那么本篇就尝试用这个模块来做几个简单的配置来实现上面的需求，这里纯架构的尝试，真正上生产还需要做大量的验证和修改的测试的</p><h2 id="结构设想"><a href="#结构设想" class="headerlink" title="结构设想"></a>结构设想</h2><p><img src="/images/blog/o_200901081105nginxs3.png" alt="nginxs3.png-30.8kB"></p><p>当数据传到nginx的server的时候，nginx本地进行负载均衡到两个本地端口上面，本地的两个端口对应到两个集群上面,一个主写集群1，一个主写集群2，这个是最简结构，集群的civetweb可以是很多机器，nginx这个也可以是多台的机器，在一台上面之所以做个均衡是可以让两个集群是对等关系，而不是一个只用nginx写，另一个只mirror写</p><h2 id="环境准备"><a href="#环境准备" class="headerlink" title="环境准备"></a>环境准备</h2><p>准备两个完全独立的集群，分别配置一个s3的网关，我的环境为：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">192.168.19.101:8080<br>192.168.19.102:8080<br></code></pre></td></tr></table></figure><p>在每个机器上都创建一个管理员的账号，这个用于后面的通过restapi来进行管理的,其他的后面的操作都通过http来做能保证两个集群的数据是一致的</p><blockquote><p>nginx的机器在192.168.19.104</p></blockquote><p>在两个集群当中都创建相同的管理用户</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">radosgw-admin user create --uid=admin --display-name=admin --access_key=admin --secret=123456<br></code></pre></td></tr></table></figure><p>这里为了测试方便使用了简单密码</p><p>此时admin还仅仅是普通的权限，需要通过–cap添加user的capabilities，例如：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">radosgw-admin caps add --uid=admin --caps=<span class="hljs-string">&quot;users=read, write&quot;</span><br>radosgw-admin caps add --uid=admin --caps=<span class="hljs-string">&quot;usage=read, write&quot;</span> <br></code></pre></td></tr></table></figure><p>下面就用到了nginx的最新的模块了<br>Nginx 1.13.4 发布，新增 ngx_http_mirror_module 模块</p><p>软件下载：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">wget https://nginx.org/packages/mainline/centos/7/x86_64/RPMS/nginx-1.13.4-1.el7.ngx.x86_64.rpm<br></code></pre></td></tr></table></figure><p>下载rpm包然后安装<br>安装：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">rpm -ivh nginx-1.13.4-1.el7.ngx.x86_64.rpm<br></code></pre></td></tr></table></figure><p>修改nginx配置文件：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><code class="hljs bash">upstream s3 &#123;<br>      server 127.0.0.1:81;<br>      server 127.0.0.1:82;<br>&#125;<br><br>server &#123;<br>    listen       81;<br>    server_name  localhost;<br><br>    location / &#123;<br>    mirror /mirror;<br>    proxy_pass http://192.168.19.101:8080;<br>    &#125;<br><br>    location /mirror &#123;<br>    internal;<br>    proxy_pass http://192.168.19.102:8080<span class="hljs-variable">$request_uri</span>;<br>    &#125;<br>&#125;<br><br>server &#123;<br>    listen       82;<br>    server_name  localhost;<br>    <br>    location / &#123;<br>    mirror /mirror;<br>    proxy_pass http://192.168.19.102:8080;<br>    &#125;<br><br>    location /mirror &#123;<br>    internal;<br>    proxy_pass http://192.168.19.101:8080<span class="hljs-variable">$request_uri</span>;<br>    &#125;<br>&#125;<br>server&#123;<br>    listen 80;<br>    location / &#123;<br>        proxy_pass         http://s3;<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure><p>负载均衡的设置有很多种，这里用最简单的轮训的模式，想配置其他负载均衡模式可以参考我的《关于nginx-upstream的几种配置方式》</p><p>重启进程并检查服务</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@node04 ~]<span class="hljs-comment"># systemctl restart nginx</span><br>[root@node04 ~]<span class="hljs-comment"># netstat -tunlp|grep nginx</span><br>tcp        0      0 0.0.0.0:80              0.0.0.0:*               LISTEN      1582973/nginx: mast <br>tcp        0      0 0.0.0.0:81              0.0.0.0:*               LISTEN      1582973/nginx: mast <br>tcp        0      0 0.0.0.0:82              0.0.0.0:*               LISTEN      1582973/nginx: mast <br></code></pre></td></tr></table></figure><p>整个环境就配置完成了，下面我们就来验证下这个配置的效果是什么样的，下面会提供几个s3用户的相关的脚本</p><h2 id="s3用户相关脚本"><a href="#s3用户相关脚本" class="headerlink" title="s3用户相关脚本"></a>s3用户相关脚本</h2><h3 id="创建用户的脚本"><a href="#创建用户的脚本" class="headerlink" title="创建用户的脚本"></a>创建用户的脚本</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-meta">#!/bin/bash</span><br><span class="hljs-comment">###</span><br><span class="hljs-comment">#S3 USER ADMIN </span><br><span class="hljs-comment">###</span><br><br><span class="hljs-comment">###==============WRITE BEGIN=============###</span><br>ACCESS_KEY=admin <span class="hljs-comment">## ADMIN_USER_TOKEN</span><br>SECRET_KEY=123456 <span class="hljs-comment">## ADMIN_USER_SECRET</span><br>HOST=192.168.19.104:80<br>USER_ACCESS_KEY=<span class="hljs-string">&quot;&amp;access-key=user1&quot;</span><br>USER_SECRET_KEY=<span class="hljs-string">&quot;&amp;secret-key=123456&quot;</span><br><span class="hljs-comment">###==============WRITE  FINAL=======FINAL=====###</span><br><br>query2=admin/user<br>userid=<span class="hljs-variable">$1</span><br>name=<span class="hljs-variable">$2</span><br>uid=<span class="hljs-string">&quot;&amp;uid=&quot;</span><br><span class="hljs-built_in">date</span>=`TZ=GMT LANG=en_US <span class="hljs-built_in">date</span> <span class="hljs-string">&quot;+%a, %d %b %Y %H:%M:%S GMT&quot;</span>`<br>header=<span class="hljs-string">&quot;PUT\n\n\n<span class="hljs-variable">$&#123;date&#125;</span>\n/<span class="hljs-variable">$&#123;query2&#125;</span>&quot;</span><br>sig=$(<span class="hljs-built_in">echo</span> -en <span class="hljs-variable">$&#123;header&#125;</span> | openssl sha1 -hmac <span class="hljs-variable">$&#123;SECRET_KEY&#125;</span> -binary | <span class="hljs-built_in">base64</span>)<br>curl -v -H <span class="hljs-string">&quot;Date: <span class="hljs-variable">$&#123;date&#125;</span>&quot;</span> -H <span class="hljs-string">&quot;Authorization: AWS <span class="hljs-variable">$&#123;ACCESS_KEY&#125;</span>:<span class="hljs-variable">$&#123;sig&#125;</span>&quot;</span> -L -X PUT <span class="hljs-string">&quot;http://<span class="hljs-variable">$&#123;HOST&#125;</span>/<span class="hljs-variable">$&#123;query2&#125;</span>?format=json<span class="hljs-variable">$&#123;uid&#125;</span><span class="hljs-variable">$&#123;userid&#125;</span>&amp;display-name=<span class="hljs-variable">$&#123;name&#125;</span><span class="hljs-variable">$&#123;USER_ACCESS_KEY&#125;</span><span class="hljs-variable">$&#123;USER_SECRET_KEY&#125;</span>&quot;</span> -H <span class="hljs-string">&quot;Host: <span class="hljs-variable">$&#123;HOST&#125;</span>&quot;</span><br><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;&quot;</span><br></code></pre></td></tr></table></figure><p>运行脚本：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@node01 ~]<span class="hljs-comment"># sh  addusernew.sh user1 USER1</span><br>* About to connect() to 192.168.19.104 port 80 (<span class="hljs-comment">#0)</span><br>*   Trying 192.168.19.104...<br>* Connected to 192.168.19.104 (192.168.19.104) port 80 (<span class="hljs-comment">#0)</span><br>&gt; PUT /admin/user?format=json&amp;uid=user1&amp;display-name=USER1&amp;access-key=user1&amp;secret-key=123456 HTTP/1.1<br>&gt; User-Agent: curl/7.29.0<br>&gt; Accept: */*<br>&gt; Date: Wed, 09 Aug 2017 07:51:58 GMT<br>&gt; Authorization: AWS admin:wuqQUUXhhar5nQS5D5B14Dpx+Rw=<br>&gt; Host: 192.168.19.104:80<br>&gt; <br>&lt; HTTP/1.1 200 OK<br>&lt; Server: nginx/1.13.4<br>&lt; Date: Wed, 09 Aug 2017 07:51:58 GMT<br>&lt; Content-Type: application/json<br>&lt; Content-Length: 195<br>&lt; Connection: keep-alive<br>&lt; <br>* Connection <span class="hljs-comment">#0 to host 192.168.19.104 left intact</span><br>&#123;<span class="hljs-string">&quot;user_id&quot;</span>:<span class="hljs-string">&quot;user1&quot;</span>,<span class="hljs-string">&quot;display_name&quot;</span>:<span class="hljs-string">&quot;USER1&quot;</span>,<span class="hljs-string">&quot;email&quot;</span>:<span class="hljs-string">&quot;&quot;</span>,<span class="hljs-string">&quot;suspended&quot;</span>:0,<span class="hljs-string">&quot;max_buckets&quot;</span>:1000,<span class="hljs-string">&quot;subusers&quot;</span>:[],<span class="hljs-string">&quot;keys&quot;</span>:[&#123;<span class="hljs-string">&quot;user&quot;</span>:<span class="hljs-string">&quot;user1&quot;</span>,<span class="hljs-string">&quot;access_key&quot;</span>:<span class="hljs-string">&quot;user1&quot;</span>,<span class="hljs-string">&quot;secret_key&quot;</span>:<span class="hljs-string">&quot;123456&quot;</span>&#125;],<span class="hljs-string">&quot;swift_keys&quot;</span>:[],<span class="hljs-string">&quot;caps&quot;</span>:[]&#125;<br></code></pre></td></tr></table></figure><p>在两个集群中检查：<br><img src="/images/blog/o_200901081112usercreate.png" alt="usercreate.png-36.5kB"></p><p>可以看到两个集群当中都产生了相同的用户信息</p><h3 id="修改用户"><a href="#修改用户" class="headerlink" title="修改用户"></a>修改用户</h3><p>直接把上面的创建脚本里面的PUT改成POST就是修改用户的脚本</p><h3 id="删除用户脚本"><a href="#删除用户脚本" class="headerlink" title="删除用户脚本"></a>删除用户脚本</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-meta">#!/bin/bash</span><br><span class="hljs-comment">###</span><br><span class="hljs-comment">#S3 USER ADMIN</span><br><span class="hljs-comment">###</span><br><br><span class="hljs-comment">###==============WRITE BEGIN=============###</span><br>ACCESS_KEY=admin <span class="hljs-comment">## ADMIN_USER_TOKEN</span><br>SECRET_KEY=123456 <span class="hljs-comment">## ADMIN_USER_SECRET</span><br>HOST=192.168.19.104:80<br><span class="hljs-comment">###==============WRITE  FINAL=======FINAL=====###</span><br><br>query2=admin/user<br>userid=<span class="hljs-variable">$1</span><br>uid=<span class="hljs-string">&quot;&amp;uid=&quot;</span><br><span class="hljs-built_in">date</span>=`TZ=GMT LANG=en_US <span class="hljs-built_in">date</span> <span class="hljs-string">&quot;+%a, %d %b %Y %H:%M:%S GMT&quot;</span>`<br>header=<span class="hljs-string">&quot;DELETE\n\n\n<span class="hljs-variable">$&#123;date&#125;</span>\n/<span class="hljs-variable">$&#123;query2&#125;</span>&quot;</span><br>sig=$(<span class="hljs-built_in">echo</span> -en <span class="hljs-variable">$&#123;header&#125;</span> | openssl sha1 -hmac <span class="hljs-variable">$&#123;SECRET_KEY&#125;</span> -binary | <span class="hljs-built_in">base64</span>)<br>curl -v -H <span class="hljs-string">&quot;Date: <span class="hljs-variable">$&#123;date&#125;</span>&quot;</span> -H <span class="hljs-string">&quot;Authorization: AWS <span class="hljs-variable">$&#123;ACCESS_KEY&#125;</span>:<span class="hljs-variable">$&#123;sig&#125;</span>&quot;</span> -L -X DELETE <span class="hljs-string">&quot;http://<span class="hljs-variable">$&#123;HOST&#125;</span>/<span class="hljs-variable">$&#123;query2&#125;</span>?format=json<span class="hljs-variable">$&#123;uid&#125;</span><span class="hljs-variable">$&#123;userid&#125;</span>&quot;</span> -H <span class="hljs-string">&quot;Host: <span class="hljs-variable">$&#123;HOST&#125;</span>&quot;</span><br><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;&quot;</span><br></code></pre></td></tr></table></figure><p>执行删除用户：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@node01 ~]<span class="hljs-comment"># sh deluser.sh user1</span><br></code></pre></td></tr></table></figure><p><img src="/images/blog/o_200901081124deluser.png" alt="deluser.png-6.3kB"></p><p>可以看到两边都删除了</p><h3 id="获取用户的信息脚本"><a href="#获取用户的信息脚本" class="headerlink" title="获取用户的信息脚本"></a>获取用户的信息脚本</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-meta">#! /bin/sh</span><br><span class="hljs-comment">###</span><br><span class="hljs-comment">#S3 USER ADMIN </span><br><span class="hljs-comment">###</span><br><br><span class="hljs-comment">###==============WRITE BEGIN=============###</span><br>ACCESS_KEY=admin <span class="hljs-comment">## ADMIN_USER_TOKEN</span><br>SECRET_KEY=123456 <span class="hljs-comment">## ADMIN_USER_SECRET</span><br>HOST=192.168.19.101:8080<br><span class="hljs-comment">###==============WRITE  FINAL=======FINAL=====###</span><br><br>query2=admin/user<br>userid=<span class="hljs-variable">$1</span><br>uid=<span class="hljs-string">&quot;&amp;uid=&quot;</span><br><span class="hljs-built_in">date</span>=`TZ=GMT LANG=en_US <span class="hljs-built_in">date</span> <span class="hljs-string">&quot;+%a, %d %b %Y %H:%M:%S GMT&quot;</span>`<br>header=<span class="hljs-string">&quot;GET\n\n\n<span class="hljs-variable">$&#123;date&#125;</span>\n/<span class="hljs-variable">$&#123;query2&#125;</span>&quot;</span><br>sig=$(<span class="hljs-built_in">echo</span> -en <span class="hljs-variable">$&#123;header&#125;</span> | openssl sha1 -hmac <span class="hljs-variable">$&#123;SECRET_KEY&#125;</span> -binary | <span class="hljs-built_in">base64</span>)<br>curl -v -H <span class="hljs-string">&quot;Date: <span class="hljs-variable">$&#123;date&#125;</span>&quot;</span> -H <span class="hljs-string">&quot;Authorization: AWS <span class="hljs-variable">$&#123;ACCESS_KEY&#125;</span>:<span class="hljs-variable">$&#123;sig&#125;</span>&quot;</span> -L -X GET <span class="hljs-string">&quot;http://<span class="hljs-variable">$&#123;HOST&#125;</span>/<span class="hljs-variable">$&#123;query2&#125;</span>?format=json<span class="hljs-variable">$&#123;uid&#125;</span><span class="hljs-variable">$&#123;userid&#125;</span>&amp;display-name=<span class="hljs-variable">$&#123;name&#125;</span>&quot;</span>  -H <span class="hljs-string">&quot;Host: <span class="hljs-variable">$&#123;HOST&#125;</span>&quot;</span><br></code></pre></td></tr></table></figure><h3 id="测试上传一个文件"><a href="#测试上传一个文件" class="headerlink" title="测试上传一个文件"></a>测试上传一个文件</h3><p>通过192.168.19.104:80端口上传一个文件，然后通过nginx的端口，以及两个集群的端口进行查看</p><p><img src="/images/blog/o_200901081240same.png" alt="same.png-24.6kB"></p><p>可以看到在上传一次的情况下，两个集群里面同时拥有了这个文件</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>真正将方案运用到生产还需要做大量的验证测试，中间的失效处理，以及是否可以将写镜像，读取的时候不镜像，这些都需要进一步做相关的验证工作</p><p>本篇中的S3用户的管理接口操作参考了网上的其他资料</p><h2 id="变更记录"><a href="#变更记录" class="headerlink" title="变更记录"></a>变更记录</h2><table><thead><tr><th align="center">Why</th><th align="center">Who</th><th align="center">When</th></tr></thead><tbody><tr><td align="center">创建</td><td align="center">武汉-运维-磨渣</td><td align="center">2017-08-10</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>RBD快速删除的方法分析与改进</title>
    <link href="/2017/07/27/RBD%E5%BF%AB%E9%80%9F%E5%88%A0%E9%99%A4%E7%9A%84%E6%96%B9%E6%B3%95%E5%88%86%E6%9E%90%E4%B8%8E%E6%94%B9%E8%BF%9B/"/>
    <url>/2017/07/27/RBD%E5%BF%AB%E9%80%9F%E5%88%A0%E9%99%A4%E7%9A%84%E6%96%B9%E6%B3%95%E5%88%86%E6%9E%90%E4%B8%8E%E6%94%B9%E8%BF%9B/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>这个问题在很久以前就有一篇文章进行过讨论 <a href="http://cephnotes.ksperis.com/blog/2014/07/04/remove-big-rbd-image">remove-big-rbd</a>,这个文章写的比较清楚了，并且对不同的方法做了分析，这里先把结论说下</p><!--break--><table><thead><tr><th align="center">rbd类型</th><th align="center">rbd rm 方法</th><th align="center">rados -p rm方法</th></tr></thead><tbody><tr><td align="center">未填充很多</td><td align="center">慢</td><td align="center">快</td></tr><tr><td align="center">已填充很多</td><td align="center">快</td><td align="center">慢</td></tr></tbody></table><p>在rbd进行删除的时候，即使内部没有对象数据，也一样需要一个个对象去发请求，即使对象不存在，这个可以开日志看到</p><h2 id="实验过程"><a href="#实验过程" class="headerlink" title="实验过程"></a>实验过程</h2><h3 id="开启日志的方法"><a href="#开启日志的方法" class="headerlink" title="开启日志的方法"></a>开启日志的方法</h3><p>在&#x2F;etc&#x2F;ceph&#x2F;ceph.conf中添加</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">[client]<br>debug_ms=1<br>log_file=/var/log/ceph/rados.log<br></code></pre></td></tr></table></figure><p>这个默认也会在执行命令的时候打印到前台，所以处理下比较好，最简单的办法就是做alias<br>添加下面内容到 &#x2F;etc&#x2F;bashrc</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">alias</span> ceph=<span class="hljs-string">&#x27;ceph  --debug-ms=0&#x27;</span><br><span class="hljs-built_in">alias</span> rados=<span class="hljs-string">&#x27;rados  --debug-ms=0&#x27;</span><br></code></pre></td></tr></table></figure><p>然后命令行执行</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">source</span> /etc/bashrc<br></code></pre></td></tr></table></figure><p>在做操作的时候就只会记录日志，前台不会打印调试信息了,但是这个会影响到ceph daemon的命令，这个可以用这种方式在线屏蔽即可</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">ceph --debug_ms=0  -s<br></code></pre></td></tr></table></figure><p>然后执行操作后，去分析每秒钟的操作数目即可,类似下面的这个，也可以用日志系统进行分析，这里不赘述</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">cat</span>  /var/log/ceph/rados.log|grep delete|grep -v <span class="hljs-string">&quot;&gt;&quot;</span>|grep 13:29:46|<span class="hljs-built_in">wc</span> -l<br></code></pre></td></tr></table></figure><p>原始的快速删除方法</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">rados -p rbd <span class="hljs-built_in">ls</span> | grep <span class="hljs-string">&#x27;^rbd_data.25ae86b8b4567&#x27;</span> | xargs -n 200  rados -p rbd <span class="hljs-built_in">rm</span><br></code></pre></td></tr></table></figure><h2 id="开启多进程删除的方法"><a href="#开启多进程删除的方法" class="headerlink" title="开启多进程删除的方法"></a>开启多进程删除的方法</h2><p>这个比上面那种方法好的是：</p><ul><li>可以显示当前删除的进度</li><li>可以指定删除的进程并发数</li><li>可以显示当时正在删除的对象</li><li>可以增加一个中断时间降低负载</li></ul><p>首先获取一个需要快速删除的rbd的列表<br>获取prifix</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 put]<span class="hljs-comment"># rbd info testrbd|grep prefix</span><br>block_name_prefix: rbd_data.32c0f6b8b4567<br></code></pre></td></tr></table></figure><p>获取列表</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 put]<span class="hljs-comment"># rados -p rbd ls |grep rbd_data.32c0f6b8b4567 &gt; delobject</span><br></code></pre></td></tr></table></figure><p>这里可以看下内容有没有问题，检查确认下</p><p>删除的fastremove.sh脚本如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-meta">#!/bin/bash</span><br><br><span class="hljs-comment">#####config</span><br>process=5<br>objectlistfile=<span class="hljs-string">&quot;./delobject&quot;</span><br>deletepool=rbd<br><span class="hljs-comment">#####</span><br><br>  <span class="hljs-function"><span class="hljs-title">delete_fun</span></span>()<br>  &#123;<br>      <span class="hljs-built_in">date</span> <span class="hljs-string">&quot;+%Y-%m-%d %H:%M:%S&quot;</span><br>      rados -p <span class="hljs-variable">$deletepool</span> <span class="hljs-built_in">rm</span> <span class="hljs-variable">$1</span><br>  <span class="hljs-comment">#sleep 1</span><br>  &#125;<br><br> <span class="hljs-function"><span class="hljs-title">concurrent</span></span>()<br> &#123;<br>     start=<span class="hljs-variable">$1</span> &amp;&amp; end=<span class="hljs-variable">$2</span> &amp;&amp; cur_num=<span class="hljs-variable">$3</span><br>     <span class="hljs-built_in">mkfifo</span>   ./fifo.$$ &amp;&amp;  <span class="hljs-built_in">exec</span> 4&lt;&gt; ./fifo.$$ &amp;&amp; <span class="hljs-built_in">rm</span> -f ./fifo.$$<br>     <span class="hljs-keyword">for</span> ((i=<span class="hljs-variable">$start</span>; i&lt;<span class="hljs-variable">$cur_num</span>+<span class="hljs-variable">$start</span>; i++)); <span class="hljs-keyword">do</span><br>         <span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;init  start delete process <span class="hljs-variable">$i</span>&quot;</span> &gt;&amp;4<br>     <span class="hljs-keyword">done</span><br><br>     <span class="hljs-keyword">for</span>((i=<span class="hljs-variable">$start</span>; i&lt;=<span class="hljs-variable">$end</span>; i++)); <span class="hljs-keyword">do</span><br>         <span class="hljs-built_in">read</span> -u 4<br>         &#123;<br>             <span class="hljs-built_in">echo</span> -e <span class="hljs-string">&quot;-- current delete: [:delete <span class="hljs-variable">$i</span>/<span class="hljs-variable">$objectnum</span>  <span class="hljs-variable">$REPLY</span>]&quot;</span><br>             delob=`sed -n <span class="hljs-string">&quot;<span class="hljs-variable">$&#123;i&#125;</span>p&quot;</span> <span class="hljs-variable">$objectlistfile</span>`<br>             delete_fun <span class="hljs-variable">$delob</span><br>             <span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;delete <span class="hljs-variable">$delob</span> done&quot;</span>  1&gt;&amp;4 <span class="hljs-comment"># write to $ff_file</span><br>         &#125; &amp;<br>     <span class="hljs-keyword">done</span><br>     <span class="hljs-built_in">wait</span><br> &#125;<br><br>objectnum=`<span class="hljs-built_in">cat</span> <span class="hljs-variable">$objectlistfile</span>|<span class="hljs-built_in">wc</span> -l`<br>concurrent 1 <span class="hljs-variable">$objectnum</span> <span class="hljs-variable">$process</span><br><br></code></pre></td></tr></table></figure><p>上面直接把配置写到脚本里面了，根据需要进行修改</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment">#####config</span><br>process=10<br>objectlistfile=<span class="hljs-string">&quot;./delobject&quot;</span><br>deletepool=rbd<br><span class="hljs-comment">#####</span><br></code></pre></td></tr></table></figure><p>指定并发数目，指定准备删除的对象的list文件，指定对象所在的存储池</p><p>然后执行即可</p><h2 id="本次测试删除的性能差别"><a href="#本次测试删除的性能差别" class="headerlink" title="本次测试删除的性能差别"></a>本次测试删除的性能差别</h2><p>准备对象数据</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">rbd map testrbd<br><span class="hljs-built_in">dd</span> <span class="hljs-keyword">if</span>=/dev/zero of=/dev/rbd2 bs=4M count=1200<br></code></pre></td></tr></table></figure><p>获取列表</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 put]<span class="hljs-comment"># rados -p rbd ls |grep rbd_data.32c0f6b8b4567 &gt; delobject</span><br></code></pre></td></tr></table></figure><p>执行删除脚本</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 put]<span class="hljs-comment"># sh fastremove.sh</span><br></code></pre></td></tr></table></figure><p>测试结果如下：</p><table><thead><tr><th align="center">并发数</th><th align="center">删除时间</th></tr></thead><tbody><tr><td align="center">1</td><td align="center">71s</td></tr><tr><td align="center">2</td><td align="center">35s</td></tr><tr><td align="center">5</td><td align="center">5s</td></tr><tr><td align="center">25</td><td align="center">6s</td></tr><tr><td align="center">50</td><td align="center">5s</td></tr><tr><td align="center">100</td><td align="center">5s</td></tr></tbody></table><p>从测试结果来看在并发数为5的时候就能达到每秒删除200个对象了，根据自己的需要进行增减，也可以增减删除的间隔加上sleep</p><h3 id="删除rbd的元数据信息"><a href="#删除rbd的元数据信息" class="headerlink" title="删除rbd的元数据信息"></a>删除rbd的元数据信息</h3><p>因为只删除了对象没删除元数据信息</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ceph]<span class="hljs-comment"># rados -p rbd listomapvals rbd_directory</span><br>id_519216b8b4567<br>value (6 bytes) :<br>00000000  02 00 00 00 7a 70                                 |....zp|<br>00000006<br><br>name_zp<br>value (17 bytes) :<br>00000000  0d 00 00 00 35 31 39 32  31 36 62 38 62 34 35 36  |....519216b8b456|<br>00000010  37                                                |7|<br>00000011<br><br>[root@lab101 ceph]<span class="hljs-comment"># rados -p rbd rmomapkey rbd_directory id_519216b8b4567 </span><br>[root@lab101 ceph]<span class="hljs-comment"># rados -p rbd listomapvals rbd_directory</span><br>name_zp<br>value (17 bytes) :<br>00000000  0d 00 00 00 35 31 39 32  31 36 62 38 62 34 35 36  |....519216b8b456|<br>00000010  37                                                |7|<br>00000011<br><br>[root@lab101 ceph]<span class="hljs-comment"># rados -p rbd rmomapkey rbd_directory name_zp</span><br></code></pre></td></tr></table></figure><p>这样就把名称为zp的rbd删除了</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>在ceph里面一些系统的操作默认是单进程去处理的，一般情况下都没什么问题，在数据量超大，追求效率的时候，我们可以通过加上一些并发加速这个过程，本篇脚本当中的并发同样适用于其他需要并发的场景</p><h2 id="变更记录"><a href="#变更记录" class="headerlink" title="变更记录"></a>变更记录</h2><table><thead><tr><th align="center">Why</th><th align="center">Who</th><th align="center">When</th></tr></thead><tbody><tr><td align="center">创建</td><td align="center">武汉-运维-磨渣</td><td align="center">2017-07-27</td></tr><tr><td align="center">增加前台调试信息的屏蔽</td><td align="center">武汉-运维-磨渣</td><td align="center">2017-07-28</td></tr><tr><td align="center">更新元数据的清理</td><td align="center">武汉-运维-磨渣</td><td align="center">2018-04-12</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>从ceph对象中提取RBD中的指定文件</title>
    <link href="/2017/07/22/%E4%BB%8Eceph%E5%AF%B9%E8%B1%A1%E4%B8%AD%E6%8F%90%E5%8F%96RBD%E4%B8%AD%E7%9A%84%E6%8C%87%E5%AE%9A%E6%96%87%E4%BB%B6/"/>
    <url>/2017/07/22/%E4%BB%8Eceph%E5%AF%B9%E8%B1%A1%E4%B8%AD%E6%8F%90%E5%8F%96RBD%E4%B8%AD%E7%9A%84%E6%8C%87%E5%AE%9A%E6%96%87%E4%BB%B6/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>之前有个想法，是不是有办法找到rbd中的文件与对象的关系，想了很久但是一直觉得文件系统比较复杂，在fs 层的东西对ceph来说是透明的，并且对象大小是4M，而文件很小，可能在fs层进行了合并，应该很难找到对应关系，最近看到小胖有提出这个问题，那么就再次尝试了，现在就是把这个实现方法记录下来</p><!--break--><p>这个提取的作用个人觉得最大的好处就是一个rbd设备，在文件系统层被破坏以后，还能够从rbd提取出文件，我们知道很多情况下设备的文件系统一旦破坏，无法挂载，数据也就无法读取，而如果能从rbd中提取出文件，这就是保证了即使文件系统损坏的情况下，数据至少不丢失</p><p>本篇是基于xfs文件系统情况下的提取，其他文件系统有时间再看看，因为目前使用的比较多的就是xfs文件系统</p><p>本篇也回答了一个可能会经常被问起的问题，能告诉我虚拟机里面的文件在后台存储在哪里么，看完本篇就知道存储在哪里了</p><h2 id="XFS文件系统介绍"><a href="#XFS文件系统介绍" class="headerlink" title="XFS文件系统介绍"></a>XFS文件系统介绍</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ~]<span class="hljs-comment"># mkfs.xfs -f /dev/rbd0p1 </span><br>warning: device is not properly aligned /dev/rbd0p1<br>meta-data=/dev/rbd0p1            isize=256    agcount=9, agsize=162816 blks<br>         =                       sectsz=512   attr=2, projid32bit=1<br>         =                       crc=0        finobt=0<br>data     =                       bsize=4096   blocks=1310475, imaxpct=25<br>         =                       sunit=1024   swidth=1024 blks<br>naming   =version 2              bsize=4096   ascii-ci=0 ftype=0<br><span class="hljs-built_in">log</span>      =internal <span class="hljs-built_in">log</span>           bsize=4096   blocks=2560, version=2<br>         =                       sectsz=512   sunit=8 blks, lazy-count=1<br>realtime =none                   extsz=4096   blocks=0, rtextents=0<br></code></pre></td></tr></table></figure><p>XFS文件系统采取是AG管理的，每个AG维护自己的inode和数据，所以XFS文件系统是一种很容易扩展的文件系统，本篇里面主要用到的命令是xfs_bmap这个命令</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ~]<span class="hljs-comment"># xfs_bmap -lvp /etc/fstab</span><br>/etc/fstab:<br> EXT: FILE-OFFSET      BLOCK-RANGE        AG AG-OFFSET        TOTAL FLAGS<br>   0: [0..7]:          26645424..26645431  1 (431024..431031)     8 00000<br></code></pre></td></tr></table></figure><p>一个文件最小就是8个block（512b），也就是4k,这个因为上面默认的xfs的格式化就是data bsize&#x3D;4K,这个值可以自行调整的，本篇尽量用默认常规的参数来讲例子</p><p>查看man xfs_bmap这个命令可以看到：</p><blockquote><p>Holes are marked by replacing the startblock..endblock with hole.  All the file offsets and disk blocks are in units of 512-byte blocks, no matter what the filesystem’s block size is.</p></blockquote><p>意思是这个查询到的里面的计数单位都是512-byte，不管上层设置的block大小是多少，我们知道文件系统底层的sector就是512-byte，所以这个查询到的结果就可以跟当前的文件系统的sector的偏移量联系起来，这里强调一下，这个偏移量的起始位子为当前文件系统所在分区的偏移量，如果是多分区的情况，在计算整个偏移量的时候就要考虑分区的偏移量了，这个会在后面用实例进行讲解的</p><p>rbd的对象是不清楚内部分区的偏移量，所以在rbd层进行提取的时候是需要得到的是分区当中的文件相对整个磁盘的一个sector的偏移量</p><h2 id="rbd的对象结构"><a href="#rbd的对象结构" class="headerlink" title="rbd的对象结构"></a>rbd的对象结构</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ~]<span class="hljs-comment"># rados -p rbd ls|grep data</span><br>rbd_data.25a636b8b4567.00000000000009ff<br>rbd_data.25a636b8b4567.00000000000001dd<br>rbd_data.25a636b8b4567.0000000000000000<br>rbd_data.25a636b8b4567.000000000000009f<br>rbd_data.25a636b8b4567.0000000000000459<br>rbd_data.25a636b8b4567.000000000000027e<br>rbd_data.25a636b8b4567.00000000000004ff<br>rbd_data.25a636b8b4567.000000000000027c<br>rbd_data.25a636b8b4567.000000000000027d<br>rbd_data.25a636b8b4567.0000000000000001<br>rbd_data.25a636b8b4567.000000000000013e<br>rbd_data.25a636b8b4567.00000000000003ba<br>rbd_data.25a636b8b4567.000000000000031b<br>rbd_data.25a636b8b4567.00000000000004f8<br></code></pre></td></tr></table></figure><p>rbd被xfs格式化以后会产生一些对象，这些对象是以16进制名称的方式存储在后台的，也就是rbd大小一定的情况下对象数目是一定的，也就是名称也是一定的</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ~]<span class="hljs-comment"># parted -s /dev/rbd0 unit s print</span><br>Model: Unknown (unknown)<br>Disk /dev/rbd0: 20971520s<br>Sector size (logical/physical): 512B/512B<br>Partition Table: gpt<br>Disk Flags: <br><br>Number  Start      End        Size       File system  Name     Flags<br> 1      1953s      10485759s  10483807s  xfs          primari<br> 2      10485760s  20963327s  10477568s               primari<br></code></pre></td></tr></table></figure><p>上面可以看到rbd0的sector个数为20971520s<br>20971520s*512byte&#x3D;10737418240byte&#x3D;10485760KB&#x3D;10240MB<br>sector的大小一定，总rbd大小一定的情况下sector的数目也是一定的，本篇实例的rbd大小</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ~]<span class="hljs-comment"># rbd info zp</span><br>rbd image <span class="hljs-string">&#x27;zp&#x27;</span>:<br>size 10000 MB <span class="hljs-keyword">in</span> 2500 objects<br>order 22 (4096 kB objects)<br>block_name_prefix: rbd_data.25a776b8b4567<br>format: 2<br>features: layering<br>flags: <br>create_timestamp: Sat Jul 22 18:04:12 2017<br></code></pre></td></tr></table></figure><h2 id="sector和ceph-object的对应关系的查询"><a href="#sector和ceph-object的对应关系的查询" class="headerlink" title="sector和ceph object的对应关系的查询"></a>sector和ceph object的对应关系的查询</h2><p>这个就像个map一样，需要把这个关系给找到，一个sector的区间对应到object的map，这里我用python写个简单的方法来做查询，也可以自己用其他语言来实现</p><p>首先查询到rbd的对象数目</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ~]<span class="hljs-comment"># rbd info zp</span><br>rbd image <span class="hljs-string">&#x27;zp&#x27;</span>:<br>size 10000 MB <span class="hljs-keyword">in</span> 2500 objects<br>order 22 (4096 kB objects)<br>block_name_prefix: rbd_data.25a776b8b4567<br>format: 2<br>features: layering<br>flags: <br>create_timestamp: Sat Jul 22 18:04:12 2017<br></code></pre></td></tr></table></figure><p>处理脚本如下:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">vim getsecob.py<br></code></pre></td></tr></table></figure><p>添加下面内容</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-meta">#! /bin/python</span><br><span class="hljs-comment"># *-* conding=UTF-8 *-*</span><br><br>import commands<br><br>def main():<br>    getmap(2500)<br><br><br>def getmap(object):<br>    sector=int(object)*4096*1024/512<br>    <span class="hljs-built_in">print</span> <span class="hljs-string">&quot;object:&quot;</span>+str(object)<br>    <span class="hljs-built_in">print</span> <span class="hljs-string">&quot;sector:&quot;</span>+str(sector)<br>    incre=sector/object<br>    <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> range(int(object)):<br>        a=int(item*8192)<br>        b=int((item+<span class="hljs-number">1</span>)*<span class="hljs-number">8192</span>-<span class="hljs-number">1</span>)<br>        print str([a,b])+&quot;  --&gt;  &quot;+&quot;%<span class="hljs-number">016</span>x&quot; %item<br><br>if __name__ == &#x27;__main__&#x27;:<br>    main()<br></code></pre></td></tr></table></figure><p>其中getmap后面为对象数目<br>输出是这个形式的：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ~]<span class="hljs-comment"># python getsecob.py</span><br>object:2500<br>sector:20480000<br>[0, 8191]  --&gt;  0000000000000000<br>[8192, 16383]  --&gt;  0000000000000001<br>[16384, 24575]  --&gt;  0000000000000002<br>[24576, 32767]  --&gt;  0000000000000003<br>[32768, 40959]  --&gt;  0000000000000004<br>[40960, 49151]  --&gt;  0000000000000005<br>···<br></code></pre></td></tr></table></figure><p>对rbd0进行分区，分区后的结果如下</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ~]<span class="hljs-comment"># parted -s /dev/rbd0 unit s print</span><br>Model: Unknown (unknown)<br>Disk /dev/rbd0: 20480000s<br>Sector size (logical/physical): 512B/512B<br>Partition Table: gpt<br>Disk Flags: <br><br>Number  Start      End        Size       File system  Name     Flags<br> 1      1953s      10240000s  10238048s               primari<br> 2      10248192s  20471807s  10223616s               primari<br></code></pre></td></tr></table></figure><p>这个是个测试用的image，大小为10G分成两个5G的分区，现在我们在两个分区里面分别写入两个测试文件，然后经过计算后，从后台的对象中把文件读出</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">mount /dev/rbd0p1 /mnt1<br>mount /dev/rbd0p2 /mnt2<br><span class="hljs-built_in">cp</span> /etc/fstab /mnt1<br><span class="hljs-built_in">cp</span> /etc/hostname /mnt2<br></code></pre></td></tr></table></figure><p>首先获取文件在分区上的sector的偏移量</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ~]<span class="hljs-comment"># xfs_bmap -lvp /mnt1/fstab </span><br>/mnt1/fstab:<br> EXT: FILE-OFFSET      BLOCK-RANGE      AG AG-OFFSET        TOTAL FLAGS<br>   0: [0..7]:          8224..8231        0 (8224..8231)         8 01111<br></code></pre></td></tr></table></figure><p>可以得到是(8224..8231)共8个sector<br>从上面的分区1的start的sector可以知道起始位置是1953，那么相对于磁盘的偏移量就变成了</p><blockquote><p>(8224+1953..8231+1953) &#x3D; (10177..10184)</p></blockquote><p>这里说下，这个地方拿到偏移量后，直接通过对rbd设备进行dd读取也可以把这个文件读取出来，这个顺带讲下，本文主要是从对象提取：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">dd</span> <span class="hljs-keyword">if</span>=/dev/rbd0 of=a bs=512 count=8 skip=10177<br></code></pre></td></tr></table></figure><p>bs取512是因为sector的单位就是512b<br>这样就把刚刚的fstab文件读取出来了，skip就是文件的sector相对磁盘的起始位置，count就是文件所占的block数目</p><p>继续我们的对象提取方式，上面的（10177..10184）这个我们根据上面那个脚本输出的对象列表来找到对象</p><blockquote><p>[8192, 16383]  –&gt;  0000000000000001<br>获取名称，这个因为我的是测试环境，就只有一个匹配，多个image的时候要过滤出对用的rbd的对象，用prifix过滤即可</p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ~]<span class="hljs-comment"># rados -p rbd ls|grep 0000000000000001</span><br>rbd_data.25a776b8b4567.0000000000000001<br></code></pre></td></tr></table></figure><p>下载对象</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ~]<span class="hljs-comment"># rados -p rbd get rbd_data.25a776b8b4567.0000000000000001 rbd_data.25a776b8b4567.0000000000000001</span><br></code></pre></td></tr></table></figure><p>根据偏移量计算对象中的偏移量</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">（10177..10184）<br>[8192, 16383]  --&gt;  0000000000000001<br></code></pre></td></tr></table></figure><p>得到</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">10177-8192=1985<br><br><span class="hljs-built_in">dd</span> <span class="hljs-keyword">if</span>=rbd_data.25a776b8b4567.0000000000000001 of=a bs=512 count=8 skip=1985<br></code></pre></td></tr></table></figure><p>得到的文件a的内容即为之前文件的内容</p><p>准备取第二个分区的文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ~]<span class="hljs-comment"># xfs_bmap -lvp /mnt2/hostname </span><br>/mnt2/hostname:<br> EXT: FILE-OFFSET      BLOCK-RANGE      AG AG-OFFSET        TOTAL FLAGS<br>   0: [0..7]:          8224..8231        0 (8224..8231)         8 01111<br></code></pre></td></tr></table></figure><p>8224+10248192..8231+10248192&#x3D;10256416..10256423</p><p>从磁盘方式</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ~]<span class="hljs-comment"># dd if=/dev/rbd0 of=a bs=512 count=8 skip=10256416</span><br></code></pre></td></tr></table></figure><p>从对象方式<br>10256416..10256423 对应<br>[10256384, 10264575]  –&gt;  00000000000004e4<br>对象偏移量</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">10256416-10256384=32<br></code></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">rados -p rbd get <br>[root@lab8106 ~]<span class="hljs-comment"># rados -p rbd get rbd_data.25a776b8b4567.00000000000004e4 rbd_data.25a776b8b4567.00000000000004e4</span><br></code></pre></td></tr></table></figure><p>获取文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ~]<span class="hljs-comment"># dd if=rbd_data.25a776b8b4567.00000000000004e4 of=a bs=512 count=8 skip=32</span><br></code></pre></td></tr></table></figure><p>如果文件比较大的情况，可能出现就是文件是跨对象的，那么还是跟上面的提取方法一样，然后进行提取后的文件进行合并即可</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>在存储系统上面存储的文件必然会对应到底层磁盘的sector，而sector也是会一一对应到后台的对象的，这个在本文当中得到了验证，所以整个逻辑就是，在文件系统层找到文件对应的sector位置，然后再在底层把sector和对象关系找好，就能从找到文件在对象当中的具体的位置，也就能定位并且能提取了，本篇是基于xfs的，其他文件系统只要能定位文件的sector，就可以在底层找到文件，这个以后会补充其他文件系统进来</p><h2 id="变更记录"><a href="#变更记录" class="headerlink" title="变更记录"></a>变更记录</h2><table><thead><tr><th align="center">Why</th><th align="center">Who</th><th align="center">When</th></tr></thead><tbody><tr><td align="center">创建</td><td align="center">武汉-运维-磨渣</td><td align="center">2017-07-22</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>利用火焰图分析ceph pg分布</title>
    <link href="/2017/07/18/%E5%88%A9%E7%94%A8%E7%81%AB%E7%84%B0%E5%9B%BE%E5%88%86%E6%9E%90ceph%20pg%E5%88%86%E5%B8%83/"/>
    <url>/2017/07/18/%E5%88%A9%E7%94%A8%E7%81%AB%E7%84%B0%E5%9B%BE%E5%88%86%E6%9E%90ceph%20pg%E5%88%86%E5%B8%83/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>性能优化大神Brendan Gregg发明了火焰图来定位性能问题，通过图表就可以发现问题出在哪里，通过svg矢量图来查看性能卡在哪个点，哪个操作占用的资源最多</p><p>在查看了原始数据后，这个分析的原理是按层级来对调用进行一个计数，然后以层级去做比对，来看横向的占用的比例情况</p><p>基于这个原理，把osd tree的数据和pg数据可以做一个层级的组合，从而可以很方便的看出pg的分布情况，主机的分布情况，还可以进行搜索，在一个简单的图表内汇聚了大量的信息</p><h2 id="实践"><a href="#实践" class="headerlink" title="实践"></a>实践</h2><p>获取需要的数据，这个获取数据是我用一个脚本解析的osd tree 和pg dump，然后按照需要的格式进行输出</p><blockquote><p>default;lab8106;osd.2;0.0 6<br><br>default;lab8106;osd.3;0.0 6<br><br>default;rack1;lab8107;osd.0;0.0 6</p></blockquote><p>需要的格式是这个样的，最后一个为权重，使用的是对象数，因为对象数可能为0，所以默认在每个数值进行了加一的操作，前面就是osd的分布的位置</p><p>脚本&#x2F;sbin&#x2F;stackcollapse-crush内容如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-meta">#! /bin/python</span><br><span class="hljs-comment"># -*- coding: UTF-8 -*-</span><br>import os<br>import commands<br>import json<br><br><br>def main():<br>    global list_all_host<br>    list_all_host = commands.getoutput(<span class="hljs-string">&#x27;ceph osd tree -f json-pretty  2&gt;/dev/null&#x27;</span>)<br>    getpgmap()<br>def getosd(osd):<br>    mylist=[]<br>    crushid=&#123;&#125;<br>    json_str = json.loads(list_all_host)<br>    <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> json_str[<span class="hljs-string">&#x27;nodes&#x27;</span>]:<br>        <span class="hljs-keyword">if</span> item.has_key(<span class="hljs-string">&#x27;children&#x27;</span>):<br>            crushid[str(item[<span class="hljs-string">&#x27;id&#x27;</span>])]=str(item[<span class="hljs-string">&#x27;name&#x27;</span>])<br>            <span class="hljs-keyword">for</span> child <span class="hljs-keyword">in</span> item[<span class="hljs-string">&#x27;children&#x27;</span>]:<br>                tmplist=[item[<span class="hljs-string">&#x27;id&#x27;</span>],child]<br>                mylist.append(tmplist)<br>        <span class="hljs-keyword">if</span> item[<span class="hljs-string">&#x27;type&#x27;</span>] == <span class="hljs-string">&quot;osd&quot;</span>:<br>            crushid[str(item[<span class="hljs-string">&#x27;id&#x27;</span>])]=str(item[<span class="hljs-string">&#x27;name&#x27;</span>])<br>    listnum=len(mylist)<br>    compareindex=0<br><span class="hljs-comment">###从数组开始跟后面的数组进行比较，如果有就改变后面的数组，然后删除当前比较的list(index),进行list更新</span><br><span class="hljs-comment">###如果没有改变，就把索引往后推即可</span><br>    <span class="hljs-keyword">while</span> compareindex &lt; len(mylist):<br>        change = False<br>        <span class="hljs-keyword">for</span> index,num <span class="hljs-keyword">in</span> enumerate(mylist):<br>            <span class="hljs-keyword">if</span> compareindex != index and compareindex &lt; index:<br>                <span class="hljs-keyword">if</span> str(mylist[compareindex][-1]) == str(num[0]):<br>                    del mylist[index][0]<br>                    mylist[index]=mylist[compareindex]+mylist[index]<br>                    change=True<br>                <span class="hljs-keyword">if</span> str(mylist[compareindex][0]) == str(num[-1]):<br>                    del mylist[index][-1]<br>                    mylist[index]=mylist[index]+mylist[compareindex]<br>                    change=True<br>        <span class="hljs-keyword">if</span> change == True:<br>            del mylist[compareindex]<br>        <span class="hljs-keyword">if</span> change == False:<br>            compareindex = compareindex + 1<br><br>    <span class="hljs-keyword">for</span> index,crushlist <span class="hljs-keyword">in</span> enumerate(mylist):<br>        osdcrushlist=[]<br>        <span class="hljs-keyword">for</span> osdlocaltion <span class="hljs-keyword">in</span> crushlist:<br>            <span class="hljs-built_in">local</span>=str(crushid[<span class="hljs-string">&#x27;%s&#x27;</span> %osdlocaltion])<br>            osdcrushlist.append(<span class="hljs-built_in">local</span>)<br>        <span class="hljs-keyword">if</span> osdcrushlist[-1] == osd:<br>            <span class="hljs-built_in">return</span> osdcrushlist<br><br>def getpgmap():<br>    list_all_host = commands.getoutput(<span class="hljs-string">&#x27;ceph pg  ls --format json-pretty  2&gt;/dev/null&#x27;</span>)<br>    json_str = json.loads(list_all_host)<br>    <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> json_str:<br>        <span class="hljs-keyword">for</span> osdid <span class="hljs-keyword">in</span> item[<span class="hljs-string">&#x27;up&#x27;</span>]:<br>            osd=<span class="hljs-string">&quot;osd.&quot;</span>+str(osdid)<br>            b=<span class="hljs-string">&quot;&quot;</span><br>            <span class="hljs-keyword">for</span> a <span class="hljs-keyword">in</span> getosd(osd):<br>                b=b+str(a)+<span class="hljs-string">&quot;;&quot;</span><br>            <span class="hljs-built_in">print</span> b+item[<span class="hljs-string">&#x27;pgid&#x27;</span>]+<span class="hljs-string">&quot; &quot;</span>+str(item[<span class="hljs-string">&#x27;stat_sum&#x27;</span>][<span class="hljs-string">&#x27;num_objects&#x27;</span>]+1)<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    main()<br></code></pre></td></tr></table></figure><h3 id="获取数据"><a href="#获取数据" class="headerlink" title="获取数据"></a>获取数据</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">/sbin/stackcollapse-crush &gt; /tmp/mydata<br></code></pre></td></tr></table></figure><h3 id="解析数据"><a href="#解析数据" class="headerlink" title="解析数据"></a>解析数据</h3><p>获取解析脚本，这个脚本是Brendan Gregg写好的，这地方托管到我的github里面了</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">wget -O /sbin/flamegraph https://bash.githubusercontent.com/zphj1987/cephcrushflam/master/flamegraph.pl<br></code></pre></td></tr></table></figure><p>对数据进行解析</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">/sbin/flamegraph  --title  <span class="hljs-string">&quot;Ceph crush flame graph&quot;</span> --width <span class="hljs-string">&quot;1800&quot;</span> --countname <span class="hljs-string">&quot;num&quot;</span> /tmp/mydata &gt; /tmp/mycrush.svg<br></code></pre></td></tr></table></figure><p>将&#x2F;tmp&#x2F;mycrush.svg拷贝到windows机器，然后用浏览器打开即可，推荐chrome</p><h3 id="效果图如下"><a href="#效果图如下" class="headerlink" title="效果图如下"></a>效果图如下</h3><p>Example :<br>[<img src="/images/blog/o_200901080558mycrush.png" alt="Example"></p><ul><li>通过颜色来区分比例占用的区别</li><li>支持搜索</li><li>tree方式，可以清楚看到分布</li><li>可以查看pg对象数目</li><li>可以查看osd上面有哪些pg，主机上有哪些osd</li></ul><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>通过ceph osd tree可以查到整个的信息，但是一个屏幕的信息量有限，而通过滚屏或者过滤进行查询的信息，需要做一下关联，而这种可以缩放的svg位图的方式，可以包含大量的信息，如果是做分析的时候还是能比较直观的看到，上面的难点在于获取数据部分，而绘图的部分是直接用的现有的处理，比自己重新开发一个要简单的多，类似的工具还有个桑基图方式，这个在inkscope这个管理平台里面有用到</p><p>本篇就是在最小的视野里容纳尽量多的信息量一个实例，其他的数据有类似模型的也可以做相似的处理</p><h2 id="变更记录"><a href="#变更记录" class="headerlink" title="变更记录"></a>变更记录</h2><table><thead><tr><th align="center">Why</th><th align="center">Who</th><th align="center">When</th></tr></thead><tbody><tr><td align="center">创建</td><td align="center">武汉-运维-磨渣</td><td align="center">2017-07-18</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Cephfs 操作输出到日志查询系统</title>
    <link href="/2017/07/13/Cephfs%20%E6%93%8D%E4%BD%9C%E8%BE%93%E5%87%BA%E5%88%B0%E6%97%A5%E5%BF%97%E6%9F%A5%E8%AF%A2%E7%B3%BB%E7%BB%9F/"/>
    <url>/2017/07/13/Cephfs%20%E6%93%8D%E4%BD%9C%E8%BE%93%E5%87%BA%E5%88%B0%E6%97%A5%E5%BF%97%E6%9F%A5%E8%AF%A2%E7%B3%BB%E7%BB%9F/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>文件系统当中如果某些文件不见了，有什么办法判断是删除了还是自己不见了，这个就需要去日志里面定位了，通常情况下是去翻日志，而日志是会进行压缩的，并且查找起来非常的不方便,还有可能并没有开启</p><p>这个时候就需要日志系统了，最近正好看到一篇<a href="https://zhuanlan.zhihu.com/p/27363484">最佳日志实践（v2.0）</a>，一篇非常好的文章，本篇日志属于文章里面所提到的统计日志，统计客户端做了什么操作</p><p>对于日志系统来说，很重要的一点，能够很方便的进行查询，这就需要对日志信息进行一些处理了，怎么处理就是设计问题，要求就是不多不少</p><h2 id="结构"><a href="#结构" class="headerlink" title="结构"></a>结构</h2><p><img src="/images/blog/o_200901080128mdslogsystem.png" alt="mdslogsystem.png-32.4kB"></p><p>其中graylog配置部分在这篇<a href="/images/blog/13575431.html">使用日志系统graylog获取Ceph集群状态</a>，根据这篇的操作，配置出12201的udp监听端口即可，剩余部分就是本篇中的配置</p><h2 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h2><h3 id="集群的配置"><a href="#集群的配置" class="headerlink" title="集群的配置"></a>集群的配置</h3><p>需要对MDS的配置进行debug_ms&#x3D;1,在&#x2F;etc&#x2F;ceph&#x2F;ceph.conf当中添加下面配置</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">[mds.lab8106]<br>debug_ms=1<br>hostname=lab8106<br></code></pre></td></tr></table></figure><p>这个地方集群的文件操作日志是记录在message里面的1级别的，所以把mds的debug_ms开到1<br>日志长这个样子：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">2017-07-13 11:26:23.703624 7fc3128c3700  1 -- 192.168.8.106:6804/3280969928 &lt;== client.14180 192.168.8.106:0/1092795882 2384 ==== client_request(client.14180:2346 <span class="hljs-built_in">mkdir</span> <span class="hljs-comment">#1/ppop 2017-07-13 11:26:23.702532 caller_uid=0, caller_gid=0&#123;&#125;) v2 ==== 170+0+0 (843685338 0 0) 0x5645ec243600 con 0x5645ec247000</span><br></code></pre></td></tr></table></figure><p>下面会对这个日志进行提取</p><h3 id="下载logstash"><a href="#下载logstash" class="headerlink" title="下载logstash"></a>下载logstash</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">https://artifacts.elastic.co/downloads/logstash/logstash-5.5.0.rpm<br></code></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ~]<span class="hljs-comment"># rpm -ivh logstash-5.5.0.rpm</span><br></code></pre></td></tr></table></figure><p>修改启动进程为root权限<br>修改&#x2F;etc&#x2F;systemd&#x2F;system&#x2F;logstash.service文件当中的</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">User=root<br>Group=root<br></code></pre></td></tr></table></figure><p>因为logstash需要本地文件的读取权限，这里是为了方便直接给的root权限，方便使用，如果对权限要求比较严的环境，就给文件</p><h3 id="创建一个配置文件"><a href="#创建一个配置文件" class="headerlink" title="创建一个配置文件"></a>创建一个配置文件</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">vim /etc/logstash/conf.d/logstash.conf<br></code></pre></td></tr></table></figure><p>添加下面的配置文件，这个配置文件包含的内容比较多，会在后面详细的介绍下处理过程</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><code class="hljs bash">input &#123;<br>    file &#123;<br>    path =&gt; <span class="hljs-string">&quot;/var/log/ceph/ceph-mds.*.log&quot;</span><br>    &#125;<br>&#125;<br><br>filter &#123;<br>    grok &#123;<br>     match =&gt;&#123;<span class="hljs-string">&quot;message&quot;</span> =&gt; <span class="hljs-string">&quot;%&#123;TIMESTAMP_ISO8601:logtime&#125; %&#123;BASE16FLOAT&#125;  %&#123;BASE10NUM&#125; -- %&#123;HOSTPORT:mdsip&#125;%&#123;NOTSPACE&#125; &lt;== %&#123;NOTSPACE:clientid&#125; %&#123;IP:clientip&#125;%&#123;NOTSPACE&#125; %&#123;INT&#125; ==== client_request\(%&#123;NOTSPACE&#125; %&#123;WORD:do&#125; %&#123;NOTSPACE:where&#125; %&#123;TIMES</span><br><span class="hljs-string">TAMP_ISO8601:dotime&#125;%&#123;GREEDYDATA&#125;&quot;</span>&#125;<br>    overwrite =&gt; [<span class="hljs-string">&quot;message&quot;</span>]<br>    remove_field =&gt;[<span class="hljs-string">&quot;logtime&quot;</span>]<br>    &#125;<br><br>    <span class="hljs-keyword">if</span> ![dotime] &#123;<br>        drop &#123;&#125;<br>    &#125;<br>    <span class="hljs-keyword">if</span> [<span class="hljs-keyword">do</span>] == <span class="hljs-string">&quot;mkdir&quot;</span> &#123;<br>        mutate &#123;<br>        replace =&gt; &#123; <span class="hljs-string">&quot;do&quot;</span> =&gt; <span class="hljs-string">&quot;创建目录&quot;</span>&#125;<br>        &#125;<br>    &#125;<br>    <span class="hljs-keyword">if</span> [<span class="hljs-keyword">do</span>] == <span class="hljs-string">&quot;create&quot;</span> &#123;<br>        mutate &#123;<br>        replace =&gt; &#123; <span class="hljs-string">&quot;do&quot;</span> =&gt; <span class="hljs-string">&quot;创建文件&quot;</span>&#125;<br>        &#125;<br>    &#125;<br>    <span class="hljs-keyword">if</span> [<span class="hljs-keyword">do</span>] == <span class="hljs-string">&quot;unlink&quot;</span> &#123;<br>        mutate &#123;<br>        replace =&gt; &#123; <span class="hljs-string">&quot;do&quot;</span> =&gt; <span class="hljs-string">&quot;删除文件(或链接)&quot;</span>&#125;<br>        &#125;<br>    &#125;<br>    <span class="hljs-keyword">if</span> [<span class="hljs-keyword">do</span>] == <span class="hljs-string">&quot;rmdir&quot;</span> &#123;<br>        mutate &#123;<br>        replace =&gt; &#123; <span class="hljs-string">&quot;do&quot;</span> =&gt; <span class="hljs-string">&quot;删除目录&quot;</span>&#125;<br>        &#125;<br>    &#125;<br>    <span class="hljs-keyword">if</span> [<span class="hljs-keyword">do</span>] == <span class="hljs-string">&quot;rename&quot;</span> &#123;<br>        mutate &#123;<br>        replace =&gt; &#123; <span class="hljs-string">&quot;do&quot;</span> =&gt; <span class="hljs-string">&quot;重命名&quot;</span>&#125;<br>        &#125;<br>    &#125;<br>    <span class="hljs-keyword">if</span> [<span class="hljs-keyword">do</span>] == <span class="hljs-string">&quot;symlink&quot;</span> &#123;<br>        mutate &#123;<br>        replace =&gt; &#123; <span class="hljs-string">&quot;do&quot;</span> =&gt; <span class="hljs-string">&quot;链接&quot;</span>&#125;<br>        &#125;<br>    &#125;<br><br>    mutate &#123;<br>        replace =&gt; &#123; <span class="hljs-string">&quot;message&quot;</span> =&gt; <span class="hljs-string">&quot;time:%&#123;dotime&#125; ClientIp:%&#123;clientip&#125;  ClintId:%&#123;clientid&#125;  MdsIp: %&#123;mdsip&#125; do:%&#123;do&#125; where:%&#123;where&#125;&quot;</span>&#125;<br>    &#125;<br>    <span class="hljs-built_in">date</span> &#123;<br>        match =&gt; [ <span class="hljs-string">&quot;dotime&quot;</span>, <span class="hljs-string">&quot;yyyy-MM-dd HH:mm:ss.SSSSSS&quot;</span>]<br>        target =&gt; <span class="hljs-string">&quot;@timestamp&quot;</span><br>    &#125;<br>&#125;<br><br>output &#123;<br>    gelf &#123;<br>        host =&gt; <span class="hljs-string">&quot;192.168.8.106&quot;</span><br>    &#125;<br><br>    stdout &#123; codec =&gt; json_lines&#125;<br>&#125;<br><br></code></pre></td></tr></table></figure><p>处理后的日志是这个样子：</p><blockquote><p>{“path”:”&#x2F;var&#x2F;log&#x2F;ceph&#x2F;ceph-mds.lab8106.log”,”@timestamp”:”2017-07-13T04:01:01.251Z”,”clientid”:”client.14180”,”clientip”:”192.168.8.106”,”@version”:”1”,”host”:”lab8106”,”where”:”#1&#x2F;test1”,”do”:”创建目录”,”message”:”time:2017-07-13 12:01:01.251358 ClientIp:192.168.8.106  ClintId:client.14180  MdsIp: 192.168.8.106:6804 do:创建目录 where:#1&#x2F;test1”,”mdsip”:”192.168.8.106:6804”,”dotime”:”2017-07-13 12:01:01.251358”}</p></blockquote><p>是一个json形式的，可以根据自己的需要增加减少字段，这些信息都会传递到graylog当中</p><h3 id="解析配置文件"><a href="#解析配置文件" class="headerlink" title="解析配置文件"></a>解析配置文件</h3><p>logstash配置文件的结构包括三个大模块：</p><ul><li>input</li><li>filter</li><li>output</li></ul><p>input是文件的来源，也就是我们需要解析的日志，filter是处理日志的模块，output是输出的模块，这里我们需要使用的是gelf的输出模式，在本地进行调试的时候，可以开启stdout来进行调试</p><p>采用grok进行正则匹配，这个里面的匹配正则可以用 <a href="http://grokconstructor.appspot.com/do/construction">http://grokconstructor.appspot.com/do/construction</a> 这个进行正则表达式的验证和书写，可以一步步的进行匹配，还是很方便的工具</p><p>output输出gelf的信息流</p><h3 id="grok内部解析"><a href="#grok内部解析" class="headerlink" title="grok内部解析"></a>grok内部解析</h3><ul><li>remove_field可以用来删除无用的字段</li><li>if ![dotime] 这个是用来过滤消息的，如果没拿到这个值，也就是没匹配上的时候，就把消息丢弃</li><li>使用mutate replace模块来进行字段的替换，将固定操作转换为中文</li><li>使用mutate replace模块来重写message，根据自己定义的格式进行输出</li><li>使用date 模块进行@timestamp的重写，将日志内的时间写入到这个里面</li></ul><p>查询插件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">/usr/share/logstash/bin/logstash-plugin list<br></code></pre></td></tr></table></figure><p>logstash-output-gelf默认没有安装的,需要安装一下</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">/usr/share/logstash/bin/logstash-plugin install logstash-output-gelf<br></code></pre></td></tr></table></figure><p>这个安装可能有点慢，稍微多等一下</p><p>调试方式的启动</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">/usr/share/logstash/bin/logstash -f /etc/logstash/conf.d/logstash.conf<br></code></pre></td></tr></table></figure><p>如果调试完毕了后就用系统命令启动</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ~]<span class="hljs-comment"># systemctl restart logstash</span><br></code></pre></td></tr></table></figure><p><img src="/images/blog/o_200901080134graylogfs.png" alt="graylogfs.png-188.3kB"></p><p>通过graylog系统就可以很方便的看到日志里面节获取的内容了</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>对于一套系统来说，日志系统是一个很重要的组成部分，可以更好的掌握系统内部的运行情况，并不是说出了问题再去找日志，这个日志的需求来源其实很简单</p><blockquote><p>哪个客户端对着哪个MDS做了一个什么操作</p></blockquote><p>然后就可以用这个搜索引擎去进行相关的搜索了，可以查询一段时间创建了多少文件，是不是删除了哪个文件</p><p>本次实践的难点在于logstash对日志的相关解析的操作，掌握了方法以后，对于其他日志的提取也可以用类似的方法，提取自己需要的信息，然后进行整合，输出到一个系统当中，剩下的就是在界面上获取信息</p><h2 id="变更记录"><a href="#变更记录" class="headerlink" title="变更记录"></a>变更记录</h2><table><thead><tr><th align="center">Why</th><th align="center">Who</th><th align="center">When</th></tr></thead><tbody><tr><td align="center">创建</td><td align="center">武汉-运维-磨渣</td><td align="center">2017-07-13</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>ceph luminous 新功能之磁盘智能分组</title>
    <link href="/2017/06/28/ceph%20luminous%20%E6%96%B0%E5%8A%9F%E8%83%BD%E4%B9%8B%E7%A3%81%E7%9B%98%E6%99%BA%E8%83%BD%E5%88%86%E7%BB%84/"/>
    <url>/2017/06/28/ceph%20luminous%20%E6%96%B0%E5%8A%9F%E8%83%BD%E4%B9%8B%E7%A3%81%E7%9B%98%E6%99%BA%E8%83%BD%E5%88%86%E7%BB%84/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>本篇是luminous一个新功能介绍，关于磁盘智能分组的，这个在ceph里面叫crush class，这个我自己起名叫磁盘智能分组，因为这个实现的功能就是根据磁盘类型进行属性关联，然后进行分类，减少了很多的人为操作</p><p>以前我们需要对ssd和hdd进行分组的时候，需要大量的修改crush map，然后绑定不同的存储池到不同的 crush 树上面，现在这个逻辑简化了很多</p><blockquote><p>ceph crush class {create,rm,ls} manage the new CRUSH device<br><br>class feature. ceph crush set-device-class <osd> <class><br><br>will set the clas for a particular device.</p></blockquote><blockquote><p>Each OSD can now have a device class associated with it (e.g., hdd or<br><br>ssd), allowing CRUSH rules to trivially map data to a subset of devices<br><br>in the system. Manually writing CRUSH rules or manual editing of the CRUSH is normally not required.</p></blockquote><p>这个是发布的公告里面关于这两个功能的说明的，本篇就来看看这个功能怎么用</p><h2 id="实践"><a href="#实践" class="headerlink" title="实践"></a>实践</h2><h3 id="首先创建分类的规则"><a href="#首先创建分类的规则" class="headerlink" title="首先创建分类的规则"></a>首先创建分类的规则</h3><p>创建一个ssd的分组</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ceph]# ceph osd crush class create  ssd<br>created class ssd with id 0 to crush map<br></code></pre></td></tr></table></figure><p>也就是一个名称，这里我认为是ssd的分组就创建名词为ssd</p><p>再创建一个hdd的分组</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ceph]# ceph osd crush class create  hdd<br>created class hdd with id 1 to crush map<br></code></pre></td></tr></table></figure><h3 id="查询分组规则"><a href="#查询分组规则" class="headerlink" title="查询分组规则"></a>查询分组规则</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ceph]# ceph osd crush class ls<br>[<br>    &quot;ssd&quot;,<br>    &quot;hdd&quot;<br>]<br></code></pre></td></tr></table></figure><h3 id="把osd绑定不同的属性-属性名称就是上面的分类"><a href="#把osd绑定不同的属性-属性名称就是上面的分类" class="headerlink" title="把osd绑定不同的属性(属性名称就是上面的分类)"></a>把osd绑定不同的属性(属性名称就是上面的分类)</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ceph]# ceph osd crush set-device-class osd.0  ssd<br>set-device-class item id 0 name &#x27;osd.0&#x27; device_class ssd<br>[root@lab8106 ceph]# ceph osd crush set-device-class osd.2  ssd<br>set-device-class item id 2 name &#x27;osd.2&#x27; device_class ssd<br>[root@lab8106 ceph]# ceph osd crush set-device-class osd.1  hdd<br>set-device-class item id 1 name &#x27;osd.1&#x27; device_class hdd<br>[root@lab8106 ceph]# ceph osd crush set-device-class osd.3  hdd<br>set-device-class item id 3 name &#x27;osd.3&#x27; device_class hdd<br></code></pre></td></tr></table></figure><p>查询设置以后的效果</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ceph]# ceph osd tree<br>ID WEIGHT  TYPE NAME            UP/DOWN REWEIGHT PRIMARY-AFFINITY <br>-6 0.54559 root default~hdd                                       <br>-5 0.54559     host lab8106~hdd                                   <br> 1 0.27280         osd.1             up  1.00000          1.00000 <br> 3 0.27280         osd.3             up  1.00000          1.00000 <br>-4 0.54559 root default~ssd                                       <br>-3 0.54559     host lab8106~ssd                                   <br> 0 0.27280         osd.0             up  1.00000          1.00000 <br> 2 0.27280         osd.2             up  1.00000          1.00000 <br>-1 1.09119 root default                                           <br>-2 1.09119     host lab8106                                       <br> 0 0.27280         osd.0             up  1.00000          1.00000 <br> 1 0.27280         osd.1             up  1.00000          1.00000 <br> 2 0.27280         osd.2             up  1.00000          1.00000 <br> 3 0.27280         osd.3             up  1.00000          1.00000 <br></code></pre></td></tr></table></figure><p>这个就是这个功能比较核心的地方，会根据磁盘类型不同，自动的创建了不同的树，并且把磁盘放入到了树里面去了</p><h3 id="根据根创建规则（这个地方有bug，下面会提及）"><a href="#根据根创建规则（这个地方有bug，下面会提及）" class="headerlink" title="根据根创建规则（这个地方有bug，下面会提及）"></a>根据根创建规则（这个地方有bug，下面会提及）</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ceph]# ceph osd crush rule create-simple ssd default~ssd host firstn<br></code></pre></td></tr></table></figure><p>检查创建的rule规则：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 build]# ceph   osd  crush rule  dump ssd<br>&#123;<br>    &quot;rule_id&quot;: 1,<br>    &quot;rule_name&quot;: &quot;ssd&quot;,<br>    &quot;ruleset&quot;: 1,<br>    &quot;type&quot;: 1,<br>    &quot;min_size&quot;: 1,<br>    &quot;max_size&quot;: 10,<br>    &quot;steps&quot;: [<br>        &#123;<br>            &quot;op&quot;: &quot;take&quot;,<br>            &quot;item&quot;: -4,<br>            &quot;item_name&quot;: &quot;default~ssd&quot;<br>        &#125;,<br>        &#123;<br>            &quot;op&quot;: &quot;chooseleaf_firstn&quot;,<br>            &quot;num&quot;: 0,<br>            &quot;type&quot;: &quot;host&quot;<br>        &#125;,<br>        &#123;<br>            &quot;op&quot;: &quot;emit&quot;<br>        &#125;<br>    ]<br>&#125;<br></code></pre></td></tr></table></figure><h3 id="根据rule创建存储池"><a href="#根据rule创建存储池" class="headerlink" title="根据rule创建存储池"></a>根据rule创建存储池</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs raw">ceph  osd pool create testpool 64 64 ssd<br>ceph   osd dump|grep pool<br>pool 3 &#x27;testpool&#x27; replicated size 3 min_size 1 crush_rule 1 object_hash rjenkins pg_num 64 pgp_num 64 last_change 27 flags hashpspool stripe_width 0<br></code></pre></td></tr></table></figure><p>这里有个验证规则的小bug  代码在src&#x2F;mon&#x2F;MonCommands.h  </p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs raw"> COMMAND(&quot;osd crush rule create-simple &quot; \<br>&quot;name=name,type=CephString,goodchars=[A-Za-z0-9-_.] &quot; \<br>&quot;name=root,type=CephString,goodchars=[A-Za-z0-9-_.] &quot; \<br>&quot;name=type,type=CephString,goodchars=[A-Za-z0-9-_.] &quot; \<br></code></pre></td></tr></table></figure><p>默认的goodchars不包含’~’,这里不清楚社区是准备去改创建的逻辑去掉这个特殊符号，还是去改创建rule相关的规则，我已经提交了<a href="http://tracker.ceph.com/issues/20446">issue#20446</a>，等待社区的修改方案</p><h2 id="功能逻辑"><a href="#功能逻辑" class="headerlink" title="功能逻辑"></a>功能逻辑</h2><h3 id="现在方法"><a href="#现在方法" class="headerlink" title="现在方法"></a>现在方法</h3><p>创建一个磁盘类型的class，给磁盘标记class的统一标签，自动会根据class的类型创建一个树，根据树创建rule，根据rule创建存储池，整个操作没有动crushmap的操作</p><p>增加或修改盘的时候，设置下属性即可</p><h3 id="以前的方法"><a href="#以前的方法" class="headerlink" title="以前的方法"></a>以前的方法</h3><p>先添加盘，手动创建树，新加的osd要找下原来的树的名称，然后把osd放到这个树里面去，然后创建规则，根据rule创建存储池</p><p>增加盘或修改盘的时候，需要查找下，然后根据查找的规则进行相关操作</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>现在方法对用户操作来说更透明，直接对磁盘进行分类打标签即可，减少了一些复杂的操作逻辑，是一个很不错的功能</p><h2 id="更新"><a href="#更新" class="headerlink" title="更新"></a>更新</h2><p>后面会在crush rule创建的时候指定一个class的选项，就可以不改规则，也不改命令了<br><a href="https://www.spinics.net/lists/ceph-devel/msg37343.html%EF%BC%8C%E4%B8%8B%E4%B8%AA%E7%89%88%E6%9C%AC%E7%9A%84rc%E5%BA%94%E8%AF%A5%E4%BC%9A%E8%A7%A3%E5%86%B3">https://www.spinics.net/lists/ceph-devel/msg37343.html，下个版本的rc应该会解决</a></p><h2 id="变更记录"><a href="#变更记录" class="headerlink" title="变更记录"></a>变更记录</h2><table><thead><tr><th align="center">Why</th><th align="center">Who</th><th align="center">When</th></tr></thead><tbody><tr><td align="center">创建</td><td align="center">武汉-运维-磨渣</td><td align="center">2017-06-28</td></tr><tr><td align="center">更新进度</td><td align="center">武汉-运维-磨渣</td><td align="center">2017-06-28</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>ceph luminous 新功能之内置dashboard</title>
    <link href="/2017/06/25/ceph%20luminous%20%E6%96%B0%E5%8A%9F%E8%83%BD%E4%B9%8B%E5%86%85%E7%BD%AEdashboard/"/>
    <url>/2017/06/25/ceph%20luminous%20%E6%96%B0%E5%8A%9F%E8%83%BD%E4%B9%8B%E5%86%85%E7%BD%AEdashboard/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>ceph luminous版本新增加了很多有意思的功能，这个也是一个长期支持版本，所以这些新功能的特性还是很值得期待的，从底层的存储改造，消息方式的改变，以及一些之前未实现的功能的完成，都让ceph变得更强，这里面有很多核心模块来自中国的开发者，在这里准备用一系列的文章对这些新功能进行一个简单的介绍，也是自己的一个学习的过程</p><h2 id="相关配置"><a href="#相关配置" class="headerlink" title="相关配置"></a>相关配置</h2><h3 id="配置ceph国内源"><a href="#配置ceph国内源" class="headerlink" title="配置ceph国内源"></a>配置ceph国内源</h3><p>修改 &#x2F;etc&#x2F;yum.repos.d&#x2F;ceph.repo文件</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs raw">[ceph]<br>name=ceph<br>baseurl=http://mirrors.163.com/ceph/rpm-luminous/el7/x86_64/<br>gpgcheck=0<br>[ceph-noarch]<br>name=cephnoarch<br>baseurl=http://mirrors.163.com/ceph/rpm-luminous/el7/noarch/<br>gpgcheck=0<br></code></pre></td></tr></table></figure><p>添加完更新下缓存</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">yum makecache<br></code></pre></td></tr></table></figure><p>前一段时间163源上的ceph没有了，可能是误操作的，现在的163源已经恢复，上面添加的是最新的luminous版本源，本篇实践的功能是在这个版本才加入的</p><h3 id="安装ceph相关软件包"><a href="#安装ceph相关软件包" class="headerlink" title="安装ceph相关软件包"></a>安装ceph相关软件包</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]# yum install ceph-deploy ceph<br></code></pre></td></tr></table></figure><p>检查版本</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]# ceph -v<br>ceph version 12.1.0 (262617c9f16c55e863693258061c5b25dea5b086) luminous (dev)<br></code></pre></td></tr></table></figure><h3 id="搭建一个集群"><a href="#搭建一个集群" class="headerlink" title="搭建一个集群"></a>搭建一个集群</h3><p>这个就不描述配置集群的步骤，这个网上很多资料，也是很基础的操作<br>这里提几个luminous重要的变化</p><ul><li>默认的消息处理从simple变成了async了（ms_type &#x3D; async+posix）</li><li>默认的后端存储从filestore变成了bluestore了</li><li>ceph-s的命令的输出发生了改变(显示如下)</li></ul><p>添加mgr</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs raw">ceph-deploy mgr create lab8106<br>ceph mgr module enable dashboard<br></code></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ceph]# ceph -s<br>  cluster:<br>    id:     49ee8a7f-fb7c-4239-a4b7-acf0bc37430d<br>    health: HEALTH_OK<br> <br>  services:<br>    mon: 1 daemons, quorum lab8106<br>    mgr: lab8106(active)<br>    osd: 2 osds: 2 up, 2 in<br> <br>  data:<br>    pools:   1 pools, 64 pgs<br>    objects: 0 objects, 0 bytes<br>    usage:   2110 MB used, 556 GB / 558 GB avail<br>    pgs:     64 active+clean<br></code></pre></td></tr></table></figure><h3 id="开启监控模块"><a href="#开启监控模块" class="headerlink" title="开启监控模块"></a>开启监控模块</h3><p>在&#x2F;etc&#x2F;ceph&#x2F;ceph.conf中添加</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs raw">[mgr]<br>mgr modules = dashboard<br></code></pre></td></tr></table></figure><p>设置dashboard的ip和端口</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs raw">ceph config-key put mgr/dashboard/server_addr 192.168.8.106<br>ceph config-key put mgr/dashboard/server_port 7000<br></code></pre></td></tr></table></figure><p>这个从代码上看应该是可以支持配置文件方式的设置，目前还没看到具体的文档，先按这个设置即可，默认的端口是7000</p><p>重启mgr服务</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ceph]# systemctl restart ceph-mgr@lab8106<br></code></pre></td></tr></table></figure><p>检查端口</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ceph]# netstat -tunlp|grep 7000<br>tcp        0      0 192.168.8.106:7000      0.0.0.0:*               LISTEN      31485/ceph-mgr<br></code></pre></td></tr></table></figure><h3 id="访问界面"><a href="#访问界面" class="headerlink" title="访问界面"></a>访问界面</h3><p><img src="/images/blog/o_200901075847health.png" alt="dashboard"><br>这个是首页的信息</p><p><img src="/images/blog/o_200901075621server.png" alt="image.png-137.3kB"><br>这个是主机的相关信息</p><p><img src="/images/blog/o_200901075629osdstat.png" alt="servers"><br>这个界面是显示的osd相关的信息的</p><p><img src="/images/blog/o_200901075634rbd.png" alt="rbd"></p><p>rbd相关的信息</p><p><img src="/images/blog/o_200901075641filesystem.png" alt="filesystem"><br>文件系统相关的信息</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>从部署方便性来说，这个部署还是非常的方便的，而且走的是ceph原生接口，ceph通过增加一个mgr模块，可以把一些管理的功能独立出来，从而让mon自己做最重要的一些事情</p><p>目前的监控功能还比较少，主要是监控功能，未来应该会慢慢增加更多的功能，从产品角度来看，一个原生的UI监控使得ceph整个模块更加的完整了</p><p>有的时候也许 simple is the best</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>&#x2F;usr&#x2F;lib64&#x2F;ceph&#x2F;mgr&#x2F;dashboard&#x2F;README.rst</p><h2 id="补充"><a href="#补充" class="headerlink" title="补充"></a>补充</h2><p>目前还缺iscsi部分的，这个需要看下底层iscsi的实现方法</p><h2 id="变更记录"><a href="#变更记录" class="headerlink" title="变更记录"></a>变更记录</h2><table><thead><tr><th align="center">Why</th><th align="center">Who</th><th align="center">When</th></tr></thead><tbody><tr><td align="center">创建</td><td align="center">武汉-运维-磨渣</td><td align="center">2017-06-26</td></tr><tr><td align="center">更新最新版的</td><td align="center">武汉-运维-磨渣</td><td align="center">2017-08-29</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>调整PG分多次调整和一次到位的迁移差别分析</title>
    <link href="/2017/06/14/%E8%B0%83%E6%95%B4PG%E5%88%86%E5%A4%9A%E6%AC%A1%E8%B0%83%E6%95%B4%E5%92%8C%E4%B8%80%E6%AC%A1%E5%88%B0%E4%BD%8D%E7%9A%84%E8%BF%81%E7%A7%BB%E5%B7%AE%E5%88%AB%E5%88%86%E6%9E%90/"/>
    <url>/2017/06/14/%E8%B0%83%E6%95%B4PG%E5%88%86%E5%A4%9A%E6%AC%A1%E8%B0%83%E6%95%B4%E5%92%8C%E4%B8%80%E6%AC%A1%E5%88%B0%E4%BD%8D%E7%9A%84%E8%BF%81%E7%A7%BB%E5%B7%AE%E5%88%AB%E5%88%86%E6%9E%90/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>这个问题来源于我们研发的一个问题，在进行pg调整的时候，是一次调整到位好，还是分多次调整比较好，分多次调整的时候会不会出现某个pg反复挪动的问题，造成整体迁移量大于一次调整的</p><p>最近自己的项目上也有pg调整的需求，这个需求一般来源于pg规划好了，后期出现节点扩容的情况，需要对pg进行增加的调整</p><p>本篇用具体的数据来分析两种方式的差别</p><p>因为本篇的篇幅较长，直接先把结论拿出来</p><h2 id="数据结论"><a href="#数据结论" class="headerlink" title="数据结论"></a>数据结论</h2><table><thead><tr><th align="center">调整pg</th><th align="center">迁移pg</th><th align="center">迁移对象</th></tr></thead><tbody><tr><td align="center">1200-&gt;1440</td><td align="center">460</td><td align="center">27933</td></tr><tr><td align="center">1440-&gt;1680</td><td align="center">458</td><td align="center">27730</td></tr><tr><td align="center">1680-&gt;1920</td><td align="center">465</td><td align="center">27946</td></tr><tr><td align="center">1920-&gt;2160</td><td align="center">457</td><td align="center">21141</td></tr><tr><td align="center">2160-&gt;2400</td><td align="center">458</td><td align="center">13938</td></tr><tr><td align="center">总和</td><td align="center">2305</td><td align="center">132696</td></tr></tbody></table><table><thead><tr><th align="center">调整pg</th><th align="center">迁移pg</th><th align="center">迁移对象</th></tr></thead><tbody><tr><td align="center">1200-&gt;2400</td><td align="center">2299</td><td align="center">115361</td></tr></tbody></table><p>结论：<br>分多次调整的时候，PG迁移量比一次调整多了6个，多了0.2%，对象的迁移量多了17335，多了15%</p><p>从数据上看pg迁移的数目基本一样，但是数据量是多了15%，这个是因为分多次迁移的时候，在pg基数比较小的时候，迁移一个pg里面的对象要比后期分裂以后的对象要多，就产生了这个数据量的差别</p><p>从整体上来看二者需要迁移的pg基本差不多，数据量上面会增加15%，分多次的时候是可以进行周期性调整的，拆分到不同的时间段来做，所以各有好处</p><h2 id="实践"><a href="#实践" class="headerlink" title="实践"></a>实践</h2><h3 id="环境准备"><a href="#环境准备" class="headerlink" title="环境准备"></a>环境准备</h3><p>本次测试采用的是开发环境，使用开发环境可以很快的部署一个需要的环境，本次分析采用的就是一台机器模拟的4台机器48个 4T osd的环境</p><h4 id="环境搭建"><a href="#环境搭建" class="headerlink" title="环境搭建"></a>环境搭建</h4><p>生成集群</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">./vstart.sh -n  --mon_num 1 --osd_num 48 --mds_num 1 --short  -d<br></code></pre></td></tr></table></figure><p>后续操作都在源码的src目录下面执行</p><p>设置存储池副本为2</p><p>修改crush weight 为3.7模拟4T盘</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">seq 0 47| xargs -i ./ceph -c ceph.conf osd crush reweight osd.&#123;&#125; 3.8<br></code></pre></td></tr></table></figure><p>模拟主机分组</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs raw">seq 0 11 |xargs -i ./ceph -c ceph.conf osd crush set osd.&#123;&#125; 3.8 host=lab8106 root=default<br>seq 12 23 |xargs -i ./ceph -c ceph.conf osd crush set osd.&#123;&#125; 3.8 host=lab8107 root=default<br>seq 24 35 |xargs -i ./ceph -c ceph.conf osd crush set osd.&#123;&#125; 3.8 host=lab8108 root=default<br>seq 36 47 |xargs -i ./ceph -c ceph.conf osd crush set osd.&#123;&#125; 3.8 host=lab8109 root=default<br></code></pre></td></tr></table></figure><p>48个osd设置初始pg为1200，让每个osd上面差不多50个pg左右，做一下均衡操作，后续目标调整为pg为2400</p><p>准备120000个小文件准备put进去集群，使每个pg上面对象100个左右</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">./rados -c ceph.conf -p rbd bench -b 1K 600 write --no-cleanup<br></code></pre></td></tr></table></figure><h3 id="一次调整pg到2400"><a href="#一次调整pg到2400" class="headerlink" title="一次调整pg到2400"></a>一次调整pg到2400</h3><p>统计一次调整到位的情况下的数据迁移情况</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">./ceph  -c ceph.conf  osd pool set rbd pg_num 2400<br></code></pre></td></tr></table></figure><p>记录当前的pg分布的情况</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">./ceph -c ceph.conf pg dump pgs|awk &#x27;&#123;print $1,$2,$15,$17&#125;&#x27; &gt; pgmappg_1200_pgp_2400<br></code></pre></td></tr></table></figure><p>调整存储池的pgp为2400</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">./ceph -c ceph.conf osd pool set rbd  pgp_num 2400<br></code></pre></td></tr></table></figure><p>等迁移完成以后，统计最终的pg分布情况</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">./ceph -c ceph.conf pg dump pgs|awk &#x27;&#123;print $1,$2,$15,$17&#125;&#x27; &gt; pgmappg2400_pgp2400<br></code></pre></td></tr></table></figure><p>这里说明一下，调整pg的时候只会触发pg的分裂，并不会影响集群的分布，也就是不会出现pg迁移的情况，调整pgp以后才会去改变pg的分布，所以本次数据分析统计的是pgp变动后的迁移的数据量，这个量才是集群的真正的迁移量</p><p>用比较的脚本来进行统计（脚本会在本文文末提供）</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 src]#python compair.py pgmappg_1200_pgp_2400 pgmappg2400_pgp2400<br>| pgs | objects |<br>-----------------<br>[2299, 115361]<br></code></pre></td></tr></table></figure><p>也就是整个环境有2299次pg的变动，总共迁移的对象数目为115361个</p><h3 id="分五次调整到2400PG"><a href="#分五次调整到2400PG" class="headerlink" title="分五次调整到2400PG"></a>分五次调整到2400PG</h3><h4 id="初始pg为1200个第一次调整，1200PG调整到1440PG"><a href="#初始pg为1200个第一次调整，1200PG调整到1440PG" class="headerlink" title="初始pg为1200个第一次调整，1200PG调整到1440PG"></a>初始pg为1200个第一次调整，1200PG调整到1440PG</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">./ceph -c ceph.conf osd pool set rbd pg_num 1440<br></code></pre></td></tr></table></figure><p>调整pg为1440,当前pgp为1200<br>记录当前的pg分布数据</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">./ceph -c ceph.conf pg dump pgs|awk &#x27;&#123;print $1,$2,$15,$17&#125;&#x27; &gt; pgmappaira1<br></code></pre></td></tr></table></figure><p>调整pgp为1440,当前pg为1440<br>记录当前的pg分布数据</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">./ceph -c ceph.conf pg dump pgs|awk &#x27;&#123;print $1,$2,$15,$17&#125;&#x27; &gt; pgmappaira2<br></code></pre></td></tr></table></figure><p>统计第一次调整后的迁移量</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 pgdata]# python compair.py pgmappaira1 pgmappaira2<br>| pgs | objects |<br>-----------------<br>[460, 27933]<br></code></pre></td></tr></table></figure><h4 id="第二次调整，1440PG调整到1680PG"><a href="#第二次调整，1440PG调整到1680PG" class="headerlink" title="第二次调整，1440PG调整到1680PG"></a>第二次调整，1440PG调整到1680PG</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">./ceph -c ceph.conf osd pool set rbd pg_num 1680<br></code></pre></td></tr></table></figure><p>调整pg为1680,当前pgp为1440<br>记录当前的pg分布数据</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">./ceph -c ceph.conf pg dump pgs|awk &#x27;&#123;print $1,$2,$15,$17&#125;&#x27; &gt; pgmappairb1<br></code></pre></td></tr></table></figure><p>调整pgp为1680,当前pg为1680<br>记录当前的pg分布数据</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">./ceph -c ceph.conf pg dump pgs|awk &#x27;&#123;print $1,$2,$15,$17&#125;&#x27; &gt; pgmappairb2<br></code></pre></td></tr></table></figure><p>统计第二次调整后的迁移量</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 pgdata]# python compair.py pgmappairb1 pgmappairb2<br>| pgs | objects |<br>-----------------<br>[458, 27730]<br></code></pre></td></tr></table></figure><h4 id="第三次调整，1680PG调整到1920PG"><a href="#第三次调整，1680PG调整到1920PG" class="headerlink" title="第三次调整，1680PG调整到1920PG"></a>第三次调整，1680PG调整到1920PG</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">./ceph -c ceph.conf osd pool set rbd pg_num 1920<br></code></pre></td></tr></table></figure><p>调整pg为1920,当前pgp为1680<br>记录当前的pg分布数据</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">./ceph -c ceph.conf pg dump pgs|awk &#x27;&#123;print $1,$2,$15,$17&#125;&#x27; &gt; pgmappairc1<br></code></pre></td></tr></table></figure><p>调整pgp为1920,当前pg为1920<br>记录当前的pg分布数据</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">./ceph -c ceph.conf pg dump pgs|awk &#x27;&#123;print $1,$2,$15,$17&#125;&#x27; &gt; pgmappairc2<br></code></pre></td></tr></table></figure><p>统计第三次调整后的迁移量</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 pgdata]# python compair.py  pgmappairc1 pgmappairc2<br>| pgs | objects |<br>-----------------<br>[465, 27946]<br></code></pre></td></tr></table></figure><h4 id="第四次调整，1920PG调整到2160PG"><a href="#第四次调整，1920PG调整到2160PG" class="headerlink" title="第四次调整，1920PG调整到2160PG"></a>第四次调整，1920PG调整到2160PG</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">./ceph -c ceph.conf osd pool set rbd pg_num 2160<br></code></pre></td></tr></table></figure><p>调整pg为2160,当前pgp为1920<br>记录当前的pg分布数据</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">./ceph -c ceph.conf pg dump pgs|awk &#x27;&#123;print $1,$2,$15,$17&#125;&#x27; &gt; pgmappaird1<br></code></pre></td></tr></table></figure><p>调整pgp为2160,当前pg为2160<br>记录当前的pg分布数据</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">./ceph -c ceph.conf pg dump pgs|awk &#x27;&#123;print $1,$2,$15,$17&#125;&#x27; &gt; pgmappaird2<br></code></pre></td></tr></table></figure><p>统计第四次调整后的迁移量</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 pgdata]# python compair.py pgmappaird1 pgmappaird2<br>| pgs | objects |<br>-----------------<br>[457, 21141]<br></code></pre></td></tr></table></figure><h4 id="第五次调整，2160PG调整到2400PG"><a href="#第五次调整，2160PG调整到2400PG" class="headerlink" title="第五次调整，2160PG调整到2400PG"></a>第五次调整，2160PG调整到2400PG</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">./ceph -c ceph.conf osd pool set rbd pg_num 2400<br></code></pre></td></tr></table></figure><p>调整pg为2400,当前pgp为2160<br>记录当前的pg分布数据</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">./ceph -c ceph.conf pg dump pgs|awk &#x27;&#123;print $1,$2,$15,$17&#125;&#x27; &gt; pgmappaire1<br></code></pre></td></tr></table></figure><p>调整pgp为2400,当前pg为2400<br>记录当前的pg分布数据</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">./ceph -c ceph.conf pg dump pgs|awk &#x27;&#123;print $1,$2,$15,$17&#125;&#x27; &gt; pgmappaire2<br></code></pre></td></tr></table></figure><p>统计第五次调整后的迁移量</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 pgdata]# python compair.py pgmappaire1 pgmappaire2<br>| pgs | objects |<br>-----------------<br>[458, 13938]<br></code></pre></td></tr></table></figure><p>上面五次加起来的总量为<br>2305 PGS132696 objects</p><h2 id="统计的脚本"><a href="#统计的脚本" class="headerlink" title="统计的脚本"></a>统计的脚本</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><code class="hljs raw">#!/usr/bin/env python<br> # -*- coding: utf-8 -*-<br>__author__ =&quot;zp&quot;<br>import os,sys<br><br>class filetojson(object):<br>    def __init__(self,orin,new):<br>        self.origin=orin<br>        self.new=new<br><br>    def tojson(self,filename):<br>        data=&#123;&#125;<br>        pginfo=&#123;&#125;<br>        for line in open(filename):<br>            if &quot;pg_stat&quot; in line:<br>                continue<br>            lines=line.split()<br>            pg=lines[0]<br>            objects=lines[1]<br>            withosd=lines[2]<br><br>            data[pg]=&#123;&#x27;objects&#x27;:objects,&#x27;osd&#x27;:list(eval(withosd))&#125;<br>        return data<br><br>    def compare(self):<br>        movepg=0<br>        allmovepg=0<br>        allmoveobject=0<br>        moveobject=0<br>        oringinmap=self.tojson(self.origin)<br>        newmap=self.tojson(self.new)<br>        for key in oringinmap:<br>            amapn=set(oringinmap[key][&#x27;osd&#x27;])<br>            bmapn=set(newmap[key][&#x27;osd&#x27;])<br>            movepg=len(list(amapn.difference(bmapn)))<br>            if movepg != 0:<br>                moveobject=int(oringinmap[key][&#x27;objects&#x27;]) * int(movepg)<br>                allmovepg=allmovepg+movepg<br>                allmoveobject=allmoveobject+moveobject<br>        return [allmovepg,allmoveobject]<br><br>mycom=filetojson(sys.argv[1],sys.argv[2])<br>print &quot;| pgs | objects |&quot;<br>print &quot;-----------------&quot;<br>print mycom.compare()<br></code></pre></td></tr></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本篇是对集群进行pg调整的这个场景下迁移的数据进行分析的，对于一个集群来说，还是要用数据来进行问题的说明会比较有说服力，凭感觉还是没有那么强的说服力，本篇因为环境所限，所以在模拟的时候采用的是单个pg100个对象的样本，如果需要更精确的数据可以采用多次测试，并且加大这个单个pg的对象数目</p><h2 id="变更记录"><a href="#变更记录" class="headerlink" title="变更记录"></a>变更记录</h2><table><thead><tr><th align="center">Why</th><th align="center">Who</th><th align="center">When</th></tr></thead><tbody><tr><td align="center">创建</td><td align="center">武汉-运维-磨渣</td><td align="center">2017-06-14</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>使用日志系统graylog获取Ceph集群状态</title>
    <link href="/2017/06/09/%E4%BD%BF%E7%94%A8%E6%97%A5%E5%BF%97%E7%B3%BB%E7%BB%9Fgraylog%E8%8E%B7%E5%8F%96Ceph%E9%9B%86%E7%BE%A4%E7%8A%B6%E6%80%81/"/>
    <url>/2017/06/09/%E4%BD%BF%E7%94%A8%E6%97%A5%E5%BF%97%E7%B3%BB%E7%BB%9Fgraylog%E8%8E%B7%E5%8F%96Ceph%E9%9B%86%E7%BE%A4%E7%8A%B6%E6%80%81/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>在看集群的配置文件的时候看到ceph里面有一个graylog的输出选择，目前看到的是可以收集mon日志和clog，osd单个的日志没有看到，Elasticsearch有整套的日志收集系统，可以很方便的将所有日志汇总到一起，这个graylog的收集采用的是自有的udp协议，从配置上来说可以很快的完成，这里只做一个最基本的实践</p><h2 id="系统实践"><a href="#系统实践" class="headerlink" title="系统实践"></a>系统实践</h2><p>graylog日志系统主要由三个组件组成的</p><ul><li>MongoDB – 存储配置信息和一些元数据信息的，MongoDB (&gt;&#x3D; 2.4)</li><li>Elasticsearch – 用来存储Graylog server收取的log messages的，Elasticsearch (&gt;&#x3D; 2.x)</li><li>Graylog server – 用来解析日志的并且提供内置的web的访问接口</li></ul><p>配置好基础源文件</p><blockquote><p>CentOS-Base.repo<br><br>epel.repo</p></blockquote><h3 id="安装java"><a href="#安装java" class="headerlink" title="安装java"></a>安装java</h3><p>要求版本Java (&gt;&#x3D; 8)</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">yum install java-1.8.0-openjdk<br></code></pre></td></tr></table></figure><h3 id="安装MongoDB"><a href="#安装MongoDB" class="headerlink" title="安装MongoDB"></a>安装MongoDB</h3><p>安装软件</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">yum install mongodb mongodb-server<br></code></pre></td></tr></table></figure><p>启动服务并且加入自启动</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs raw">systemctl restart mongod<br>systemctl enable mongod<br></code></pre></td></tr></table></figure><p>安装完成检查服务启动端口</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab102 ~]# netstat -tunlp|grep 27017<br>tcp        0      0 127.0.0.1:27017         0.0.0.0:*               LISTEN      151840/mongod<br></code></pre></td></tr></table></figure><h3 id="安装Elasticsearch"><a href="#安装Elasticsearch" class="headerlink" title="安装Elasticsearch"></a>安装Elasticsearch</h3><p>导入认证文件</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">rpm --import https://packages.elastic.co/GPG-KEY-elasticsearch<br></code></pre></td></tr></table></figure><p>添加源文件</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs raw">vim /etc/yum.repos.d/elasticsearch.repo<br>添加<br>[elasticsearch-2.x]<br>name=Elasticsearch repository for 2.x packages<br>baseurl=https://packages.elastic.co/elasticsearch/2.x/centos<br>gpgcheck=1<br>gpgkey=https://packages.elastic.co/GPG-KEY-elasticsearch<br>enabled=1<br></code></pre></td></tr></table></figure><p>安装elasticsearch包</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">yum install elasticsearch<br></code></pre></td></tr></table></figure><p>配置自启动</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">systemctl enable elasticsearch<br></code></pre></td></tr></table></figure><p>修改配置文件</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs raw"># vim /etc/elasticsearch/elasticsearch.yml<br><br>cluster.name: graylog<br></code></pre></td></tr></table></figure><p>重启服务</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">systemctl restart  elasticsearch<br></code></pre></td></tr></table></figure><p>检查运行服务端口</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab102 ~]# netstat -tunlp|grep java<br>tcp        0      0 127.0.0.1:9200          0.0.0.0:*               LISTEN      154116/java <br>tcp        0      0 127.0.0.1:9300          0.0.0.0:*               LISTEN      154116/java <br></code></pre></td></tr></table></figure><p>检查elasticsearch状态</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab102 ~]#  curl -X GET http://localhost:9200<br>&#123;<br>  &quot;name&quot; : &quot;Vibro&quot;,<br>  &quot;cluster_name&quot; : &quot;graylog&quot;,<br>  &quot;cluster_uuid&quot; : &quot;11Y2GOTmQ9ynNbTlruFcyA&quot;,<br>  &quot;version&quot; : &#123;<br>    &quot;number&quot; : &quot;2.4.5&quot;,<br>    &quot;build_hash&quot; : &quot;c849dd13904f53e63e88efc33b2ceeda0b6a1276&quot;,<br>    &quot;build_timestamp&quot; : &quot;2017-04-24T16:18:17Z&quot;,<br>    &quot;build_snapshot&quot; : false,<br>    &quot;lucene_version&quot; : &quot;5.5.4&quot;<br>  &#125;,<br>  &quot;tagline&quot; : &quot;You Know, for Search&quot;<br>&#125;<br></code></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab102 ~]# curl -XGET &#x27;http://localhost:9200/_cluster/health?pretty=true&#x27;<br>&#123;<br>  &quot;cluster_name&quot; : &quot;graylog&quot;,<br>  &quot;status&quot; : &quot;green&quot;,<br></code></pre></td></tr></table></figure><p>状态应该是green</p><h3 id="安装graylog"><a href="#安装graylog" class="headerlink" title="安装graylog"></a>安装graylog</h3><p>安装源</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">rpm -Uvh https://packages.graylog2.org/repo/packages/graylog-2.2-repository_latest.rpm<br></code></pre></td></tr></table></figure><p>安装软件包</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">yum install graylog-server pwgen<br></code></pre></td></tr></table></figure><p>生成password_secret</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab102 ~]# pwgen -N 1 -s 96<br>DoqTYuvQPHaNW6XGFj5jru3FH8qxMjehj7Xk9OaVxhxaLYphF871CyiCMOKuAsHsJc0DtUUkK3ioFeqYo73mkMDUN7YklqgS<br></code></pre></td></tr></table></figure><p>在配置文件&#x2F;etc&#x2F;graylog&#x2F;server&#x2F;server.conf中password_secret填上上面的输出</p><p>生成root_password_sha2（后面生成的-不需要）</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab102 ~]# echo -n 123456 |shasum -a 256<br>8d969eef6ecad3c29a3a629280e686cf0c3f5d5a86aff3ca12020c923adc6c92  -<br></code></pre></td></tr></table></figure><p>123456是我设置的密码<br>在配置文件&#x2F;etc&#x2F;graylog&#x2F;server&#x2F;server.conf中root_password_sha2填上上面的输出</p><p>设置时区</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">root_timezone = Asia/Shanghai<br></code></pre></td></tr></table></figure><p>配置web监听端口</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs raw">rest_listen_uri = http://192.168.10.2:9000/api/<br>web_listen_uri = http://192.168.10.2:9000/<br></code></pre></td></tr></table></figure><p>这里注意写上你的web准备使用的那个网卡的IP地址，不要全局监听</p><p>启动服务并配置自启动</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab102 ~]# systemctl restart graylog-server<br>[root@lab102 ~]# systemctl enable graylog-server<br></code></pre></td></tr></table></figure><p>检查服务端口</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab102 ~]# netstat -tunlp|grep 9000<br>tcp        0      0 192.168.10.2:9000       0.0.0.0:*               LISTEN      160129/java  <br></code></pre></td></tr></table></figure><h3 id="使用web进行访问"><a href="#使用web进行访问" class="headerlink" title="使用web进行访问"></a>使用web进行访问</h3><p>使用地址<a href="http://192.168.10.2:9000进行访问">http://192.168.10.2:9000进行访问</a><br><img src="/images/blog/o_200901075143image1.png" alt="image.png-312kB"><br>用户名admin<br>密码123456</p><p><img src="/images/blog/o_200901075152image2.png" alt="image.png-69.9kB"><br>进来就是引导界面，这个地方是</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs">1、把日志发送到graylog<br>2、对收集到的数据做点搜索<br>3、创建一个图表<br>4、创建告警<br></code></pre></td></tr></table></figure><p>到这里配置graylog平台的基础工作就完成了，现在看下怎么跟ceph对接</p><p><img src="/images/blog/o_200901075200image3.png" alt="image.png-38.6kB"></p><h2 id="配置ceph的支持"><a href="#配置ceph的支持" class="headerlink" title="配置ceph的支持"></a>配置ceph的支持</h2><p>日志从ceph里面输出是采用的GELF UDP方式的</p><p>GELF is Graylog2 的json格式的数据，内部采用键值对的方式，ceoh内部传输出来的数据不光有message还有下面的</p><ul><li>hostname</li><li>thread id</li><li>priority</li><li>subsystem name and id</li><li>fsid</li></ul><p><img src="/images/blog/o_200901075207image4.png" alt="image.png-68.5kB"></p><p>选择GELF UDP协议 </p><p><img src="/images/blog/o_200901075222image5.png" alt="image.png-77.1kB"></p><p>选择节点，配置监听端口为12201，保存</p><p>在lab102上检查端口的监听情况</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab102 ~]# netstat -tunlp|grep 12201<br>udp        0      0 0.0.0.0:12201           0.0.0.0:*                           160129/java<br></code></pre></td></tr></table></figure><p>可以看到已经监听好了</p><p>修改ceph的配置文件</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs raw">#log_to_graylog = true<br>#err_to_graylog = true<br>#log_graylog_host = 192.168.10.2<br>#log_graylog_port = 12201<br>clog_to_graylog = true<br>clog_to_graylog_host = 192.168.10.2<br>clog_to_graylog_port = 12201<br>mon_cluster_log_to_graylog = true<br>mon_cluster_log_to_graylog_host = 192.168.10.2<br>mon_cluster_log_to_graylog_port = 12201<br></code></pre></td></tr></table></figure><p>ceph.conf当中跟graylog有关的就是这些配置文件了，配置好端口是刚刚监听的那个udp端口，然后重启ceph服务，这里我只需要mon_cluster日志和clog，这个根据自己的需要选择</p><p><img src="/images/blog/o_200901075232image6.png" alt="image.png-199.9kB"></p><p>可以看到ceph -w的输出都可以在这个里面查询了</p><h3 id="配置告警"><a href="#配置告警" class="headerlink" title="配置告警"></a>配置告警</h3><p><img src="/images/blog/o_200901075240image7.png" alt="image.png-128.7kB"><br>出现异常的时候<br><img src="/images/blog/o_200901075253image8.png" alt="image.png-62.2kB"></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>这个系统支持原生的接口接入，未来应该可以支持更多类型的日志倒入，这样相当于很容易就部署了一个日志搜索系统了，当然还有很多其他的方案，从功能完整性来说Elasticsearch要比这个强大，这套系统目前来看配置是非常的简单，也是一个优势</p><h2 id="变更记录"><a href="#变更记录" class="headerlink" title="变更记录"></a>变更记录</h2><table><thead><tr><th align="center">Why</th><th align="center">Who</th><th align="center">When</th></tr></thead><tbody><tr><td align="center">创建</td><td align="center">武汉-运维-磨渣</td><td align="center">2017-06-09</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Ceph部署mon出现0.0.0.0地址</title>
    <link href="/2017/06/06/Ceph%E9%83%A8%E7%BD%B2mon%E5%87%BA%E7%8E%B00.0.0.0%E5%9C%B0%E5%9D%80/"/>
    <url>/2017/06/06/Ceph%E9%83%A8%E7%BD%B2mon%E5%87%BA%E7%8E%B00.0.0.0%E5%9C%B0%E5%9D%80/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>最近在群里两次看到出现mon地址不对的问题，都是显示0.0.0.0:0地址，如下所示：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ceph]# ceph -s<br>    cluster 3137d009-e41e-41f0-b8f8-5cb574502572<br>     health HEALTH_ERR<br>            1 mons down, quorum 0,1,2 lab8106,node8107,lab104<br>     monmap e2: 4 mons at &#123;lab104=192.168.10.4:6789/0,lab8106=192.168.8.106:6789/0,lab8107=0.0.0.0:0/2,node8107=192.168.8.107:6789/0&#125;<br></code></pre></td></tr></table></figure><!--break--><p>这个之前偶尔会看到有出现这个问题，但是自己一直没碰到过，想看下是什么情况下触发的，在征得这个cepher的同意后，登录上他的环境检查了一下，发现是主机名引起的这个问题</p><h2 id="问题复现"><a href="#问题复现" class="headerlink" title="问题复现"></a>问题复现</h2><p>在部署的过程中，已经规划好了主机名，而又去修改了这个机器的主机名的情况下就会出现这个问题<br>比如我的这个机器，开始规划好lab8107主机名是这个，然后再lab8107上执行hostname node8107，就会触发这个问题</p><p>这个在deploy的部署输出日志中可以看得到 </p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs raw">[lab8107][WARNIN] ********************************************************************************<br>[lab8107][WARNIN] provided hostname must match remote hostname<br>[lab8107][WARNIN] provided hostname: lab8107<br>[lab8107][WARNIN] remote hostname: node8107<br>[lab8107][WARNIN] monitors may not reach quorum and create-keys will not complete<br>[lab8107][WARNIN] ********************************************************************************<br></code></pre></td></tr></table></figure><p>可以看到 provided hostname: lab8107 而remote hostname: node8107，就会出现这个问题了</p><p>如果下次出现这个问题，首先就检查下规划的mon的主机名与真实的主机名是否一致</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>新手在部署环境的时候，经常会犯一些比较基础的错误，这个是一个经验积累的过程，当然对于已经比较熟悉的cepher来说，也去尝试多看下各种异常问题，这个对于以后定位异常还是很有帮助的</p><h2 id="变更记录"><a href="#变更记录" class="headerlink" title="变更记录"></a>变更记录</h2><table><thead><tr><th align="center">Why</th><th align="center">Who</th><th align="center">When</th></tr></thead><tbody><tr><td align="center">创建</td><td align="center">武汉-运维-磨渣</td><td align="center">2017-06-06</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Centos7升级内核后无法启动解决办法</title>
    <link href="/2017/06/01/Centos7%E5%8D%87%E7%BA%A7%E5%86%85%E6%A0%B8%E5%90%8E%E6%97%A0%E6%B3%95%E5%90%AF%E5%8A%A8%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95/"/>
    <url>/2017/06/01/Centos7%E5%8D%87%E7%BA%A7%E5%86%85%E6%A0%B8%E5%90%8E%E6%97%A0%E6%B3%95%E5%90%AF%E5%8A%A8%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>这个问题存在有一段时间了，之前做的centos7的ISO，在进行内核的升级以后就存在这个问题：</p><ul><li>系统盘在板载sata口上是可以正常启动新内核并且能识别面板硬盘</li><li>系统盘插在面板口上新内核无法启动，调试发现无法找到系统盘</li><li>系统盘插在面板上默认的3.10内核可以正常启动</li></ul><p>暂时的解决办法就是让系统插在板载的sata口上，因为当时没找到具体的解决办法，在这个问题持续了一段时间后，最近再次搜索资料的时候，把问题定位在了initramfs内的驱动的问题，并且对问题进行了解决</p><h2 id="解决过程"><a href="#解决过程" class="headerlink" title="解决过程"></a>解决过程</h2><p>查询initramfs的驱动</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab103 lab103]<span class="hljs-comment"># lsinitrd -k 3.10.0-327.el7.x86_64|grep mpt[23]sas</span><br>drwxr-xr-x   2 root     root            0 Apr 17 12:05 usr/lib/modules/3.10.0-327.el7.x86_64/kernel/drivers/scsi/mpt2sas<br>-rw-r--r--   1 root     root       337793 Nov 20  2015 usr/lib/modules/3.10.0-327.el7.x86_64/kernel/drivers/scsi/mpt2sas/mpt2sas.ko<br></code></pre></td></tr></table></figure><p>可以看到在3.10内核的时候是mpt2sas驱动</p><p>可以在4.x内核中看到<br>新版的内核已经把mpt2sas升级为mpt3sas</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">/lib/modules/4.4.46/kernel/drivers/scsi/mpt3sas/mpt3sas.ko<br></code></pre></td></tr></table></figure><p>查询initramfs内的模块</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">lsinitrd -k  4.4.46|grep mpt[23]sas<br></code></pre></td></tr></table></figure><p>可以看到并没有输出，说明initramfs并没有把这个驱动打进去</p><p>这个地方有两种方式来解决</p><h3 id="方法一："><a href="#方法一：" class="headerlink" title="方法一："></a>方法一：</h3><p>修改 &#x2F;etc&#x2F;dracut.conf文件，增加字段</p><blockquote><p>add_drivers+&#x3D;” mpt3sas”</p></blockquote><p>注意增加的模块前后都要加上空格，解析的地方不加空格会解析不到模块名称会报错</p><p>重新生成initramfs</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">dracut -f /boot/initramfs-4.4.46.img 4.4.46<br></code></pre></td></tr></table></figure><h3 id="方法二："><a href="#方法二：" class="headerlink" title="方法二："></a>方法二：</h3><p>强制加载驱动</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">dracut --force --add-drivers mpt3sas --kver=4.4.46<br></code></pre></td></tr></table></figure><p>以上方法二选一做下驱动的集成，然后做下面的检查</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">lsinitrd -k  4.4.46|grep mpt[23]sas<br></code></pre></td></tr></table></figure><p>如果有输出就是正常了的</p><p>然后重启操作系统即可</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>目前出现这个问题的原因不清楚来自内核还是dracut生成的地方，如果遇到这个问题就按照上面的方法进行处理下即可，问题能找到解决办法后就会发现只是小问题，没找到的时候，完全不知道问题在哪里</p><h2 id="变更记录"><a href="#变更记录" class="headerlink" title="变更记录"></a>变更记录</h2><table><thead><tr><th align="center">Why</th><th align="center">Who</th><th align="center">When</th></tr></thead><tbody><tr><td align="center">创建</td><td align="center">武汉-运维-磨渣</td><td align="center">2017-06-01</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Freebsd10.2安装包升级pkg引起环境破坏的解决</title>
    <link href="/2017/05/24/Freebsd10.2%E5%AE%89%E8%A3%85%E5%8C%85%E5%8D%87%E7%BA%A7pkg%E5%BC%95%E8%B5%B7%E7%8E%AF%E5%A2%83%E7%A0%B4%E5%9D%8F%E7%9A%84%E8%A7%A3%E5%86%B3/"/>
    <url>/2017/05/24/Freebsd10.2%E5%AE%89%E8%A3%85%E5%8C%85%E5%8D%87%E7%BA%A7pkg%E5%BC%95%E8%B5%B7%E7%8E%AF%E5%A2%83%E7%A0%B4%E5%9D%8F%E7%9A%84%E8%A7%A3%E5%86%B3/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>freebsd10.2环境在安装一个新软件包的时候提示升级pkg到1.10.1，然后点击了升级，然后整个pkg环境就无法使用了</p><h2 id="记录"><a href="#记录" class="headerlink" title="记录"></a>记录</h2><p>升级完了软件包以后第一个错误提示</p><blockquote><p>FreeBSD: &#x2F;usr&#x2F;local&#x2F;lib&#x2F;libpkg.so.3: Undefined symbol “utimensat” </p></blockquote><p>这个是因为这个库是在freebsd的10.3当中才有的库，而我的环境是10.2的环境</p><h3 id="网上有一个解决办法"><a href="#网上有一个解决办法" class="headerlink" title="网上有一个解决办法"></a>网上有一个解决办法</h3><p>更新源</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs raw"># cat /usr/local/etc/pkg/repos/FreeBSD.conf<br>FreeBSD: &#123;<br>  url: &quot;pkg+http://pkg.FreeBSD.org/$&#123;ABI&#125;/release_2&quot;,<br>  enabled: yes<br>&#125;<br></code></pre></td></tr></table></figure><p>检查当前版本</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs raw"># pkg --version<br>1.10.1<br></code></pre></td></tr></table></figure><p>更新缓存</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw"># pkg update<br></code></pre></td></tr></table></figure><p>卸载</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw"># pkg delete -f pkg<br></code></pre></td></tr></table></figure><p>重新安装</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs raw"># pkg install -y pkg<br># pkg2ng<br></code></pre></td></tr></table></figure><p>检查版本</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs raw"># pkg --version<br>1.5.4<br></code></pre></td></tr></table></figure><p>这个在我的环境下没有生效</p><h3 id="还有一个办法"><a href="#还有一个办法" class="headerlink" title="还有一个办法"></a>还有一个办法</h3><p>有个pkg-static命令可以使用，，然后&#x2F;var&#x2F;cache&#x2F;pkg里边缓存的包。执行命令： </p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw"># pkg-static install -f /var/cache/pkg/pkg-1.5.4.txz<br></code></pre></td></tr></table></figure><p>这个在我的环境下报错</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs raw">root@mkiso:/usr/ports/ports-mgmt/pkg # pkg info sqlite3<br>pkg: warning: database version 34 is newer than libpkg(3) version 33, but still compatible<br>pkg: sqlite error while executing INSERT OR ROLLBACK INTO pkg_search(id, name, origin) VALUES (?1, ?2 || &#x27;-&#x27; || ?3, ?4); in file pkgdb.c:1544: no such table: pkg_search<br></code></pre></td></tr></table></figure><p>这个在网上看到有很多人出现了</p><h3 id="最终解决的办法"><a href="#最终解决的办法" class="headerlink" title="最终解决的办法"></a>最终解决的办法</h3><p>在邮件列表里面看到一个解决办法，我是用的这个办法解决了的</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">#pkg shell<br></code></pre></td></tr></table></figure><p>进入交互模式,执行下面的操作</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs raw">CREATE VIRTUAL TABLE pkg_search USING fts4(id, name, origin);<br>pragma user_version=33;<br></code></pre></td></tr></table></figure><p>执行完了以后pkg 环境可用了</p><h2 id="避免这个问题"><a href="#避免这个问题" class="headerlink" title="避免这个问题"></a>避免这个问题</h2><p>锁定本机的pkg版本</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">pkg lock pkg<br></code></pre></td></tr></table></figure><p>如果需要手动找包就是这个路径</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">http://pkg.freebsd.org/FreeBSD:10:amd64/<br></code></pre></td></tr></table></figure><p>我的机器最终版本是</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs raw">#pkg -v<br>1.8.7<br></code></pre></td></tr></table></figure><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="http://www.07net01.com/2017/02/1816847.html">freebsd pkg升级问题报错</a><br><a href="http://glasz.org/sheeplog/2017/02/freebsd-usrlocalliblibpkgso3-undefined-symbol-utimensat.html">FreeBSD: &#x2F;usr&#x2F;local&#x2F;lib&#x2F;libpkg.so.3: Undefined symbol “utimensat” </a><br><a href="http://bbs.chinaunix.net/thread-4260263-1-1.html">升级pkg失败, 安装低版本pkg失败</a><br><a href="https://lists.freebsd.org/pipermail/freebsd-ports/2017-January/106799.html">pkg database issue: database version 34 is newer than libpkg(3) version 33 ?</a></p><h2 id="变更记录"><a href="#变更记录" class="headerlink" title="变更记录"></a>变更记录</h2><table><thead><tr><th align="center">Why</th><th align="center">Who</th><th align="center">When</th></tr></thead><tbody><tr><td align="center">创建</td><td align="center">武汉-运维-磨渣</td><td align="center">2017-05-24</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>多MDS变成单MDS的方法</title>
    <link href="/2017/05/03/%E5%A4%9AMDS%E5%8F%98%E6%88%90%E5%8D%95MDS%E7%9A%84%E6%96%B9%E6%B3%95/"/>
    <url>/2017/05/03/%E5%A4%9AMDS%E5%8F%98%E6%88%90%E5%8D%95MDS%E7%9A%84%E6%96%B9%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>之前有个cepher的环境上是双活MDS的，需要变成MDS，目前最新版本是支持这个操作的</p><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><h3 id="设置最大mds"><a href="#设置最大mds" class="headerlink" title="设置最大mds"></a>设置最大mds</h3><p>多活的mds的max_mds会超过1，这里需要先将max_mds设置为1</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">ceph mds <span class="hljs-built_in">set</span> max_mds 1<br></code></pre></td></tr></table></figure><h3 id="deactive-mds"><a href="#deactive-mds" class="headerlink" title="deactive mds"></a>deactive mds</h3><p>看下需要停掉的mds是rank 0 还是rank1,然后执行下面的命令即可</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@server8 ~]<span class="hljs-comment"># ceph -s|grep mdsmap</span><br>     mdsmap e13: 1/1/1 up &#123;0=lab8106=up:clientreplay&#125;<br></code></pre></td></tr></table></figure><p>这个输出的lab8106前面的0，就是这个mds的rank，根据需要停止对应的rank</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">ceph mds deactivate 1<br></code></pre></td></tr></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>不建议用多活mds</p><h2 id="变更记录"><a href="#变更记录" class="headerlink" title="变更记录"></a>变更记录</h2><table><thead><tr><th align="center">Why</th><th align="center">Who</th><th align="center">When</th></tr></thead><tbody><tr><td align="center">创建</td><td align="center">武汉-运维-磨渣</td><td align="center">2017-05-03</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Ceph OSD从filestore 转换到 bluestore的方法</title>
    <link href="/2017/05/03/Ceph%20OSD%E4%BB%8Efilestore%20%E8%BD%AC%E6%8D%A2%E5%88%B0%20bluestore%E7%9A%84%E6%96%B9%E6%B3%95/"/>
    <url>/2017/05/03/Ceph%20OSD%E4%BB%8Efilestore%20%E8%BD%AC%E6%8D%A2%E5%88%B0%20bluestore%E7%9A%84%E6%96%B9%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>前段时间看到<a href="https://mp.weixin.qq.com/s?__biz=MzI0NDE0NjUxMQ==&mid=2651256389&idx=1&sn=e11edcce5722853f442b9a7b8211787e&chksm=f2901e65c5e79773c7690f29e35dbd1870a5bfdb92c70541979f5d080d6580e3af9ba85fff66&mpshare=1&scene=23&srcid=0502SazrSPsWnszP3xfdEId4#rd">豪迈的公众号</a>上提到了这个离线转换工具，最近看到群里有人问，找了下没什么相关文档，就自己写了一个，供参考</p><h2 id="实践步骤"><a href="#实践步骤" class="headerlink" title="实践步骤"></a>实践步骤</h2><h3 id="获取代码并安装"><a href="#获取代码并安装" class="headerlink" title="获取代码并安装"></a>获取代码并安装</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">git <span class="hljs-built_in">clone</span> https://github.com/ceph/ceph.git<br><span class="hljs-built_in">cd</span> ceph<br>git submodule update --init --recursive<br>./make-dist<br>rpm -bb ceph.spec<br></code></pre></td></tr></table></figure><p>生成rpm安装包后进行安装,这个过程就不讲太多，根据各种文档安装上最新的版本即可，这个代码合进去时间并不久，大概是上个月才合进去的</p><h3 id="配置集群"><a href="#配置集群" class="headerlink" title="配置集群"></a>配置集群</h3><p>首先配置一个filestore的集群，这个也是很简单的，我的环境配置一个单主机三个OSD的集群</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ceph]<span class="hljs-comment"># ceph -s</span><br>    cluster 3daaf51a-eeba-43a6-9f58-c26c5796f928<br>     health HEALTH_WARN<br>            mon.lab8106 low disk space<br>     monmap e2: 1 mons at &#123;lab8106=192.168.8.106:6789/0&#125;<br>            election epoch 4, quorum 0 lab8106<br>        mgr active: lab8106 <br>     osdmap e16: 3 osds: 3 up, 3 <span class="hljs-keyword">in</span><br>      pgmap v34: 64 pgs, 1 pools, 0 bytes data, 0 objects<br>            323 MB used, 822 GB / 822 GB avail<br>                  64 active+clean<br>[root@lab8106 ceph]<span class="hljs-comment"># ceph osd tree</span><br>ID WEIGHT  TYPE NAME        UP/DOWN REWEIGHT PRIMARY-AFFINITY <br>-1 0.80338 root default                                       <br>-2 0.80338     host lab8106                                   <br> 0 0.26779         osd.0         up  1.00000          1.00000 <br> 1 0.26779         osd.1         up  1.00000          1.00000 <br> 2 0.26779         osd.2         up  1.00000          1.00000<br></code></pre></td></tr></table></figure><h3 id="写入少量数据"><a href="#写入少量数据" class="headerlink" title="写入少量数据"></a>写入少量数据</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ~]<span class="hljs-comment"># rados -p rbd bench 10 write --no-cleanup</span><br></code></pre></td></tr></table></figure><h3 id="设置noout"><a href="#设置noout" class="headerlink" title="设置noout"></a>设置noout</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ~]<span class="hljs-comment"># ceph osd set noout</span><br>noout is <span class="hljs-built_in">set</span><br></code></pre></td></tr></table></figure><h3 id="停止OSD-0"><a href="#停止OSD-0" class="headerlink" title="停止OSD.0"></a>停止OSD.0</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ~]<span class="hljs-comment"># systemctl stop ceph-osd@0</span><br>[root@lab8106 ~]<span class="hljs-comment"># ceph osd down 0</span><br>osd.0 is already down.<br></code></pre></td></tr></table></figure><p>将数据换个目录挂载，换个新盘挂载到原路径</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ~]<span class="hljs-comment"># mkdir /var/lib/ceph/osd/ceph-0.old/</span><br>[root@lab8106 ~]<span class="hljs-comment"># umount /var/lib/ceph/osd/ceph-0</span><br>[root@lab8106 ~]<span class="hljs-comment"># mount /dev/sdb1 /var/lib/ceph/osd/ceph-0.old/</span><br>[root@lab8106 ~]<span class="hljs-comment"># mount /dev/sde1 /var/lib/ceph/osd/ceph-0/</span><br><br>[root@lab8106 ~]<span class="hljs-comment"># df -h|grep osd</span><br>/dev/sdc1       275G  833M  274G   1% /var/lib/ceph/osd/ceph-1<br>/dev/sdd1       275G  833M  274G   1% /var/lib/ceph/osd/ceph-2<br>/dev/sdb1       275G  759M  274G   1% /var/lib/ceph/osd/ceph-0.old<br>/dev/sde1       280G   33M  280G   1% /var/lib/ceph/osd/ceph-0<br></code></pre></td></tr></table></figure><p>在配置文件&#x2F;etc&#x2F;ceph&#x2F;ceph.conf中添加</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">enable_experimental_unrecoverable_data_corrupting_features = bluestore<br></code></pre></td></tr></table></figure><p>如果需要指定osd的block的路径需要写配置文件<br>在做<code>ceph-objectstore-tool --type bluestore --data-path  --op mkfs</code>这个操作之前，在配置文件的全局里面添加上</p><blockquote><p>bluestore_block_path &#x3D; &#x2F;dev&#x2F;sde2</p></blockquote><p>然后再创建的时候就可以是链接到设备了，这个地方写全局变量，然后创建完了后就删除掉这项配置文件，写单独的配置文件的时候发现没读取成功,生成后应该是这样的</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ceph]<span class="hljs-comment"># ll /var/lib/ceph/osd/ceph-0</span><br>total 20<br>lrwxrwxrwx 1 root root  9 May  3 17:40 block -&gt; /dev/sde2<br>-rw-r--r-- 1 root root  2 May  3 17:40 bluefs<br>-rw-r--r-- 1 root root 37 May  3 17:40 fsid<br>-rw-r--r-- 1 root root  8 May  3 17:40 kv_backend<br>-rw-r--r-- 1 root root  4 May  3 17:40 mkfs_done<br>-rw-r--r-- 1 root root 10 May  3 17:40 <span class="hljs-built_in">type</span><br></code></pre></td></tr></table></figure><p>如果不增加这个就是以文件形式的存在</p><h3 id="获取osd-0的fsid"><a href="#获取osd-0的fsid" class="headerlink" title="获取osd.0的fsid"></a>获取osd.0的fsid</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ~]<span class="hljs-comment"># cat /var/lib/ceph/osd/ceph-0.old/fsid </span><br>b2f73450-5c4a-45fb-9c24-8218a5803434<br></code></pre></td></tr></table></figure><h3 id="创建一个bluestore的osd-0"><a href="#创建一个bluestore的osd-0" class="headerlink" title="创建一个bluestore的osd.0"></a>创建一个bluestore的osd.0</h3><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs css"><span class="hljs-selector-attr">[root@lab8106 ~]</span># ceph-objectstore-tool <span class="hljs-attr">--type</span> bluestore <span class="hljs-attr">--data-path</span> /<span class="hljs-selector-tag">var</span>/lib/ceph/osd/ceph-<span class="hljs-number">0</span> <span class="hljs-attr">--fsid</span> b2f73450-<span class="hljs-number">5</span>c4a-<span class="hljs-number">45</span>fb-<span class="hljs-number">9</span>c24-<span class="hljs-number">8218</span>a5803434 <span class="hljs-attr">--op</span> mkfs<br></code></pre></td></tr></table></figure><h3 id="转移数据"><a href="#转移数据" class="headerlink" title="转移数据"></a>转移数据</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ~]<span class="hljs-comment"># ceph-objectstore-tool --data-path /var/lib/ceph/osd/ceph-0.old --target-data-path /var/lib/ceph/osd/ceph-0 --op dup</span><br>[root@lab8106 ~]<span class="hljs-comment"># chown -R ceph:ceph /var/lib/ceph/osd/ceph-0</span><br></code></pre></td></tr></table></figure><p>这个操作是将之前的filestore的数据转移到新的bluestore上了</p><h3 id="启动OSD-0"><a href="#启动OSD-0" class="headerlink" title="启动OSD.0"></a>启动OSD.0</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 osd]<span class="hljs-comment"># systemctl restart ceph-osd@0</span><br></code></pre></td></tr></table></figure><p>检查状态</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 osd]<span class="hljs-comment"># ceph -s</span><br>2017-05-03 17:05:13.119492 7f20a501b700 -1 WARNING: the following dangerous and experimental features are enabled: bluestore<br>2017-05-03 17:05:13.150181 7f20a501b700 -1 WARNING: the following dangerous and experimental features are enabled: bluestore<br>    cluster 3daaf51a-eeba-43a6-9f58-c26c5796f928<br>     health HEALTH_WARN<br>            noout flag(s) <span class="hljs-built_in">set</span><br>            mon.lab8106 low disk space<br>     monmap e2: 1 mons at &#123;lab8106=192.168.8.106:6789/0&#125;<br>            election epoch 4, quorum 0 lab8106<br>        mgr active: lab8106 <br>     osdmap e25: 3 osds: 3 up, 3 <span class="hljs-keyword">in</span><br>            flags noout<br>      pgmap v80: 64 pgs, 1 pools, 724 MB data, 182 objects<br>            3431 MB used, 555 GB / 558 GB avail<br>                  64 active+clean<br></code></pre></td></tr></table></figure><p>成功转移</p><h3 id="不同的block方式"><a href="#不同的block方式" class="headerlink" title="不同的block方式"></a>不同的block方式</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ceph]<span class="hljs-comment"># ll /var/lib/ceph/osd/ceph-0/ -al|grep block</span><br>-rw-r--r--  1 ceph ceph 10737418240 May  3 17:32 block<br>[root@lab8106 ceph]<span class="hljs-comment"># ll /var/lib/ceph/osd/ceph-4/ -al|grep block</span><br>lrwxrwxrwx  1 ceph ceph  58 May  3 17:16 block -&gt; /dev/disk/by-partuuid/846e93a2-0f6d-47d4-8a90-85ab3cf4ec4e<br>-rw-r--r--  1 ceph ceph  37 May  3 17:16 block_uuid<br></code></pre></td></tr></table></figure><p>可以看到直接创建的时候的block是以链接的方式链接到一个分区的，而不改配置文件的转移的方式里面是一个文件的形式，根据需要进行选择</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>转移工具的出现方便了以后从filestore到bluestore的转移，可以采取一个个osd的转移方式将整个集群进行转移，而免去了剔除osd，再添加的方式，减少了迁移量，可以一个个的离线进行操作</p><p>ceph的工具集越来越完整了</p><h2 id="变更记录"><a href="#变更记录" class="headerlink" title="变更记录"></a>变更记录</h2><table><thead><tr><th align="center">Why</th><th align="center">Who</th><th align="center">When</th></tr></thead><tbody><tr><td align="center">创建</td><td align="center">武汉-运维-磨渣</td><td align="center">2017-05-03</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>mdsjournalFAILEDassert解决</title>
    <link href="/2017/04/27/mdsjournalFAILEDassert%E8%A7%A3%E5%86%B3/"/>
    <url>/2017/04/27/mdsjournalFAILEDassert%E8%A7%A3%E5%86%B3/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>在处理一个其他双活MDS无法启动环境的时候，查看mds的日志看到了这个错误mds&#x2F;journal.cc: 2929: FAILED assert(mds-&gt;sessionmap.get_version() &#x3D;&#x3D; cmapv)，在查询资料以后，暂时得到了解决,在生产环境下还是不建议使用双活MDS</p><h2 id="处理步骤"><a href="#处理步骤" class="headerlink" title="处理步骤"></a>处理步骤</h2><p>这个是双MDS多活情况下出现的一个问题，在什么情况下出现还无法判断，目前只看到是有这个问题，并且有其他人也出现了 <a href="http://tracker.ceph.com/issues/17113">issue17113</a><br>按照<a href="http://docs.ceph.com/docs/master/cephfs/disaster-recovery/">disaster-recovery</a>建议的步骤做了如下处理：</p><h3 id="备份下journal"><a href="#备份下journal" class="headerlink" title="备份下journal"></a>备份下journal</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">cephfs-journal-tool journal export backup.bin<br></code></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs raw">cephfs-journal-tool journal reset<br>cephfs-table-tool all reset session<br></code></pre></td></tr></table></figure><p>做了上两步后环境并没有恢复,还有个下面的操作没有做，这个操作会引起数据的丢失， MDS ranks other than 0 will be ignored: as a result it is possible for this to result in data loss，所以暂缓操作</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">ceph fs reset &lt;fs name&gt; --yes-i-really-mean-it<br></code></pre></td></tr></table></figure><p>再次启动后还是，看到日志提示的是sessionmap的问题，正常情况下这个地方重置了session应该是可以好的</p><p>Yan, Zheng 2014年的时候在<a href="https://www.mail-archive.com/ceph-devel@vger.kernel.org/msg18629.html">邮件列表</a>里面提过一个配置</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">mds wipe_sessions = 1<br></code></pre></td></tr></table></figure><p>当时解决一个replay的问题，尝试加入这个参数，然后启动mds</p><p>环境恢复了变成了双active，提示还有damage，但是数据属于可访问了</p><h3 id="后续操作"><a href="#后续操作" class="headerlink" title="后续操作"></a>后续操作</h3><p>建议是导出数据，重新配置为主备MDS集群，然后倒入数据</p><h2 id="变更记录"><a href="#变更记录" class="headerlink" title="变更记录"></a>变更记录</h2><table><thead><tr><th align="center">Why</th><th align="center">Who</th><th align="center">When</th></tr></thead><tbody><tr><td align="center">创建</td><td align="center">武汉-运维-磨渣</td><td align="center">2017-04-27</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Ceph根据Crush位置读取数据</title>
    <link href="/2017/04/27/Ceph%E6%A0%B9%E6%8D%AECrush%E4%BD%8D%E7%BD%AE%E8%AF%BB%E5%8F%96%E6%95%B0%E6%8D%AE/"/>
    <url>/2017/04/27/Ceph%E6%A0%B9%E6%8D%AECrush%E4%BD%8D%E7%BD%AE%E8%AF%BB%E5%8F%96%E6%95%B0%E6%8D%AE/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>在ceph研发群里面看到一个cepher在问关于怎么读取ceph的副本的问题，这个功能应该在2012年的时候,我们公司的研发就修改了代码去实现这个功能，只是当时的硬件条件所限，以及本身的稳定性问题，后来没有在生产当中使用</p><p>我们都知道ceph在写数据的时候，是先写主本，然后去写副本，而读取的时候，实际上只有主本能够提供服务，这对于磁盘的整体带宽来说，并没有充分的发挥其性能，所以能够读取副本当然是会有很大好处的，特别是对于读场景比较多的情况</p><p>那么在ceph当中是不是有这个功能呢？其实是有的，这个地方ceph更往上走了一层，是基于crush定义的地址去进行文件的读取，这样在读取的客户端眼里，就没有什么主副之分，他会按自己想要的区域去尽量读取，当然这个区域没有的时候就按正常读取就可以了</p><h2 id="实践"><a href="#实践" class="headerlink" title="实践"></a>实践</h2><p>如果你看过关于ceph hadoop的相关配置文档，应该会看到这么一个配置</p><blockquote><p>ceph.localize.reads<br><br>Allow reading from file replica objects<br><br>Default value: true</p></blockquote><p>显示的是可以从非主本去读取对象，这个对于hadoop场景肯定是越近越好的，可以在ceph的代码里面搜索下 localize-reads<br><a href="https://github.com/ceph/ceph/blob/master/src/ceph_fuse.cc">https://github.com/ceph/ceph/blob/master/src/ceph_fuse.cc</a></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs raw">for (std::vector&lt;const char*&gt;::iterator i = args.begin(); i != args.end(); ) &#123;<br>  if (ceph_argparse_double_dash(args, i)) &#123;<br>    break;<br>  &#125; else if (ceph_argparse_flag(args, i, &quot;--localize-reads&quot;, (char*)NULL)) &#123;<br>    cerr &lt;&lt; &quot;setting CEPH_OSD_FLAG_LOCALIZE_READS&quot; &lt;&lt; std::endl;<br>    filer_flags |= CEPH_OSD_FLAG_LOCALIZE_READS;<br>  &#125; else if (ceph_argparse_flag(args, i, &quot;-h&quot;, &quot;--help&quot;, (char*)NULL)) &#123;<br>    usage();<br>  &#125; else &#123;<br>    ++i;<br>  &#125;<br>&#125;<br></code></pre></td></tr></table></figure><p>可以看到在ceph-fuse的情况下，是有这个隐藏的一个参数的，本篇就是用这个隐藏的参数来进行实践</p><h3 id="配置一个两节点集群"><a href="#配置一个两节点集群" class="headerlink" title="配置一个两节点集群"></a>配置一个两节点集群</h3><p>配置完成了以后ceph的目录树如下,mon部署在lab8106上面</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8107 ~]# ceph osd tree<br>ID WEIGHT  TYPE NAME        UP/DOWN REWEIGHT PRIMARY-AFFINITY <br>-1 1.07336 root default                                       <br>-2 0.53778     host lab8106                                   <br> 1 0.26779         osd.1         up  1.00000          1.00000 <br> 0 0.26999         osd.0         up  1.00000          1.00000 <br>-3 0.53558     host lab8107                                   <br> 2 0.26779         osd.2         up  1.00000          1.00000 <br> 3 0.26779         osd.3         up  1.00000          1.00000<br>[root@lab8107 ~]# ceph -s|grep mon<br>monmap e1: 1 mons at &#123;lab8106=192.168.8.106:6789/0&#125;<br></code></pre></td></tr></table></figure><h3 id="在lab8107上挂载客户端"><a href="#在lab8107上挂载客户端" class="headerlink" title="在lab8107上挂载客户端"></a>在lab8107上挂载客户端</h3><p>在&#x2F;etc&#x2F;ceph&#x2F;ceph.conf中增加一个配置</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs raw">[client]<br>crush_location = &quot;host=lab8107 root=default&quot;<br></code></pre></td></tr></table></figure><p>这个配置的作用是告诉这个客户端尽量去读取lab8107上面的对象</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8107 ~]# ceph-fuse -m lab8106:6789 /mnt  --localize-reads<br></code></pre></td></tr></table></figure><p>写入一个大文件</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8107 ~]# dd if=/dev/zero of=a bs=4M count=4000<br></code></pre></td></tr></table></figure><p>在lab8106和lab8107上监控磁盘</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8107 ~]# iostat -dm 1<br></code></pre></td></tr></table></figure><p>读取数据</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8107 ~]# dd if=a of=/dev/null<br></code></pre></td></tr></table></figure><p>可以看到只有lab8107上有磁盘的读取，也就是读取的数据里面肯定也有副本，都是从lab8107上面读取了</p><p>如果需要多次测试，需要清除下缓存</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">sync; echo 3 &gt; /proc/sys/vm/drop_caches<br></code></pre></td></tr></table></figure><p>并且重新挂载客户端，这个读取crush的位置的操作是在mount的时候读取的</p><h2 id="使用场景"><a href="#使用场景" class="headerlink" title="使用场景"></a>使用场景</h2><p>上面的配置是可以指定多个平级的位置的</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs raw">[client]<br>crush_location = &quot;host=lab8106 host=lab8107 root=default&quot;<br></code></pre></td></tr></table></figure><p>这样，在一些读请求很多的场景下，可以把整个后端按逻辑上划分为一个个的区域，然后前面的客户端就可以平级分配到这些区域当中，这样就可以比较大的限度去把副本的读取也调动起来的</p><p>目前在ceph-fuse上已经实现，rbd里面也有类似的一些处理，这个是一个很不错的功能</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>ceph里面有很多可配置的东西，怎么用好它，最大限度的去适配使用场景，还是有很大的可调的空间的，所谓学无止境，我也在学习python coding了，有很多想法等着去实现</p><h2 id="变更记录"><a href="#变更记录" class="headerlink" title="变更记录"></a>变更记录</h2><table><thead><tr><th align="center">Why</th><th align="center">Who</th><th align="center">When</th></tr></thead><tbody><tr><td align="center">创建</td><td align="center">武汉-运维-磨渣</td><td align="center">2017-04-27</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>关于backfill参数建议</title>
    <link href="/2017/04/27/%E5%85%B3%E4%BA%8Ebackfill%E5%8F%82%E6%95%B0%E5%BB%BA%E8%AE%AE/"/>
    <url>/2017/04/27/%E5%85%B3%E4%BA%8Ebackfill%E5%8F%82%E6%95%B0%E5%BB%BA%E8%AE%AE/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>在做一个比较满的集群的扩容的时候，遇到了一些问题，在这里做下总结，一般来说很难遇到，扩容要趁早，不然出的问题都是稀奇古怪的一些问题</p><h2 id="建议"><a href="#建议" class="headerlink" title="建议"></a>建议</h2><p>环境一般来说在70%左右就需要考虑扩容了，这个时候的扩容数据迁移的少，遇到的问题自然会少很多，所谓的参数设置并不是一个单纯的参数的设置，所以一般来说在调优参数的时候，个人觉得只有适配硬件进行调优，所以本篇的参数同样是一个组合形式的</p><p>首先罗列出本篇涉及的所有参数</p><blockquote><p>mon_osd_full_ratio &#x3D; 0.95<br><br>sd_backfill_full_ratio &#x3D; 0.85<br><br>sd_max_backfills &#x3D; 1</p></blockquote><p>最少的OSD的PG数目</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">min_pg=`ceph osd df|awk &#x27;&#123;print $9&#125;&#x27;|awk &#x27;NF&#x27;|grep -v PGS|sort -n|head -n 1`<br></code></pre></td></tr></table></figure><p>那么最好满足</p><blockquote><p>(osd_max_backfills&#x2F;min_pg)+osd_backfill_full_ratio &lt; mon_osd_full_ratio</p></blockquote><p>这个在老版本里面进行backfill full的检测的时候，只在启动backfill的时候做了检测，如果设置的backfill足够大，而迁移的又足够多的时候，就会一下涌过去，直径把OSD给弄full然后挂掉了，新版本还没验证是否做了实时控制，但是如果遵循了上面的设置，即使没控制一样不会出问题</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>有的参数不光对速度有控制，对量上面同样可能有影响，所以在设置的时候，需要尽量综合考虑</p><h2 id="变更记录"><a href="#变更记录" class="headerlink" title="变更记录"></a>变更记录</h2><table><thead><tr><th align="center">Why</th><th align="center">Who</th><th align="center">When</th></tr></thead><tbody><tr><td align="center">创建</td><td align="center">武汉-运维-磨渣</td><td align="center">2017-04-27</td></tr><tr><td align="center">处理取min_pg字符串比较问题</td><td align="center">武汉-运维-磨渣</td><td align="center">2018-02-24</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>rados put striper功能的调试</title>
    <link href="/2017/04/26/rados%20put%20striper%E5%8A%9F%E8%83%BD%E7%9A%84%E8%B0%83%E8%AF%95/"/>
    <url>/2017/04/26/rados%20put%20striper%E5%8A%9F%E8%83%BD%E7%9A%84%E8%B0%83%E8%AF%95/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>之前对于striper这个地方的功能并没研究太多，只是知道这个里面可以以条带方式并行的去写对象，从而加大并发性来提高性能，而默认的条带数目为1，也就是以对象大小去写，并没有条带，所以不是很好感觉到差别，今天就尝试下用rados命令来看下这个条带是怎么回事</p><!--break--><h2 id="实践过程"><a href="#实践过程" class="headerlink" title="实践过程"></a>实践过程</h2><p>最开始我的集群是用rpm包进行安装的，这个可以做一些常规的测试，如果需要改动一些代码的话，就比较麻烦了，本文后面会讲述怎么改动一点点代码，然后进行测试</p><p>我们一般来说用rados put操作就是一个完整的文件，并不会进行拆分，我们尝试下看下</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]# dd if=/dev/zero of=16M bs=4M count=4<br>[root@lab8106 ~]# rados  -p rbd put 16M 16M<br>[root@lab8106 ~]# rados  -p rbd stat 16M<br>rbd/16M mtime 2017-04-26 15:08:14.000000, size 16777216<br></code></pre></td></tr></table></figure><p>可以看到我们put 16M的文件，在后台就是一个16M的对象</p><p>这个rados命令还有个参数是striper</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]# rados  --help|grep stri<br>   --striper<br>        Use radostriper interface rather than pure rados<br></code></pre></td></tr></table></figure><p>我们来用这个命令试一下</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]# dd if=/dev/zero of=strip16M bs=4M count=4<br>[root@lab8106 ~]# rados  -p rbd put strip16M strip16M --striper<br>[root@lab8106 ~]# rados  -p rbd ls |grep strip<br>strip16M.0000000000000002<br>strip16M.0000000000000003<br>strip16M.0000000000000000<br>strip16M.0000000000000001<br>[root@lab8106 ~]# rados  -p rbd  --striper ls |grep strip<br>strip16M<br>[root@lab8106 ~]#  rados  -p rbd stat strip16M.0000000000000002<br>rbd/strip16M.0000000000000002 mtime 2017-04-26 15:11:06.000000, size 4194304<br></code></pre></td></tr></table></figure><p>可以看到这个16M的文件是被拆分成了4M一个的对象，存储到了后台的,我们开启下日志后看下有没有什么详细的信息，因为在rados参数当中确实没有找到可配置的选项<br>在&#x2F;etc&#x2F;ceph&#x2F;ceph.conf当中添加</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs raw">debug_rados=20<br>debug_striper=20<br></code></pre></td></tr></table></figure><p>再次测试</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]# dd if=/dev/zero of=strip116M bs=4M count=4<br>[root@lab8106 ~]# rados  -p rbd put strip116M strip116M --striper<br>···<br>sc is one, reset su to os<br>su 4194304 sc 1 os 4194304 stripes_per_object 1<br>···<br></code></pre></td></tr></table></figure><p>这个地方解释下意思</p><blockquote><p>strip count is 1,重置strip unit为object size ，也就是4M<br>strip unit 4194304 ，strip count 1，object size 4194304,每个对象的条带为1</p></blockquote><p>这个代码里面写了<br><a href="https://github.com/ceph/ceph/blob/master/src/tools/rados/rados.cc">https://github.com/ceph/ceph/blob/master/src/tools/rados/rados.cc</a></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs raw">--striper<br>       Use radostriper interface rather than pure rados<br></code></pre></td></tr></table></figure><p>也就是这个rados在加了参数之后是调用了radostriper interface这个接口的，所以猜测这个条带的相关参数应该是在接口里面写死了的<br><a href="https://github.com/ceph/ceph/blob/master/src/libradosstriper/RadosStriperImpl.cc">https://github.com/ceph/ceph/blob/master/src/libradosstriper/RadosStriperImpl.cc</a></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs raw">/// default object layout<br>struct ceph_file_layout default_file_layout = &#123;<br> fl_stripe_unit: init_le32(1&lt;&lt;22),<br> fl_stripe_count: init_le32(1),<br> fl_object_size: init_le32(1&lt;&lt;22),<br> fl_cas_hash: init_le32(0),<br> fl_object_stripe_unit: init_le32(0),<br> fl_unused: init_le32(-1),<br> fl_pg_pool : init_le32(-1),<br>&#125;;<br></code></pre></td></tr></table></figure><p>下面开始看下调试模式下改下这几个数值</p><h3 id="下载代码"><a href="#下载代码" class="headerlink" title="下载代码"></a>下载代码</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs raw">git clone https://github.com/ceph/ceph.git<br>git checkout -b myceph2 v10.2.3<br>git submodule update --init --recursive<br></code></pre></td></tr></table></figure><p>切换到10.2.3版本,用的make模式，没用cmake</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs raw">cd ceph<br>./install-deps.sh<br>./autogen.sh<br>./configure<br>make -j 12<br></code></pre></td></tr></table></figure><p>启动开发模式服务</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs raw">cd src<br>./vstart.sh --mon_num 1 --osd_num 3 --mds_num 1  --short -n -d<br></code></pre></td></tr></table></figure><p>这样，dev cluster就起来了。修改部分源码重新make之后，需要关闭cluster，重启让代码生效，当然最好的是，你修改哪个模块，就重启那个模块就行，这里使用重启集群</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs raw">./stop.sh all<br>./vstart.sh --mon_num 1 --osd_num 3 --mds_num 1 --short  -d<br></code></pre></td></tr></table></figure><p>查看状态</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 src]# ./ceph -s -c ./ceph.conf<br></code></pre></td></tr></table></figure><p>我们修改下代码<br>vim libradosstriper&#x2F;RadosStriperImpl.cc</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs raw">/// default object layout<br>struct ceph_file_layout default_file_layout = &#123;<br> fl_stripe_unit: init_le32(1&lt;&lt;21),<br> fl_stripe_count: init_le32(2),<br> fl_object_size: init_le32(1&lt;&lt;22),<br> fl_cas_hash: init_le32(0),<br> fl_object_stripe_unit: init_le32(0),<br> fl_unused: init_le32(-1),<br> fl_pg_pool : init_le32(-1),<br>&#125;;<br></code></pre></td></tr></table></figure><p>修改的是stripe_unit为2M，stripe_count为2，object_size为4M，也就是条带为2<br>修改完了后重新make</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs raw">./stop.sh all<br>./vstart.sh --mon_num 1 --osd_num 3 --mds_num 1 --short  -d<br></code></pre></td></tr></table></figure><p>初始化集群，修改下配置文件增加调试信息<br>vim .&#x2F;ceph.conf</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs raw">debug_rados=20<br>debug_striper=20<br></code></pre></td></tr></table></figure><p>创建文件</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]# dd if=/dev/zero of=debugstrip16M bs=4M count=4<br>[root@lab8106 src]# ./rados -c ./ceph.conf --striper  -p rbd  put  debugstrip16M debugstrip16M<br>[root@lab8106 src]#./rados -c ./ceph.conf  -p rbd  stat debugstrip16M.0000000000000001<br>rbd/debugstrip16M.0000000000000001 mtime 2017-04-26 15:38:41.483464 <br>2017-04-26 15:37:27.000000, size 4194304<br></code></pre></td></tr></table></figure><p>可以看到对象还是4M<br>我们截取下日志分析</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs raw">su 2097152 sc 2 os 4194304 stripes_per_object 2<br>off 0 blockno 0 stripeno 0 stripepos 0 objectsetno 0 objectno 0 block_start 0<br>added new extent(debugstrip16M.0000000000000000 (0) <br>off 2097152 blockno 1 stripeno 0 stripepos 1 objectsetno 0 objectno 1 block_start 0 <br>added new extent(debugstrip16M.0000000000000001 (1) <br>off 4194304 blockno 2 stripeno 1 stripepos 0 objectsetno 0 objectno 0 block_start 2097152<br>added new extent(debugstrip16M.0000000000000000 (0)   <br>off 6291456 blockno 3 stripeno 1 stripepos 1 objectsetno 0 objectno 1 block_start 2097152<br>added new extent(debugstrip16M.0000000000000001 (1)<br>off 8388608 blockno 4 stripeno 2 stripepos 0 objectsetno 1 objectno 2 block_start 0<br>added new extent(debugstrip16M.0000000000000002 (2) <br>off 10485760 blockno 5 stripeno 2 stripepos 1 objectsetno 1 objectno 3 block_start 0<br>added new extent(debugstrip16M.0000000000000003 (3) <br>off 12582912 blockno 6 stripeno 3 stripepos 0 objectsetno 1 objectno 2 block_start 2097152 <br>added new extent(debugstrip16M.0000000000000002 (2)<br>off 14680064 blockno 7 stripeno 3 stripepos 1 objectsetno 1 objectno 3 block_start 2097152 <br>added new extent(debugstrip16M.0000000000000003 (3) <br></code></pre></td></tr></table></figure><p>从上面可以看到先在debugstrip16M.0000000000000000写了2M，在debugstrip16M.0000000000000001写了2M，<br>然后在debugstrip16M.0000000000000000追加写了2M，并且是从block_start 2097152开始的，每个对象是写了两次的并且每次写的就是条带的大小的2M，跟修改上面的条带大小和对象大小是一致的，并且可以很清楚的看到写对象的过程</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本篇尝试了用rados来测试strip功能，并且顺带讲了下怎么在开发模式下修改代码并测试，如果自己写客户端的话，利用librados的时候，可以考虑使用libradosstriper条带来增加一定的性能</p><h2 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h2><p><a href="http://ivanjobs.github.io/2016/05/11/prepare-ceph-dev-env/">准备Ceph开发环境</a></p><h2 id="变更记录"><a href="#变更记录" class="headerlink" title="变更记录"></a>变更记录</h2><table><thead><tr><th align="center">Why</th><th align="center">Who</th><th align="center">When</th></tr></thead><tbody><tr><td align="center">创建</td><td align="center">武汉-运维-磨渣</td><td align="center">2017-04-26</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Cephfs的文件存到哪里了</title>
    <link href="/2017/04/20/Cephfs%E7%9A%84%E6%96%87%E4%BB%B6%E5%AD%98%E5%88%B0%E5%93%AA%E9%87%8C%E4%BA%86/"/>
    <url>/2017/04/20/Cephfs%E7%9A%84%E6%96%87%E4%BB%B6%E5%AD%98%E5%88%B0%E5%93%AA%E9%87%8C%E4%BA%86/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>在ceph里面使用rbd接口的时候，存储的数据在后台是以固定的prifix的对象存在的，这样就能根据相同的前缀对象去对image文件进行拼接或者修复</p><p>在文件系统里面这一块就要复杂一些，本篇就写的关于这个，文件和对象的对应关系是怎样的，用系统命令怎么定位，又是怎么得到这个路径的</p><h2 id="实践"><a href="#实践" class="headerlink" title="实践"></a>实践</h2><h3 id="根据系统命令进行文件的定位"><a href="#根据系统命令进行文件的定位" class="headerlink" title="根据系统命令进行文件的定位"></a>根据系统命令进行文件的定位</h3><p>写入测试文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">dd</span> <span class="hljs-keyword">if</span>=/dev/zero of=/mnt/testfile bs=4M count=10<br></code></pre></td></tr></table></figure><p>查看文件的映射</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 mnt]<span class="hljs-comment"># cephfs /mnt/testfile  map</span><br>WARNING: This tool is deprecated.  Use the layout.* xattrs to query and modify layouts.<br>    FILE OFFSET                    OBJECT        OFFSET        LENGTH  OSD<br>              0      10000001188.00000000             0       4194304  1<br>        4194304      10000001188.00000001             0       4194304  0<br>        8388608      10000001188.00000002             0       4194304  1<br>       12582912      10000001188.00000003             0       4194304  0<br>       16777216      10000001188.00000004             0       4194304  1<br>       20971520      10000001188.00000005             0       4194304  0<br>       25165824      10000001188.00000006             0       4194304  0<br>       29360128      10000001188.00000007             0       4194304  1<br>       33554432      10000001188.00000008             0       4194304  1<br>       37748736      10000001188.00000009             0       4194304  0<br></code></pre></td></tr></table></figure><p>查找文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 mnt]<span class="hljs-comment"># ceph osd map data 10000001188.00000000</span><br>osdmap e109 pool <span class="hljs-string">&#x27;data&#x27;</span> (2) object <span class="hljs-string">&#x27;10000001188.00000000&#x27;</span> -&gt; pg 2.9865f84d (2.d) -&gt; up ([1], p1) acting ([1], p1)<br>[root@lab8106 mnt]<span class="hljs-comment"># ll /var/lib/ceph/osd/ceph-1/current/2.d_head/10000001188.00000000__head_9865F84D__2 </span><br>-rw-r--r-- 1 ceph ceph 4194304 Apr 20 09:35 /var/lib/ceph/osd/ceph-1/current/2.d_head/10000001188.00000000__head_9865F84D__2<br></code></pre></td></tr></table></figure><p>根据上面的命令已经把文件和对象的关系找到了，我们要看下这个关系是根据什么计算出来的</p><h3 id="根据算法进行文件定位"><a href="#根据算法进行文件定位" class="headerlink" title="根据算法进行文件定位"></a>根据算法进行文件定位</h3><p>写入测试文件(故意用bs&#x3D;3M模拟后台不为整的情况)</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ~]<span class="hljs-comment"># dd if=/dev/zero of=/mnt/myfile bs=3M count=10</span><br></code></pre></td></tr></table></figure><p>获取文件的inode信息</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ~]<span class="hljs-comment"># fileinode=`stat  -c %i  &quot;/mnt/myfile&quot;`</span><br>[root@lab8106 ~]<span class="hljs-comment"># echo $fileinode</span><br></code></pre></td></tr></table></figure><p>获取文件的大小和对象个数信息</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ~]<span class="hljs-comment"># filesize=`stat  -c %s  &quot;/mnt/myfile&quot;`</span><br>[root@lab8106 ~]<span class="hljs-comment"># echo $filesize</span><br>31457280<br>[root@lab8106 ~]<span class="hljs-comment"># objectnumori=`echo &quot;scale = 1; $filesize/$objectsize&quot;|bc`</span><br>[root@lab8106 ~]<span class="hljs-comment"># echo $objectnumori</span><br>7.5<br>[root@lab8106 ~]<span class="hljs-comment"># objectnum=`echo $(($&#123;objectnumori//.*/+1&#125;))`</span><br>[root@lab8106 ~]<span class="hljs-comment"># echo $objectnum</span><br>8<br></code></pre></td></tr></table></figure><p>获取对象名称前缀</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ~]<span class="hljs-comment"># declare -l $objectname</span><br>[root@lab8106 ~]<span class="hljs-comment"># objectname=`echo &quot;obase=16;$fileinode&quot;|bc`</span><br>[root@lab8106 ~]<span class="hljs-comment"># echo $objectname</span><br>1000000118b<br></code></pre></td></tr></table></figure><p>上面的declare -l操作后，对象名称的变量才能自动赋值为小写的，否则的话就是大写的，会出现对应不上的问题<br>对象的后缀(后面的0即为编号)</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ~]<span class="hljs-comment">#objectbackname=`printf &quot;%.8x\n&quot; 0`</span><br>[root@lab8106 ~]<span class="hljs-comment">#echo $objectbackname</span><br></code></pre></td></tr></table></figure><p>真正的对象名称为：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ~]<span class="hljs-comment">#realobjectback=$objectname.$objectbackname</span><br></code></pre></td></tr></table></figure><p>打印出所有对象名称</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ~]<span class="hljs-comment"># for num in `seq  0 $objectnum` ;do backname=`printf &quot;%.8x\n&quot; $num`;echo $objectname.$backname;done;</span><br>1000000118b.00000000<br>1000000118b.00000001<br>1000000118b.00000002<br>1000000118b.00000003<br>1000000118b.00000004<br>1000000118b.00000005<br>1000000118b.00000006<br>1000000118b.00000007<br>1000000118b.00000008<br></code></pre></td></tr></table></figure><p>可以看到用算法进行定位的时候，整个过程都没有跟集群ceph进行查询交互，只用到了获取文件的stat的信息，所以根据算法就可以完全定位到具体的对象名称了</p><p>##根据对象名称查找文件所在的位置<br>假如一个对象名称如下：</p><blockquote><p>10000000010.00000003</p></blockquote><p>对象名称取10000000010前面加上0x,并且去掉.后面的，得到:</p><blockquote><p>0x10000000010</p></blockquote><p>计算出inode:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 mnt]<span class="hljs-comment">#printf &quot;%d\n&quot; 0x10000000010</span><br>1099511627792<br></code></pre></td></tr></table></figure><p>根据inode查找文件路径</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 mnt]<span class="hljs-comment"># find /mnt/  -inum 1099511627792 -printf &quot;%i %p\n&quot;</span><br>1099511627792 /mnt/testfile<br></code></pre></td></tr></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本篇是介绍了cephfs中文件跟后台具体对象对应的关系，这个对于系统的可恢复性上面还是有很大的作用的，在cephfs当中只要对象还在，数据就还在，哪怕所有的服务全挂掉，这个在之前的某个别人的生产环境当中已经实践过一次，当然那个是rbd的相对来说要简单一些，当然文件系统的恢复也可以用OSD重构集群的方式进行恢复，本篇的对于元数据丢失的情况下文件恢复会有一定的指导作用</p><h2 id="变更记录"><a href="#变更记录" class="headerlink" title="变更记录"></a>变更记录</h2><table><thead><tr><th align="center">Why</th><th align="center">Who</th><th align="center">When</th></tr></thead><tbody><tr><td align="center">创建</td><td align="center">武汉-运维-磨渣</td><td align="center">2017-04-20</td></tr><tr><td align="center">增加根据对象查找文件位置</td><td align="center">武汉-运维-磨渣</td><td align="center">2017-07-12</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>为什么删除的Ceph对象还能get</title>
    <link href="/2017/04/19/%E4%B8%BA%E4%BB%80%E4%B9%88%E5%88%A0%E9%99%A4%E7%9A%84Ceph%E5%AF%B9%E8%B1%A1%E8%BF%98%E8%83%BDget/"/>
    <url>/2017/04/19/%E4%B8%BA%E4%BB%80%E4%B9%88%E5%88%A0%E9%99%A4%E7%9A%84Ceph%E5%AF%B9%E8%B1%A1%E8%BF%98%E8%83%BDget/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>在很久以前在研究一套文件系统的时候，当时发现一个比较奇怪的现象，没有文件存在，磁盘容量还在增加，在研究了一段时间后，发现这里面有一种比较奇特的处理逻辑</p><p>这套文件系统在处理一个文件的时候放入的是一个临时目录，最开始在发送第一个写请求后，在操作系统层面马上进行了一个delete操作，而写还在继续，并且需要处理这个数据的进程一直占着的，一旦使用完这个文件，不需要做处理，这个文件就会自动释放掉，而无需担心临时文件占用空间的问题</p><p>在Ceph集群当中，有人碰到了去后台的OSD直接rm一个对象后，在前端通过rados还能get到这个删除的对象，而不能rados ls到，我猜测就是这个原因，我们来看下怎么验证这个问题</p><h2 id="验证步骤"><a href="#验证步骤" class="headerlink" title="验证步骤"></a>验证步骤</h2><h3 id="准备测试数据，并且put进去集群"><a href="#准备测试数据，并且put进去集群" class="headerlink" title="准备测试数据，并且put进去集群"></a>准备测试数据，并且put进去集群</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]# cat zp1 <br>sdasdasd<br>[root@lab8106 ~]# rados  -p rbd put zp1 zp1<br>[root@lab8106 ~]# rados -p rbd ls<br>zp1<br></code></pre></td></tr></table></figure><h3 id="找到测试数据并且直接-rm-删除"><a href="#找到测试数据并且直接-rm-删除" class="headerlink" title="找到测试数据并且直接 rm 删除"></a>找到测试数据并且直接 rm 删除</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]# ceph osd map rbd zp1<br>osdmap e90 pool &#x27;rbd&#x27; (3) object &#x27;zp1&#x27; -&gt; pg 3.43eb7bdb (3.1b) -&gt; up ([0], p0) acting ([0], p0)<br>[root@lab8106 ~]# ll /var/lib/ceph/osd/ceph-0/current/3.1b_head/DIR_B/DIR_D/zp1__head_43EB7BDB__3 <br>-rw-r--r-- 1 ceph ceph 9 Apr 19 14:46 /var/lib/ceph/osd/ceph-0/current/3.1b_head/DIR_B/DIR_D/zp1__head_43EB7BDB__3<br>[root@lab8106 ~]# rm -rf /var/lib/ceph/osd/ceph-0/current/3.1b_head/DIR_B/DIR_D/zp1__head_43EB7BDB__3<br></code></pre></td></tr></table></figure><h3 id="尝试查询数据，get数据"><a href="#尝试查询数据，get数据" class="headerlink" title="尝试查询数据，get数据"></a>尝试查询数据，get数据</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 tmp]# rados  -p rbd ls<br>[root@lab8106 tmp]# rados  -p rbd get zp1 zp1<br>[root@lab8106 tmp]# cat zp1<br>sdasdasd<br></code></pre></td></tr></table></figure><p>可以看到数据确实可以查询不到，但是能get下来，并且数据是完整的</p><h3 id="验证我的猜测"><a href="#验证我的猜测" class="headerlink" title="验证我的猜测"></a>验证我的猜测</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 tmp]# lsof |grep zp1<br>ms_pipe_w  4737  5620           ceph   86u      REG               8,33          9  201496748 /var/lib/ceph/osd/ceph-0/current/3.1b_head/DIR_B/DIR_D/zp1__head_43EB7BDB__3 (deleted)<br>···<br></code></pre></td></tr></table></figure><p>可以看到这个标记为delete的对象就是我们删除的zp1，这个输出的意思是，进程4737上面删除了一个文件，文件描述符为86的</p><p>我们直接去拷贝下这个数据看下</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 tmp]# cp  /proc/4737/fd/86 /tmp/zp_save<br>[root@lab8106 tmp]# cat /tmp/zp_save <br>sdasdasd<br></code></pre></td></tr></table></figure><p>可以看到这个数据确实是存在的，还没有释放，所有可以get的到</p><p>我们试下重启下这个进程，看下delete的文件是不是会释放</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 tmp]# systemctl restart ceph-osd@0<br></code></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 tmp]# lsof |grep zp1<br></code></pre></td></tr></table></figure><p>可以看到已经没有这个delete了，现在我们尝试下get</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 tmp]# rados  -p rbd get zp1 zp1<br>error getting rbd/zp1: (2) No such file or directory<br></code></pre></td></tr></table></figure><p>可以看到文件释放掉了，问题确实跟我猜测的是一致的，当然这并不是什么问题</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本篇是对删除了的对象还能get的现象进行了解释</p><h2 id="变更记录"><a href="#变更记录" class="headerlink" title="变更记录"></a>变更记录</h2><table><thead><tr><th align="center">Why</th><th align="center">Who</th><th align="center">When</th></tr></thead><tbody><tr><td align="center">创建</td><td align="center">武汉-运维-磨渣</td><td align="center">2017-04-19</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Ceph删除OSD上一个异常object</title>
    <link href="/2017/04/19/Ceph%E5%88%A0%E9%99%A4OSD%E4%B8%8A%E4%B8%80%E4%B8%AA%E5%BC%82%E5%B8%B8object/"/>
    <url>/2017/04/19/Ceph%E5%88%A0%E9%99%A4OSD%E4%B8%8A%E4%B8%80%E4%B8%AA%E5%BC%82%E5%B8%B8object/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>ceph里面的数据是以对象的形式存储在OSD当中的，有的时候因为磁盘的损坏或者其它的一些特殊情况，会引起集群当中的某一个对象的异常，那么我们需要对这个对象进行处理</p><p>在对象损坏的情况下，启动OSD有的时候都会有问题，那么通过rados rm的方式是没法发送到这个无法启动的OSD的，也就无法删除，所以需要用其他的办法来处理这个情况</p><h2 id="处理步骤"><a href="#处理步骤" class="headerlink" title="处理步骤"></a>处理步骤</h2><h3 id="查找对象的路径"><a href="#查找对象的路径" class="headerlink" title="查找对象的路径"></a>查找对象的路径</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ~]<span class="hljs-comment"># ceph osd map rbd  rbd_data.857e6b8b4567.00000000000000ba</span><br>osdmap e53 pool <span class="hljs-string">&#x27;rbd&#x27;</span> (0) object <span class="hljs-string">&#x27;rbd_data.857e6b8b4567.00000000000000ba&#x27;</span> -&gt; pg 0.2daee1ba (0.3a) -&gt; up ([1], p1) acting ([1], p1)<br></code></pre></td></tr></table></figure><p>先找到这个对象所在的OSD以及PG</p><h3 id="设置集群的noout"><a href="#设置集群的noout" class="headerlink" title="设置集群的noout"></a>设置集群的noout</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ~]<span class="hljs-comment">#ceph osd set noout </span><br></code></pre></td></tr></table></figure><p>这个是为了防止osd的停止产生不必要的删除</p><h3 id="停止OSD"><a href="#停止OSD" class="headerlink" title="停止OSD"></a>停止OSD</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ]<span class="hljs-comment">#systemctl stop ceph-osd@1</span><br></code></pre></td></tr></table></figure><p>如果osd已经是停止的状态就不需要做这一步</p><h3 id="使用ceph-objectstore-tool工具删除单个对象"><a href="#使用ceph-objectstore-tool工具删除单个对象" class="headerlink" title="使用ceph-objectstore-tool工具删除单个对象"></a>使用ceph-objectstore-tool工具删除单个对象</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ]<span class="hljs-comment">#ceph-objectstore-tool --data-path /var/lib/ceph/osd/ceph-1/ --journal-path /var/lib/ceph/osd/ceph-1/journal --pgid 0.3a  rbd_data.857e6b8b4567.00000000000000ba remove</span><br></code></pre></td></tr></table></figure><p>如果有多个副本的情况下，最好都删除掉，影响的数据就是包含这个对象的数据，这个操作的前提是这个对象数据已经被破坏了，如果是部分破坏，可以用集群的repair进行修复，这个是无法修复的情况下的删除对象，来实现启动OSD而不影响其它的数据的</p><h3 id="启动OSD"><a href="#启动OSD" class="headerlink" title="启动OSD"></a>启动OSD</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ]# systemctl start ceph-osd@1<br></code></pre></td></tr></table></figure><h3 id="解除noout"><a href="#解除noout" class="headerlink" title="解除noout"></a>解除noout</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]#ceph osd unset noout <br></code></pre></td></tr></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>一般情况下比较少出现这个情况，如果有这样的删除损坏的对象的需求，就可以这么处理</p><h2 id="变更记录"><a href="#变更记录" class="headerlink" title="变更记录"></a>变更记录</h2><table><thead><tr><th align="center">Why</th><th align="center">Who</th><th align="center">When</th></tr></thead><tbody><tr><td align="center">创建</td><td align="center">武汉-运维-磨渣</td><td align="center">2017-04-19</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>查看ceph集群被哪些客户端连接</title>
    <link href="/2017/04/13/%E6%9F%A5%E7%9C%8Bceph%E9%9B%86%E7%BE%A4%E8%A2%AB%E5%93%AA%E4%BA%9B%E5%AE%A2%E6%88%B7%E7%AB%AF%E8%BF%9E%E6%8E%A5/"/>
    <url>/2017/04/13/%E6%9F%A5%E7%9C%8Bceph%E9%9B%86%E7%BE%A4%E8%A2%AB%E5%93%AA%E4%BA%9B%E5%AE%A2%E6%88%B7%E7%AB%AF%E8%BF%9E%E6%8E%A5/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>我们在使用集群的时候，一般来说比较关注的是后台的集群的状态，但是在做一些更人性化的管理功能的时候，就需要考虑到更多的细节</p><p>本篇就是其中的一个点，查询ceph被哪些客户端连接了</p><h2 id="实践"><a href="#实践" class="headerlink" title="实践"></a>实践</h2><p>从接口上来说，ceph提供了文件，块，和对象的接口，所以不同的接口需要不同的查询方式，因为我接触文件和块比较多，并且文件和块存储属于长连接类型，对象属于请求类型，所以主要关注文件和块存储的连接信息查询</p><p>我的集群状态如下</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]# ceph -s<br>    cluster 3daaf51a-eeba-43a6-9f58-c26c5796f928<br>     health HEALTH_WARN<br>            mon.lab8106 low disk space<br>     monmap e1: 1 mons at &#123;lab8106=192.168.8.106:6789/0&#125;<br>            election epoch 6, quorum 0 lab8106<br>      fsmap e20: 1/1/1 up &#123;0=lab8106=up:active&#125;<br>     osdmap e52: 2 osds: 2 up, 2 in<br>            flags sortbitwise,require_jewel_osds<br>      pgmap v27223: 96 pgs, 3 pools, 2579 MB data, 4621 objects<br>            2666 MB used, 545 GB / 548 GB avail<br>                  96 active+clean<br><br></code></pre></td></tr></table></figure><h3 id="文件接口的连接信息查询"><a href="#文件接口的连接信息查询" class="headerlink" title="文件接口的连接信息查询"></a>文件接口的连接信息查询</h3><p>文件接口的连接信息是保存在MDS的，所以需要通过跟MDS进行交互查询,我的0h环境的MDS在lab8106，登陆到lab8106这台机器执行下面命令</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]# ceph daemon mds.lab8106 session ls|grep &#x27;inst\|hostname\|kernel_version&#x27;<br>        &quot;inst&quot;: &quot;client.34157 192.168.8.106:0\/3325402310&quot;,<br>            &quot;hostname&quot;: &quot;lab8106&quot;,<br>            &quot;kernel_version&quot;: &quot;4.9.5-1.el7.elrepo.x86_64&quot;,<br>        &quot;inst&quot;: &quot;client.14118 192.168.8.107:0\/2202227749&quot;,<br>            &quot;hostname&quot;: &quot;lab8107&quot;,<br>            &quot;kernel_version&quot;: &quot;4.1.12-37.5.1.el7uek.x86_64&quot;<br></code></pre></td></tr></table></figure><p>输出结果我做了过滤，主要信息是机器的IP，主机名，和内核版本</p><h3 id="块接口的连接信息查询"><a href="#块接口的连接信息查询" class="headerlink" title="块接口的连接信息查询"></a>块接口的连接信息查询</h3><p>块接口也就是rbd的接口的</p><p>首先在一台机器上map</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]# rbd map rbd/zp1<br></code></pre></td></tr></table></figure><p>执行查询</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]# rbd status zp1<br>Watchers:<br>watcher=192.168.8.106:0/1837592013 client.34246 cookie=1844646259873284096<br></code></pre></td></tr></table></figure><p>可以看到是被192.168.8.106使用了，也就是watcher</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>命令都比较简单，如果做成一个监控平台，这种连接信息还是有个地方进行查询比较好</p><h2 id="变更记录"><a href="#变更记录" class="headerlink" title="变更记录"></a>变更记录</h2><table><thead><tr><th align="center">Why</th><th align="center">Who</th><th align="center">When</th></tr></thead><tbody><tr><td align="center">创建</td><td align="center">武汉-运维-磨渣</td><td align="center">2017-04-13</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Ceph如何实现文件系统的横向扩展</title>
    <link href="/2017/03/29/Ceph%E5%A6%82%E4%BD%95%E5%AE%9E%E7%8E%B0%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E7%9A%84%E6%A8%AA%E5%90%91%E6%89%A9%E5%B1%95/"/>
    <url>/2017/03/29/Ceph%E5%A6%82%E4%BD%95%E5%AE%9E%E7%8E%B0%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E7%9A%84%E6%A8%AA%E5%90%91%E6%89%A9%E5%B1%95/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>在跟一个朋友聊天的时候，聊到一个技术问题，他们的一个环境上面小文件巨多，是我目前知道的集群里面规模算非常大的了，但是目前有个问题，一方面会进行一倍的硬件的扩容，而文件的数量也在剧烈的增长着，所以有没有什么办法来 缓解这个增长的压力</p><p>当时也没想到太多的办法,只是觉得这么用下去风险太大</p><p>后来在思考了一段时间后，大概有了一个想法，这个就要看是否能把方案做下去，如果是我自己在用的集群，而非客户，我会这么去用的</p><h2 id="方案介绍"><a href="#方案介绍" class="headerlink" title="方案介绍"></a>方案介绍</h2><h3 id="方案一"><a href="#方案一" class="headerlink" title="方案一"></a>方案一</h3><p>也就是默认的方案，一般来说就是一个主MDS，然后几个备用MDS，整个一个挂载点，全局混用的空间</p><p>存在问题：</p><ul><li>扩容以后，有大量的数据迁移</li><li>所有的元数据请求，只有一个MDS服务，到了巨型数据的时候，可能出现卡顿或MDS卡掉的问题</li></ul><p>优点：</p><ul><li>全局统一命名空间</li></ul><h3 id="方案二："><a href="#方案二：" class="headerlink" title="方案二："></a>方案二：</h3><p>采用分存储池的结构，也就是将集群内的目录树分配到整个集群的多个相互独立的空间里面</p><p>存在问题：</p><ul><li>同样是所有的元数据请求，只有一个MDS服务，到了巨型数据的时候，可能出现卡顿或MDS卡掉的问题</li></ul><p>优点：</p><ul><li>全局统一命名空间下面对应目录到不同的存储池当中，在进行扩容的时候，不会影响原有的数据，基本是没有迁移数据</li></ul><h3 id="方案三："><a href="#方案三：" class="headerlink" title="方案三："></a>方案三：</h3><p>物理分存储池的结构并没有解决元数据压力过大的问题，而元数据的处理能力并非横向扩展的，而文件数量和集群规模都是在横向增长，所以必然是一个瓶颈点</p><p>这个方案其实很简单，相当于方案二的扩展，我们在方案二中进行了物理存储池的分离，然后把空间映射到子目录，来实现数据的分离，既然规模能够大到分物理空间，那么我们可以考虑部署多套集群，并且来真正的实现了数据处理能力的横向扩展，因为MDS，可以是多个的了，那么比较重要的问题就是统一命名空间的问题了，怎么实现，这个也简单，主要是跟客户沟通好，让客户接受提出的方案</p><p>我们在一些商业系统上面可以看到一些限制，比如单卷的大小最大支持多大，在这里我们需要跟客户沟通好，无限的扩展，会带来一些压力的风险，有方案能够解决这种问题，而这种数据量在之前是没有太多的案例可借鉴的，所以需要人为控制一个目录的最大空间，也就是单套集群的大小，下面举例来说明下</p><p>假设我们的空间一期规模为2P，二期规模要4P，三期规模6P<br>那么我们的命名空间上就分离出三个逻辑空间，也就是对应三套集群</p><p>弄清楚客户的存储的目录结构，一般来说客户并不太关心目录的设计，如果能够引导的情况下，可以引导客户，我们需要弄清楚目录可变化的那个点在哪里，举例说明，假如客户的数据可以去按年进行分类的话，数据就可以是</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs raw">2014<br>2015 <br>2016<br>2017<br></code></pre></td></tr></table></figure><p>这样的增长趋势，并且数据量之前的肯定已知，未来可大概估计，并且集群准备存储多少年的数据，也是可大概预估的，那么这个环境我们就先认为到2017的数据我们放在集群一内，2017年以后的数据放在集群二内，那么挂载点是这样的</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs raw">192.168.10.101:/2014<br>192.168.10.101:/2015<br>192.168.10.101:/2016<br>192.168.10.101:/2017<br><br>192.168.10.102:/2018<br>192.168.10.102:/2019<br>192.168.10.102:/2020<br>192.168.10.102:/2021<br></code></pre></td></tr></table></figure><p>挂载到本地的服务的机器上<br>本地创建好目录</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs raw">/share/2014<br>/share/2015<br>/share/2016<br>/share/2017<br>/share/2018<br>/share/2019<br>/share/2020<br>/share/2021<br></code></pre></td></tr></table></figure><p>然后把上面的集群挂载点按名称挂载到本地的这些目录上面</p><p>本地的共享就把&#x2F;share共享出去，那么用户看到的就是一个全局命名空间了，这个是用本地子目录映射的方式来实现统一命名空间，技术难度小，难点在于跟客户沟通好数据的层级结构，如果客户能够自己随意增加目录，那么更好实现了，随意的将目录分配到两个集群即可，最终能达到满意的效果就行</p><p>当然主要还是需要客户能够接受你的方案，海量小文件的情况下，分开到多个集群当然会更好些，并且集群万一崩溃，也是只会影响局部的集群了</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>我们在利用一些新的技术的时候我们很多时候关注的是他最好的那个点，而这个点有的时候反而阻碍了我们的想法，比如集群，那就是把所有硬盘管理起来，搞成一个集群，那么为什么不能往上再走一层，我用管理的方式把多套集群在管理的层面组合成一个集群池呢？然后从多个集群里面来分配我们需要的资源即可</p><h2 id="变更记录"><a href="#变更记录" class="headerlink" title="变更记录"></a>变更记录</h2><table><thead><tr><th align="center">Why</th><th align="center">Who</th><th align="center">When</th></tr></thead><tbody><tr><td align="center">创建</td><td align="center">武汉-运维-磨渣</td><td align="center">2017-03-29</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>小文件测试数据准备</title>
    <link href="/2017/03/24/%E5%B0%8F%E6%96%87%E4%BB%B6%E6%B5%8B%E8%AF%95%E6%95%B0%E6%8D%AE%E5%87%86%E5%A4%87/"/>
    <url>/2017/03/24/%E5%B0%8F%E6%96%87%E4%BB%B6%E6%B5%8B%E8%AF%95%E6%95%B0%E6%8D%AE%E5%87%86%E5%A4%87/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>在看一个Linux Vault 2017的资料的时候，看到红帽分享的一个测试的过程，里面关于小文件元数据性能测试的，环境准备的还比较好,可以作为一种测试模型</p><h2 id="测试用例"><a href="#测试用例" class="headerlink" title="测试用例"></a>测试用例</h2><h3 id="测试用例一："><a href="#测试用例一：" class="headerlink" title="测试用例一："></a>测试用例一：</h3><p><img src="/images/blog/o_200901074233small1.png" alt="small"></p><p>使用find -name 测试 find -size 测试</p><h3 id="测试用例二："><a href="#测试用例二：" class="headerlink" title="测试用例二："></a>测试用例二：</h3><p><img src="/images/blog/o_200901074240lardir.png" alt="large"></p><p>使用find -name 测试 find -size 测试</p><h3 id="测试用例三："><a href="#测试用例三：" class="headerlink" title="测试用例三："></a>测试用例三：</h3><p><img src="/images/blog/o_200901074246onlydir.png" alt="onlydir"></p><p>使用rmdir进行测试</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本篇就是记录一个测试模型</p><h2 id="变更记录"><a href="#变更记录" class="headerlink" title="变更记录"></a>变更记录</h2><table><thead><tr><th align="center">Why</th><th align="center">Who</th><th align="center">When</th></tr></thead><tbody><tr><td align="center">创建</td><td align="center">武汉-运维-磨渣</td><td align="center">2017-03-24</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>编译的Ceph二进制文件过大问题</title>
    <link href="/2017/03/23/%E7%BC%96%E8%AF%91%E7%9A%84Ceph%E4%BA%8C%E8%BF%9B%E5%88%B6%E6%96%87%E4%BB%B6%E8%BF%87%E5%A4%A7%E9%97%AE%E9%A2%98/"/>
    <url>/2017/03/23/%E7%BC%96%E8%AF%91%E7%9A%84Ceph%E4%BA%8C%E8%BF%9B%E5%88%B6%E6%96%87%E4%BB%B6%E8%BF%87%E5%A4%A7%E9%97%AE%E9%A2%98/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>在ceph的研发群里看到一个cepher提出一个问题，编译的ceph的二进制文件过大，因为我一直用的打包好的rpm包，没有关注这个问题，重新编译了一遍发现确实有这个问题</p><p>本篇就是记录如何解决这个问题的</p><h2 id="打rpm包的方式"><a href="#打rpm包的方式" class="headerlink" title="打rpm包的方式"></a>打rpm包的方式</h2><p>用我自己的环境编译的时候发现一个问题，编译出来的rpm包还是很大，开始怀疑是机器的原因，换了一台发现二进制包就很小了，然后查询了很多资料以后，找到了问题所在</p><p>在打rpm包的时候可以通过宏变量去控制是否打出一个的debug的包，这个包的作用就是把二进制文件当中包含的debug的相关的全部抽离出来形成一个新的rpm包，而我的环境不知道什么时候在&#x2F;root&#x2F;.rpmmacros添加进去了一个</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">%debug_package      %&#123;nil&#125;<br></code></pre></td></tr></table></figure><p>搜寻资料后确定就是这个的问题,这个变量添加了以后，在打包的时候就不会进行debug相关包的剥离，然后打出的包就是巨大的，可以这样检查自己的rpmbuild的宏变量信息</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@host1 ceph-10.2.6]<span class="hljs-comment">#  rpmbuild --showrc|grep debug_package</span><br>    %&#123;!?__debug_package:<br>    %&#123;?__debug_package:%&#123;__debug_install_post&#125;&#125;<br>-14: _enable_debug_packages1<br>-14: debug_package<br>%global __debug_package 1<br>-14: install%&#123;?_enable_debug_packages:%&#123;?buildsubdir:%&#123;debug_package&#125;&#125;&#125;<br></code></pre></td></tr></table></figure><p>如果开启了debug包抽离（默认就是开启的），那么rpmbuild在打包的过程中会有个调用</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">/usr/lib/rpm/find-debuginfo.sh --strict-build-id -m --run-dwz --dwz-low-mem-die-limit 10000000 --dwz-max-die-limit 110000000 /root/rpmbuild/BUILD/ceph-10.2.5<br></code></pre></td></tr></table></figure><p>这个就是rpmbuild过程中，进行抽离debug信息的操作，也就是缩小二进制的过程，这个并不能直接执行命令，需要用rpmbuild -bb ceph.spec 打包的时候内部自动进行调用的</p><p>上面是rpm打包过程中进行的二进制缩小，那么如果我们是源码编译安装时候，如何缩小这个二进制，答案当然是可以的</p><h2 id="源码编译安装的方式"><a href="#源码编译安装的方式" class="headerlink" title="源码编译安装的方式"></a>源码编译安装的方式</h2><p>.&#x2F;configure 后make生成的二进制文件就在.&#x2F;src下面了<br>我们以ceph-mon为例进行抽离</p><p>这个-O3并没有影响到太多的生成的二进制的大小，–with-debug会有一定的影响，关键还是strip的这个操作</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">./configure --with-debug  CXXFLAGS=-O3 CFLAGS=-O3 CCASFLAGS=-O3<br></code></pre></td></tr></table></figure><p>所以默认的就行</p><p>如果整体进行安装就使用make install-strip安装即可</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@host1 ceph-10.2.6]<span class="hljs-comment"># make install-strip</span><br>[root@host1 ceph-10.2.6]<span class="hljs-comment"># ll /usr/local/bin/ceph-osd</span><br>-rwxr-xr-x 1 root root 14266576 Mar 23 17:57 /usr/local/bin/ceph-osd<br>[root@host1 ceph-10.2.6]<span class="hljs-comment"># ll /usr/local/bin/ceph-osd -hl</span><br>-rwxr-xr-x 1 root root 14M Mar 23 17:57 /usr/local/bin/ceph-osd<br>[root@host1 ceph-10.2.6]<span class="hljs-comment"># ll src/ceph-osd -hl</span><br>-rwxr-xr-x 1 root root 248M Mar 23 17:54 src/ceph-osd<br><br></code></pre></td></tr></table></figure><h2 id="关键的strip的用法"><a href="#关键的strip的用法" class="headerlink" title="关键的strip的用法"></a>关键的strip的用法</h2><p>gcc编译的时候带上-g参数,就是添加了debug的信息</p><blockquote><p>gcc -g -o</p></blockquote><h3 id="分离debug-information"><a href="#分离debug-information" class="headerlink" title="分离debug information"></a>分离debug information</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@host1 ceph-10.2.6]<span class="hljs-comment">#objcopy --only-keep-debug src/ceph-osd src/ceph-osd.debug</span><br>[root@host1 ceph-10.2.6]<span class="hljs-comment"># ll src/ceph-osd -hl</span><br>-rwxr-xr-x 1 root root 248M Mar 23 17:54 src/ceph-osd<br>[root@host1 ceph-10.2.6]<span class="hljs-comment"># ll src/ceph-osd.debug -hl</span><br>-rwxr-xr-x 1 root root 235M Mar 23 18:08 src/ceph-osd.debug<br></code></pre></td></tr></table></figure><p>另外一种方法：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@host1 ceph-10.2.6]<span class="hljs-comment"># cp src/ceph-osd src/ceph-osd.debug</span><br>[root@host1 ceph-10.2.6]<span class="hljs-comment"># strip --only-keep-debug src/ceph-osd.debug</span><br>[root@host1 ceph-10.2.6]<span class="hljs-comment"># ll src/ceph-osd.debug -hl</span><br>-rwxr-xr-x 1 root root 235M Mar 23 18:10 src/ceph-osd.debug<br></code></pre></td></tr></table></figure><h3 id="从原始文件去掉-debug-information"><a href="#从原始文件去掉-debug-information" class="headerlink" title="从原始文件去掉 debug information"></a>从原始文件去掉 debug information</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@host1 ceph-10.2.6]<span class="hljs-comment"># objcopy --strip-debug src/ceph-osd</span><br>[root@host1 ceph-10.2.6]<span class="hljs-comment"># ll src/ceph-osd -hl</span><br>-rwxr-xr-x 1 root root 18M Mar 23 18:11 src/ceph-osd<br>objcopy --strip-debug main<br></code></pre></td></tr></table></figure><p>另外一种方法：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@host1 ceph-10.2.6]<span class="hljs-comment"># strip --strip-debug --strip-unneeded src/ceph-osd</span><br>[root@host1 ceph-10.2.6]<span class="hljs-comment"># ll src/ceph-osd -hl</span><br>-rwxr-xr-x 1 root root 14M Mar 23 18:12 src/ceph-osd<br></code></pre></td></tr></table></figure><h3 id="启用debuglink模式"><a href="#启用debuglink模式" class="headerlink" title="启用debuglink模式"></a>启用debuglink模式</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@host1 ceph-10.2.6]<span class="hljs-comment"># objcopy --add-gnu-debuglink  src/ceph-osd.debug src/ceph-osd</span><br>[root@host1 ceph-10.2.6]<span class="hljs-comment"># gdb src/ceph-osd</span><br></code></pre></td></tr></table></figure><p>或者</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@host1 ceph-10.2.6]<span class="hljs-comment"># gdb -s src/ceph-osd.debug -e src/ceph-osd</span><br></code></pre></td></tr></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>二进制包里面包含了debug的一些相关信息，可以通过strip的方式将内部的debug内容清理掉，这样就可以得到比较小的二进制包了</p><h2 id="变更记录"><a href="#变更记录" class="headerlink" title="变更记录"></a>变更记录</h2><table><thead><tr><th align="center">Why</th><th align="center">Who</th><th align="center">When</th></tr></thead><tbody><tr><td align="center">创建</td><td align="center">武汉-运维-磨渣</td><td align="center">2017-03-23</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Ceph实现数据的&#39;不拆分&#39;</title>
    <link href="/2017/03/22/Ceph%E5%AE%9E%E7%8E%B0%E6%95%B0%E6%8D%AE%E7%9A%84&#39;%E4%B8%8D%E6%8B%86%E5%88%86&#39;/"/>
    <url>/2017/03/22/Ceph%E5%AE%9E%E7%8E%B0%E6%95%B0%E6%8D%AE%E7%9A%84&#39;%E4%B8%8D%E6%8B%86%E5%88%86&#39;/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>之前看过一个朋友一篇文章，讲述的是Vsan为什么使用的是两副本，而ceph则大多数情况下需要三副本，当时个人观点是这个并不是关键点，但是在仔细考虑了问题的出发点以后，这个也可以说是其中的一个点</p><p>一个集群数据丢失可以从多方面去看</p><ul><li>发生丢失数据的事件，这个来说，出现这个事件的概率是一致的，同等硬件情况下没有谁的系统能够说在两副本情况下把这个出现坏盘概率做的比其他系统更低</li><li>发生坏盘事件以后，数据丢失波及的范围，这个就是那个朋友提出的一个观点，对于Vsan来说因为文件的不拆分，也就是在丢了的情况下，只是局部数据的丢失，而ceph的数据因为拆分到整个集群，基本上说就是全军覆没了，这一点没有什么争议</li></ul><p>一般来说，ceph都是配置的分布式文件系统，也就是数据以PG为组合，以对象为最小单元的形式分布到整个集群当中去，通过控制crush能够增加一定的可用概率，但是有没有办法实现真的丢盘的情况下，数据波及没有那么广，答案当然是有的，只是需要做一些更细微的控制，前端的使用的接口也需要做一定的改动，本篇将讲述这个如何去实现，以及前端可能需要的变动</p><h2 id="方案实现"><a href="#方案实现" class="headerlink" title="方案实现"></a>方案实现</h2><p>首先来一张示意图，来介绍大致的实现方式，下面再给出操作步骤</p><p><img src="/images/blog/o_200901074045osd%E4%B8%8D%E6%8B%86%E5%88%86.png" alt="osd不拆分.png-15.7kB"></p><p>主要包括三步</p><ul><li>横向划条带 </li><li>创建对应规则 </li><li>根据规则创建相关存储池</li></ul><h3 id="横向划条带"><a href="#横向划条带" class="headerlink" title="横向划条带"></a>横向划条带</h3><p>创建虚拟根</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs raw">ceph osd crush add-bucket default-a root<br>ceph osd crush add-bucket default-b root<br>ceph osd crush add-bucket default-c root<br>ceph osd crush add-bucket default-d root<br></code></pre></td></tr></table></figure><p>创建虚拟主机</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs raw">ceph  osd crush add-bucket host1-a host<br>ceph  osd crush add-bucket host2-a host<br>ceph  osd crush add-bucket host3-a host<br>ceph  osd crush add-bucket host1-b host<br>ceph  osd crush add-bucket host2-b host<br>ceph  osd crush add-bucket host3-b host<br>ceph  osd crush add-bucket host1-c host<br>ceph  osd crush add-bucket host2-c host<br>ceph  osd crush add-bucket host3-c host<br>ceph  osd crush add-bucket host1-d host<br>ceph  osd crush add-bucket host2-d host<br>ceph  osd crush add-bucket host3-d host<br></code></pre></td></tr></table></figure><p>将虚拟主机挪到虚拟根里面</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs raw">ceph osd crush move host1-a root=default-a<br>ceph osd crush move host2-a root=default-a<br>ceph osd crush move host3-a root=default-a<br>ceph osd crush move host1-b root=default-b<br>ceph osd crush move host2-b root=default-b<br>ceph osd crush move host3-b root=default-b<br>ceph osd crush move host1-c root=default-c<br>ceph osd crush move host2-c root=default-c<br>ceph osd crush move host3-c root=default-c<br>ceph osd crush move host1-d root=default-d<br>ceph osd crush move host2-d root=default-d<br>ceph osd crush move host3-d root=default-d<br></code></pre></td></tr></table></figure><p>将osd塞入到指定的bucker内</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs raw">ceph osd  crush create-or-move  osd.0 1.83  host=host1-a<br>ceph osd  crush create-or-move  osd.4 1.83  host=host2-a<br>ceph osd  crush create-or-move  osd.8 1.83  host=host3-a<br>ceph osd  crush create-or-move  osd.1 1.83  host=host1-b<br>ceph osd  crush create-or-move  osd.5 1.83  host=host2-b<br>ceph osd  crush create-or-move  osd.9 1.83  host=host3-b<br>ceph osd  crush create-or-move  osd.2 1.83  host=host1-c<br>ceph osd  crush create-or-move  osd.6 1.83  host=host2-c<br>ceph osd  crush create-or-move  osd.10 1.83  host=host3-c<br>ceph osd  crush create-or-move  osd.3 1.83  host=host1-d<br>ceph osd  crush create-or-move  osd.7 1.83  host=host2-d<br>ceph osd  crush create-or-move  osd.11 1.83  host=host3-d<br></code></pre></td></tr></table></figure><p>以上的这么多的操作可以用比较简单的命令实现</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs raw">ceph osd crush set osd.0 1.83  host=host1-a root=default-a<br>ceph osd crush set osd.1 1.83  host=host1-b root=default-b<br>ceph osd crush set osd.2 1.83  host=host1-c root=default-c<br>ceph osd crush set osd.3 1.83  host=host1-d root=default-d<br>ceph osd crush set osd.4 1.83  host=host2-a root=default-a<br>ceph osd crush set osd.5 1.83  host=host2-b root=default-b<br>ceph osd crush set osd.6 1.83  host=host2-c root=default-c<br>ceph osd crush set osd.7 1.83  host=host2-d root=default-d<br>ceph osd crush set osd.8 1.83  host=host3-a root=default-a<br>ceph osd crush set osd.9 1.83  host=host3-b root=default-b<br>ceph osd crush set osd.10 1.83 host=host3-c root=default-c<br>ceph osd crush set osd.11 1.83 host=host3-d root=default-d<br></code></pre></td></tr></table></figure><p>查看现在的树</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@host1 ceph]# ceph osd tree<br>ID  WEIGHT  TYPE NAME        UP/DOWN REWEIGHT PRIMARY-AFFINITY <br> -8 5.44080 root default-d                                     <br>-18 1.81360     host host1-d                                   <br>  3 1.81360         osd.3         up  1.00000          1.00000 <br>-19 1.81360     host host2-d                                   <br>  7 1.81360         osd.7         up  1.00000          1.00000 <br>-20 1.81360     host host3-d                                   <br> 11 1.81360         osd.11        up  1.00000          1.00000 <br> -7 5.44080 root default-c                                     <br>-15 1.81360     host host1-c                                   <br>  2 1.81360         osd.2         up  1.00000          1.00000 <br>-16 1.81360     host host2-c                                   <br>  6 1.81360         osd.6         up  1.00000          1.00000 <br>-17 1.81360     host host3-c                                   <br> 10 1.81360         osd.10        up  1.00000          1.00000 <br> -6 5.44080 root default-b                                     <br>-12 1.81360     host host1-b                                   <br>  1 1.81360         osd.1         up  1.00000          1.00000 <br>-13 1.81360     host host2-b                                   <br>  5 1.81360         osd.5         up  1.00000          1.00000 <br>-14 1.81360     host host3-b                                   <br>  9 1.81360         osd.9         up  1.00000          1.00000 <br> -5 5.44080 root default-a                                     <br> -9 1.81360     host host1-a                                   <br>  0 1.81360         osd.0         up  1.00000          1.00000 <br>-10 1.81360     host host2-a                                   <br>  4 1.81360         osd.4         up  1.00000          1.00000 <br>-11 1.81360     host host3-a                                   <br>  8 1.81360         osd.8         up  1.00000          1.00000 <br> -1       0 root default                                       <br> -2       0     host host1                                     <br> -3       0     host host2                                     <br> -4       0     host host3  <br></code></pre></td></tr></table></figure><p>下面老的一些bucket可以清理掉</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs raw">ceph osd pool delete rbd rbd  --yes-i-really-really-mean-it<br>ceph osd crush rule rm replicated_ruleset<br>ceph osd crush remove host1<br>ceph osd crush remove host2<br>ceph osd crush remove host3<br>ceph osd crush remove default<br></code></pre></td></tr></table></figure><h3 id="创建对应规则"><a href="#创建对应规则" class="headerlink" title="创建对应规则"></a>创建对应规则</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs raw">ceph osd crush rule create-simple rule048  default-a host<br>ceph osd crush rule create-simple rule159  default-b host<br>ceph osd crush rule create-simple rule2610  default-c host<br>ceph osd crush rule create-simple rule3711  default-d host<br></code></pre></td></tr></table></figure><p>检查下规则</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@host1 ceph]# ceph osd crush rule dump|grep &quot;rule_name\|item_name&quot;<br>        &quot;rule_name&quot;: &quot;rule048&quot;,<br>                &quot;item_name&quot;: &quot;default-a&quot;<br>        &quot;rule_name&quot;: &quot;rule159&quot;,<br>                &quot;item_name&quot;: &quot;default-b&quot;<br>        &quot;rule_name&quot;: &quot;rule2610&quot;,<br>                &quot;item_name&quot;: &quot;default-c&quot;<br>        &quot;rule_name&quot;: &quot;rule3711&quot;,<br>                &quot;item_name&quot;: &quot;default-d&quot;<br></code></pre></td></tr></table></figure><h3 id="根据规则创建相关存储池"><a href="#根据规则创建相关存储池" class="headerlink" title="根据规则创建相关存储池"></a>根据规则创建相关存储池</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@host1 ceph]# ceph osd pool create poola048 64 64 replicated rule048<br>pool &#x27;poola048&#x27; created<br>[root@host1 ceph]# ceph osd pool create poolb159 64 64 replicated rule159<br>pool &#x27;poolb159&#x27; created<br>[root@host1 ceph]# ceph osd pool create poolc2610 64 64 replicated rule2610<br>pool &#x27;poolc2610&#x27; created<br>[root@host1 ceph]# ceph osd pool create poold3711 64 64 replicated rule3711<br>pool &#x27;poold3711&#x27; created<br></code></pre></td></tr></table></figure><p>检查存储池</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@host1 ceph]# ceph osd dump|grep pool<br>pool 1 &#x27;poola048&#x27; replicated size 2 min_size 1 crush_ruleset 0 object_hash rjenkins pg_num 64 pgp_num 64 last_change 145 flags hashpspool stripe_width 0<br>pool 2 &#x27;poolb159&#x27; replicated size 2 min_size 1 crush_ruleset 1 object_hash rjenkins pg_num 64 pgp_num 64 last_change 147 flags hashpspool stripe_width 0<br>pool 3 &#x27;poolc2610&#x27; replicated size 2 min_size 1 crush_ruleset 2 object_hash rjenkins pg_num 64 pgp_num 64 last_change 149 flags hashpspool stripe_width 0<br>pool 4 &#x27;poold3711&#x27; replicated size 2 min_size 1 crush_ruleset 3 object_hash rjenkins pg_num 64 pgp_num 64 last_change 151 flags hashpspool stripe_width 0<br></code></pre></td></tr></table></figure><p>到这里基本的环境就配置好了，采用的是副本2，但是虚拟组里面留了三个osd，这个后面会解释</p><h2 id="如何使用"><a href="#如何使用" class="headerlink" title="如何使用"></a>如何使用</h2><p>假设现在前端需要8个image用来使用了，那么我们创建的时候，就将这个8个平均分布到上面的四个存储里面去，这里是因为是划成了四个条带，在实际环境当中，可以根据需要进行划分，在选择用哪个存储的时候可以去用轮询的算法，进行轮询，也可以自定义去选择在哪个存储池创建，这个都是可以控制的</p><h3 id="创建image"><a href="#创建image" class="headerlink" title="创建image"></a>创建image</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs raw">rbd -p poola048 create image1 --size 1G<br>rbd -p poola048 create image2 --size 1G<br>rbd -p poolb159 create image3 --size 1G<br>rbd -p poolb159 create image4 --size 1G<br>rbd -p poolc2610 create image6 --size 1G<br>rbd -p poolc2610 create image7 --size 1G<br>rbd -p poold3711 create image8 --size 1G<br>rbd -p poold3711 create image9 --size 1G<br></code></pre></td></tr></table></figure><h3 id="如何跟virsh对接"><a href="#如何跟virsh对接" class="headerlink" title="如何跟virsh对接"></a>如何跟virsh对接</h3><p>如果你熟悉virsh配置文件的话，可以看到rbd相关的配置文件是这样的</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs raw">&lt;source protocol=&#x27;rbd&#x27; name=&#x27;volumes/volume-f20fd994-e600-41da-a6d8-6e216044dbb1&#x27;&gt;<br>        &lt;host name=&#x27;192.168.10.4&#x27; port=&#x27;6789&#x27;/&gt;<br>&lt;/source&gt;<br></code></pre></td></tr></table></figure><p>在cinder的相关配置当中虽然我们指定了volume这个存储池值是一个定值，在这个配置文件当中也就读取了这个值，那么需要改造的接口就是在创建云盘的时候，不去将cinder的存储池固定死，volumes&#x2F;volume-f20fd994-e600-41da-a6d8-6e216044dbb1这样的值可以是上面的poola048&#x2F;image1,也可以是poolc2610&#x2F;image6,这个地方就是需要改动的地方，将整个值包含存储池的值作为一个变量，这个改动应该属于可改的</p><h2 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h2><p>按上面的进行处理以后，那么再出现同时坏了两个盘的情况下，数据丢失的波及范围跟Vsan已经是一致了，因为数据打散也只是在这个三个里面打散了，真的出现磁盘损坏波及的也是局部的数据了</p><p>问题：</p><p>1、分布范围小了性能怎么样<br>比完全分布来说性能肯定降低了一些，但是如果说对于负载比较高的情况，每个盘都在跑的情况下，这个性能是一定的，底层的磁盘提供的带宽是一定的，这个跟VSAN一样的</p><p>并且这个上面所示的是极端的情况下的，缩小到3个OSD一组条带，也可以自行放宽到6个一个条带，这个只是提供了一种方法，缩小了波及范围</p><p>2、副本2为什么留3个osd一个条带<br>比副本数多1的话，这样在坏了一个盘也可以迁移，所以一般来说，至少比副本数多1的故障域</p><p>3、如何扩容<br>扩容就增加条带即可，并且可以把老的存储池规则指定到新的磁盘的条带上面</p><p>4、这个方法还可以用故障域增加可用性么<br>可以的，可以从每个故障域里面抽出OSD即可，只要保证底层的数据不重叠，实际是两个不同的需求</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本篇是提供了一种可能性，在实际运行环境当中，可以根据自己的环境进行设计，设计的方法就是，假设一个数据的全部副本都丢了的情况，允许的数据波及范围是多少，如果拆分两份就是波及二分之一，我的测试环境是分成了四个条带，也就是只影响四分之一的数据</p><h2 id="变更记录"><a href="#变更记录" class="headerlink" title="变更记录"></a>变更记录</h2><table><thead><tr><th align="center">Why</th><th align="center">Who</th><th align="center">When</th></tr></thead><tbody><tr><td align="center">创建</td><td align="center">武汉-运维-磨渣</td><td align="center">2017-03-22</td></tr><tr><td align="center">补充OSD设置crush的简单方法</td><td align="center">武汉-运维-磨渣</td><td align="center">2017-04-19</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>基于Docker UI 配置ceph集群</title>
    <link href="/2017/03/16/%E5%9F%BA%E4%BA%8EDocker%20UI%20%E9%85%8D%E7%BD%AEceph%E9%9B%86%E7%BE%A4/"/>
    <url>/2017/03/16/%E5%9F%BA%E4%BA%8EDocker%20UI%20%E9%85%8D%E7%BD%AEceph%E9%9B%86%E7%BE%A4/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>前一篇介绍了docker在命令行下面进行的ceph部署，本篇用docker的UI进行ceph的部署，目前来说市面上还没有一款能够比较简单就能直接在OS上面去部署Ceph的管理平台，这是因为OS的环境差异化太大，并且包的版本，以及各种软件的适配都可能造成失败，而docker比较固化环境，因此即使一个通用的UI也能很方便的部署出一个Cpeh集群</p><p>本篇就是对Docker UI部署集群做一个实践，对ceph了解，对docker了解，对dokcer的UI操作进行一定的了解的情况下，再做实践会比较好，总体上还是比较简单的</p><h2 id="安装并运行portainer"><a href="#安装并运行portainer" class="headerlink" title="安装并运行portainer"></a>安装并运行portainer</h2><h3 id="安装软件"><a href="#安装软件" class="headerlink" title="安装软件"></a>安装软件</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs raw">cd /opt<br>wget https://github.com/portainer/portainer/releases/download/1.12.1/portainer-1.12.1-linux-amd64.tar.gz<br>tar xvpfz portainer-1.12.1-linux-amd64.tar.gz<br>cd portainer<br></code></pre></td></tr></table></figure><h3 id="运行软件"><a href="#运行软件" class="headerlink" title="运行软件"></a>运行软件</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">./portainer -H unix:///var/run/docker.sock  -p &quot;:9999&quot;<br></code></pre></td></tr></table></figure><p>注意下这里-H是指定的docker的连接，也就是要控制哪个docker，这个支持本地的sock的方式，也支持远程的tcp的方式，这个进入ui界面后还可以添加更多的<br>-p是指定的访问的接口</p><h3 id="扩展知识"><a href="#扩展知识" class="headerlink" title="扩展知识"></a>扩展知识</h3><p>如何在centos7下面启用 remote api<br>打开文件</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">/usr/lib/systemd/system/docker.service<br></code></pre></td></tr></table></figure><p>在  $INSECURE_REGISTRY 后面添加  -H tcp:&#x2F;&#x2F;0.0.0.0:2376 -H unix:&#x2F;&#x2F;&#x2F;var&#x2F;run&#x2F;docker.sock </p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs raw">ExecStart=/usr/bin/dockerd-current \<br>          --add-runtime docker-runc=/usr/libexec/docker/docker-runc-current \<br>          --default-runtime=docker-runc \<br>          --exec-opt native.cgroupdriver=systemd \<br>          --userland-proxy-path=/usr/libexec/docker/docker-proxy-current \<br>          $OPTIONS \<br>          $DOCKER_STORAGE_OPTIONS \<br>          $DOCKER_NETWORK_OPTIONS \<br>          $ADD_REGISTRY \<br>          $BLOCK_REGISTRY \<br>          $INSECURE_REGISTRY  -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock<br></code></pre></td></tr></table></figure><p>修改好了后</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]#systemctl daemon-reload<br>[root@lab8106 ~]#systemctl restart docker<br></code></pre></td></tr></table></figure><p>检查端口和asok</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]# netstat -tunlp|grep 2376<br>tcp6       0      0 :::2376                 :::*                    LISTEN      24484/dockerd-curre <br>[root@lab8106 ~]# ll /var/run/docker.sock<br>srw-rw---- 1 root root 0 Mar 16 16:39 /var/run/docker.sock<br></code></pre></td></tr></table></figure><p>生成了配置没有问题</p><h4 id="portainer的自身数据"><a href="#portainer的自身数据" class="headerlink" title="portainer的自身数据"></a>portainer的自身数据</h4><p>默认情况下portainer的数据是存储在&#x2F;data目录下面的，如果想重新配置密码或者内容的话，删除这个目录里面的数据就行</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]# ll /data/<br>total 24<br>-rw------- 1 root root 32768 Mar 16 16:32 portainer.db<br>drwx------ 2 root root     6 Mar 16 16:32 tls<br></code></pre></td></tr></table></figure><h2 id="UI界面登陆"><a href="#UI界面登陆" class="headerlink" title="UI界面登陆"></a>UI界面登陆</h2><p>直接访问宿主机的<code>http://ip:9999</code><br><img src="/images/blog/o_200901073515image_1bbb4ogmqu1ir8049n1okfq4j9.png" alt="login"><br>输入一个8位数的密码<br>输入好了以后，登陆即可</p><p><img src="/images/blog/o_200901073508image_1bbb4r1eb1qnj0pcjmsbf1ucgm.png" alt="endponit"></p><p>检查endpoint，可以看到就是我刚才命令行当中加入的sock</p><h2 id="获取image"><a href="#获取image" class="headerlink" title="获取image"></a>获取image</h2><p><img src="/images/blog/o_200901073501image_1bbb4vs5h1ri522q8avkrb1ko716.png" alt="get ceph"></p><p>在上面填写<code>ceph/daemon</code> 然后点击pull</p><p>有可能会超时，如果多次失败，就去后台命令行执行，这个地方等同于后台的命令</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">docker pull ceph/daemon<br></code></pre></td></tr></table></figure><p>也可以直接在后台执行这个命令<br>可以用dstat -n观察下载的速度</p><p>下载好了去页面上看下是否好了<br><img src="/images/blog/o_200901073454image_1bbb6c50tip1iud1gfv9m4uku1j.png" alt="download"></p><h2 id="配置CEPH集群"><a href="#配置CEPH集群" class="headerlink" title="配置CEPH集群"></a>配置CEPH集群</h2><p>配置集群可以都在页面做了，因为之前有篇命令行部署docker的ceph，建议先回顾一下，再看这个比较好</p><h3 id="创建MON"><a href="#创建MON" class="headerlink" title="创建MON"></a>创建MON</h3><p>点击增加容器<br><img src="/images/blog/o_200901073448image_1bbb6fpgmpgh1enf6pm1kk818q920.png" alt="add comn"></p><p>注意创建好两个目录</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs raw">mkdir -p /etc/ceph<br>mkdir -p /var/lib/ceph/<br></code></pre></td></tr></table></figure><p>这两个目录里面不要有任何东西,保持空目录状态</p><p><img src="/images/blog/o_200901073440image_1bbb6pbf811pesikkrmemt9du2d.png" alt="ceph mon"></p><ul><li>填写名称为mon，这个是容器名称，可以自定义</li><li>填写Image，这个填写下载好的ceph&#x2F;daemon</li><li>填写command,这个填写mon，为固定值</li><li>填写Entry Ponit ,这个填写&#x2F;entrypoint.sh，为固定值</li><li>填写Environment variable，这个填写两个变量<ul><li>MON_IP  192.168.8.106</li><li>CEPH_PUBLIC_NETWORK 192.168.0.0&#x2F;16</li></ul></li></ul><p>填写完了切换第二个标签页Volumes<br><img src="/images/blog/o_200901073432image_1bbb6rsb01etg1ebt1hr317lo1met2q.png" alt="volume"></p><ul><li>填写Volume<ul><li>&#x2F;etc&#x2F;ceph &#x2F;etc&#x2F;ceph</li><li>&#x2F;var&#x2F;lib&#x2F;ceph&#x2F; &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;</li></ul></li></ul><p><img src="/images/blog/o_200901073423image_1bbb6tgov1kvr1rcc1keg1e0a1i4537.png" alt="network"></p><ul><li>填写Network为host</li><li>填写hostname为宿主机的主机名<br>上面都填写完了后就点击create</li></ul><p>没出异常的话，就可以进入console进行查询了<br><img src="/images/blog/o_200901073414image_1bbb726491l5it2d1kf31at614lb3k.png" alt="console"><br>点击connect<br><img src="/images/blog/o_200901073404image_1bbb73gjif91s70a6f8pg1vg141.png" alt="image_1bbb73gjif91s70a6f8pg1vg141.png-79.5kB"><br>没有问题</p><h3 id="创建OSD"><a href="#创建OSD" class="headerlink" title="创建OSD"></a>创建OSD</h3><p>点击增加容器<br><img src="/images/blog/o_200901073448image_1bbb6fpgmpgh1enf6pm1kk818q920.png" alt="add comn"></p><p><img src="/images/blog/o_200901073357image_1bbb7a1dm1gv1n4j1odoo3k1n2u4e.png" alt="osd0"></p><ul><li>填写Name，这个为容器名称，可以自定义</li><li>填写Image,这个为ceph&#x2F;daemon,固定的值</li><li>填写command,这个为osd_ceph_disk，为定值</li><li>填写Entry Ponit ,这个填写&#x2F;entrypoint.sh，为固定值</li><li>填写Environment variable，这个填写一个OSD磁盘变量</li><li>OSD_DEVICE &#x2F;dev&#x2F;sdb</li></ul><p>切换到第二个Volume标签页</p><ul><li>填写Volume<ul><li>&#x2F;etc&#x2F;ceph &#x2F;etc&#x2F;ceph</li><li>&#x2F;var&#x2F;lib&#x2F;ceph&#x2F; &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;</li><li>&#x2F;dev&#x2F; &#x2F;dev&#x2F;</li></ul></li></ul><p><img src="/images/blog/o_200901073349image_1bbb7aqg21jso1ku51mdgajtr0p4r.png" alt="osd0 add"></p><p>切换到Network标签页</p><ul><li>填写Network为host</li><li>填写hostname为宿主机的主机名<br>上面都填写完了后就点击create</li></ul><p><img src="/images/blog/o_200901073342image_1bbb7c5d17b21o1uoc1i7h1cr458.png" alt="osdsd add"><br>切换到Security&#x2F;Host标签页<br>勾选上  privileged ,一定要选上，不然没有权限去格式化磁盘</p><p><img src="/images/blog/o_200901073335image_1bbb7okcj8mj1c301tdb16mtecn5l.png" alt="osd addd "><br>上面都填写完了后就点击create<br>没出异常的话，就可以进入console进行查询了<br><img src="/images/blog/o_200901073328image_1bbb7ufgk12nj1unpoq5taa1iah9.png" alt="good"></p><p>基本上一个简单的集群就配置好了，跨主机的情况，就提前把配置文件拷贝到另外一台主机，还有bootstrap keyring也拷贝过去，就可以了，这里就不做过多的赘述</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本篇基于portainer以及一个现有的ceph容器做的部署实践，从整个操作来说，UI的部署，环境的搭建都非常的简单，这个得益于UI环境的简单，还有docker的封装，更多的玩法可以自己去探索，也可以运用这个UI做更多其他的容器操作</p><h2 id="变更记录"><a href="#变更记录" class="headerlink" title="变更记录"></a>变更记录</h2><table><thead><tr><th align="center">Why</th><th align="center">Who</th><th align="center">When</th></tr></thead><tbody><tr><td align="center">创建</td><td align="center">武汉-运维-磨渣</td><td align="center">2017-03-16</td></tr></tbody></table><p>&#x2F;assets&#x2F;images&#x2F;blogimg&#x2F;base-on-docker-ui-deploy-ceph&#x2F;</p>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>基于docker部署ceph以及修改docker image</title>
    <link href="/2017/03/15/%E5%9F%BA%E4%BA%8Edocker%E9%83%A8%E7%BD%B2ceph%E4%BB%A5%E5%8F%8A%E4%BF%AE%E6%94%B9docker%20image/"/>
    <url>/2017/03/15/%E5%9F%BA%E4%BA%8Edocker%E9%83%A8%E7%BD%B2ceph%E4%BB%A5%E5%8F%8A%E4%BF%AE%E6%94%B9docker%20image/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>容器和ceph的结合已经在一些生产环境当中做了尝试，容器的好处就是对运行环境的一个封装，传统的方式是集成为ISO，这个需要一定的维护量，而容器的相关操作会简单很多，也就有了一些尝试，个人觉得如果玩的转容器可以考虑，当然得懂ceph，不然两套系统在一起，问题都不知道是哪个的，就比较麻烦了</p><p>本篇是基于之前我的填坑群里面的牛鹏举的一个问题，他的环境出现了创建osd的时候权限问题，我这边没遇到，现在实践了一遍，感觉应该是之前目录提前创建了的问题</p><h2 id="实践步骤"><a href="#实践步骤" class="headerlink" title="实践步骤"></a>实践步骤</h2><h3 id="安装docker"><a href="#安装docker" class="headerlink" title="安装docker"></a>安装docker</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">yum install docker<br></code></pre></td></tr></table></figure><h3 id="下载ceph镜像"><a href="#下载ceph镜像" class="headerlink" title="下载ceph镜像"></a>下载ceph镜像</h3><p>这个镜像是sebastien维护的，他是redhat的ceph工程师，ceph-ansible的负责人,很多一线的资料都是来自他的分享，这个是一个集成好的镜像</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">docker pull ceph/daemon<br></code></pre></td></tr></table></figure><p>准备好一些目录</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs raw">mkdir -p /etc/ceph<br>mkdir -p /var/lib/ceph/<br></code></pre></td></tr></table></figure><p>注意只需要做这个两个目录，不要创建子目录，docker内部有相关的操作</p><h3 id="创建一个mon"><a href="#创建一个mon" class="headerlink" title="创建一个mon"></a>创建一个mon</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs raw">sudo docker run -d --net=host  --name=mon \<br>-v /etc/ceph:/etc/ceph \<br>-v /var/lib/ceph/:/var/lib/ceph \<br>-e MON_IP=192.168.8.106 \<br>-e CEPH_PUBLIC_NETWORK=192.168.0.0/16 \<br>ceph/daemon mon<br></code></pre></td></tr></table></figure><p>MON_IP就是宿主机的IP地址</p><p>执行完了后</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ceph]#  docker ps -l<br>CONTAINER ID        IMAGE               COMMAND                CREATED              STATUS              PORTS               NAMES<br>86ed05173432        ceph/daemon         &quot;/entrypoint.sh mon&quot;   About a minute ago   Up 59 seconds                           mon<br></code></pre></td></tr></table></figure><p>可以看到退出了，我们来docker logs -f mon看下日志的输出</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ceph]# docker logs -f mon<br>/sbin/ip<br>creating /etc/ceph/ceph.client.admin.keyring<br>creating /etc/ceph/ceph.mon.keyring<br>creating /var/lib/ceph/bootstrap-osd/ceph.keyring<br>creating /var/lib/ceph/bootstrap-mds/ceph.keyring<br>creating /var/lib/ceph/bootstrap-rgw/ceph.keyring<br>monmaptool: monmap file /etc/ceph/monmap-ceph<br>monmaptool: set fsid to cb5df106-25b3-4f93-9f54-baca2976a47b<br>monmaptool: writing epoch 0 to /etc/ceph/monmap-ceph (1 monitors)<br>creating /tmp/ceph.mon.keyring<br>importing contents of /etc/ceph/ceph.client.admin.keyring into /tmp/ceph.mon.keyring<br>importing contents of /var/lib/ceph/bootstrap-osd/ceph.keyring into /tmp/ceph.mon.keyring<br>importing contents of /var/lib/ceph/bootstrap-mds/ceph.keyring into /tmp/ceph.mon.keyring<br>importing contents of /var/lib/ceph/bootstrap-rgw/ceph.keyring into /tmp/ceph.mon.keyring<br>importing contents of /etc/ceph/ceph.mon.keyring into /tmp/ceph.mon.keyring<br>ceph-mon: set fsid to cb5df106-25b3-4f93-9f54-baca2976a47b<br>ceph-mon: created monfs at /var/lib/ceph/mon/ceph-lab8106 for mon.lab81<br></code></pre></td></tr></table></figure><p>提示成功了</p><p>我们看下生成的文件</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ceph]# ll /etc/ceph<br>total 16<br>-rw------- 1 root  root  137 Mar 14 17:53 ceph.client.admin.keyring<br>-rw-r--r-- 1 root  root  285 Mar 14 17:53 ceph.conf<br>-rw------- 1 64045 64045  77 Mar 14 17:53 ceph.mon.keyring<br>-rw-r--r-- 1 64045 64045 187 Mar 14 17:53 monmap-ceph<br></code></pre></td></tr></table></figure><p>从这里可以看到内部的cpeh的用户的id是64045，所以在docker宿主机不要随便去给ceph权限，可能id不匹配，容器内部还是无法操作</p><h3 id="创建一个osd"><a href="#创建一个osd" class="headerlink" title="创建一个osd"></a>创建一个osd</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs raw">sudo docker run -d --net=host --name=myosd1 \<br>--privileged=true \<br>-v /etc/ceph:/etc/ceph \<br>-v /var/lib/ceph/:/var/lib/ceph \<br>-v /dev/:/dev/ \<br>-e OSD_DEVICE=/dev/sdb \<br>ceph/daemon osd_ceph_disk<br></code></pre></td></tr></table></figure><p>如果查询日志</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">docker logs -f myosd1<br></code></pre></td></tr></table></figure><p>如果执行命令</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">docker exec -it mon ceph -s<br></code></pre></td></tr></table></figure><p>如果想进入容器内部</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">docker exec -it mon  /bin/bash<br></code></pre></td></tr></table></figure><p>修改集群的副本数</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">docker exec -it mon  ceph osd pool set rbd size 1<br></code></pre></td></tr></table></figure><p>查看集群状态</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ceph]# docker exec -it mon  ceph -s<br>    cluster cb5df106-25b3-4f93-9f54-baca2976a47b<br>     health HEALTH_WARN<br>            mon.lab8106 low disk space<br>     monmap e2: 1 mons at &#123;lab8106=192.168.8.106:6789/0&#125;<br>            election epoch 4, quorum 0 lab8106<br>        mgr no daemons active <br>     osdmap e7: 1 osds: 1 up, 1 in<br>            flags sortbitwise,require_jewel_osds,require_kraken_osds<br>      pgmap v15: 64 pgs, 1 pools, 0 bytes data, 0 objects<br>            34288 kB used, 279 GB / 279 GB avail<br>                  64 active+clean<br></code></pre></td></tr></table></figure><p>上面的操作都很顺利，但是某些情况可能出现异常情况，或者镜像内部本身就有问题需要自己修改，这个怎么处理</p><h2 id="碰上问题想修改image"><a href="#碰上问题想修改image" class="headerlink" title="碰上问题想修改image"></a>碰上问题想修改image</h2><p>我们看下我们运行的docker</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ceph]# docker ps <br>CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS               NAMES<br>874d78ccae55        ceph/daemon         &quot;/entrypoint.sh osd_c&quot;   14 hours ago        Up 14 hours                             myosd1<br>86ed05173432        ceph/daemon         &quot;/entrypoint.sh mon&quot;     15 hours ago        Up 15 hours                             mon<br></code></pre></td></tr></table></figure><p>COMMAND这里有个&#x2F;entrypoint.sh</p><p>如果存在ENTRYPOINT和CMD，那么CMD就是ENTRYPOINT的参数，如果没有ENTRYPOINT，则CMD就是默认执行指令<br>也就是容器启动的时候默认是会去执行&#x2F;entrypoint.sh 这个了</p><p>我们不需要他执行这个，就需要加参数了</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ceph]# docker run -i -t --entrypoint /bin/bash ceph/daemon<br></code></pre></td></tr></table></figure><p>比如我上次做的一个操作，把ceph用户绑定到root的id</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs raw">root@9b269bf751f9:/# cat /etc/passwd|grep ceph<br>ceph:x:64045:64045:Ceph storage service:/var/lib/ceph:/bin/false<br>root@9b269bf751f9:/# sed -i &#x27;s/64045/0/g&#x27; /etc/passwd<br>root@9b269bf751f9:/# cat /etc/passwd|grep ceph<br>ceph:x:0:0:Ceph storage service:/var/lib/ceph:/bin/false<br></code></pre></td></tr></table></figure><p>退出容器</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">root@9b269bf751f9:/# exit<br></code></pre></td></tr></table></figure><p>查询我们最后运行的容器，修改回entrypoint我们再把容器修改提交到基础image</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ceph]# docker ps -l<br>CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS                      PORTS               NAMES<br>9b269bf751f9        ceph/daemon         &quot;/bin/bash&quot;         2 minutes ago       Exited (0) 15 seconds ago                       angry_hawking<br><br>[root@lab8106 ceph]#  docker commit 9b269bf751f9 ceph/daemon<br><br>[root@lab8106 ~]# docker run -i -t --entrypoint /entrypoint.sh ceph/daemon<br>[root@lab8106 ~]# docker ps -l<br>CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS                     PORTS               NAMES<br>c2ea602c18ac        ceph/daemon         &quot;/entrypoint.sh&quot;    10 seconds ago      Exited (1) 7 seconds ago                       ecstatic_bartik<br><br>[root@lab8106 ceph]# docker commit c2ea602c18ac ceph/daemon<br></code></pre></td></tr></table></figure><p>再次启动容器,并且检查内容，可以看到已经修改好了</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ceph]# docker run -i -t --entrypoint /bin/bash ceph/daemon<br>root@65b538fdc61e:/# cat /etc/passwd|grep ceph<br>ceph:x:0:0:Ceph storage service:/var/lib/ceph:/bin/false<br></code></pre></td></tr></table></figure><p>如果需要做其他的改动，这样改下就行</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本篇主要是根据sebastien的镜像做的部署，并且给出一些常用的命令，以及如何进入固化的容器的内部进行修改，方便自己调试环境</p><h2 id="相关资料"><a href="#相关资料" class="headerlink" title="相关资料"></a>相关资料</h2><p><a href="http://www.sebastien-han.fr/blog/2015/06/23/bootstrap-your-ceph-cluster-in-docker/">bootstrap-your-ceph-cluster-in-docker&#x2F;</a></p><h2 id="变更记录"><a href="#变更记录" class="headerlink" title="变更记录"></a>变更记录</h2><table><thead><tr><th align="center">Why</th><th align="center">Who</th><th align="center">When</th></tr></thead><tbody><tr><td align="center">创建</td><td align="center">武汉-运维-磨渣</td><td align="center">2017-03-15</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>在线动态修改ulimit</title>
    <link href="/2017/03/06/%E5%9C%A8%E7%BA%BF%E5%8A%A8%E6%80%81%E4%BF%AE%E6%94%B9ulimit/"/>
    <url>/2017/03/06/%E5%9C%A8%E7%BA%BF%E5%8A%A8%E6%80%81%E4%BF%AE%E6%94%B9ulimit/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>系统中有些地方会进行资源的限制，其中的一个就是open file的限制，操作系统默认限制的是1024,这个值可以通过各种方式修改，本篇主要讲的是如何在线修改，生产上是不可能随便重启进程的</p><h2 id="实践"><a href="#实践" class="headerlink" title="实践"></a>实践</h2><h3 id="查看系统默认的限制"><a href="#查看系统默认的限制" class="headerlink" title="查看系统默认的限制"></a>查看系统默认的限制</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ~]<span class="hljs-comment"># ulimit -a|grep open</span><br>open files                      (-n) 1024<br></code></pre></td></tr></table></figure><p>默认的打开文件是1024</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ~]<span class="hljs-comment"># ps -ef|grep ceph-osd</span><br>ceph     28176     1  0 18:08 ?        00:00:00 /usr/bin/ceph-osd -f --cluster ceph --<span class="hljs-built_in">id</span> 0 --setuser ceph --setgroup ceph<br>root     28619 26901  0 18:10 pts/3    00:00:00 grep --color=auto ceph-osd<br>[root@lab8106 ~]<span class="hljs-comment"># cat /proc/28176/limits |grep open</span><br>Max open files            1048576              1048576              files<br></code></pre></td></tr></table></figure><p>ceph osd的进程的这个参数是1048576 </p><h3 id="通过配置文件修改"><a href="#通过配置文件修改" class="headerlink" title="通过配置文件修改"></a>通过配置文件修改</h3><p>这个参数控制是放在：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ~]<span class="hljs-comment"># cat  /usr/lib/systemd/system/ceph-osd@.service |grep LimitNOFILE -B 1</span><br>[Service]<br>LimitNOFILE=1048576<br></code></pre></td></tr></table></figure><p>这个地方设置的，如果我们有需要修改，那么可以修改这里，这不是本篇的重点，对于运行中的进程如何修改呢</p><h3 id="在线修改进程的limit"><a href="#在线修改进程的limit" class="headerlink" title="在线修改进程的limit"></a>在线修改进程的limit</h3><p>这里调用的是prlimit进行的在线修改<br>查询指定进程的限制</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ~]<span class="hljs-comment"># prlimit --pid 28176</span><br>RESOURCE   DESCRIPTION                             SOFT      HARD UNITS<br>AS         address space <span class="hljs-built_in">limit</span>                unlimited unlimited bytes<br>CORE       max core file size                         0 unlimited blocks<br>CPU        CPU time                           unlimited unlimited seconds<br>DATA       max data size                      unlimited unlimited bytes<br>FSIZE      max file size                      unlimited unlimited blocks<br>LOCKS      max number of file locks held      unlimited unlimited <br>MEMLOCK    max locked-in-memory address space     65536     65536 bytes<br>MSGQUEUE   max bytes <span class="hljs-keyword">in</span> POSIX mqueues            819200    819200 bytes<br>NICE       max <span class="hljs-built_in">nice</span> prio allowed to raise             0         0 <br>NOFILE     max number of open files             1048576   1048576 <br>NPROC      max number of processes              1048576   1048576 <br>RSS        max resident <span class="hljs-built_in">set</span> size              unlimited unlimited pages<br>RTPRIO     max real-time priority                     0         0 <br>RTTIME     <span class="hljs-built_in">timeout</span> <span class="hljs-keyword">for</span> real-time tasks        unlimited unlimited microsecs<br>SIGPENDING max number of pending signals         192853    192853 <br>STACK      max stack size                       8388608 unlimited bytes<br></code></pre></td></tr></table></figure><p>修改指定运行进程的限制</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ~]<span class="hljs-comment"># prlimit --pid 28176 --nofile=104857</span><br>[root@lab8106 ~]<span class="hljs-comment"># prlimit --pid 28176 |grep NOFILE</span><br>NOFILE     max number of open files              104857    104857 <br></code></pre></td></tr></table></figure><p>可以看到修改成功了</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>一般来说ulimit这个限制都是在终端上修改对下次生效，本篇用来记录如何在线修改，如果碰到了，可以这样处理</p><h2 id="变更记录"><a href="#变更记录" class="headerlink" title="变更记录"></a>变更记录</h2><table><thead><tr><th align="center">Why</th><th align="center">Who</th><th align="center">When</th></tr></thead><tbody><tr><td align="center">创建</td><td align="center">武汉-运维-磨渣</td><td align="center">2017-03-06</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>python执行rados命令例子</title>
    <link href="/2017/02/28/python%E6%89%A7%E8%A1%8Crados%E5%91%BD%E4%BB%A4%E4%BE%8B%E5%AD%90/"/>
    <url>/2017/02/28/python%E6%89%A7%E8%A1%8Crados%E5%91%BD%E4%BB%A4%E4%BE%8B%E5%AD%90/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>我们以前的管理平台在python平台下面做的，内部做的一些操作采用的是命令执行，然后解析的方式去做的，ceph自身有python的rados接口，可以直接调用原生接口，然后直接解析json的方式，这样更靠近底层</p><p>在看ceph-dash内部的实现的时候，发现里面的获取集群信息的代码可以留存备用</p><h2 id="代码实例"><a href="#代码实例" class="headerlink" title="代码实例"></a>代码实例</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs raw">#!/usr/bin/env python<br># -*- coding: UTF-8 -*-<br>import json<br><br>from rados import Rados<br>from rados import Error as RadosError<br><br>class CephClusterCommand(dict):<br>    &quot;&quot;&quot;<br>    Issue a ceph command on the given cluster and provide the returned json<br>    &quot;&quot;&quot;<br><br>    def __init__(self, cluster, **kwargs):<br>        dict.__init__(self)<br>        ret, buf, err = cluster.mon_command(json.dumps(kwargs), &#x27;&#x27;, timeout=5)<br>        if ret != 0:<br>            self[&#x27;err&#x27;] = err<br>        else:<br>            self.update(json.loads(buf))<br><br>config=&#123;&#x27;conffile&#x27;: &#x27;/etc/ceph/ceph.conf&#x27;, &#x27;conf&#x27;: &#123;&#125;&#125;<br>with Rados(**config) as cluster:<br>    cluster_status = CephClusterCommand(cluster, prefix=&#x27;status&#x27;, format=&#x27;json&#x27;)<br>    print cluster_status<br></code></pre></td></tr></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>调用原生接口的好处在于,只需要很少的库就可以取得监控系统所需要的值</p><p>最近在研究系统的时候发现一个问题</p><blockquote><p>跟着错误的文档实践只会掉进同一个坑</p></blockquote><p>在遇到一个小的错误的时候，翻到了一个github的Issue，然后看到一个人把自己的配置过程和配置文件详详细细的都写在Issue下面，然后就跟着他的过程走了一遍，发现不论怎么弄都是同样的错误</p><p>而返回去根据另一个正确的文档又走一遍的时候，发现终于跑通了，回顾了一遍，发现是那个错误的过程里面的配置文件里面是有配置项目，不兼容的，而软件也没有抛出相关的错误，然后在同一个地方找了两天</p><p>所以如果有碰到无法解决的操作步骤文档的时候，就尽量不要去根据那个文档操作了，除非自己对细节弄的很清楚了</p><h2 id="变更记录"><a href="#变更记录" class="headerlink" title="变更记录"></a>变更记录</h2><table><thead><tr><th align="center">Why</th><th align="center">Who</th><th align="center">When</th></tr></thead><tbody><tr><td align="center">创建</td><td align="center">武汉-运维-磨渣</td><td align="center">2017-02-28</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>记最近一次ceph故障修复</title>
    <link href="/2017/02/24/%E8%AE%B0%E6%9C%80%E8%BF%91%E4%B8%80%E6%AC%A1ceph%E6%95%85%E9%9A%9C%E4%BF%AE%E5%A4%8D/"/>
    <url>/2017/02/24/%E8%AE%B0%E6%9C%80%E8%BF%91%E4%B8%80%E6%AC%A1ceph%E6%95%85%E9%9A%9C%E4%BF%AE%E5%A4%8D/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>所谓吃一堑长一智，每次面对问题才是最好的学习机会，在面对问题的时候，尽量是能够自己去解决，或者去尝试能够最接近答案，确实无法解决再去寻求他人帮助，这样成长的会更快一些，在学校读书做题的时候，老师也是经常告诉我们要忍住，不要去直接翻答案，在当今的互联网飞速的发展下，在google的帮助下，基本上90%的问题都能找到正确的答案，而我们其实真正需要锻炼的是实践能力和甄别的能力</p><p>去年一年给不少的生产环境解决过问题，在相互交流几次以后，解决问题的过程，基本也熟悉了，一般解决问题的大致流程都是：</p><ul><li>告之我环境的当前状况，需要实现的情况</li><li>准备好远程的环境</li><li>告之对方可能出现的情况，是否可操作，然后解决问题</li><li>交流问题的出现原因以及解决的办法</li></ul><p>目前来看，基本都解决了，对于我来说是一次处理故障经验的累积，对对方来说是环境的恢复，以及下次在出现相同故障的时候，自己能够处理好类似问题</p><p>本次恢复对于我来说也是刷新了的认识，进展只到了解决问题的地方，就结束了，那么我就记录下这次解决问题当中的收获</p><h2 id="处理过程"><a href="#处理过程" class="headerlink" title="处理过程"></a>处理过程</h2><p>故障的发生应该是在一次掉电后触发的,整个集群在重新启动以后，出现了多块磁盘故障的问题，也有主机无法启动的情况，整个集群的PG状态处于一个混乱的状态，stale和incomplete以及peering状态的都很多</p><p>告之对方，需要把相关的osd节点全部都启动起来，然后再看是否有恢复的可能，常规来说，如果三台机器同时出现磁盘损坏，那么这个集群的数据必然会丢失，并且丢失的数据基本将是覆盖所有数据</p><p>在将近一周的时间以后，集群环境磁盘都能挂载，环境可以进行处理了</p><h3 id="出现pg状态一直是peering状态的情况"><a href="#出现pg状态一直是peering状态的情况" class="headerlink" title="出现pg状态一直是peering状态的情况"></a>出现pg状态一直是peering状态的情况</h3><p>用ceph -s 检查集群的状态，集群的状态所有的osd都是正常的up in状态，但是pg状态就是peering状态无法恢复，然后查看都是来自其中的某一个osd，登陆上机器后查看osd的日志，显示无法获取心跳，但是网络明明是好的，并且还能登陆到其他机器上，这就奇怪了，这里先讲下这个地方对方环境埋下的一个坑</p><p>hosts文件里面是这种组合</p><blockquote><p>10.10.10.101  node1<br><br>192.168.10.1  node1<br><br>10.10.10.102  node2<br><br>192.168.10.2  node2<br><br>10.10.10.103  node3<br><br>192.168.10.3  node3</p></blockquote><p>也就是一个主机名映射了两个IP，这个对方说没问题，我也就不多说了，只是我的环境是不会允许这么配置，正是因为这个配置，也就间接隐藏了一个错误的配置，这个错误就是居然在环境当中配置两台主机相同的IP，这也就是为什么出现相同的IP我还能登陆机器</p><p>环境配置成了 </p><blockquote><p>10.10.10.102  node3<br><br>192.168.10.3  node3</p></blockquote><p>也就是node3和node2的集群IP冲突了，所以我在ssh node3的时候能正确登陆node3 ssh node2也能正确登陆node2，只是集群用的IP冲突了，而两台机器之间网络又可以通过其他的网段通信，集群的osd状态是正常，只是pg异常了</p><p>IP冲突在生产环境中是大忌，可能会毁掉整个集群的状态，这个有多大影响？你可以试下配置好一个集群，然后把两个节点的IP配置成一样，然后检查集群的状态和你的上面运行的存储的状态，这个环境因为是在不提供服务状态下，所以带来的影响没有那么大</p><p>在排查到这个错误的时候，已经是晚上快11点了，对方也要回家了，作为运维比较苦逼的就是很多时候，需要待在公司到晚上很晚才能离开，所以问了下是否能留远程给我，得到了许可，可以继续操作，因为这个环境状态来看我觉得还在我的可控范围内，所以想继续尝试，对方也是问过几次，这个环境是否可恢复，我给出的回答也是尽量，IP冲突的问题解决后，重新启动OSD，集群基本快正常了，还是有一些异常的PG需要处理</p><h3 id="出现osd无法启动"><a href="#出现osd无法启动" class="headerlink" title="出现osd无法启动"></a>出现osd无法启动</h3><blockquote><p>verify_reply couldn’t decrypt with error: error decoding block for decryption</p></blockquote><p>这个错误之前有处理经验，时间偏移过大引起认证不通过，登陆上osd对应的机器，检查发现时间偏移了几个小时，引起错误，检查发现ntp配置文件使用的是默认配置文件，至少这台没配置ntp，调整好时间，重启osd解决无法启动问题</p><h3 id="出现PG-incomplete的状态"><a href="#出现PG-incomplete的状态" class="headerlink" title="出现PG incomplete的状态"></a>出现PG incomplete的状态</h3><p>这个状态一般是环境出现过特别的异常状况，PG无法完成状态的更新，这个只要没有OSD被踢出去或者损坏掉，就好办，这个环境的多个PG的数据是一致的，只是状态不对，这个就把PG对应的OSD全部停掉，然后用ceph-object-tool 进行mark complete,然后重启osd，一般来说这个都是能解决了，没出意外，这个环境用工具进行了修复，当然8个这样的PG操作起来还是要比较长的时间，一个个的确认状态，还好都解决了，这个工具是jewel上面集成的，所以在老版本出现这个问题，可以通过升级后进行处理，之前有个别人的生产环境这么处理过这个问题</p><h3 id="出现PG-inconsistent状态的"><a href="#出现PG-inconsistent状态的" class="headerlink" title="出现PG inconsistent状态的"></a>出现PG inconsistent状态的</h3><p>这个是环境中有数据不一致了，这个算比较小的问题，直接对pg进行了repair的修复，然后pg状态就正常了，不得不说现在的ceph比很久前的版本要好很多，Jewel版本的修复工具已经非常完善了，基本能覆盖很多常规的故障</p><h3 id="出现PG-处于activaing状态"><a href="#出现PG-处于activaing状态" class="headerlink" title="出现PG 处于activaing状态"></a>出现PG 处于activaing状态</h3><p>上面的各种问题都处理过来了，到了最后一个，有一个PG处于activating状态，对于ceph来说真是一个都不能少，这个影响的是这个PG所在的存储池当中的数据，影响的范围也是存储池级别的，所以还是希望能够修复好，在反复重启这个pg的所在的osd后，发现这个pg总是无法正常，并且这个机器所在的OSD还会down掉，开始以为是操作没完成，需要很多数据要处理，所以增加了osd_op_thread_suicide_timeout的超时值，发现增大到180s以后还是会挂掉，然后报一堆东西，这个时候想起来还没去检查下这个PG是不是数据之前掉了，检查后就发现了问题，主PG里面的目录居然是空的，而另外两个副本里面的数据都是完整的并且一样的，应该是数据出了问题，造成PG无法正常</p><p>停止掉PG所在的三个OSD，使用ceph-object-tool进行pg数据备份，然后用ceph-object-tool在主PG上删除那个空的pg，这里要注意不要手动删除数据，用工具删除会去清理相关的元数据，而手动去删除可能会残留元数据而引起异常，然后用ceph-object-tool进行数据的导入，然后重启节点，还是无法正常，然后开日志看，发现是对象权限问题，用工具导入的时候，pg内的对象是root权限的，而ceph 启动的权限无法读取，手动给这个pg目录进行给予ceph的权限，重启osd，整个集群正常了</p><p>然后通知对方，环境修复好了，对方回复，有空检查下虚拟机情况，然后就没有然后了···</p><h2 id="这个环境暴露的问题"><a href="#这个环境暴露的问题" class="headerlink" title="这个环境暴露的问题"></a>这个环境暴露的问题</h2><p>1、主机名hosts内单主机名对应多IP<br>这个对于对主机名敏感的应用会受影响<br>2、环境出现IP冲突<br>这个属于细节问题，当然也是最不好排查的一种故障<br>3、环境内没配置内网ntp<br>操作其中的某台机器的时候报了，不清楚整个环境没配置还是只是一台没配置<br>4、有一个mon挂掉了，没有先处理<br>这个对于进行故障处理的时候经常会请求到故障的mon上，造成卡操作，因为不清楚mon状态，所以没启动，直接注释配置文件进行操作<br>5、主机环境的内存分配设置有问题<br>这个因为没太多交流也就不多说了<br>6、ceph版本为10.2.2<br>这个版本有什么问题吗？用的不是好好的吗？这个问题我一直认为公司一定需要有人会选择版本，这里说下怎么选,你也可以自检下自己的版本，当然你们研发觉得没问题，我也就不做过多评论，每个人有自己的想法，别乱来就好</p><ul><li>软件的开发一般会是隔一个版本会是一个长期支持版本，所以尽量不要选择<br>INFERNALIS这个版本，Jewel刚出来，那么你应该选择harmer最后一个版本，而这个时候就会有人选择了Jewel版本，也就是上面的10.2.2或者更低10.0.x</li><li>长期支持版本出来了，那么尽量等版本出到4或者5再去用，也就是现在的Jewel的10.2.5，这个版本不会出现大的bug，K版本就不要随便上生产，等等后面的 Luminous的稳定版本</li><li>内部做升级需求的时候，在选择下个版本的时候同样是选择下一个长期支持版本的稳定版本，这样能保证软件的稳定性，以及版本的生命周期尽量长</li><li>正常运行的时候都没事，碰上异常经常一搜就是已经解决了bug，等到这个时候再升级，就是故障中升级，拉长了故障恢复时间</li><li>所以现在的版本能够用Jewel的最后一个版本和hammer的最后一个版本，一些能backports的功能,也会合成到hammer的最后一个版本</li><li>小版本中间也可能有大变化，0.94.4和0.94.7这两个都是节点版本，节点版本就是你从更低的版本往更高的版本升级的时候，需要先来到节点版本，然后才能往上走，也就是迭代升级，如果不了解清楚，直接把集群升级到起不来也是会出现的事情</li></ul><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本次处理的故障属于组合型的,本篇是博客当中贴命令最少的一篇，当然内容不少，相信你看了处理过程也会有所收获，不管你是搞云计算还是云存储，一定要重视数据的可恢复问题</p><h2 id="变更记录"><a href="#变更记录" class="headerlink" title="变更记录"></a>变更记录</h2><table><thead><tr><th align="center">Why</th><th align="center">Who</th><th align="center">When</th></tr></thead><tbody><tr><td align="center">创建</td><td align="center">武汉-运维-磨渣</td><td align="center">2017-02-24</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>预估ceph的迁移数据量</title>
    <link href="/2017/02/08/%E9%A2%84%E4%BC%B0ceph%E7%9A%84%E8%BF%81%E7%A7%BB%E6%95%B0%E6%8D%AE%E9%87%8F/"/>
    <url>/2017/02/08/%E9%A2%84%E4%BC%B0ceph%E7%9A%84%E8%BF%81%E7%A7%BB%E6%95%B0%E6%8D%AE%E9%87%8F/</url>
    
    <content type="html"><![CDATA[<h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>我们在进行 ceph 的 osd 的增加和减少的维护的时候，会碰到迁移数据，但是我们平时会怎么去回答关于迁移数据量的问题，一般来说，都是说很多，或者说根据环境来看，有没有精确的一个说法，到底要迁移多少数据?这个我以前也有思考过这个问题，当时想是对比前后的pg的分布，然后进行计算，正好在翻一些资料的时候，看到有alram写的一篇博客，alram是Inktank的程序员，也就是sage所在的公司，程序是一个python脚本，本篇会分析下这个对比的思路，以及运行效果</p><p>计算迁移量只需要一个修改后的crushmap就可以了，这个是离线计算的，所以不会对集群有什么影响</p><h2 id="运行效果"><a href="#运行效果" class="headerlink" title="运行效果"></a>运行效果</h2><h3 id="准备修改后的crushmap"><a href="#准备修改后的crushmap" class="headerlink" title="准备修改后的crushmap"></a>准备修改后的crushmap</h3><p>获取当前crushmap</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">ceph osd getcrushmap -o crushmap<br></code></pre></td></tr></table></figure><p>解码crushmap</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">crushtool -d crushmap -o crushmap.txt<br></code></pre></td></tr></table></figure><p>修改crushmap.txt<br>这个根据自己需要，修改成自己想修改成的crushmap即可，可以是增加，也可以是删除</p><h3 id="减少节点的计算"><a href="#减少节点的计算" class="headerlink" title="减少节点的计算"></a>减少节点的计算</h3><p>假如删除一个osd.5 我们需要迁移多少数据<br>将crushmap里面的osd.5的weight改成0</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">crushtool -c crushmap.txt -o crushmapnew<br></code></pre></td></tr></table></figure><p>运行计算脚本</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ceph]# python jisuan.py --crushmap-file crushmapnew<br>POOL                 REMAPPED OSDs        BYTES REBALANCE      OBJECTS REBALANCE   <br>rbd                  59                   6157238296           1469                <br>data                 54                   5918162968           1412                <br>metadata             53                   5825888280           1390<br></code></pre></td></tr></table></figure><p>可以看到迁移的数据量<br>REMAPPED OSDs 下面就是有多少份的PG数据需要迁移，这里面计算的方式是比较前后的分布</p><blockquote><p>[1,2] - &gt; [1,2] 迁移0个<br><br>[1,2] - &gt; [4,2] 迁移1个<br><br>[1,2] - &gt; [4,3] 迁移2个</p></blockquote><p>上面的统计的是这样的个数，所以不太好说是PG或者是OSD，可以理解为PG内数据的份数，因为单个PG可能需要迁移一份，也有可能迁移两份，或者多份</p><h3 id="增加节点的计算"><a href="#增加节点的计算" class="headerlink" title="增加节点的计算"></a>增加节点的计算</h3><p>如果增加一个osd.6 我们需要迁移多少数据<br>直接运行脚本</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ceph]# python jisuan.py --crushmap-file crushmapnew<br>POOL                 REMAPPED OSDs        BYTES REBALANCE      OBJECTS REBALANCE   <br>rbd                  0                    0                    0                   <br>data                 0                    0                    0                   <br>metadata             0                    0                    0 <br></code></pre></td></tr></table></figure><p>可以看到没有输出，这个是因为计算的脚本里面有个地方报错了，ceph内部有个限制，在将crushmap import进osdmap的时候，ceph会验证osdmap里面的osd个数和crushmap里面的osd个数是不是相同<br>所以这个地方需要多做一步，将osd的个数设置成跟预估的一致，这个是唯一对现有集群做的修改操作，</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ceph]# ceph osd setmaxosd 7<br>set new max_osd = 7<br></code></pre></td></tr></table></figure><p>然后再次运行就可以了</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ceph]# python jisuan.py --crushmap-file crushmapnew<br>POOL                 REMAPPED OSDs        BYTES REBALANCE      OBJECTS REBALANCE   <br>rbd                  31                   3590324224           856                 <br>data                 34                   3372220416           804                 <br>metadata             41                   4492099584           1071                <br></code></pre></td></tr></table></figure><p>上面就是运行的效果，下面我们对内部的逻辑进行分析</p><h2 id="代码和代码分析"><a href="#代码和代码分析" class="headerlink" title="代码和代码分析"></a>代码和代码分析</h2><h3 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br></pre></td><td class="code"><pre><code class="hljs raw">#!/usr/bin/env python<br><br>import ast<br>import json<br>import os<br>import subprocess<br>import argparse<br>import sys<br><br>FNULL = open(os.devnull, &#x27;w&#x27;)<br><br># assume the osdmap test output<br># is the same lenght and order...<br># if add support for PG increase<br># that&#x27;s gonna break<br>def diff_output(original, new, pools):<br>    number_of_osd_remap = 0<br>    osd_data_movement = 0<br><br>    results = &#123;&#125;<br><br>    pg_data, pg_objects = get_pg_info()<br><br>    for i in range(len(original)):<br>        orig_i = original[i]<br>        new_i = new[i]<br><br>        if orig_i[0].isdigit():<br>            pg_id = orig_i.split(&#x27;\t&#x27;)[0]<br>            pool_id = pg_id[0]<br>            pool_name = pools[pool_id][&#x27;pool_name&#x27;]<br><br>            if not pool_name in results:<br>                results[pool_name] = &#123;&#125;<br>                results[pool_name][&#x27;osd_remap_counter&#x27;] = 0<br>                results[pool_name][&#x27;osd_bytes_movement&#x27;] = 0<br>                results[pool_name][&#x27;osd_objects_movement&#x27;] = 0<br><br>            original_mappings = ast.literal_eval(orig_i.split(&#x27;\t&#x27;)[1])<br>            new_mappings = ast.literal_eval(new_i.split(&#x27;\t&#x27;)[1])<br>            intersection = list(set(original_mappings).intersection(set(new_mappings)))<br><br>            osd_movement_for_this_pg = int(pools[pool_id][&#x27;pool_size&#x27;]) - len(intersection)<br>            osd_data_movement_for_this_pg = int(osd_movement_for_this_pg) * int(pg_data[pg_id])<br>            osd_object_movement_for_this_pg = int(osd_movement_for_this_pg) * int(pg_objects[pg_id])<br><br>            results[pool_name][&#x27;osd_remap_counter&#x27;] += osd_movement_for_this_pg<br>            results[pool_name][&#x27;osd_bytes_movement&#x27;] += int(osd_data_movement_for_this_pg)<br>            results[pool_name][&#x27;osd_objects_movement&#x27;] += int(osd_object_movement_for_this_pg)<br><br>        elif orig_i.startswith(&#x27;#osd&#x27;):<br>            break<br><br>    return results<br><br>def get_pools_info(osdmap_path):<br>    pools = &#123;&#125;<br>    args = [&#x27;osdmaptool&#x27;, &#x27;--print&#x27;, osdmap_path]<br>    osdmap_out = subprocess.check_output(args, stderr=FNULL).split(&#x27;\n&#x27;)<br>    for line in osdmap_out:<br>        if line.startswith(&#x27;pool&#x27;):<br>            pool_id = line.split()[1]<br>            pool_size = line.split()[5]<br>            pool_name = line.split()[2].replace(&quot;&#x27;&quot;,&quot;&quot;)<br>            pools[pool_id] = &#123;&#125;<br>            pools[pool_id][&#x27;pool_size&#x27;] = pool_size<br>            pools[pool_id][&#x27;pool_name&#x27;] = pool_name<br>        elif line.startswith(&#x27;max_osd&#x27;):<br>            break<br><br>    return pools<br><br>def get_osd_map(osdmap_path):<br>    args = [&#x27;sudo&#x27;, &#x27;ceph&#x27;, &#x27;osd&#x27;, &#x27;getmap&#x27;, &#x27;-o&#x27;, osdmap_path]<br>    subprocess.call(args, stdout=FNULL, stderr=subprocess.STDOUT)<br><br>def get_pg_info():<br>    pg_data = &#123;&#125;<br>    pg_objects = &#123;&#125;<br>    args = [&#x27;sudo&#x27;, &#x27;ceph&#x27;, &#x27;pg&#x27;, &#x27;dump&#x27;]<br>    pgmap = subprocess.check_output(args, stderr=FNULL).split(&#x27;\n&#x27;)<br><br>    for line in pgmap:<br>        if line[0].isdigit():<br>            pg_id = line.split(&#x27;\t&#x27;)[0]<br>            pg_bytes = line.split(&#x27;\t&#x27;)[6]<br>            pg_obj = line.split(&#x27;\t&#x27;)[1]<br>            pg_data[pg_id] = pg_bytes<br>            pg_objects[pg_id] = pg_obj<br>        elif line.startswith(&#x27;pool&#x27;):<br>            break<br><br>    return pg_data, pg_objects<br><br>def osdmaptool_test_map_pgs_dump(original_osdmap_path, crushmap):<br>    new_osdmap_path = original_osdmap_path + &#x27;.new&#x27;<br>    get_osd_map(original_osdmap_path)<br>    args = [&#x27;osdmaptool&#x27;, &#x27;--test-map-pgs-dump&#x27;, original_osdmap_path]<br>    original_osdmaptool_output = subprocess.check_output(args, stderr=FNULL).split(&#x27;\n&#x27;)<br><br>    args = [&#x27;cp&#x27;, original_osdmap_path, new_osdmap_path]<br>    subprocess.call(args, stdout=FNULL, stderr=subprocess.STDOUT)<br>    args = [&#x27;osdmaptool&#x27;, &#x27;--import-crush&#x27;, crushmap, new_osdmap_path]<br>    subprocess.call(args, stdout=FNULL, stderr=subprocess.STDOUT)<br>    args = [&#x27;osdmaptool&#x27;, &#x27;--test-map-pgs-dump&#x27;, new_osdmap_path]<br>    new_osdmaptool_output = subprocess.check_output(args, stderr=FNULL).split(&#x27;\n&#x27;)<br><br>    pools = get_pools_info(original_osdmap_path)<br>    results = diff_output(original_osdmaptool_output, new_osdmaptool_output, pools)<br><br>    return results<br><br><br>def dump_plain_output(results):<br>    sys.stdout.write(&quot;%-20s %-20s %-20s %-20s\n&quot; % (&quot;POOL&quot;, &quot;REMAPPED OSDs&quot;, &quot;BYTES REBALANCE&quot;, &quot;OBJECTS REBALANCE&quot;))<br><br>    for pool in results:<br>        sys.stdout.write(&quot;%-20s %-20s %-20s %-20s\n&quot; % (<br>            pool,<br>            results[pool][&#x27;osd_remap_counter&#x27;],<br>            results[pool][&#x27;osd_bytes_movement&#x27;],<br>            results[pool][&#x27;osd_objects_movement&#x27;]<br>            ))<br><br>def cleanup(osdmap):<br>    FNULL.close()<br>    new_osdmap = osdmap + &#x27;.new&#x27;<br>    os.remove(new_osdmap)<br><br>def parse_args():<br>    parser = argparse.ArgumentParser(description=&#x27;Ceph CRUSH change data movement calculator.&#x27;)<br><br>    parser.add_argument(<br>        &#x27;--osdmap-file&#x27;,<br>        help=&quot;Where to save the original osdmap. Temp one will be &lt;location&gt;.new. Default: /tmp/osdmap&quot;,<br>        default=&quot;/tmp/osdmap&quot;,<br>        dest=&quot;osdmap_path&quot;<br>        )<br>    parser.add_argument(<br>        &#x27;--crushmap-file&#x27;,<br>        help=&quot;CRUSHmap to run the movement test against.&quot;,<br>        required=True,<br>        dest=&quot;new_crushmap&quot;<br>        )<br><br>    parser.add_argument(<br>        &#x27;--format&#x27;,<br>        help=&quot;Output format. Default: plain&quot;,<br>        choices=[&#x27;json&#x27;, &#x27;plain&#x27;],<br>        dest=&quot;format&quot;,<br>        default=&quot;plain&quot;<br>        )<br><br>    args = parser.parse_args()<br>    return args<br><br>if __name__ == &#x27;__main__&#x27;:<br>    ctx = parse_args()<br><br>    results = osdmaptool_test_map_pgs_dump(ctx.osdmap_path, ctx.new_crushmap)<br>    cleanup(ctx.osdmap_path)<br><br>    if ctx.format == &#x27;json&#x27;:<br>        print json.dumps(results)<br>    elif ctx.format == &#x27;plain&#x27;:<br>        dump_plain_output(results)<br></code></pre></td></tr></table></figure><p>直接放在这里方便拷贝，也可以去原作者的gist里面去获取</p><h3 id="主要代码分析"><a href="#主要代码分析" class="headerlink" title="主要代码分析"></a>主要代码分析</h3><p>首先获取osdmap</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">ceph osd getmap -o /tmp/osdmap<br></code></pre></td></tr></table></figure><p>获取原始pg分布</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">使用osdmaptool  --test-map-pgs-dump /tmp/osdmap<br></code></pre></td></tr></table></figure><p>获取新的crushmap</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">这个是自己编辑成需要的crushmap<br></code></pre></td></tr></table></figure><p>将新的crushmap注入到osdmap里面得到新的osdmap</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">osdmaptool --import-crush  crushmap  /tmp/new_osdmap_path<br></code></pre></td></tr></table></figure><p>根据新的osdmap进行计算新的分布</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">osdmaptool  --test-map-pgs-dump /tmp/new_osdmap_path<br></code></pre></td></tr></table></figure><p>然后比较两个输入进行对比得到结果</p><h2 id="相关链接"><a href="#相关链接" class="headerlink" title="相关链接"></a>相关链接</h2><p><a href="http://blog-fromsomedude.rhcloud.com/2017/01/19/Calculate-data-migration-when-changing-the-CRUSHmap/">Calculate data migration when changing the CRUSHmap</a><br><a href="https://gist.github.com/alram/c6b1129a4c9100ab5184197d1455a6bd">alram&#x2F;crush_data_movement_calculator.py</a></p><h2 id="变更记录"><a href="#变更记录" class="headerlink" title="变更记录"></a>变更记录</h2><table><thead><tr><th align="center">Why</th><th align="center">Who</th><th align="center">When</th></tr></thead><tbody><tr><td align="center">创建</td><td align="center">武汉-运维-磨渣</td><td align="center">2017-02-08</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Linux 升级内核开启 TCP BBR 有多大好处</title>
    <link href="/2017/01/24/Linux%20%E5%8D%87%E7%BA%A7%E5%86%85%E6%A0%B8%E5%BC%80%E5%90%AF%20TCP%20BBR%20%E6%9C%89%E5%A4%9A%E5%A4%A7%E5%A5%BD%E5%A4%84/"/>
    <url>/2017/01/24/Linux%20%E5%8D%87%E7%BA%A7%E5%86%85%E6%A0%B8%E5%BC%80%E5%90%AF%20TCP%20BBR%20%E6%9C%89%E5%A4%9A%E5%A4%A7%E5%A5%BD%E5%A4%84/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>如果你有订阅一些科技新闻，应该会有看过内核在4.9当中加入了一个新的算法，来解决在有一定的丢包率的情况下的带宽稳定的问题，这个是谷歌为我们带来的干货，新的 TCP 拥塞控制算法 BBR (Bottleneck Bandwidth and RTT)，谷歌一向的做法是，先上生产，然后发论文，然后有可能开源，所以这个已经合并到了内核4.9分支当中，算法带来的改变在出的测试报告当中有很详细的数据展示，这个看多了可能反而不知道到底会有什么明显改变，特别是对于我们自己的场景</p><p>那么本篇就是来做一个实践的，开看看在通用的一些场景下，这个改变有多大，先说下结果，是真的非常大</p><h2 id="实践"><a href="#实践" class="headerlink" title="实践"></a>实践</h2><p>还是我的两台机器lab8106和lab8107,lab8106做一个webserver，lab8107模拟客户端，用简单的wget来进行测试，环境为同一个交换机上的万兆网卡服务器</p><p>我们本次测试只测试一种丢包率的情况就是1%，有兴趣的情况下，可以自己去做些其他丢包率的测试，大多数写在丢包率20%以上的时候，效果可能没那么好，这个高丢包率不是我们探讨的情况，毕竟不是常用的场景<br>###安装新内核<br>内核可以自己选择4.9或者以上的进行安装，也可以用yum安装,这里只是测试，就yum直接安装</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">yum --enablerepo=elrepo-kernel install kernel-ml<br></code></pre></td></tr></table></figure><p>修改启动项</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs raw">grub2-editenv list<br>grub2-set-default &#x27;CentOS Linux (4.9.5-1.el7.elrepo.x86_64) 7 (Core)&#x27;<br>grub2-editenv list<br></code></pre></td></tr></table></figure><h3 id="准备下载数据"><a href="#准备下载数据" class="headerlink" title="准备下载数据"></a>准备下载数据</h3><p>准备一个web服务器然后把一个iso丢到根目录下，用于客户端的wget</p><h3 id="设置丢包率"><a href="#设置丢包率" class="headerlink" title="设置丢包率"></a>设置丢包率</h3><p>这里用tc进行控制的，也就是一条命令就可以了,这个还可以做其他很多控制，可以自行研究</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">tc qdisc add dev enp2s0f0 root netem loss 1%<br></code></pre></td></tr></table></figure><p>如果需要取消限制</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">tc qdisc del root dev enp2s0f0<br></code></pre></td></tr></table></figure><h3 id="设置新的算法"><a href="#设置新的算法" class="headerlink" title="设置新的算法"></a>设置新的算法</h3><p>讲下面的两个配置文件添加到&#x2F;etc&#x2F;sysctl.conf</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs raw">net.ipv4.tcp_congestion_control=bbr<br>net.core.default_qdisc=fq<br></code></pre></td></tr></table></figure><p>然后执行sysctl -p让它生效</p><p>检查是参数是否生效</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 rpmbuild]# sysctl net.ipv4.tcp_available_congestion_control<br>net.ipv4.tcp_available_congestion_control = bbr cubic reno<br></code></pre></td></tr></table></figure><p>检查模块是否开启</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 rpmbuild]# lsmod | grep bbr<br>tcp_bbr                16384  0 <br></code></pre></td></tr></table></figure><p>如果需要恢复成默认的就修改成下面这个值，然后执行sysct -p恢复默认</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs raw">net.ipv4.tcp_congestion_control = cubic<br>net.core.default_qdisc = pfifo_fast<br></code></pre></td></tr></table></figure><h3 id="开始测试"><a href="#开始测试" class="headerlink" title="开始测试"></a>开始测试</h3><p>为了避免磁盘本身的写入速度的影响，我们直接将数据wget到内存当中去</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8107 ~]# cd /dev/shm<br></code></pre></td></tr></table></figure><p>写入到这个目录当中的数据就是直接写入内存的<br>我们先来对比下没有丢包的时候的速度<br>####默认算法，无丢包率</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs raw"> wget http://192.168.8.106/FreeBSD-10.2-RELEASE-amd64-dvd1.iso<br>2017-01-24 12:34:01 (909 MB/s) - ‘FreeBSD-10.2-RELEASE-amd64-dvd1.iso’ saved<br></code></pre></td></tr></table></figure><h4 id="BBR算法，无丢包率"><a href="#BBR算法，无丢包率" class="headerlink" title="BBR算法，无丢包率"></a>BBR算法，无丢包率</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs raw">wget http://192.168.8.106/FreeBSD-10.2-RELEASE-amd64-dvd1.iso<br>2017-01-24 12:36:21 (913 MB/s) - ‘FreeBSD-10.2-RELEASE-amd64-dvd1.iso’ saved<br></code></pre></td></tr></table></figure><p>上面的两组数据基本一样，没有什么差别<br>下面的测试将丢包率控制到1%，然后继续测试</p><h4 id="默认算法，1-丢包率"><a href="#默认算法，1-丢包率" class="headerlink" title="默认算法，1%丢包率"></a>默认算法，1%丢包率</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs raw">wget http://192.168.8.106/FreeBSD-10.2-RELEASE-amd64-dvd1.iso<br>2017-01-24 12:38:47 (142 MB/s) - ‘FreeBSD-10.2-RELEASE-amd64-dvd1.iso’ saved<br></code></pre></td></tr></table></figure><p>可以看到在1%丢包率下，速度已经降为正常的1&#x2F;6左右了，是一个很大的衰减</p><h4 id="BBR算法，1-丢包率"><a href="#BBR算法，1-丢包率" class="headerlink" title="BBR算法，1%丢包率"></a>BBR算法，1%丢包率</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs raw">wget http://192.168.8.106/FreeBSD-10.2-RELEASE-amd64-dvd1.iso<br>2017-01-24 12:40:25 (896 MB/s) - ‘FreeBSD-10.2-RELEASE-amd64-dvd1.iso’<br></code></pre></td></tr></table></figure><p>可以看到在1%丢包率下，还能维持接近900MB&#x2F;s的下载速度，相对于默认算法，相差了真是非常非常的大，google在很多情况下技术甩了其他公司真的是几条街了</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>上面的测试通过一个简单的场景来验证了bbr算法对于丢包情况下的带宽的优化，这个对于一些提供下载服务，并且有一定的丢包率的场景的情况下，能够有很大的改善，所以算法对于技术的改变还是非常大的，很多时候就是这种异常情况下的差别，才是真正的差别</p><p>顺便提一句微博的技术经理@来去之间说的一句话：</p><blockquote><p>曾经有同事问我，为啥有些新业务给老员工做，交学费，而不是市场上招人更有效率。。。俺说渣浪业务起起伏伏，如果所有战线都用雇佣兵，顺的时候势如破竹，逆的时候兵败山倒了。。公司和员工都是相互扶持的，有些新业务，员工有能力做，只是经验不足，公司多付出一些，就当给未来不顺的时候上一份保险了</p></blockquote><h2 id="相关链接"><a href="#相关链接" class="headerlink" title="相关链接"></a>相关链接</h2><p><a href="https://www.zhihu.com/question/53559433">Linux Kernel 4.9 中的 BBR 算法与之前的 TCP 拥塞控制相比有什么优势？</a><br><a href="https://www.mf8.biz/linux-kernel-with-tcp-bbr/">Linux 升级内核开启 TCP BBR 实现高效单边加速</a></p><h2 id="变更记录"><a href="#变更记录" class="headerlink" title="变更记录"></a>变更记录</h2><table><thead><tr><th align="center">Why</th><th align="center">Who</th><th align="center">When</th></tr></thead><tbody><tr><td align="center">创建</td><td align="center">武汉-运维-磨渣</td><td align="center">2017-01-24</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>rbd-mirror配置指南-单向备份</title>
    <link href="/2017/01/22/rbd-mirror%E9%85%8D%E7%BD%AE%E6%8C%87%E5%8D%97-%E5%8D%95%E5%90%91%E5%A4%87%E4%BB%BD/"/>
    <url>/2017/01/22/rbd-mirror%E9%85%8D%E7%BD%AE%E6%8C%87%E5%8D%97-%E5%8D%95%E5%90%91%E5%A4%87%E4%BB%BD/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>RBD 的 mirroring 功能将在Jewel中实现的，这个Jewel版本已经发布了很久了,这个功能已经在这个发布的版本中实现了，本来之前写过一篇文章，但是有几个朋友根据文档配置后，发现还是有问题，自己在进行再次配置的时候也发现有些地方没讲清楚，容易造成误解，这里对文档进行再一次的梳理</p><h2 id="基本原理"><a href="#基本原理" class="headerlink" title="基本原理"></a>基本原理</h2><p>我们试图解决的或者至少需要克服的问题是，ceph在内部是强一致性的，这个对于跨区域的情况数据同步是无法接受的，一个请求需要异地返回再确认完成，这个在性能上肯定是无法接受的，这就是为什么基本上无法部署跨区域的ceph集群</p><p>因此我们需要有一种机制能够让我们在不同区域的集群之间复制块设备。这个能够帮助我们实现两个功能：</p><ul><li>灾难恢复</li><li>全球块设备分布（跨地理位置）</li></ul><h2 id="内部的实现"><a href="#内部的实现" class="headerlink" title="内部的实现"></a>内部的实现</h2><p><img src="/images/blog/o_200901034252%E7%94%BB%E5%9B%BE.png" alt="画图.png-34.8kB"></p><p>从上图所示是进行的主备模式的备份，其实这个只是看怎么应用了，在里面是自动实现的主主的模式，双向同步的，只是在应用中需要注意不要去同时操作同一个image，这个功能是作为主备去使用的，以备真正有问题的时候去实现故障恢复，这个同步是异步的</p><h2 id="一个新的进程"><a href="#一个新的进程" class="headerlink" title="一个新的进程"></a>一个新的进程</h2><p>一个新的守护程序：rbd-mirror 将会负责将一个镜像从一个集群同步到另一个，rbd-mirror需要在两个集群上都配置，它会同时连接本地和远程的集群。在jewel版本中还是一对一的方式，在以后的版本中会实现一对多的，所以在以后的版本可以配置一对多的备份</p><p>作为起点，这个功能讲使用配置文件连接集群，使用用户和密钥。使用admin用户就可以了，使用的验证方式就是默认的cephx的方式</p><p>为了相互识别，两个集群都需要相互注册使用  rbd mirror pool peer add 命令， 这个在下面会实践</p><h2 id="镜像"><a href="#镜像" class="headerlink" title="镜像"></a>镜像</h2><p><img src="/images/blog/o_200901034303ceph-rbd-mirror-inside.png" alt="ceph-rbd-mirror-inside.png-80.8kB"><br>The RBD mirroring 依赖两个新的rbd的属性</p><ul><li>journaling: 启动后会记录image的事件</li><li>mirroring: 明确告诉rbd-mirror需要复制这个镜像</li></ul><p>也有命令可以禁用单独的某个镜像。journaling可以看做是另一个rbd的image（一些rados对象），一般情况下，先写日志，然后返回客户端，然后被写入底层的rbd的image，出于性能考虑，这个journal可以跟它的镜像不在一个存储池当中，目前是一个image一个journal，最近应该会沿用这个策略，直到ceph引入一致性组。关于一致性组的概念就是一组卷，然后用的是一个RBD image。可以在所有的组中执行快照操作，有了一致性的保证，所有的卷就都在一致的状态。当一致性组实现的时候，我们就可以用一个journal来管理所有的RBD的镜像</p><p>可以给一个已经存在image开启journal么，可以的，ceph将会将你的镜像做一个快照，然后对快照做一个复制，然后开启journal，这都是后台执行的一个任务</p><p>可以启用和关闭单个镜像或者存储池的mirror功能，如果启用了journal功能，那么每个镜像将会被复制</p><p>可以使用 rbd mirror pool enable启用它</p><h2 id="灾难恢复"><a href="#灾难恢复" class="headerlink" title="灾难恢复"></a>灾难恢复</h2><p>交叉同步复制是可以的，默认的就是这个方式，这意味着<strong>两个地方的存储池名称需要相同的</strong>这个会带来两个问题</p><ul><li>使用相同的存储做备份做使用会影响性能的</li><li>相同的池名称在进行恢复的时候也更容易。openstack里面只需要记录卷ID即可</li></ul><p>每个image都有 mirroring_directory 记录当前active的地方。在本地镜像提示为 primary的时候，是可写的并且远程的站点上就会有锁，这个image就是不可写的。只有在primary镜像降级，备份的点升级就可以了，demoted 和 promoted来控制这里，这就是为什么引入了等级制度，一旦备份的地方升级了，那么主的就自动降级了，这就意味着同步的方向就会发生变化了</p><p>如果出现脑裂的情况，那么rbd-mirror将会停止同步，你自己需要判断哪个是最新的image，然后手动强制去同步<code> rbd mirror image resync</code></p><p>上面基本参照的是sebastien翻译的，原文只是做了简短的说明，下面是我的实践部分</p><h1 id="配置实践部分"><a href="#配置实践部分" class="headerlink" title="配置实践部分"></a>配置实践部分</h1><h2 id="先介绍下一些简单的概念"><a href="#先介绍下一些简单的概念" class="headerlink" title="先介绍下一些简单的概念"></a>先介绍下一些简单的概念</h2><h3 id="rbd-mirror-进程"><a href="#rbd-mirror-进程" class="headerlink" title="rbd-mirror 进程"></a>rbd-mirror 进程</h3><p>rbd-mirror进程负责将镜像从一个Ceph集群同步到另一个集群</p><p>根据复制的类型，rbd-mirror可以在单个集群上或者是镜像的两个集群上都运行</p><ul><li>单向备份<ul><li>当数据从主集群备份到备用的集群的时候，rbd-mirror仅在备份群集上运行。</li></ul></li><li>双向备份<ul><li>如果两个集群互为备份的时候，rbd-mirror需要在两个集群上都运行</li></ul></li></ul><p>为了更清晰的理解这个配置，我们本次实践只进行单向备份的配置，也就是只备份一个集群的镜像到另外一个集群</p><blockquote><p>rbd-mirror的每个实例必须能够同时连接到两个Ceph集群,因为需要同两个集群都进行数据通信<br>每个Ceph集群只运行一个rbd-mirror进程</p></blockquote><h3 id="Mirroring-模式"><a href="#Mirroring-模式" class="headerlink" title="Mirroring 模式"></a>Mirroring 模式</h3><p>mirroring是基于存储池进行的peer，ceph支持两种模式的镜像，根据镜像来划分有：</p><ul><li><p>存储池模式</p></li><li><p>一个存储池内的所有镜像都会进行备份</p></li><li><p>镜像模式</p></li><li><p>只有指定的镜像才会进行备份</p></li></ul><p>本次配置选择的模式是镜像的模式，也就是指定的镜像才会进行备份</p><h3 id="Image-状态"><a href="#Image-状态" class="headerlink" title="Image 状态"></a>Image 状态</h3><p>做了mirroring的Image的状态有:<br>primary (可以修改)<br>non-primary (不能修改).<br>当第一次对image进行开启mirroring的时候 .Images 自动 promoted 为 primary</p><h2 id="开始配置"><a href="#开始配置" class="headerlink" title="开始配置"></a>开始配置</h2><p>首先配置两个集群，配置的集群都没有更改名称，都是ceph，我们通过配置文件来控制集群的识别，我的环境是单主机集群，lab8106和lab8107两台机器<br>lab8106为local集群，lab8107为remote集群，准备把lab8106的image备份到lab8107的集群上<br>在ceph.conf当中添加：</p><blockquote><p>rbd default features &#x3D; 125</p></blockquote><p>需要exclusive-lock和journaling属性<br>开启这两个个属性可以在创建的时候指定<br>语法：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">rbd create &lt;image-name&gt; --size &lt;megabytes&gt; --pool &lt;pool-name&gt; --imagefeature &lt;feature&gt; <br></code></pre></td></tr></table></figure><p>例子：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">rbd create image-1 --size 1024 --pool rbd --image-feature exclusive-lock,journaling<br></code></pre></td></tr></table></figure><p>这个是在lab8106上执行，因为我们需要对lab8106进行备份<br>也可以在创建以后开启属性：<br>语法：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">rbd feature enable &lt;pool-name&gt;/&lt;image-name&gt; &lt;feature-name&gt;<br></code></pre></td></tr></table></figure><p>例子：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs raw">rbd feature enable rbd/image-1 exclusive-lock<br>rbd feature enable rbd/image-1 journaling<br></code></pre></td></tr></table></figure><p>上面有三种方法开启属性，选择习惯或者需要的一种就可以</p><h3 id="开启存储池的mirror的模式"><a href="#开启存储池的mirror的模式" class="headerlink" title="开启存储池的mirror的模式"></a>开启存储池的mirror的模式</h3><p>我们准备开启集群镜像备份模式<br>语法：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">rbd mirror pool enable &lt;pool-name&gt; &lt;mode&gt;<br></code></pre></td></tr></table></figure><p>在lab8106主机上执行:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">rbd mirror pool enable rbd image<br></code></pre></td></tr></table></figure><p>在lab8107主机上执行：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">rbd mirror pool enable rbd image<br></code></pre></td></tr></table></figure><p>上面的操作是对rbd存储池启动image模式的mirror配置<br>如果需要关闭：<br>语法：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">rbd mirror pool disable &lt;pool-name&gt; &lt;mode&gt;<br></code></pre></td></tr></table></figure><p>执行:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">rbd mirror pool disable rbd image<br></code></pre></td></tr></table></figure><h3 id="处理配置文件和kerring"><a href="#处理配置文件和kerring" class="headerlink" title="处理配置文件和kerring"></a>处理配置文件和kerring</h3><p>在lab8106上执行</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ceph]# scp /etc/ceph/ceph.conf lab8107:/etc/ceph/local.conf<br>[root@lab8106 ceph]# scp /etc/ceph/ceph.client.admin.keyring lab8107:/etc/ceph/local.client.admin.keyring<br>[root@lab8106 ceph]#cp /etc/ceph/ceph.conf /etc/ceph/local.conf<br>[root@lab8106 ceph]#cp /etc/ceph/ceph.client.admin.keyring /etc/ceph/local.client.admin.keyring<br></code></pre></td></tr></table></figure><p>在lab8107上执行：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8107 ceph]# scp /etc/ceph/ceph.conf lab8106:/etc/ceph/remote.conf<br>[root@lab8107 ceph]# scp /etc/ceph/ceph.client.admin.keyring lab8106:/etc/ceph/remote.client.admin.keyring<br>[root@lab8107 ceph]#cp /etc/ceph/ceph.conf /etc/ceph/remote.conf<br>[root@lab8107 ceph]#cp /etc/ceph/ceph.client.admin.keyring /etc/ceph/remote.client.admin.keyring<br></code></pre></td></tr></table></figure><p>执行完了后在两台机器上给予权限</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ceph]# chown ceph:ceph -R /etc/ceph<br>[root@lab8107 ceph]# chown ceph:ceph -R /etc/ceph<br></code></pre></td></tr></table></figure><p>检验上面设置是否完成<br>在lab8106执行</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ceph]# ceph --cluster local mon stat<br>e1: 1 mons at &#123;lab8106=192.168.8.106:6789/0&#125;, election epoch 3, quorum 0 lab8106<br>[root@lab8106 ceph]# ceph --cluster remote mon stat<br>e1: 1 mons at &#123;lab8107=192.168.8.107:6789/0&#125;, election epoch 3, quorum 0 lab8107<br></code></pre></td></tr></table></figure><p>在lab8107执行</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs raw">root@lab8107:~/ceph# ceph --cluster local mon stat<br>e1: 1 mons at &#123;lab8106=192.168.8.106:6789/0&#125;, election epoch 3, quorum 0 lab8106<br>root@lab8107:~/ceph# ceph --cluster remote mon stat<br>e1: 1 mons at &#123;lab8107=192.168.8.107:6789/0&#125;, election epoch 3, quorum 0 lab8107<br></code></pre></td></tr></table></figure><p>到这里就是两个集群可以通过local和remote进行通信了</p><h3 id="增加peer"><a href="#增加peer" class="headerlink" title="增加peer"></a>增加peer</h3><p>我们这里是做单个集群的备份，为了方便我们这里都用admin的keyring<br>语法</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">rbd mirror pool peer add &lt;pool-name&gt; &lt;client-name&gt;@&lt;cluster-name&gt;<br></code></pre></td></tr></table></figure><p>这个是为了让rbd-mirror进程找到它peer的集群的存储池<br>在lab8106上执行</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ceph]# rbd --cluster local mirror pool peer add rbd client.admin@remote<br>[root@lab8106 ceph]# rbd --cluster remote mirror pool peer add rbd client.admin@local<br></code></pre></td></tr></table></figure><p>查询peer状态</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ceph]# rbd mirror pool info rbd --cluster local<br>Mode: image<br>Peers: <br>  UUID                                 NAME   CLIENT       <br>  a050a0f5-9448-43f2-872f-87c394083871 remote client.admin<br>[root@lab8106 ceph]# rbd mirror pool info rbd --cluster remote<br>Mode: image<br>Peers: <br>  UUID                                 NAME  CLIENT       <br>  8d7b3fa4-be44-4e25-b0b7-cf4bdb62bf10 local client.admin <br></code></pre></td></tr></table></figure><p>如果需要删除peer<br>语法：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">rbd mirror pool peer remove &lt;pool-name&gt; &lt;peer-uuid&gt;<br></code></pre></td></tr></table></figure><p>查询存储池状态</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ceph]# rbd mirror pool status rbd<br>health: OK<br>images: 0 total<br></code></pre></td></tr></table></figure><h3 id="开启image的mirror"><a href="#开启image的mirror" class="headerlink" title="开启image的mirror"></a>开启image的mirror</h3><p>在lab8106执行</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">rbd mirror image enable rbd/image-1<br></code></pre></td></tr></table></figure><p>查询镜像的状态</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ceph]# rbd info rbd/image-1<br>rbd image &#x27;image-1&#x27;:<br>size 1024 MB in 256 objects<br>order 22 (4096 kB objects)<br>block_name_prefix: rbd_data.102c2ae8944a<br>format: 2<br>features: exclusive-lock, journaling<br>flags: <br>journal: 102c2ae8944a<br>mirroring state: enabled<br>mirroring global id: dabdbbed-7c06-4e1d-b860-8dd104509565<br>mirroring primary: true<br></code></pre></td></tr></table></figure><h3 id="开启rbd-mirror的同步进程"><a href="#开启rbd-mirror的同步进程" class="headerlink" title="开启rbd-mirror的同步进程"></a>开启rbd-mirror的同步进程</h3><p>先用调试模式启动进程看看情况<br>在lab8107的机器上执行</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8107 ceph]# rbd-mirror -d --setuser ceph --setgroup ceph --cluster remote -i admin<br>2017-01-22 17:43:53.688820 7fc926dc6c40  0 set uid:gid to 167:167 (ceph:ceph)<br>2017-01-22 17:43:53.688840 7fc926dc6c40  0 ceph version 10.2.5 (c461ee19ecbc0c5c330aca20f7392c9a00730367), process rbd-mirror, pid 32080<br></code></pre></td></tr></table></figure><p>如果确认没问题就用服务来控制启动</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">vim /usr/lib/systemd/system/ceph-rbd-mirror@.service<br></code></pre></td></tr></table></figure><p>修改</p><blockquote><p>Environment&#x3D;CLUSTER&#x3D;remote</p></blockquote><p>然后启动<br>语法为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8107 ceph]#systemctl start ceph-rbd-mirror@&lt;client-id&gt;<br></code></pre></td></tr></table></figure><p>在lab8107上启动进程</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8107 ceph]# systemctl start  ceph-rbd-mirror@admin<br>[root@lab8107 ceph]# ps -ef|grep rbd<br>ceph      4325     1  1 17:59 ?        00:00:00 /usr/bin/rbd-mirror -f --cluster remote --id admin --setuser ceph --setgroup ceph<br></code></pre></td></tr></table></figure><p>查询镜像的同步的状态</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]# rbd mirror image status rbd/image-1 --cluster remote<br>image-1:<br>  global_id:   dabdbbed-7c06-4e1d-b860-8dd104509565<br>  state:       up+replaying<br>  description: replaying, master_position=[object_number=2, tag_tid=2, entry_tid=3974], mirror_position=[object_number=3, tag_tid=2, entry_tid=2583], entries_behind_master=1391<br>  last_update: 2017-01-22 17:54:22<br></code></pre></td></tr></table></figure><p>检查数据是否同步<br>在lab8107执行</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8107 ceph]# rbd info rbd/image-1<br>rbd image &#x27;image-1&#x27;:<br>size 1024 MB in 256 objects<br>order 22 (4096 kB objects)<br>block_name_prefix: rbd_data.127b515f007c<br>format: 2<br>features: exclusive-lock, journaling<br>flags: <br>journal: 127b515f007c<br>mirroring state: enabled<br>mirroring global id: fb976ffb-a71e-4714-8464-06381643f984<br>mirroring primary: false<br></code></pre></td></tr></table></figure><p>可以看到数据已经同步过来了</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>通过配置文件控制，可以实现集群名称不修改<br>rbd-mirror进程是在备份的集群上面启动的，并且是要能跟主集群和备份集群都能通信的，也就是peer都需要做，并且用户权限要控制好</p><p>根据上面的操作流程操作下来，应该是能够配置好rbd-mirror的</p><h2 id="相关链接"><a href="#相关链接" class="headerlink" title="相关链接"></a>相关链接</h2><p><a href="http://www.sebastien-han.fr/blog/2016/03/28/ceph-jewel-preview-ceph-rbd-mirroring/">Ceph Jewel Preview: Ceph RBD mirroring</a></p><h2 id="变更记录"><a href="#变更记录" class="headerlink" title="变更记录"></a>变更记录</h2><table><thead><tr><th align="center">Why</th><th align="center">Who</th><th align="center">When</th></tr></thead><tbody><tr><td align="center">创建</td><td align="center">武汉-运维-磨渣</td><td align="center">2017-01-22</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>ceph的rbd备份软件ceph-backup</title>
    <link href="/2017/01/19/ceph%E7%9A%84rbd%E5%A4%87%E4%BB%BD%E8%BD%AF%E4%BB%B6ceph-backup/"/>
    <url>/2017/01/19/ceph%E7%9A%84rbd%E5%A4%87%E4%BB%BD%E8%BD%AF%E4%BB%B6ceph-backup/</url>
    
    <content type="html"><![CDATA[<p>teralytics是一家国外的大数据公司，这个是他们开源的ceph的备份的工具，在twitter上搜索相关信息的时候看到，觉得不错就拿来试用一番</p><h2 id="这是个什么软件"><a href="#这是个什么软件" class="headerlink" title="这是个什么软件"></a>这是个什么软件</h2><p>一个用来备份 ceph 的 rbd 的image的开源软件，提供了两种模式<br>增量：在给定备份时间窗口内基于 rbd 快照的增量备份<br>完全：完整镜像导出时不包含快照</p><blockquote><p>注意一致性：此工具可以生成 rbd 镜像的快照，而不会感知到它们的文件系统的状态，注意下 rbd 快照的一致性限制（<a href="http://docs.ceph.com/docs/hammer/rbd/rbd-snapshot/">官网文档</a>） 由于“完全”模式不使用快照，“完全”模式下的实时映像备份不一致（“增量”模式始终使用快照）</p></blockquote><p>超过时间窗口以后，会进行一次全量备份，并且把之前的快照删除掉，重新进行一次全量备份，并且基于这个时间窗口计算是否需要删除备份的文件</p><p>软件包含以下功能：</p><ul><li>支持存储池和多image的指定</li><li>支持自定义备份目标路径</li><li>配置文件支持</li><li>支持备份窗口设置</li><li>支持压缩选项</li><li>支持增量和全量备份的配置</li></ul><h2 id="编译安装"><a href="#编译安装" class="headerlink" title="编译安装"></a>编译安装</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ~]<span class="hljs-comment">#git clone https://github.com/teralytics/ceph-backup.git</span><br>[root@lab8106 ~]<span class="hljs-comment"># cd ceph-backup</span><br>[root@lab8106 ceph-backup]<span class="hljs-comment"># python setup.py install</span><br></code></pre></td></tr></table></figure><p>安装过程中会下载一些东西，注意要有网络，需要等待一会</p><h2 id="准备配置文件"><a href="#准备配置文件" class="headerlink" title="准备配置文件"></a>准备配置文件</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ceph-backup]<span class="hljs-comment"># mkdir /etc/cephbackup/</span><br>[root@lab8106 ceph-backup]<span class="hljs-comment"># cp ceph-backup.cfg /etc/cephbackup/cephbackup.conf</span><br></code></pre></td></tr></table></figure><p>我的配置文件如下，备份 rbd 存储的 zp 的镜像，支持多 image，images后面用逗号隔开就可以</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ~]<span class="hljs-comment"># cat /etc/cephbackup/cephbackup.conf </span><br>[rbd]<br>window size = 7<br>window unit = days<br>destination directory = /tmp/<br>images = zp<br>compress = <span class="hljs-built_in">yes</span><br>ceph config = /etc/ceph/ceph.conf<br>backup mode = full<br>check mode = no<br></code></pre></td></tr></table></figure><h2 id="开始备份"><a href="#开始备份" class="headerlink" title="开始备份"></a>开始备份</h2><h3 id="全量备份配置"><a href="#全量备份配置" class="headerlink" title="全量备份配置"></a>全量备份配置</h3><p>上面的配置文件已经写好了，直接执行备份命令就可以了</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ~]<span class="hljs-comment"># cephbackup</span><br>Starting backup <span class="hljs-keyword">for</span> pool rbd<br>Full ceph backup<br>Images to backup:<br>rbd/zp<br>Backup folder: /tmp/<br>Compression: True<br>Check mode: False<br>Taking full backup of images: zp<br>rbd image <span class="hljs-string">&#x27;zp&#x27;</span>:<br>size 40960 MB <span class="hljs-keyword">in</span> 10240 objects<br>order 22 (4096 kB objects)<br>block_name_prefix: rbd_data.25496b8b4567<br>format: 2<br>features: layering<br>flags: <br>Exporting image zp to /tmp/rbd/zp/zp_UTC20170119T092933.full<br>Compress mode activated<br><span class="hljs-comment"># rbd export rbd/zp /tmp/rbd/zp/zp_UTC20170119T092933.full</span><br>Exporting image: 100% complete...done.<br><span class="hljs-comment"># tar Scvfz /tmp/rbd/zp/zp_UTC20170119T092933.full.tar.gz /tmp/rbd/zp/zp_UTC20170119T092933.full</span><br>tar: Removing leading `/<span class="hljs-string">&#x27; from member names</span><br></code></pre></td></tr></table></figure><p>压缩的如果开了，正好文件也是稀疏文件的话，需要等很久，压缩的效果很好，dd 生成的文件可以压缩到很小</p><p>检查备份生成的文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ~]<span class="hljs-comment"># ll /tmp/rbd/zp/zp_UTC20170119T092933.full*</span><br>-rw-r--r-- 1 root root 42949672960 Jan 19 17:29 /tmp/rbd/zp/zp_UTC20170119T092933.full<br>-rw-r--r-- 1 root root           0 Jan 19 17:29 /tmp/rbd/zp/zp_UTC20170119T092933.full.tar.gz<br></code></pre></td></tr></table></figure><h3 id="全量备份的还原"><a href="#全量备份的还原" class="headerlink" title="全量备份的还原"></a>全量备份的还原</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">rbd import /tmp/rbd/zp/zp_UTC20170119T092933.full zpbk<br></code></pre></td></tr></table></figure><p>检查数据，没有问题<br>###增量备份配置<br>写下增量配置的文件，修改下备份模式的选项</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs bash">[rbd]<br>window size = 7<br>window unit = day<br>destination directory = /tmp/<br>images = zp<br>compress = <span class="hljs-built_in">yes</span><br>ceph config = /etc/ceph/ceph.conf<br>backup mode = incremental<br>check mode = no<br></code></pre></td></tr></table></figure><p>执行多次进行增量备份以后是这样的</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs bash"> [root@lab8106 ~]<span class="hljs-comment">#ll  /tmp/rbd/zpbk/</span><br>total 146452<br>-rw-r--r-- 1 root root 42949672960 Jan 19 18:04 zpbk@UTC20170119T100339.full<br>-rw-r--r-- 1 root root       66150 Jan 19 18:05 zpbk@UTC20170119T100546.diff_from_UTC20170119T100339<br>-rw-r--r-- 1 root root          68 Jan 19 18:05 zpbk@UTC20170119T100550.diff_from_UTC20170119T100546<br>-rw-r--r-- 1 root root          68 Jan 19 18:06 zpbk@UTC20170119T100606.diff_from_UTC20170119T100550<br>-rw-r--r-- 1 root root          68 Jan 19 18:06 zpbk@UTC20170119T100638.diff_from_UTC20170119T100606<br></code></pre></td></tr></table></figure><h3 id="增量备份的还原"><a href="#增量备份的还原" class="headerlink" title="增量备份的还原"></a>增量备份的还原</h3><p>分成多个步骤进行</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs bash">1、进行全量的恢复<br><span class="hljs-comment"># rbd import config@UTC20161130T170848.full dest_image</span><br>2、重新创建基础快照<br><span class="hljs-comment"># rbd snap create dest_image@UTC20161130T170848</span><br>3、还原增量的快照(多次执行)<br><span class="hljs-comment"># rbd import-diff config@UTC20161130T170929.diff_from_UTC20161130T170848 dest_image</span><br></code></pre></td></tr></table></figure><p>本测试用例还原步骤就是</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs bash">rbd  import zpbk@UTC20170119T100339.full zpnew<br>rbd snap create zpnew@UTC20170119T100339<br>rbd import-diff zpbk@UTC20170119T100546.diff_from_UTC20170119T100339  zpnew<br>rbd import-diff zpbk@UTC20170119T100550.diff_from_UTC20170119T100546  zpnew<br>rbd import-diff zpbk@UTC20170119T100606.diff_from_UTC20170119T100550  zpnew<br>rbd import-diff zpbk@UTC20170119T100638.diff_from_UTC20170119T100606  zpnew<br></code></pre></td></tr></table></figure><p>检查数据，没有问题</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>这个软件基于python的实现，可以说作者的实现逻辑是很清晰的，并且提供了配置文件的方式，基本上是各个细节都考虑的比较到位，很容易上手，可以直接拿来使用，或者集成到自己的平台中去，是一个很好的软件</p><h2 id="补充"><a href="#补充" class="headerlink" title="补充"></a>补充</h2><p>集群有个bug，在rbd import名称的时候如果带了@符号，那么导入的时候就会有问题，具体如下</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">rbd import /tmp/ls@1<br></code></pre></td></tr></table></figure><p>那么无法 rbd info ls@1，无法 rbd rm ls@1操作了，这个地方需要代码进行修改进行屏蔽，一般正常操作也没问题，但是万一出现了，怎么解决呢？</p><p>下面举个例子来讲述解决过程：<br>假设我的操作是</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ~]<span class="hljs-comment">#rbd import /tmp/ls@1</span><br></code></pre></td></tr></table></figure><p>首先查询下image的id</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ~]<span class="hljs-comment">#rados -p rbd get rbd_id.ls@1 rbd_id.ls@1</span><br>[root@lab8106 ~]<span class="hljs-comment">#cat rbd_id.ls@1</span><br>304b76b8b4567<br></code></pre></td></tr></table></figure><p>得到id是这个<br>删除header（后缀是上面获取的id）</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ~]<span class="hljs-comment">#rados -p rbd rm rbd_header.304b76b8b4567</span><br></code></pre></td></tr></table></figure><p>删除data</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ~]<span class="hljs-comment">#rados -p rbd rm rbd_data.304b76b8b4567.0000000000000000</span><br></code></pre></td></tr></table></figure><p>删除id文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ~]<span class="hljs-comment">#rados -p rbd rm rbd_id.ls@1</span><br></code></pre></td></tr></table></figure><p>查询元数据信息进行删除</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ~]<span class="hljs-comment">#rados -p rbd listomapvals rbd_directory</span><br></code></pre></td></tr></table></figure><p>删除的一个是上面的获取的id，一个是名称</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ~]<span class="hljs-comment"># rados -p rbd rmomapkey rbd_directory id_304b76b8b4567</span><br>[root@lab8106 ~]<span class="hljs-comment"># rados -p rbd rmomapkey rbd_directory name_ls@1</span><br></code></pre></td></tr></table></figure><p>再次检查</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">rados -p rbd listomapvals rbd_directory<br></code></pre></td></tr></table></figure><p>再次rbd ls检查，已经好了</p><h2 id="相关链接"><a href="#相关链接" class="headerlink" title="相关链接"></a>相关链接</h2><p><a href="/images/blog/13575355.html">rbd的增量备份和恢复</a><br><a href="https://github.com/teralytics/ceph-backup">ceph-backup的github</a></p><h2 id="变更记录"><a href="#变更记录" class="headerlink" title="变更记录"></a>变更记录</h2><table><thead><tr><th align="center">Why</th><th align="center">Who</th><th align="center">When</th></tr></thead><tbody><tr><td align="center">创建</td><td align="center">武汉-运维-磨渣</td><td align="center">2017-01-19</td></tr><tr><td align="center">误导入的恢复</td><td align="center">武汉-运维-磨渣</td><td align="center">2017-03-02</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>sortbitwise是什么意思</title>
    <link href="/2017/01/12/sortbitwise%E6%98%AF%E4%BB%80%E4%B9%88%E6%84%8F%E6%80%9D/"/>
    <url>/2017/01/12/sortbitwise%E6%98%AF%E4%BB%80%E4%B9%88%E6%84%8F%E6%80%9D/</url>
    
    <content type="html"><![CDATA[<h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>flag sortbitwise 在ceph中是什么意思,在Jewel版本下可以看到多了这个flags</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 current]# ceph -s<br>    cluster ffe7a8db-c671-4b45-a784-ddb41e633905<br>     health HEALTH_OK<br>     monmap e1: 1 mons at &#123;lab8106=192.168.8.106:6789/0&#125;<br>            election epoch 4, quorum 0 lab8106<br>      fsmap e4: 1/1/1 up &#123;0=lab8106=up:active&#125;<br>     osdmap e132: 8 osds: 8 up, 8 in<br>            flags sortbitwise<br>      pgmap v206294: 201 pgs, 5 pools, 4684 MB data, 1214 objects<br>            9669 MB used, 2216 GB / 2226 GB avail<br>                 201 active+clean<br></code></pre></td></tr></table></figure><h2 id="找到的相关资料"><a href="#找到的相关资料" class="headerlink" title="找到的相关资料"></a>找到的相关资料</h2><blockquote><p>After upgrading, users should set the ‘sortbitwise’ flag to enable the new internal object sort order: ceph osd set sortbitwise<br>This flag is important for the new object enumeration API and for new backends like BlueStore.</p></blockquote><p>From <a href="http://docs.ceph.com/docs/master/release-notes/#upgrading-from-infernalis-or-hammer">Ceph release notes</a></p><blockquote><p>commit 383185bfbae74797cdb44f50b4bf651422800ff1<br><br>Author: Sage Weil <a href="mailto:&#x73;&#x61;&#x67;&#x65;&#64;&#114;&#101;&#x64;&#104;&#97;&#x74;&#x2e;&#99;&#111;&#x6d;">&#x73;&#x61;&#x67;&#x65;&#64;&#114;&#101;&#x64;&#104;&#97;&#x74;&#x2e;&#99;&#111;&#x6d;</a><br><br>Date: Fri Aug 7 16:14:09 2015 -0400<br><br>mon&#x2F;OSDMonitor: osd set&#x2F;unset sortbitwise<br><br> Add monitor command to flip the switch on the OSD hobject_t sort<br><br> order.</p></blockquote><p>From git</p><p>第一次在源码中出现:</p><blockquote><p>commit 138f58493715e386929f152424b70df37843541b<br><br>Author: John Spray <a href="mailto:&#106;&#x6f;&#104;&#x6e;&#x2e;&#x73;&#112;&#114;&#x61;&#121;&#x40;&#114;&#x65;&#x64;&#x68;&#97;&#116;&#x2e;&#x63;&#111;&#109;">&#106;&#x6f;&#104;&#x6e;&#x2e;&#x73;&#112;&#114;&#x61;&#121;&#x40;&#114;&#x65;&#x64;&#x68;&#97;&#116;&#x2e;&#x63;&#111;&#109;</a><br><br>Date: Mon Aug 17 14:40:46 2015 -0400<br><br>osdc&#x2F;Objecter: new-style pgls<br><br>Signed-off-by: John Spray <a href="mailto:&#106;&#111;&#x68;&#110;&#x2e;&#x73;&#112;&#x72;&#97;&#x79;&#x40;&#x72;&#x65;&#x64;&#104;&#x61;&#x74;&#x2e;&#x63;&#111;&#x6d;">&#106;&#111;&#x68;&#110;&#x2e;&#x73;&#112;&#x72;&#97;&#x79;&#x40;&#x72;&#x65;&#x64;&#104;&#x61;&#x74;&#x2e;&#x63;&#111;&#x6d;</a><br><br>Signed-off-by: Sage Weil <a href="mailto:&#x73;&#97;&#103;&#101;&#x40;&#x72;&#x65;&#100;&#104;&#x61;&#x74;&#x2e;&#x63;&#111;&#x6d;">&#x73;&#97;&#103;&#101;&#x40;&#x72;&#x65;&#100;&#104;&#x61;&#x74;&#x2e;&#x63;&#111;&#x6d;</a></p></blockquote><p>From git</p><p>Related github issue: <a href="https://github.com/ceph/ceph/pull/4919/commits">https://github.com/ceph/ceph/pull/4919/commits</a></p><p>初步结论: sortbitwise 内部排序算法的一个变化.之所以暴露出来是因为要兼容一些pre-jewel版本.在新的版本中应该保持开启状态.</p><p>以上转载自博客：<a href="https://medium.com/@george.shuklin/what-is-sortbitwise-flag-means-in-ceph-b4176748da42#.dfaw54rpf">What ‘sortbitwise’ flag means in Ceph?</a></p><h2 id="红帽的官方回答"><a href="#红帽的官方回答" class="headerlink" title="红帽的官方回答"></a>红帽的官方回答</h2><ul><li>如果你使用dev版本Infernalis或仍在开发中的LTS版本的Ceph的Jewel版本，你会看到这个标志在ceph状态输出默认启用</li><li>这个标志sortbitwise在Infernalis版本中引入</li><li>这个标志是在这个版本提交的upstream commit 968261b11ac30622c0606d1e2ddf422009e7d330</li></ul><p>下载ceph的源码，进入源码目录</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 all]# git show 968261b11ac30622c0606d1e2ddf422009e7d330<br>commit 968261b11ac30622c0606d1e2ddf422009e7d330<br>Author: Sage Weil &lt;sage@redhat.com&gt;<br>Date:   Fri Aug 7 16:01:12 2015 -0400<br><br>    osd/OSDMap: add a SORTBITWISE OSDMap flag<br><br>    This flag will indicate that hobject_t&#x27;s shall hence-forth be<br>    sorted in a bitwise fashion.<br><br>    Signed-off-by: Sage Weil &lt;sage@redhat.com&gt;<br></code></pre></td></tr></table></figure><p>正如我们在上面给定的提交的描述中所说，该标志将表明hobject_t的将以 bitwise fashion方式排序。<br>现在意味着现在的对象将在OSDs中以按位方式排序，并且此标志默认在Infernalis和Jewel发布版本中启用。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>目前来看这个是底层的一个排序的算法的变动，对上层目前还不清楚是有什么可以可见的变化，总之，这个让它默认开启就行，不要去修改它就可以了</p><h2 id="变更记录"><a href="#变更记录" class="headerlink" title="变更记录"></a>变更记录</h2><table><thead><tr><th align="center">Why</th><th align="center">Who</th><th align="center">When</th></tr></thead><tbody><tr><td align="center">创建</td><td align="center">武汉-运维-磨渣</td><td align="center">2017-01-12</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>解决calamari无法获取节点信息的bug</title>
    <link href="/2017/01/09/%E8%A7%A3%E5%86%B3calamari%E6%97%A0%E6%B3%95%E8%8E%B7%E5%8F%96%E8%8A%82%E7%82%B9%E4%BF%A1%E6%81%AF%E7%9A%84bug/"/>
    <url>/2017/01/09/%E8%A7%A3%E5%86%B3calamari%E6%97%A0%E6%B3%95%E8%8E%B7%E5%8F%96%E8%8A%82%E7%82%B9%E4%BF%A1%E6%81%AF%E7%9A%84bug/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>一直在做calamari的相关的一些打包和安装的工作，都是业余弄的东西，所以并没有仔细的进行功能点的验证测试，正好ceph社区群里面有人问了个问题</p><blockquote><p>calamari上是不是能看到ceph的version?</p></blockquote><!--break--><p>对于这个问题，好像确实没有见到过，而之前正好有个页面看到是空的，当时还不清楚这个是什么用的</p><p><img src="/images/blog/o_200901072134image_1b611hfic17lj15ej1jp1pb31m979.png" alt="origin"></p><p>而另外一位群友贴出了这个地方的是有值的，这个地方是有BUG的，在咨询了相关的问题描述以后，我们来看下，可以如何解决这个问题</p><h2 id="问题解决过程"><a href="#问题解决过程" class="headerlink" title="问题解决过程"></a>问题解决过程</h2><p>salt的软件版本：</p><ul><li>salt-master-2015.8.1-1.el7.noarch</li><li>salt-2015.8.1-1.el7.noarch</li><li>salt-minion-2015.8.1-1.el7.noarch</li></ul><h3 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h3><p>calamari的salt-master节点在读取</p><blockquote><p>&#x2F;var&#x2F;cache&#x2F;salt&#x2F;master&#x2F;minions&#x2F;{minion-hostname}&#x2F;data.p</p></blockquote><p>的时候有权限问题，在修改权限以后，可以读取到了，但是在重启了salt-minion以后，这个文件会被更新，然后权限又变成无法读取的</p><h3 id="相关知识补充"><a href="#相关知识补充" class="headerlink" title="相关知识补充"></a>相关知识补充</h3><p>Grains - salt-minion 自身的一些静态信息</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs raw">salt &#x27;*&#x27; grains.ls       # 查看 grains 分类<br>salt &#x27;*&#x27; grains.items    # 查看 grains 所有信息<br>salt &#x27;*&#x27; grains.item os  # 查看 grains 某个信息<br>salt &#x27;*&#x27; grains.get os<br></code></pre></td></tr></table></figure><p>上面的是salt-minion的静态信息的查询的相关的命令，salt-minion在进行重启的时候会将一些静态的信息推送到salt-master上面去，而这个生成的信息正好就是我们上面提出有权限问题的data.p这个存储的文件的，那么解决问题就是修改这个地方的权限的问题了<br>###修改salt-master代码<br>这个问题通过修改salt-master的master.py代码可以解决</p><p>写入这个grains信息的代码在&#x2F;usr&#x2F;lib&#x2F;python2.7&#x2F;site-packages&#x2F;salt&#x2F;master.py这个文件当中，代码段如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs raw">  def _pillar(self, load):<br>···<br>        if self.opts.get(&#x27;minion_data_cache&#x27;, False):<br>            cdir = os.path.join(self.opts[&#x27;cachedir&#x27;], &#x27;minions&#x27;, load[&#x27;id&#x27;])<br>            if not os.path.isdir(cdir):<br>                os.makedirs(cdir)<br>            datap = os.path.join(cdir, &#x27;data.p&#x27;)<br>            tmpfh, tmpfname = tempfile.mkstemp(dir=cdir)<br>            os.close(tmpfh)<br>            with salt.utils.fopen(tmpfname, &#x27;w+b&#x27;) as fp_:<br>                fp_.write(<br>                    self.serial.dumps(<br>                        &#123;&#x27;grains&#x27;: load[&#x27;grains&#x27;],<br>                         &#x27;pillar&#x27;: data&#125;)<br>                    )<br>            # On Windows, os.rename will fail if the destination file exists.<br>            salt.utils.atomicfile.atomic_rename(tmpfname, datap)<br>        return data<br></code></pre></td></tr></table></figure><p>就是这个函数就是负责这个文件写入的，我们只在这个代码里面增加一个文件的权限的控制，在<code> salt.utils.atomicfile.atomic_rename(tmpfname, datap)</code>这行之上增加一行代码<code>os.chmod(tmpfname, 0o644)</code></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs raw"> def _pillar(self, load):<br>···<br>        if self.opts.get(&#x27;minion_data_cache&#x27;, False):<br>            cdir = os.path.join(self.opts[&#x27;cachedir&#x27;], &#x27;minions&#x27;, load[&#x27;id&#x27;])<br>            if not os.path.isdir(cdir):<br>                os.makedirs(cdir)<br>            datap = os.path.join(cdir, &#x27;data.p&#x27;)<br>            tmpfh, tmpfname = tempfile.mkstemp(dir=cdir)<br>            os.close(tmpfh)<br>            with salt.utils.fopen(tmpfname, &#x27;w+b&#x27;) as fp_:<br>                fp_.write(<br>                    self.serial.dumps(<br>                        &#123;&#x27;grains&#x27;: load[&#x27;grains&#x27;],<br>                         &#x27;pillar&#x27;: data&#125;)<br>                    )<br>            # On Windows, os.rename will fail if the destination file exists.<br>            os.chmod(tmpfname, 0o644)<br>            salt.utils.atomicfile.atomic_rename(tmpfname, datap)<br>        return data<br></code></pre></td></tr></table></figure><p>修改好了以后，重启下salt-master，然后重启下salt-minion</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">systemctl restart salt-minion<br></code></pre></td></tr></table></figure><p>检查权限，已经看到权限变成了644了</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]# ll /var/cache/salt/master/minions/lab8106/data.p <br>-rw-r--r-- 1 root root 5331 Jan  9 15:33 /var/cache/salt/master/minions/lab8106/data.p<br></code></pre></td></tr></table></figure><p>现在再看下前台页面效果:</p><p><img src="/images/blog/o_200901072140image_1b612ekp81ec0uk140t1b5tdjfm.png" alt="changge"></p><p>问题解决</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>calamari有一些各种各样的小问题，总体上还是一款非常简洁漂亮的管理界面，在没有监控系统的情况下，还是一个不错的选择</p><h2 id="变更记录"><a href="#变更记录" class="headerlink" title="变更记录"></a>变更记录</h2><table><thead><tr><th align="center">Why</th><th align="center">Who</th><th align="center">When</th></tr></thead><tbody><tr><td align="center">创建</td><td align="center">武汉-运维-磨渣</td><td align="center">2017-01-09</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>ceph 的crush算法 straw</title>
    <link href="/2017/01/05/ceph%20%E7%9A%84crush%E7%AE%97%E6%B3%95%20straw/"/>
    <url>/2017/01/05/ceph%20%E7%9A%84crush%E7%AE%97%E6%B3%95%20straw/</url>
    
    <content type="html"><![CDATA[<p>很多年以前，Sage 在写CRUSH的原始算法的时候，写了不同的Bucket类型，可以选择不同的伪随机选择算法，大部分的模型是基于RJ Honicky写的RUSH algorithms 这个算法，这个在网上可以找到资料，这里面有一个新的特性是sage很引以为豪的，straw算法，也就是我们现在常用的一些算法，这个算法有下面的特性：</p><ul><li>items 可以有任意的weight</li><li>选择一个项目的算法复杂度是O(n)</li><li>如果一个item的weight调高或者调低，只会在调整了的item直接变动，而没有调整的item是不会变动的</li></ul><blockquote><p>O(n)找到一个数组里面最大的一个数，你要把n个变量都扫描一遍，操作次数为n，那么算法复杂度是O(n)<br>冒泡法的算法复杂度是O(n²)</p></blockquote><p>这个过程的算法基本动机看起来像画画的颜料吸管，最长的一个将会获胜，每个item 基于weight有自己的随机straw长度</p><p>这些看上去都很好，但是第三个属性实际上是不成立的，这个straw 长度是基于bucket中的其他的weights来进行的一个复杂的算法的，虽然iteam的PG的计算方法是很独立的，但是一个iteam的权重变化实际上影响了其他的iteam的比例因子，这意味着一个iteam的变化可能会影响其他的iteam</p><p>这个看起来是显而易见的，但是事实上证明，8年都没有人去仔细研究底层的代码或者算法，这个影响就是用户做了一个很小的权重变化，但是看到了一个很大的数据变动过程，sage 在做的时候写过一个很好的测试，来验证了第三个属性是真的，但是当时的测试只用了几个比较少的组合，如果大量测试是会发现这个问题的</p><p>sage注意到这个问题也是很多人抱怨在迁移的数据超过了预期的数据，但是这个很难量化和验证，所以被忽视了很久</p><p>无论如何，这是个坏消息</p><p>好消息是，sage找到了如何解决分布算法来的实现这三个属性，新的算法被称为 ‘straw2’,下面是不同的算法<br>straw的算法</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs bash">max_x = -1<br>max_item = -1<br><span class="hljs-keyword">for</span> each item:<br>    x = random value from 0..65535<br>    x *= scaling <span class="hljs-built_in">factor</span><br>    <span class="hljs-keyword">if</span> x &gt; max_x:<br>       max_x = x<br>       max_item = item<br><span class="hljs-built_in">return</span> item<br></code></pre></td></tr></table></figure><p>这个就有问题了scaling factor(比例因子) 是其他iteam的权重所有的，这个就意味着改变A的权重，可能会影响到B和C的权重了</p><p>新的straw2的算法是这样的</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs bash">max_x = -1<br>max_item = -1<br><span class="hljs-keyword">for</span> each item:<br>   x = random value from 0..65535<br>   x = <span class="hljs-built_in">ln</span>(x / 65536) / weight<br>   <span class="hljs-keyword">if</span> x &gt; max_x:<br>      max_x = x<br>      max_item = item<br><span class="hljs-built_in">return</span> item<br></code></pre></td></tr></table></figure><p>可以看到这个是一个weight的简单的函数，这个意味着改变一个item的权重不会影响到其他的项目</p><p>sage发现问题的一半，然后 sam根据<a href="https://en.wikipedia.org/wiki/Exponential_distribution#Distribution_of_the_minimum_of_exponential_random_variables">这个算法</a>解决了问题</p><p>计算ln()函数有点讨厌，因为这个是一个浮点功能，CRUSH是定点运算（整数型），当前的实施方法是128KB的查找表，在做一个小的单元测试的时候比straw慢了25%，单这个可能跟一些缓存和输入也有关系</p><p>以上是2014年sage在开发者邮件列表里面提出来的，相信到现在为止straw2的算法已经改进了很多，目前默认的还是straw算法，内核在kernel4.1以后才支持的这个属性的</p><p>那么我们在0.9x中来看下这个属性,来从实际环境中看下具体有什么区别</p><h2 id="实践过程"><a href="#实践过程" class="headerlink" title="实践过程"></a>实践过程</h2><p><img src="/images/blog/o_200901072029%E5%9F%BA%E7%A1%80%E7%8E%AF%E5%A2%83.png" alt="基础环境.png-8.6kB"></p><p>基础的环境为这个，我的机器为8个osd的单机节点，通过修改crush模拟成如上图所示的环境，设置的pg数目为800，保证每个osd上的pg为100左右，这个增加pg的数目，来扩大测试的样本</p><p>straw2和straw的区别在于，straw算法改变一个bucket的权重的时候，因为内部算法的问题，造成了其他机器的item的计算因子也会变化，就会出现其他没修改权重的bucket也会出现pg的相互间的流动，这个跟设计之初的想法是不一致的，造成的后果就是，在增加或者减少存储节点的时候，如果集群比较大，数据比较多，就会造成很大的无关数据的迁移，这个就是上面提到的问题</p><p>为了解决这个问题就新加入了算法straw2，这个算法保证在bucket的crush权重发生变化的时候，只会在变化的bucket有数据流入或者流出，不会出现其他bucket间的数据流动，减少数据的迁移量，下面的测试将会直观的看到这种变化</p><h2 id="环境配置"><a href="#环境配置" class="headerlink" title="环境配置"></a>环境配置</h2><p>调整tunables 为 hammer，这个里面才支持crush v4(straw2)属性</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">root@lab8107:~/ceph/crush<span class="hljs-comment"># ceph osd crush tunables hammer</span><br>adjusted tunables profile to hammer<br>root@lab8107:~/ceph/crush<span class="hljs-comment"># ceph osd crush set-tunable straw_calc_version 1</span><br>adjusted tunable straw_calc_version to 1<br></code></pre></td></tr></table></figure><p>设置完了检查这两个个属性，如果是straw_calc_version 0的时候profile会显示unknow</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs bash">root@lab8107:~/ceph/crush<span class="hljs-comment"># ceph osd crush dump|egrep &quot;allowed_bucket_algs|profile&quot;</span><br>        <span class="hljs-string">&quot;allowed_bucket_algs&quot;</span>: 54,<br>        <span class="hljs-string">&quot;profile&quot;</span>: <span class="hljs-string">&quot;hammer&quot;</span>,<br>root@lab8107:~/ceph/crush<span class="hljs-comment"># ceph osd crush dump|grep alg</span><br>            <span class="hljs-string">&quot;alg&quot;</span>: <span class="hljs-string">&quot;straw&quot;</span>,<br>            <span class="hljs-string">&quot;alg&quot;</span>: <span class="hljs-string">&quot;straw&quot;</span>,<br>            <span class="hljs-string">&quot;alg&quot;</span>: <span class="hljs-string">&quot;straw&quot;</span>,<br>            <span class="hljs-string">&quot;alg&quot;</span>: <span class="hljs-string">&quot;straw&quot;</span>,<br>            <span class="hljs-string">&quot;alg&quot;</span>: <span class="hljs-string">&quot;straw&quot;</span>,<br>            <span class="hljs-string">&quot;alg&quot;</span>: <span class="hljs-string">&quot;straw&quot;</span>,<br></code></pre></td></tr></table></figure><p>设置完了后并不能马上生效的，这个是为了防止集群大的变动,可以用这个触发，或者等待下次crush发生变动的时候会自动触发</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">ceph osd crush reweight-all<br></code></pre></td></tr></table></figure><p>##先来测试straw<br>开始第一步测试，将osd.7从集群中crush改为0，那么变动的就是host4的crush，那么我们来看下数据的变化<br>首先需要记录原始的pg分布</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">root@lab8107:~ ceph pg dump pgs|awk <span class="hljs-string">&#x27;&#123;print $1,$15&#125;&#x27;</span> &gt; oringin<br>root@lab8107:~/ceph/crush<span class="hljs-comment"># ceph osd crush reweight osd.7 0</span><br>reweighted item <span class="hljs-built_in">id</span> 7 name <span class="hljs-string">&#x27;osd.7&#x27;</span> to 0 <span class="hljs-keyword">in</span> crush map<br>root@lab8107:~ceph pg dump pgs|awk <span class="hljs-string">&#x27;&#123;print $1,$15&#125;&#x27;</span> &gt; rewei70<br></code></pre></td></tr></table></figure><p>现在比较oringin 和rewei70 的变化</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">diff oringin rewei70 -y -W 30 --suppress-common-lines<br></code></pre></td></tr></table></figure><p>查看非调整节点的数据流动</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs bash">0.3d [2]      | 0.3d [5]<br>0.316 [2]     | 0.316 [5]<br>0.26c [5]     | 0.26c [1]<br>0.241 [2]     | 0.241 [0]<br>0.235 [5]     | 0.235 [2]<br>0.128 [0]     | 0.128 [3]<br></code></pre></td></tr></table></figure><p>再来一次将osd.6的crush weight弄成0</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">ceph osd crush reweight osd.6 0<br></code></pre></td></tr></table></figure><p>再次查看变化</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs bash">0.cb [4]      | 0.cb [2]<br>0.30b [4]     | 0.30b [2]<br>0.2e9 [1]     | 0.2e9 [4]<br>0.2d8 [3]     | 0.2d8 [1]<br>0.28e [3]     | 0.28e [4]<br>0.286 [1]     | 0.286 [4]<br>0.1f7 [3]     | 0.1f7 [1]<br>0.1b6 [1]     | 0.1b6 [4]<br>0.163 [0]     | 0.163 [3]<br>0.14f [2]     | 0.14f [4]<br>0.10a [0]     | 0.10a [3]<br></code></pre></td></tr></table></figure><p>上面的两组就是在一个bucket的里面的出现单点和整个bucket的crush weight减少的时候触发的其他节点的数据变动</p><h2 id="现在把环境恢复后再来测试straw2"><a href="#现在把环境恢复后再来测试straw2" class="headerlink" title="现在把环境恢复后再来测试straw2"></a>现在把环境恢复后再来测试straw2</h2><p>修改crush map 里面的bucket的alg</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs bash">root@lab8107:~/ceph/crush<span class="hljs-comment"># ceph osd getcrushmap -o crushmap.txt</span><br>got crush map from osdmap epoch 390<br>root@lab8107:~/ceph/crush<span class="hljs-comment"># crushtool -d crushmap.txt -o crushmap-decompile</span><br>root@lab8107:~/ceph/crush<span class="hljs-comment"># vim crushmap-decompile</span><br>将文件里面的所有straw修改成straw2<br>root@lab8107:~/ceph/crush<span class="hljs-comment"># crushtool -c crushmap-decompile  -o crushmap-compile</span><br>root@lab8107:~/ceph/crush<span class="hljs-comment"># ceph osd setcrushmap -i crushmap-compile</span><br></code></pre></td></tr></table></figure><blockquote><p>如果出现报错就把crushmap里面的straw2_calc_version改成straw_calc_version</p></blockquote><p>并且设置算法(最关键的一步，否则即使设置straw2也不生效)(这里之前版本有version 2 现在已经没那个字段了)</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">ceph osd crush set-tunable straw_calc_version 1<br></code></pre></td></tr></table></figure><p>查询当前的crush算法</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs bash">root@lab8107:~/ceph/crush<span class="hljs-comment"># ceph osd crush dump|grep alg</span><br>            <span class="hljs-string">&quot;alg&quot;</span>: <span class="hljs-string">&quot;straw2&quot;</span>,<br>            <span class="hljs-string">&quot;alg&quot;</span>: <span class="hljs-string">&quot;straw2&quot;</span>,<br>            <span class="hljs-string">&quot;alg&quot;</span>: <span class="hljs-string">&quot;straw2&quot;</span>,<br>            <span class="hljs-string">&quot;alg&quot;</span>: <span class="hljs-string">&quot;straw2&quot;</span>,<br>            <span class="hljs-string">&quot;alg&quot;</span>: <span class="hljs-string">&quot;straw2&quot;</span>,<br>            <span class="hljs-string">&quot;alg&quot;</span>: <span class="hljs-string">&quot;straw2&quot;</span>,<br>        <span class="hljs-string">&quot;allowed_bucket_algs&quot;</span>: 54,<br></code></pre></td></tr></table></figure><p>做一次重新内部算法</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">ceph osd crush reweight-all<br></code></pre></td></tr></table></figure><p>可以重复上面的测试了</p><p>获取当前的pg分布</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 pgf]<span class="hljs-comment"># ceph pg dump pgs|awk &#x27;&#123;print $1,$15&#125;&#x27; &gt; oringin</span><br>root@lab8107:~/ceph/crush<span class="hljs-comment"># ceph osd crush reweight osd.7 0</span><br>[root@lab8106 pgf]<span class="hljs-comment"># ceph pg dump pgs|awk &#x27;&#123;print $1,$15&#125;&#x27; &gt; rewei70</span><br></code></pre></td></tr></table></figure><p>比较调整前后</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">diff oringin rewei70  -y -W 30 --suppress-common-lines|less<br></code></pre></td></tr></table></figure><p>再次调整osd.6</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">ceph osd crush reweight osd.6 0<br>ceph pg dump pgs|awk <span class="hljs-string">&#x27;&#123;print $1,$15&#125;&#x27;</span> &gt; rewei60<br></code></pre></td></tr></table></figure><p>已经没有非调整bucket的pg在节点间的变化了</p><h2 id="简短的做个总结就是"><a href="#简短的做个总结就是" class="headerlink" title="简短的做个总结就是"></a>简短的做个总结就是</h2><p>straw算法里面添加节点或者减少节点，其他服务器上的osd之间会有pg的流动<br>straw2算法里面添加节点或者减少节点，只会pg从变化的节点移出或者从其他点移入，其他节点间没有数据流动</p><h4 id="设置方法"><a href="#设置方法" class="headerlink" title="设置方法"></a>设置方法</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">ceph osd crush tunables hammer<br>ceph osd crush set-tunable straw_calc_version 1<br></code></pre></td></tr></table></figure><p>开始设置好了 新创建的默认就是会straw2就会省去修改crushmap的操作</p><p>注意librados是服务端支持，客户端就支持，涉及到内核客户端的，就需要内核版本的支持，内核从4.1开始支持，也就是cephfs和rbd的块设备方式需要内核4.1及以上支持，openstack对接的是librados可以默认支持，其他的也都默认可以支持的</p><h2 id="相关链接"><a href="#相关链接" class="headerlink" title="相关链接"></a>相关链接</h2><p><a href="https://en.wikipedia.org/wiki/Exponential_distribution#Distribution_of_the_minimum_of_exponential_random_variables">https://en.wikipedia.org/wiki/Exponential_distribution#Distribution_of_the_minimum_of_exponential_random_variables</a></p>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Ceph编译加速的小技巧</title>
    <link href="/2017/01/05/Ceph%E7%BC%96%E8%AF%91%E5%8A%A0%E9%80%9F%E7%9A%84%E5%B0%8F%E6%8A%80%E5%B7%A7/"/>
    <url>/2017/01/05/Ceph%E7%BC%96%E8%AF%91%E5%8A%A0%E9%80%9F%E7%9A%84%E5%B0%8F%E6%8A%80%E5%B7%A7/</url>
    
    <content type="html"><![CDATA[<p>总结了几个小技巧，用于在ceph编译过程中，能够更快一点</p><h2 id="修改clone的地址"><a href="#修改clone的地址" class="headerlink" title="修改clone的地址"></a>修改clone的地址</h2><blockquote><p>git clone  <a href="https://github.com/ceph/ceph.git">https://github.com/ceph/ceph.git</a></p></blockquote><p>可以修改成</p><blockquote><p>git clone  git:&#x2F;&#x2F;github.com&#x2F;ceph&#x2F;ceph.git</p></blockquote><p>某些时候可能可以加快一些<br><img src="/images/blog/o_200901071852%E5%8A%A0%E9%80%9F-1.png" alt="1.png-5.9kB"></p><p><img src="/images/blog/o_200901071907%E5%8A%A0%E9%80%9F-2.png" alt="1.png-5."></p><h2 id="根据需要下载分支"><a href="#根据需要下载分支" class="headerlink" title="根据需要下载分支"></a>根据需要下载分支</h2><p>假如现在想看10.2.5版本的代码</p><h3 id="常规做法"><a href="#常规做法" class="headerlink" title="常规做法"></a>常规做法</h3><p>先下载整个库</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">git <span class="hljs-built_in">clone</span> git://github.com/ceph/ceph.git all<br></code></pre></td></tr></table></figure><p>总共的下载对象数目为46万</p><blockquote><p>Counting objects: 460384</p></blockquote><p>这个是包含所有的分支和分支内的文件的所有版本的<br>我们切换到分支</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 mytest]<span class="hljs-comment">#cd all</span><br>[root@lab8106 all]<span class="hljs-comment"># git branch</span><br>* master<br>[root@lab8106 all]<span class="hljs-comment"># git checkout -b all10.2.5  v10.2.5</span><br>Switched to a new branch <span class="hljs-string">&#x27;all10.2.5&#x27;</span><br>[root@lab8106 all]<span class="hljs-comment"># git branch</span><br>* all10.2.5<br>  master<br>[root@lab8106 all]<span class="hljs-comment"># ls -R|wc -l</span><br>4392<br>可以看到有这么多的文件<br></code></pre></td></tr></table></figure><h3 id="现在只复制一个分支的"><a href="#现在只复制一个分支的" class="headerlink" title="现在只复制一个分支的"></a>现在只复制一个分支的</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 mytest]<span class="hljs-comment"># git clone -b v10.2.5 --single-branch   git://github.com/ceph/ceph.git single</span><br></code></pre></td></tr></table></figure><p>总共下载的对象数目为34万</p><blockquote><p>Counting objects: 344026</p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 mytest]<span class="hljs-comment"># cd single/</span><br>[root@lab8106 single]<span class="hljs-comment"># git checkout -b single10.2.5</span><br>Switched to a new branch <span class="hljs-string">&#x27;single10.2.5&#x27;</span><br>[root@lab8106 single]<span class="hljs-comment"># git branch</span><br>* single10.2.5<br>[root@lab8106 single]<span class="hljs-comment"># ls -R |wc -l</span><br>4392<br></code></pre></td></tr></table></figure><h3 id="现在只复制一个分支的最后一个版本的代码"><a href="#现在只复制一个分支的最后一个版本的代码" class="headerlink" title="现在只复制一个分支的最后一个版本的代码"></a>现在只复制一个分支的最后一个版本的代码</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 mytest]<span class="hljs-comment"># git clone -b v10.2.5 --single-branch --depth 1  git://github.com/ceph/ceph.git singledep1</span><br></code></pre></td></tr></table></figure><p>总共下载的对象数目为3682</p><blockquote><p>Counting objects: 3682</p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 mytest]<span class="hljs-comment">#  cd singledep1/</span><br> [root@lab8106 singledep1]<span class="hljs-comment"># git checkout -b singledep110.2.5</span><br>Switched to a new branch <span class="hljs-string">&#x27;singledep110.2.5&#x27;</span><br>[root@lab8106 singledep1]<span class="hljs-comment"># git branch</span><br>* singledep110.2.5<br>[root@lab8106 singledep1]<span class="hljs-comment"># ls -R |wc -l</span><br>4392<br></code></pre></td></tr></table></figure><p>从上面的可以看到三个版本的代码是一致的，那么区别在哪里</p><ul><li>clone：包含所有分支和分支的所有文件版本</li><li>clone single-branch：包含指定分支和指定分支的所有文件的版本</li><li>clone single-branch depth 1 ：包含指定分支和指定分支的最后一个版本的文件</li></ul><h2 id="准备编译前的install-deps慢"><a href="#准备编译前的install-deps慢" class="headerlink" title="准备编译前的install-deps慢"></a>准备编译前的install-deps慢</h2><p>提前准备好epel</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">yum install http://mirrors.aliyun.com/epel/7/x86_64/e/epel-release-7-8.noarch.rpm<br><span class="hljs-built_in">rm</span> -rf /etc/yum.repos.d/epel*<br></code></pre></td></tr></table></figure><p>装完了删除，这个是为了绕过包验证</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">wget -O /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repo<br></code></pre></td></tr></table></figure><p>删除慢速的 aliyuncs</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">sed -i <span class="hljs-string">&#x27;/aliyuncs/d&#x27;</span> /etc/yum.repos.d/epel.repo <br></code></pre></td></tr></table></figure><p>install-deps.sh第72行的需要修改</p><blockquote><p>yum-config-manager –add-repo <a href="https://dl.fedoraproject.org/pub/epel/$MAJOR_VERSION/x86_64/">https://dl.fedoraproject.org/pub/epel/$MAJOR_VERSION/x86_64/</a><br>执行下面的命令</p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">sed -i <span class="hljs-string">&#x27;s/https:\/\/dl.fedoraproject.org\/pub\//http:\/\/mirrors.aliyun.com\//g&#x27;</span> install-deps.sh<br></code></pre></td></tr></table></figure><p>然后执行install-deps.sh，这样会快很多的</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>目前就这么多，后续有更多的影响速度的地方会增加上去</p>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>rbd的image对象数与能写入文件数的关系</title>
    <link href="/2017/01/03/rbd%E7%9A%84image%E5%AF%B9%E8%B1%A1%E6%95%B0%E4%B8%8E%E8%83%BD%E5%86%99%E5%85%A5%E6%96%87%E4%BB%B6%E6%95%B0%E7%9A%84%E5%85%B3%E7%B3%BB/"/>
    <url>/2017/01/03/rbd%E7%9A%84image%E5%AF%B9%E8%B1%A1%E6%95%B0%E4%B8%8E%E8%83%BD%E5%86%99%E5%85%A5%E6%96%87%E4%BB%B6%E6%95%B0%E7%9A%84%E5%85%B3%E7%B3%BB/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>收到一个问题如下：</p><blockquote><p>一个300TB 的RBD，只有7800万的objects，如果存储小文件的话，感觉不够用</p></blockquote><!--break--><p>对于这个问题，我原来的理解是：对象默认设置的大小是4M一个，存储下去的数据，如果小于4M，就会占用一个小于4M的对象，如果超过4M，那么存储的数据就会进行拆分成多个4M，这个地方其实是不严谨的</p><p>对于rados接口来说，数据是多大对象put进去就是多大的对象，并没有进行拆分，进行拆分的是再上一层的应用，比如rbd，比如cephfs</p><p>那么对于rbd的image显示的对象数目和文件数目有什么关系呢？本篇将来看看这个问题，到底会不会出现上面的问题</p><h2 id="实践过程"><a href="#实践过程" class="headerlink" title="实践过程"></a>实践过程</h2><p>创建一个image</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]# rbd create --image zpsize --size 100M<br>[root@lab8106 ~]# rbd info zpsize<br>rbd image &#x27;zpsize&#x27;:<br>size 102400 kB in 25 objects<br>order 22 (4096 kB objects)<br>block_name_prefix: rbd_data.85c66b8b4567<br>format: 2<br>features: layering<br>flags: <br></code></pre></td></tr></table></figure><p>可以看到，这个image从集群中分配到了25个对象，每个对象的大小为4M，假如我们写入1000个小文件看下会是什么情况</p><p>映射到本地并且格式化xfs文件系统</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]# rbd map zpsize<br>/dev/rbd0<br>[root@lab8106 ~]# mkfs.xfs -f /dev/rbd0 <br>meta-data=/dev/rbd0              isize=256    agcount=4, agsize=6144 blks<br>         =                       sectsz=512   attr=2, projid32bit=1<br>         =                       crc=0        finobt=0<br>data     =                       bsize=4096   blocks=24576, imaxpct=25<br>         =                       sunit=1024   swidth=1024 blks<br>naming   =version 2              bsize=4096   ascii-ci=0 ftype=0<br>log      =internal log           bsize=4096   blocks=624, version=2<br>         =                       sectsz=512   sunit=8 blks, lazy-count=1<br>realtime =none                   extsz=4096   blocks=0, rtextents=0<br></code></pre></td></tr></table></figure><p>挂载到本地<br>[root@lab8106 ~]# mount &#x2F;dev&#x2F;rbd0 &#x2F;mnt</p><p>写入1000个1K小文件</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]# seq 1000|xargs -i dd if=/dev/zero of=/mnt/a&#123;&#125; bs=1K count=1<br></code></pre></td></tr></table></figure><p>没有报错提示，正常写入了，我们看下写入了多少对象</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]# rados  -p rbd ls|grep rbd_data.85c66b8b4567<br>rbd_data.85c66b8b4567.0000000000000018<br>rbd_data.85c66b8b4567.0000000000000000<br>rbd_data.85c66b8b4567.0000000000000006<br>rbd_data.85c66b8b4567.0000000000000001<br>rbd_data.85c66b8b4567.0000000000000017<br>rbd_data.85c66b8b4567.000000000000000c<br>rbd_data.85c66b8b4567.0000000000000012<br>rbd_data.85c66b8b4567.0000000000000002<br></code></pre></td></tr></table></figure><p>只写入了少量的对象，我们尝试下载下来看看</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]# ll -hl rbd_data.85c66b8b4567.0000000000000018<br>-rw-r--r-- 1 root root 4.0M Jan  3 14:27 rbd_data.85c66b8b4567.0000000000000018<br>[root@lab8106 ~]# rados  -p rbd get rbd_data.85c66b8b4567.0000000000000000 rbd_data.85c66b8b4567.0000000000000000<br>[root@lab8106 ~]# ll -hl rbd_data.85c66b8b4567.0000000000000000<br>-rw-r--r-- 1 root root 4.0M Jan  3 14:27 rbd_data.85c66b8b4567.0000000000000000<br></code></pre></td></tr></table></figure><p>可以看到还是4M的对象，实际上写入的小文件已经进行了合并了，在底层已经是一个4M的对象文件了</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本篇的结论就是，rbd层之上的写入的文件的个数与底层的对象数目是没有关系的，对象数目和对象大小是底层处理的，再上一层就是文件系统去处理的了，总空间占用上是一致的</p>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>处理Ceph osd的journal的uuid问题</title>
    <link href="/2016/12/26/%E5%A4%84%E7%90%86Ceph%20osd%E7%9A%84journal%E7%9A%84uuid%E9%97%AE%E9%A2%98/"/>
    <url>/2016/12/26/%E5%A4%84%E7%90%86Ceph%20osd%E7%9A%84journal%E7%9A%84uuid%E9%97%AE%E9%A2%98/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>之前有一篇文章介绍的是，在centos7的jewel下面如果自己做的分区如何处理自动挂载的问题，当时的环境对journal的地方采取的是文件的形式处理的，这样就没有了重启后journal的磁盘偏移的问题</p><p>如果采用的是ceph自带的deploy去做分区的处理的时候，是调用的sgdisk去对磁盘做了一些处理的，然后deploy能够识别一些特殊的标记，然后去做了一些其他的工作，而自己分区的时候，是没有做这些标记的这样就可能会有其他的问题</p><p>我们看下如何在部署的时候就处理好journal的uuid的问题</p><h2 id="实践"><a href="#实践" class="headerlink" title="实践"></a>实践</h2><h3 id="按常规流程部署OSD"><a href="#按常规流程部署OSD" class="headerlink" title="按常规流程部署OSD"></a>按常规流程部署OSD</h3><p>准备测试的自分区磁盘</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs raw">dd if=/dev/zero of=/dev/sde bs=4M count=100;<br>dd if=/dev/zero of=/dev/sdf bs=4M count=100; parted /dev/sde mklabel gpt;<br>parted /dev/sdf mklabel gpt;<br>parted /dev/sde mkpart primary 1 100%;<br>parted /dev/sdf mkpart primary 1 100%<br></code></pre></td></tr></table></figure><p>使用的sde1作为数据盘，使用sdf1作为ssd的独立分区的journal磁盘</p><p>我们线按照常规的步骤去部署下</p><h5 id="做osd的prepare操作"><a href="#做osd的prepare操作" class="headerlink" title="做osd的prepare操作"></a>做osd的prepare操作</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ceph]# ceph-deploy osd prepare lab8106:/dev/sde1:/dev/sdf1<br>···<br>[lab8106][WARNIN] adjust_symlink: Creating symlink /var/lib/ceph/tmp/mnt.7HuS8k/journal -&gt; /dev/sdf1<br>···<br></code></pre></td></tr></table></figure><h5 id="做osd的activate操作"><a href="#做osd的activate操作" class="headerlink" title="做osd的activate操作"></a>做osd的activate操作</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ceph]# ceph-deploy osd activate lab8106:/dev/sde1:/dev/sdf<br>···<br>[lab8106][WARNIN] ceph_disk.main.Error: Error: [&#x27;ceph-osd&#x27;, &#x27;--cluster&#x27;, &#x27;ceph&#x27;, &#x27;--mkfs&#x27;, &#x27;--mkkey&#x27;, &#x27;-i&#x27;, &#x27;7&#x27;, &#x27;--monmap&#x27;, &#x27;/var/lib/ceph/tmp/mnt.yOP4gv/activate.monmap&#x27;, &#x27;--osd-data&#x27;, &#x27;/var/lib/ceph/tmp/mnt.yOP4gv&#x27;, &#x27;--osd-journal&#x27;, &#x27;/var/lib/ceph/tmp/mnt.yOP4gv/journal&#x27;, &#x27;--osd-uuid&#x27;, &#x27;5c59284b-8d82-4cc6-b566-8b102dc25568&#x27;, &#x27;--keyring&#x27;, &#x27;/var/lib/ceph/tmp/mnt.yOP4gv/keyring&#x27;, &#x27;--setuser&#x27;, &#x27;ceph&#x27;, &#x27;--setgroup&#x27;, &#x27;ceph&#x27;] failed : 2016-12-26 13:11:54.211543 7f585e926800 -1 filestore(/var/lib/ceph/tmp/mnt.yOP4gv) mkjournal error creating journal on /var/lib/ceph/tmp/mnt.yOP4gv/journal: (13) Permission denied<br>[lab8106][WARNIN] 2016-12-26 13:11:54.211564 7f585e926800 -1 OSD::mkfs: ObjectStore::mkfs failed with error -13<br>[lab8106][WARNIN] 2016-12-26 13:11:54.211616 7f585e926800 -1  ** ERROR: error creating empty object store in /var/lib/ceph/tmp/mnt.yOP4gv: (13) Permission denied<br>···<br></code></pre></td></tr></table></figure><p>可以看到提示的是权限不足，我们检查下权限</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ceph]# mount /dev/sde1 /mnt<br>[root@lab8106 ceph]# ll /mnt/<br>total 32<br>-rw-r--r-- 1 root root 193 Dec 26 13:11 activate.monmap<br>-rw-r--r-- 1 ceph ceph  37 Dec 26 13:11 ceph_fsid<br>drwxr-xr-x 3 ceph ceph  37 Dec 26 13:11 current<br>-rw-r--r-- 1 ceph ceph  37 Dec 26 13:11 fsid<br>lrwxrwxrwx 1 ceph ceph   9 Dec 26 13:11 journal -&gt; /dev/sdf1<br>-rw-r--r-- 1 ceph ceph  37 Dec 26 13:11 journal_uuid<br>-rw-r--r-- 1 ceph ceph  21 Dec 26 13:11 magic<br>-rw-r--r-- 1 ceph ceph   4 Dec 26 13:11 store_version<br>-rw-r--r-- 1 ceph ceph  53 Dec 26 13:11 superblock<br>-rw-r--r-- 1 ceph ceph   2 Dec 26 13:11 whoami<br>[root@lab8106 ceph]# ll /dev/sdf1<br>brw-rw---- 1 root disk 8, 81 Dec 26 13:03 /dev/sdf1<br></code></pre></td></tr></table></figure><p>创建sdf1的journal的时候权限有问题，我们给下磁盘权限</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ceph]# chown ceph:ceph /dev/sdf1 <br>[root@lab8106 ceph]# ceph-deploy osd activate lab8106:/dev/sde1:/dev/sdf1<br></code></pre></td></tr></table></figure><p>可以看到成功了</p><h5 id="检查下osd的目录："><a href="#检查下osd的目录：" class="headerlink" title="检查下osd的目录："></a>检查下osd的目录：</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ceph]# ll /var/lib/ceph/osd/ceph-7<br>total 56<br>-rw-r--r--   1 root root  193 Dec 26 13:15 activate.monmap<br>-rw-r--r--   1 ceph ceph    3 Dec 26 13:15 active<br>-rw-r--r--   1 ceph ceph   37 Dec 26 13:11 ceph_fsid<br>drwxr-xr-x 166 ceph ceph 4096 Dec 26 13:16 current<br>-rw-r--r--   1 ceph ceph   37 Dec 26 13:11 fsid<br>lrwxrwxrwx   1 ceph ceph    9 Dec 26 13:11 journal -&gt; /dev/sdf1<br></code></pre></td></tr></table></figure><p>可以看到journal链接到了&#x2F;dev&#x2F;sdf1，这次的部署是成功了，但是这里就有个问题，如果下次重启的时候，sdf1不是sdf1盘符变了，那么问题就会产生了，osd可能就无法启动了</p><h3 id="优化下部署流程"><a href="#优化下部署流程" class="headerlink" title="优化下部署流程"></a>优化下部署流程</h3><p>这里是优化后的流程，解决上面的问题的<br>准备测试的自分区磁盘</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs raw">dd if=/dev/zero of=/dev/sde bs=4M count=100;<br>dd if=/dev/zero of=/dev/sdf bs=4M count=100; <br>parted /dev/sde mklabel gpt;<br>parted /dev/sdf mklabel gpt;<br>parted /dev/sde mkpart primary 1 100%;<br>parted /dev/sdf mkpart primary 1 100%<br></code></pre></td></tr></table></figure><p>给jounral盘做一个标记(特殊标记，下面的字符串不要变动固定写法)</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">/usr/sbin/sgdisk  --change-name=1:&#x27;ceph journal&#x27; --typecode=1:45b0969e-9b03-4f30-b4c6-b4b80ceff106  -- /dev/sdf<br></code></pre></td></tr></table></figure><p>给数据盘做一个标记(特殊标记，下面的字符串不要变动固定写法)</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">/usr/sbin/sgdisk  --change-name=1:&#x27;ceph data&#x27; --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sde<br></code></pre></td></tr></table></figure><p>检查下当前的分区标记情况</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]# ceph-disk list<br>/dev/sde :<br> /dev/sde1 ceph data, unprepared<br>/dev/sdf :<br> /dev/sdf1 ceph journal<br></code></pre></td></tr></table></figure><h5 id="做osd的prepare操作-1"><a href="#做osd的prepare操作-1" class="headerlink" title="做osd的prepare操作"></a>做osd的prepare操作</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs raw">ceph-deploy osd prepare lab8106:/dev/sde1:/dev/sdf1<br>ceph-deploy osd activate lab8106:/dev/sde1:/dev/sdf1<br></code></pre></td></tr></table></figure><p>再次检查下当前的分区标记情况</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ceph]# ceph-disk list<br>···<br>/dev/sde :<br> /dev/sde1 ceph data, active, cluster ceph, osd.8, journal /dev/sdf1<br>/dev/sdf :<br> /dev/sdf1 ceph journal, for /dev/sde1<br></code></pre></td></tr></table></figure><h5 id="查看jounral的数据"><a href="#查看jounral的数据" class="headerlink" title="查看jounral的数据"></a>查看jounral的数据</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ceph]# ll /var/lib/ceph/osd/ceph-8<br>total 56<br>-rw-r--r--   1 root root  193 Dec 26 13:26 activate.monmap<br>-rw-r--r--   1 ceph ceph    3 Dec 26 13:26 active<br>-rw-r--r--   1 ceph ceph   37 Dec 26 13:25 ceph_fsid<br>drwxr-xr-x 164 ceph ceph 4096 Dec 26 13:26 current<br>-rw-r--r--   1 ceph ceph   37 Dec 26 13:25 fsid<br>lrwxrwxrwx   1 ceph ceph   58 Dec 26 13:25 journal -&gt; /dev/disk/by-partuuid/cd72d6e8-07d0-4cd3-8c6b-a33d624cae36<br>···<br></code></pre></td></tr></table></figure><p>可以看到已经正确的链接了,并且部署过程中也没有了上面的需要进行权限的处理，这个是deploy工具在中间帮做了</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>处理的核心在于做的那两个标记，其他的就交给deploy工具自己处理就行了，如果有兴趣可以深入研究，没兴趣的话，就安装上面说的方法进行处理就行</p>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>如何避免Cephfs被完全毁掉</title>
    <link href="/2016/12/23/%E5%A6%82%E4%BD%95%E9%81%BF%E5%85%8DCephfs%E8%A2%AB%E5%AE%8C%E5%85%A8%E6%AF%81%E6%8E%89/"/>
    <url>/2016/12/23/%E5%A6%82%E4%BD%95%E9%81%BF%E5%85%8DCephfs%E8%A2%AB%E5%AE%8C%E5%85%A8%E6%AF%81%E6%8E%89/</url>
    
    <content type="html"><![CDATA[<h2 id="前提"><a href="#前提" class="headerlink" title="前提"></a>前提</h2><p>一套系统的最低要求是可恢复，也就是数据不丢失，但是在各种各样的原因下，整套系统都有被毁掉的可能，一直以来有个观点就是存储是需要两套的，一般情况下很难实现，但是如何把故障发生的概率降低到最低，这个是我们需要考虑的问题</p><!--break--><p>最近在社区群里面又听闻一个案例，一套系统的文件系统被重置掉了，也就是fs被重建了，实际上这属于一个不应该有的操作，但是已经发生的事情，就看怎么样能在下次避免或者把损失降到最低，对于hammer版本来说，重建cephfs只是把目录树给冲掉了，实际的目录还是能创建起来，但是这其实是一个BUG，并且在最新的Jewel下已经解决掉这个问题，这就造成无法重建目录树，在Jewel下，在不修改代码的情况下，文件都可以扫描回来，但是全部塞到了一个目录下，对于某些场景来说，这个已经是最大限度的恢复了，至少文件还在，如果文件类型可知，也可以一个个去人工识别的，虽然工作量异常的大，但至少文件回来了，这种情况，如果有保留文件名和文件md5值的强制要求的话，文件是可以完全找回来的，当然，这都是一些防范措施，看有没有重视，或者提前做好了预备</p><p>本篇就是对于情况下，如何基于快照做一个防范措施，以防误操作引起的数据无法挽回的措施</p><h2 id="实践"><a href="#实践" class="headerlink" title="实践"></a>实践</h2><p>对于元数据存储池来说，元数据的大小并不大，百万文件的元数据也才几百兆，所以我们有没有什么办法去形成一种保护措施，答案是有的</p><p>我们知道，ceph的存储池是有快照的，对于rbd场景来说，快照可以交给存储池去做快照管理，也可以交给Image自己做快照管理，二者差别在于，是大批量的快照还是只需要部分的快照，对于存储池快照来说，给存储池做一个快照，实际上就是对这个存储池中的所有的对象做了一个快照</p><p>我们先来看看，这个地方是如何基于快照去做文件的目录树恢复的</p><h3 id="准备测试数据"><a href="#准备测试数据" class="headerlink" title="准备测试数据"></a>准备测试数据</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 mnt]# df -TH|grep mnt<br>192.168.8.106:/ ceph      897G  110M  897G   1% /mnt<br>[root@lab8106 mnt]# cp -ra /usr/share/doc/ce* /mnt<br>[root@lab8106 mnt]# ll /mnt<br>total 0<br>drwxr-xr-x 1 root root 0 Dec 30  2015 celt051-0.5.1.3<br>drwxr-xr-x 1 root root 0 Mar  7  2016 centos-logos-70.0.6<br>drwxr-xr-x 1 root root 0 Mar  7  2016 centos-release<br>drwxr-xr-x 1 root root 0 Dec 21 15:04 ceph<br>drwxr-xr-x 1 root root 0 Sep  9 17:21 ceph-deploy-1.5.34<br>drwxr-xr-x 1 root root 0 Mar  7  2016 certmonger-0.78.4<br></code></pre></td></tr></table></figure><h3 id="准备快照和需要的相关数据"><a href="#准备快照和需要的相关数据" class="headerlink" title="准备快照和需要的相关数据"></a>准备快照和需要的相关数据</h3><p>对元数据池做一个快照</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]# ceph osd pool mksnap metadata snap1<br>created pool metadata snap snap1<br></code></pre></td></tr></table></figure><p>记录下元数据池的对象名称</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">for a in `rados -p metadata ls`;do echo $a &gt;&gt; metalist;done;<br></code></pre></td></tr></table></figure><p>一个简单的循环就可以拿到列表，注意，这里并不需要把数据get下来，我们只需要记录一次列表就行，这个过程，即使很多对象的情况，这个操作也是很快的</p><h3 id="毁掉我们的文件系统"><a href="#毁掉我们的文件系统" class="headerlink" title="毁掉我们的文件系统"></a>毁掉我们的文件系统</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]# umount /mnt<br>[root@lab8106 ~]# systemctl stop ceph-mds@lab8106<br>[root@lab8106 ~]# ceph mds fail 0<br>failed mds gid 4140<br>[root@lab8106 ~]# ceph fs rm ceph --yes-i-really-mean-it<br>[root@lab8106 ~]# ceph -s<br>    cluster ffe7a8db-c671-4b45-a784-ddb41e633905<br>     health HEALTH_OK<br>     monmap e1: 1 mons at &#123;lab8106=192.168.8.106:6789/0&#125;<br>            election epoch 3, quorum 0 lab8106<br>     osdmap e24: 3 osds: 3 up, 3 in<br>            flags sortbitwise<br>      pgmap v111: 192 pgs, 3 pools, 397 kB data, 52 objects<br>            105 MB used, 834 GB / 834 GB avail<br>                 192 active+clean<br></code></pre></td></tr></table></figure><p>可以看到上面的操作已经把文件系统给推掉了</p><h3 id="新创建一个文件系统"><a href="#新创建一个文件系统" class="headerlink" title="新创建一个文件系统"></a>新创建一个文件系统</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]# ceph fs new ceph metadata data<br>new fs with metadata pool 1 and data pool 2<br>[root@lab8106 ~]# systemctl start ceph-mds@lab8106<br>[root@lab8106 ~]# mount -t ceph 192.168.8.106:/ /mnt<br>[root@lab8106 ~]# ll /mnt<br>total 0<br></code></pre></td></tr></table></figure><p>可以看到上面的操作以后，我们的目录树已经空空如也了，到这里如果没有做上面的快照相关操作，需要恢复的话，基本需要去对源码进行修改，并且需要对代码非常的熟悉才能做，一般是没有办法了，我们来看下我们基于快照的情况下，是如何恢复的<br>先umount掉挂载点</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">umount /mnt<br></code></pre></td></tr></table></figure><p>还记得上面的快照名称和对象列表吧，我们现在对数据进行回滚：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 mds]# systemctl stop ceph-mds@lab8106<br>[root@lab8106 mds]# for a in `cat metalist`;do rados  -p metadata rollback $a snap1;done;<br>rolled back pool metadata to snapshot snap1<br>rolled back pool metadata to snapshot snap1<br>rolled back pool metadata to snapshot snap1<br>rolled back pool metadata to snapshot snap1<br>···<br></code></pre></td></tr></table></figure><p>重启一下mds</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 mds]# systemctl restart ceph-mds@lab8106<br></code></pre></td></tr></table></figure><p>检查下目录树，没问题，都恢复了</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 mds]# mount -t ceph 192.168.8.106:/ /mnt<br>[root@lab8106 mds]# ll /mnt<br>total 0<br>drwxr-xr-x 1 root root   3577 Dec 30  2015 celt051-0.5.1.3<br>drwxr-xr-x 1 root root   1787 Mar  7  2016 centos-logos-70.0.6<br>drwxr-xr-x 1 root root  20192 Mar  7  2016 centos-release<br>drwxr-xr-x 1 root root  19768 Dec 21 15:04 ceph<br>drwxr-xr-x 1 root root  13572 Sep  9 17:21 ceph-deploy-1.5.34<br>drwxr-xr-x 1 root root 147227 Mar  7  2016 certmonger-0.78.4<br></code></pre></td></tr></table></figure><h3 id="如果数据被不小心清空了"><a href="#如果数据被不小心清空了" class="headerlink" title="如果数据被不小心清空了"></a>如果数据被不小心清空了</h3><p>上面是基于重建fs情况下的恢复，下面来个更极端的，元数据池的对象全部被删除了</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 mds]# for a in `rados -p metadata ls`;do rados -p metadata rm $a ;done;<br>[root@lab8106 mds]# rados  -p metadata ls<br>[root@lab8106 mds]# systemctl restart ceph-mds@lab8106<br></code></pre></td></tr></table></figure><p>这个时候查看ceph -s状态，mds都无法启动，我们来做下恢复</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 mds]# systemctl stop ceph-mds@lab8106<br>[root@lab8106 mds]# ceph mds fail 0<br>[root@lab8106 mds]# ceph fs rm ceph --yes-i-really-mean-it<br>[root@lab8106 mds]# ceph fs new ceph metadata data<br>[root@lab8106 mds]# for a in `cat metalist`;do rados  -p metadata rollback $a snap1;done;<br>rolled back pool metadata to snapshot snap1<br>rolled back pool metadata to snapshot snap1<br>rolled back pool metadata to snapshot snap1<br>rolled back pool metadata to snapshot snap1<br>···<br>[root@lab8106 mds]# rados  -p metadata ls|wc -l<br>20<br>[root@lab8106 mds]# systemctl start ceph-mds@lab8106<br></code></pre></td></tr></table></figure><p>这个时候需要多等下mds恢复正常，有可能记录了原来的客户端信息，需要做重连，如果一直没恢复就重启下mds<br>挂载以后，可以看到，对象数据都回来了</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>这个能算一个防患于未然的办法，如果对于纯数据存储的情况，存储池的快照也是能够在某些场景下发挥很大的作用的，当然什么时机做快照，保留什么多少版本，什么时候删除快照，这个都是有学问的，需要根据实际的场景和压力去做</p>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Ceph数据盘怎样实现自动挂载</title>
    <link href="/2016/12/22/Ceph%E6%95%B0%E6%8D%AE%E7%9B%98%E6%80%8E%E6%A0%B7%E5%AE%9E%E7%8E%B0%E8%87%AA%E5%8A%A8%E6%8C%82%E8%BD%BD/"/>
    <url>/2016/12/22/Ceph%E6%95%B0%E6%8D%AE%E7%9B%98%E6%80%8E%E6%A0%B7%E5%AE%9E%E7%8E%B0%E8%87%AA%E5%8A%A8%E6%8C%82%E8%BD%BD/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>在Centos7 下，现在采用了 systemctl来控制服务，这个刚开始用起来可能不太习惯，不过这个服务比之前的服务控制要强大的多，可以做更多的控制，本节将来介绍下关于 Ceph的 osd 磁盘挂载的问题</p><p>很多人部署以后，发现在Centos7下用Jewel的时候没有去写fstab也没有写配置文件，重启服务器一样能够挂载起来了，关于这个有另外一篇文章：「<a href="/2016/03/31/ceph%E5%9C%A8centos7%E4%B8%8B%E4%B8%80%E4%B8%AA%E4%B8%8D%E5%AE%B9%E6%98%93%E5%8F%91%E7%8E%B0%E7%9A%84%E6%94%B9%E5%8F%98/">ceph在centos7下一个不容易发现的改变</a>」</p><!--break--><p>还有一些人发现自己的却启动不起来，需要写配置文件或者fstab</p><p>本篇就是来解决这个疑惑的，以及在不改变原配置方法的情况下如何加入这种自启动</p><h2 id="实践过程"><a href="#实践过程" class="headerlink" title="实践过程"></a>实践过程</h2><h3 id="首先来第一种部署的方法"><a href="#首先来第一种部署的方法" class="headerlink" title="首先来第一种部署的方法"></a>首先来第一种部署的方法</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs raw">ceph-deploy osd prepare lab8106:/dev/sde<br>ceph-deploy osd activate lab8106:/dev/sde1<br></code></pre></td></tr></table></figure><p>这个方法会把&#x2F;dev&#x2F;sde自动分成两个分区，一个分区给journal使用，一个分区给osd的数据使用，这种方法部署以后，是可以自动起来的，启动的挂载过程就是这个服务</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">systemctl start ceph-disk@/dev/sde1<br></code></pre></td></tr></table></figure><h3 id="再来看第二种方法"><a href="#再来看第二种方法" class="headerlink" title="再来看第二种方法"></a>再来看第二种方法</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ceph]# parted -s /dev/sdf mklabel gpt<br>[root@lab8106 ceph]# parted -s /dev/sdf mkpart primary 1 100%<br>[root@lab8106 ceph]# parted -s /dev/sdf print<br>Model: SEAGATE ST3300657SS (scsi)<br>Disk /dev/sdf: 300GB<br>Sector size (logical/physical): 512B/512B<br>Partition Table: gpt<br>Disk Flags: <br><br>Number  Start   End    Size   File system  Name     Flags<br> 1      1049kB  300GB  300GB               primary<br></code></pre></td></tr></table></figure><p>提前做好了分区的工作</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs raw">ceph-deploy osd prepare lab8106:/dev/sdf1<br>ceph-deploy osd activate lab8106:/dev/sdf1<br></code></pre></td></tr></table></figure><p>可以看到prepare的时候是对着分区去做的<br>这种方法journal是以文件的方式在数据目录生成的,可以看到两个目录的 df 看到的就是不一样的，多的那个是 journal 文件的大小</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs raw">/dev/sde1       279G   34M  279G   1% /var/lib/ceph/osd/ceph-6<br>/dev/sdf1       280G  1.1G  279G   1% /var/lib/ceph/osd/ceph-7<br></code></pre></td></tr></table></figure><p>重启服务器<br>可以看到上面的sde1挂载了而自己分区的sdf1没有挂载</p><p>我们去手动执行下:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]#  systemctl start ceph-disk@/dev/sdf1<br>Job for ceph-disk@-dev-sdf1.service failed because the control process exited with error code. See &quot;systemctl status ceph-disk@-dev-sdf1.service&quot; and &quot;journalctl -xe&quot; for details.<br></code></pre></td></tr></table></figure><p>看下报错</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]# systemctl status ceph-disk@-dev-sdf1.service<br>● ceph-disk@-dev-sdf1.service - Ceph disk activation: /dev/sdf1<br>   Loaded: loaded (/usr/lib/systemd/system/ceph-disk@.service; static; vendor preset: disabled)<br>   Active: failed (Result: exit-code) since Thu 2016-12-22 10:15:52 CST; 46s ago<br>  Process: 16334 ExecStart=/bin/sh -c flock /var/lock/ceph-disk /usr/sbin/ceph-disk --verbose --log-stdout trigger --sync %f (code=exited, status=1/FAILURE)<br> Main PID: 16334 (code=exited, status=1/FAILURE)<br><br>Dec 22 10:15:52 lab8106 sh[16334]: main(sys.argv[1:])<br>Dec 22 10:15:52 lab8106 sh[16334]: File &quot;/usr/lib/python2.7/site-packages/ceph_disk/main.py&quot;, line 4962, in main<br>Dec 22 10:15:52 lab8106 sh[16334]: args.func(args)<br>Dec 22 10:15:52 lab8106 sh[16334]: File &quot;/usr/lib/python2.7/site-packages/ceph_disk/main.py&quot;, line 4394, in main_trigger<br>Dec 22 10:15:52 lab8106 sh[16334]: raise Error(&#x27;unrecognized partition type %s&#x27; % parttype)<br>Dec 22 10:15:52 lab8106 sh[16334]: ceph_disk.main.Error: Error: unrecognized partition type 0fc63daf-8483-4772-8e79-3d69d8477de4<br>Dec 22 10:15:52 lab8106 systemd[1]: ceph-disk@-dev-sdf1.service: main process exited, code=exited, status=1/FAILURE<br>Dec 22 10:15:52 lab8106 systemd[1]: Failed to start Ceph disk activation: /dev/sdf1.<br>Dec 22 10:15:52 lab8106 systemd[1]: Unit ceph-disk@-dev-sdf1.service entered failed state.<br>Dec 22 10:15:52 lab8106 systemd[1]: ceph-disk@-dev-sdf1.service failed.<br></code></pre></td></tr></table></figure><p>关键在这句</p><blockquote><p>raise Error(‘unrecognized partition type %s’ % parttype)</p></blockquote><p>检查分区情况，可以看到确实跟另外一种方法部署的OSD情况不同</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ceph]# ceph-disk list<br>···<br>/dev/sde2 ceph journal, for /dev/sde1<br> /dev/sde1 ceph data, active, cluster ceph, osd.6, journal /dev/sde2<br>dev/sdf :<br> /dev/sdf1 other, xfs, mounted on /var/lib/ceph/osd/ceph-7<br></code></pre></td></tr></table></figure><p>这里要如何处理,才能实现自动挂载，方法是有的</p><p>这个地方需要做一步这个操作（注意下面的1：后面是写死的字符串固定的值）</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs raw">/usr/sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sdi<br><br>/dev/sdi :<br> /dev/sdi1 ceph data, active, cluster ceph, osd.7<br></code></pre></td></tr></table></figure><p>我们来验证一下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ceph]# systemctl stop ceph-osd@7<br>[root@lab8106 ceph]# umount /dev/sdi1 <br>[root@lab8106 ceph]# systemctl start ceph-disk@/dev/sdi1<br>[root@lab8106 ceph]# df -h|grep sdi<br>/dev/sdi1       280G  1.1G  279G   1% /var/lib/ceph/osd/ceph-7<br></code></pre></td></tr></table></figure><p>可以用服务挂载了<br>这个是代码里面写死的判断值，来判断osd是ready的了</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs raw">/usr/lib/python2.7/site-packages/ceph_disk/main.py<br><br>&#x27;osd&#x27;: &#123;<br>            &#x27;ready&#x27;: &#x27;4fbd7e29-9d25-41b8-afd0-062c0ceff05d&#x27;,<br>            &#x27;tobe&#x27;: &#x27;89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be&#x27;,<br>        &#125;,<br></code></pre></td></tr></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>通过本篇的介绍，应该能够清楚什么情况下不自动挂载，什么情况下自动挂载，怎么去实现自动挂载，虽然上面只用了一调命令就实现了，不过我找了很久才定位到这个命令的，当然自己也掌握了这个知识点，公众号已经可以留言了，欢迎留言</p>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>cephonebox发布</title>
    <link href="/2016/12/18/cephonebox%E5%8F%91%E5%B8%83/"/>
    <url>/2016/12/18/cephonebox%E5%8F%91%E5%B8%83/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>现在已经是2016年收官的一个月了，之前一直想做一个calamari的集成版本，之所以有这个想法，是因为，即使在已经打好包的情况下，因为各种软件版本的原因，造成很多人无法配置成功，calamari发布以后，因为版本的更迭，经常出现软件版本引起的BUG</p><p>这里直接把需要的软件集成在一起了，按照本篇指导，一步一步是能够很简单的配置起来的，并且提供了一个视频的指导，应该能够很大程度上降低calamari的部署难度，希望能够帮助到更多的人</p><h2 id="集成软件版本说明"><a href="#集成软件版本说明" class="headerlink" title="集成软件版本说明"></a>集成软件版本说明</h2><h3 id="操作系统-Centos-7-3"><a href="#操作系统-Centos-7-3" class="headerlink" title="操作系统 Centos 7.3"></a>操作系统 Centos 7.3</h3><p>之所以选择这个版本是因为这个是最新发布的，centos小版本的发布能够解决一些BUG，并且不会做很大的改动，并且能够让这个集成系统保持一定的生命周期</p><h3 id="ceph"><a href="#ceph" class="headerlink" title="ceph"></a>ceph</h3><p>ceph采用的是Jewel版本的10.2.5，这个是最新发布的，因为Jewel版本是一个长期支持版本，并且小版本号已经到5了，已经在一定程度上达到了稳定</p><h3 id="calamari"><a href="#calamari" class="headerlink" title="calamari"></a>calamari</h3><p>calamari采用的是1.3版本，因为calamari要做新的接口，基本上这个就是当前系统的最后的一个稳定版本，集成的版本适配Jewel版本接口名称的变化<br>集成版本的diamond解决了无法获取iops的BUG</p><h2 id="视频演示教程"><a href="#视频演示教程" class="headerlink" title="视频演示教程"></a>视频演示教程</h2><p><video play="false" poster="/wp-content/uploads/2016/12/cephoneboxpre2.jpg" src="/wp-content/uploads/2020/06/cephonebox.mp4" controls="controls" width="710" height="400"  ></video></p><h2 id="配置教程"><a href="#配置教程" class="headerlink" title="配置教程"></a>配置教程</h2><h3 id="获取ISO"><a href="#获取ISO" class="headerlink" title="获取ISO"></a>获取ISO</h3><p>地址如下：<br>链接：<a href="http://pan.baidu.com/s/1cimGYa">http://pan.baidu.com/s/1cimGYa</a> 密码：1bs6<br>大小为811M</p><p>安装完成后默认：<br>用户名为root ,密码为123456</p><h3 id="安装操作系统"><a href="#安装操作系统" class="headerlink" title="安装操作系统"></a>安装操作系统</h3><p>本系统是定制的系统，默认本地硬盘启动，选择到第二项进行安装<br>需要配置的地方：</p><ul><li>配置磁盘分区 </li><li>配置主机名和IP</li></ul><h3 id="进行ceph集群的配置"><a href="#进行ceph集群的配置" class="headerlink" title="进行ceph集群的配置"></a>进行ceph集群的配置</h3><p>这个地方可以参考网上的教程进行配置，或者已经有的集群就可以直接跳过这步，这里不做过多的讲解</p><h3 id="配置calamari"><a href="#配置calamari" class="headerlink" title="配置calamari"></a>配置calamari</h3><p>安装好机器后，在服务器上进行calamari的初始化</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">calamari-ctl  initialize<br></code></pre></td></tr></table></figure><p>输入用户名，和密码 ，这个是用于登录web的用户名密码</p><h3 id="配置集群到calamari的连接"><a href="#配置集群到calamari的连接" class="headerlink" title="配置集群到calamari的连接"></a>配置集群到calamari的连接</h3><p>集群到calamari连接一共有两个地方，一个是diamond来采集一些监控信息，一个是salt-minion来采集集群的一些信息以及接收控制</p><h4 id="修改diamond-conf"><a href="#修改diamond-conf" class="headerlink" title="修改diamond.conf"></a>修改diamond.conf</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">cp /etc/diamond/diamond.conf.example /etc/diamond/diamond.conf<br></code></pre></td></tr></table></figure><p>然后修改&#x2F;etc&#x2F;diamond&#x2F;diamond.conf</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs raw">[[GraphiteHandler]]<br>### Options for GraphiteHandler<br># Graphite server host<br>host = graphite<br># Port to send metrics to<br>port = 2003<br># Socket timeout (seconds)<br>timeout = 15<br># Batch size for metrics<br>batch = 1<br></code></pre></td></tr></table></figure><p>修改为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs raw">[[GraphiteHandler]]<br>### Options for GraphiteHandler<br># Graphite server host<br>host = cephoneweb<br># Port to send metrics to<br>port = 2003<br># Socket timeout (seconds)<br>timeout = 15<br># Batch size for metrics<br>batch = 1<br></code></pre></td></tr></table></figure><p>修改集群节点的这个配置文件当中的上面的host字段的主机名为运行calamariweb机器的主机名</p><blockquote><p>注意要在集群节点的hosts文件当中配置好calamari的web的主机名和IP的对应关系</p></blockquote><p>重启diamond</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">systemctl restart diamond<br></code></pre></td></tr></table></figure><h4 id="修改-etc-salt-minion"><a href="#修改-etc-salt-minion" class="headerlink" title="修改&#x2F;etc&#x2F;salt&#x2F;minion"></a>修改&#x2F;etc&#x2F;salt&#x2F;minion</h4><p>修改下面的master后面字段为cephoneweb的主机名</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs raw"># Set the location of the salt master server. If the master server cannot be<br># resolved, then the minion will fail to start.<br>master: cephoneweb<br></code></pre></td></tr></table></figure><p>重启salt-minion</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">systemctl restart salt-minion<br></code></pre></td></tr></table></figure><h3 id="访问WEB"><a href="#访问WEB" class="headerlink" title="访问WEB"></a>访问WEB</h3><p>配置已经完成通过web 访问cephoneweb的IP即可</p><h2 id="故障处理"><a href="#故障处理" class="headerlink" title="故障处理"></a>故障处理</h2><p><img src="/images/blog/o_200901071350%E6%95%85%E9%9A%9C%E4%B8%80%E5%A4%84%E7%90%86.jpg" alt="故障一处理.jpg-42.6kB"><br><img src="/images/blog/o_200901071357%E6%95%85%E9%9A%9C%E4%BA%8C%E5%A4%84%E7%90%86.jpg" alt="故障二处理.jpg-54.8kB"></p>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>rbd的image快照与Pool快照</title>
    <link href="/2016/12/14/rbd%E7%9A%84image%E5%BF%AB%E7%85%A7%E4%B8%8EPool%E5%BF%AB%E7%85%A7/"/>
    <url>/2016/12/14/rbd%E7%9A%84image%E5%BF%AB%E7%85%A7%E4%B8%8EPool%E5%BF%AB%E7%85%A7/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>这个问题是不久前在ceph社区群里看到的，创建image的时候，当时的报错如下：</p><blockquote><p>2016-12-13 23:13:10.266865 7efbfb7fe700 -1 librbd::image::CreateRequest: pool not configured for self-managed RBD snapshot support<br><br>rbd: create error: (22) Invalid argument</p></blockquote><p>这个错我之前也没见过，并且因为很少用到快照，所以可能也就没有触发这个问题,在查看了一些资料以后，明白了原因，这里就梳理一下</p><blockquote><p>Ceph实质上有两种Snapshot模式，并且两种Snapshot是不能同时应用到同一个Pool中。<br><br>Pool Snapshot: 对整个Pool打一个Snapshot，该Pool中所有的对象都会受影响<br><br>Self Managed Snapshot: 用户管理的Snapshot，简单的理解就是这个Pool受影响的对象是受用户控制的。这里的用户往往是应用如librbd。</p></blockquote><p>上面这段话引用自 <a href="http://www.wzxue.com/%E8%A7%A3%E6%9E%90ceph-snapshot/">麦子迈：解析ceph-snapshot</a></p><p>本篇就将讲述下用两种的互斥是如何实现的，又如何解决</p><h2 id="实践"><a href="#实践" class="headerlink" title="实践"></a>实践</h2><p>我们创建两个存储池</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]# ceph osd pool create testpool1 8<br>pool &#x27;testpool1&#x27; created<br>[root@lab8106 ~]# ceph osd pool create testpool2 8<br>pool &#x27;testpool2&#x27; created<br></code></pre></td></tr></table></figure><p>我们在testpool1当中创建一个image，testpool2保留为空</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]# rbd -p testpool1 create testimage --size 4000<br></code></pre></td></tr></table></figure><p>我们检查下存储池</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]# ceph osd dump<br>···<br>pool 5 &#x27;testpool1&#x27; replicated size 2 min_size 1 crush_ruleset 0 object_hash rjenkins pg_num 8 pgp_num 8 last_change 85 flags hashpspool stripe_width 0<br>removed_snaps [1~3]<br>pool 6 &#x27;testpool2&#x27; replicated size 2 min_size 1 crush_ruleset 0 object_hash rjenkins pg_num 8 pgp_num 8 last_change 82 flags hashpspool stripe_width 0<br>···<br></code></pre></td></tr></table></figure><p>创建了image的存储池多了一个标记 removed_snaps [1~3] ,而没有创建的存储池没有这个标记 </p><p>我们现在来对两个存储池创建快照</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]# ceph osd pool mksnap testpool1 testpool1-snap<br>Error EINVAL: pool testpool1 is in unmanaged snaps mode<br>[root@lab8106 ~]# ceph osd pool mksnap testpool2 testpool2-snap<br>created pool testpool2 snap testpool2-snap<br></code></pre></td></tr></table></figure><p>可以看到创建了image的存储池无法创建存储池的快照，因为存储池当前已经为unmanaged snaps mode了，而没有创建image的 就可以做存储池快照</p><p>我们再继续创建image，看下会发生什么</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]# rbd -p testpool1 create testimag2 --size 4000<br>[root@lab8106 ~]# rbd -p testpool2 create testimag3 --size 4000<br>2016-12-13 23:31:58.105932 7fd9cb7fe700 -1 librbd::image::CreateRequest: pool not configured for self-managed RBD snapshot support<br>rbd: create error: (22) Invalid argument<br></code></pre></td></tr></table></figure><p>可以看到做了存储池快照的存储池无法创建image了，并且提示了没有配置成self-managed RBD snapshot，创建的时候会去检查是否是自管理模式</p><p>这个地方有个配置项目是</p><blockquote><p>rbd_validate_pool &#x3D; true</p></blockquote><p>我们把这个参数改成false，然后再次创建</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]# rbd -p testpool2 create testimag3 --size 4000<br></code></pre></td></tr></table></figure><p>就可以创建了</p><p>本篇共出现了两次抛错，所以，这个是需要我们去做好选择的,总结下大概是这样的</p><p><img src="/images/blog/o_200901071245poolsnap.png" alt="此处输入图片的描述"></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本篇总结了快照的两个出错的原因，尽量提前做好规划，再去选择哪种</p>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Cephfs的快照功能</title>
    <link href="/2016/12/13/Cephfs%E7%9A%84%E5%BF%AB%E7%85%A7%E5%8A%9F%E8%83%BD/"/>
    <url>/2016/12/13/Cephfs%E7%9A%84%E5%BF%AB%E7%85%A7%E5%8A%9F%E8%83%BD/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>Cephfs的快照功能在官网都很少提及，因为即使开发了很多年，但是由于cephfs的复杂性，功能一直没能达到稳定，这里，只是介绍一下这个功能，怎么使用，并且建议不要在生产中使用，因为搞不好是会丢数据的</p><h2 id="功能介绍"><a href="#功能介绍" class="headerlink" title="功能介绍"></a>功能介绍</h2><p>首先这个功能是默认关闭的，所以需要开启</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]# ceph fs set ceph  allow_new_snaps 1<br>Error EPERM: Warning! This feature is experimental.It may cause problems up to and including data loss.Consult the documentation at ceph.com, and if unsure, do not proceed.Add --yes-i-really-mean-it if you are certain.<br>[root@lab8106 ~]# ceph fs set ceph  allow_new_snaps 1  --yes-i-really-mean-it<br></code></pre></td></tr></table></figure><p>从提示上可以看到，还是不要在生产上使用</p><p>开发者的话：</p><blockquote><p>In Jewel ceph fs snapshots are still experimental. Does someone has a clue when this would become stable, or how experimental this is ?<br><br>We’re not sure yet. Probably it will follow stable multi-MDS; we’re thinking about redoing some of the core snapshot pieces still. :&#x2F;<br><br>WIt’s still pretty experimental in Jewel. Shen had been working on this and I think it often works, but tends to fall apart under the failure of other components (eg, restarting an MDS while snapshot work is happening).<br>-Greg</p></blockquote><p>挂载集群</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]# mount -t ceph 192.168.8.106:/ /mnt<br>[root@lab8106 ~]# cd /mnt/<br></code></pre></td></tr></table></figure><p>快照是对目录创建的<br>所以我们来看下</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 mnt]# ll<br>total 0<br>drwxr-xr-x 1 root root 51341 Dec 13 11:41 test1<br>drwxr-xr-x 1 root root 51591 Dec 13 16:18 test2<br>drwxr-xr-x 1 root root 30951 Dec 13 15:19 test3<br></code></pre></td></tr></table></figure><p>创建快照</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 mnt]# mkdir .snap/snap1<br>[root@lab8106 mnt]# ll .snap/<br>total 0<br>drwxr-xr-x 1 root root 133883 Dec 13 14:21 snap1<br>[root@lab8106 mnt]# ll .snap/snap1<br>total 0<br>drwxr-xr-x 1 root root 51341 Dec 13 11:41 test1<br>drwxr-xr-x 1 root root 51591 Dec 13 16:18 test2<br>drwxr-xr-x 1 root root 30951 Dec 13 15:19 test3<br></code></pre></td></tr></table></figure><p>创建快照很简单，就是在需要做快照的目录下面执行 <code>mkdir .snap/snapname</code> 后面接快照的名称</p><p>快照的速度非常快，秒级别的</p><p>恢复快照数据</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 mnt]# cp -ra .snap/snap1/* ./<br></code></pre></td></tr></table></figure><p>删除快照</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 mnt]# rmdir .snap/snap1<br></code></pre></td></tr></table></figure><p>删除快照需要用rmdir命令</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本篇简单的介绍了下cephfs快照的相关的操作，自己很久没搞，命令都找不到了，供参考</p>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>查询Ceph的OSD占用内存</title>
    <link href="/2016/12/08/%E6%9F%A5%E8%AF%A2Ceph%E7%9A%84OSD%E5%8D%A0%E7%94%A8%E5%86%85%E5%AD%98/"/>
    <url>/2016/12/08/%E6%9F%A5%E8%AF%A2Ceph%E7%9A%84OSD%E5%8D%A0%E7%94%A8%E5%86%85%E5%AD%98/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>之前写过一篇关于查询OSD的运行的CPU的情况的分享，本篇是讲的获取内存占用的，代码包括两种输出，一种是直接的表格，一种是可以方便解析的json</p><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><p>直接上代码，python才用不久，所以可能代码实现比较低级，主要是看实现的方法</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-meta">#!/usr/bin/env python</span><br><span class="hljs-comment"># -*- coding: UTF-8 -*-</span><br>import os<br>import sys<br>import json<br>import psutil<br>import commands<br>from prettytable import PrettyTable<br>def main():<br>    <span class="hljs-keyword">if</span> len(sys.argv) == 1:<br>        printosdmemtable(<span class="hljs-string">&quot;table&quot;</span>)<br>    <span class="hljs-keyword">elif</span> sys.argv[1] == <span class="hljs-string">&#x27;json&#x27;</span>:<br>        printosdmemtable(<span class="hljs-string">&quot;json&quot;</span>)<br><br>def printosdmemtable(chosse):<br>        data_dic = &#123;&#125;<br>        osd_list=&#123;&#125;<br>        row = PrettyTable()<br>        row.header = True<br>        memlist = [<span class="hljs-string">&quot;OSD\MEM&quot;</span>]<br>        memchose = [ <span class="hljs-string">&#x27;VIRT&#x27;</span>,<span class="hljs-string">&#x27;RES&#x27;</span>]<br>        <span class="hljs-keyword">for</span> meminfo <span class="hljs-keyword">in</span> memchose:<br>            memlist.append(<span class="hljs-string">&quot;%s&quot;</span> %meminfo )<br>        row.field_names = memlist<br>        <span class="hljs-keyword">for</span> root, <span class="hljs-built_in">dirs</span>, files <span class="hljs-keyword">in</span> os.walk(<span class="hljs-string">&#x27;/var/run/ceph/&#x27;</span>):<br>            <span class="hljs-keyword">for</span> name <span class="hljs-keyword">in</span> files:<br>                <span class="hljs-keyword">if</span> <span class="hljs-string">&quot;osd&quot;</span>  <span class="hljs-keyword">in</span> name and <span class="hljs-string">&quot;pid&quot;</span> <span class="hljs-keyword">in</span> name :<br>                    osdlist = []<br>                    osdthlist=[]<br>                    <span class="hljs-keyword">for</span> osdmem <span class="hljs-keyword">in</span> range(len(memchose)):<br>                        osdlist.append(<span class="hljs-string">&quot; &quot;</span>)<br>                    pidfile=root+ name<br>                    osdid=commands.getoutput(<span class="hljs-string">&#x27;ls  %s|cut -d &quot;.&quot; -f 2 2&gt;/dev/null&#x27;</span>  %pidfile )<br>                    osdpid = commands.getoutput(<span class="hljs-string">&#x27;cat %s  2&gt;/dev/null&#x27;</span> %pidfile)<br>                    osd_runmemvsz = commands.getoutput(<span class="hljs-string">&#x27;ps -p %s  -o vsz |grep -v VSZ 2&gt;/dev/null&#x27;</span> %osdpid)<br>                    osd_runmemrsz = commands.getoutput(<span class="hljs-string">&#x27;ps -p %s  -o rsz |grep -v RSZ 2&gt;/dev/null&#x27;</span> %osdpid)<br>                    osdname=<span class="hljs-string">&quot;osd.&quot;</span>+osdid<br>                    osdlist.insert(0,osdname)<br>                    osdlist[1] = str(int(osd_runmemvsz)/1024)+<span class="hljs-string">&quot;KB&quot;</span><br>                    osdlist[2] = str(int(osd_runmemrsz)/1024)+<span class="hljs-string">&quot;KB&quot;</span><br>                    vm_dic = &#123;&#125;<br>                    vm_dic[<span class="hljs-string">&#x27;VSZ&#x27;</span>]= str(int(osd_runmemvsz)/1024)+<span class="hljs-string">&quot;KB&quot;</span><br>                    vm_dic[<span class="hljs-string">&#x27;RSZ&#x27;</span>]= str(int(osd_runmemrsz)/1024)+<span class="hljs-string">&quot;KB&quot;</span><br>                    osd_list[osdname] = vm_dic<br>                    data_dic[<span class="hljs-string">&#x27;osdmemused&#x27;</span>] = osd_list<br>                    <span class="hljs-keyword">if</span> chosse == <span class="hljs-string">&quot;table&quot;</span>:<br>                        row.add_row(osdlist)<br>                    <span class="hljs-keyword">elif</span> chosse == <span class="hljs-string">&quot;json&quot;</span>:<br>                        row = json.dumps(data_dic,separators=(<span class="hljs-string">&#x27;,&#x27;</span>, <span class="hljs-string">&#x27;:&#x27;</span>))<br>        <span class="hljs-built_in">print</span> row<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    main()<br></code></pre></td></tr></table></figure><h2 id="运行脚本"><a href="#运行脚本" class="headerlink" title="运行脚本"></a>运行脚本</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 getmem]<span class="hljs-comment"># python getmem.py </span><br>+---------+-------+------+<br>| OSD\MEM |  VIRT | RES  |<br>+---------+-------+------+<br>|  osd.0  | 825KB | 43KB |<br>|  osd.1  | 826KB | 43KB |<br>+---------+-------+------+<br>[root@lab8106 getmem]<span class="hljs-comment"># python getmem.py json</span><br>&#123;<span class="hljs-string">&quot;osdmemused&quot;</span>:&#123;<span class="hljs-string">&quot;osd.1&quot;</span>:&#123;<span class="hljs-string">&quot;VSZ&quot;</span>:<span class="hljs-string">&quot;826KB&quot;</span>,<span class="hljs-string">&quot;RSZ&quot;</span>:<span class="hljs-string">&quot;43KB&quot;</span>&#125;,<span class="hljs-string">&quot;osd.0&quot;</span>:&#123;<span class="hljs-string">&quot;VSZ&quot;</span>:<span class="hljs-string">&quot;825KB&quot;</span>,<span class="hljs-string">&quot;RSZ&quot;</span>:<span class="hljs-string">&quot;43KB&quot;</span>&#125;&#125;&#125;<br></code></pre></td></tr></table></figure><h2 id="附加"><a href="#附加" class="headerlink" title="附加"></a>附加</h2><p>如果在&#x2F;var&#x2F;run&#x2F;ceph下面没有生成pid，就在配置文件&#x2F;etc&#x2F;ceph&#x2F;ceph.conf当中提前加好配置文件然后重启进程</p><blockquote><p>pid_file&#x3D;&#x2F;var&#x2F;run&#x2F;$cluster&#x2F;$type.$id.pid</p></blockquote>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>基于发行版本内核打造自己的内核</title>
    <link href="/2016/12/07/%E5%9F%BA%E4%BA%8E%E5%8F%91%E8%A1%8C%E7%89%88%E6%9C%AC%E5%86%85%E6%A0%B8%E6%89%93%E9%80%A0%E8%87%AA%E5%B7%B1%E7%9A%84%E5%86%85%E6%A0%B8/"/>
    <url>/2016/12/07/%E5%9F%BA%E4%BA%8E%E5%8F%91%E8%A1%8C%E7%89%88%E6%9C%AC%E5%86%85%E6%A0%B8%E6%89%93%E9%80%A0%E8%87%AA%E5%B7%B1%E7%9A%84%E5%86%85%E6%A0%B8/</url>
    
    <content type="html"><![CDATA[<p>Linux当中最核心的部分就是内核，这个也是最基础，最可能被忽视的一部分，随便找一个刚入职的运维，学习个两三天，网上找些资料也能能自己安装编译内核了，很多运维的初期培训就是做的这些学习，为什么在网上已经有这么多文章的情况下，还要写一篇关于内核的文章，这是因为，我想讲的是如何去选择内核</p><!--break--><p>一般来说，找内核的时候都会去下面这个网站进行选择</p><blockquote><p><a href="https://www.kernel.org/">https://www.kernel.org/</a></p></blockquote><p>很多人在问我的时候，都会问，我该怎么去选择哪个版本的内核，一般来说我的回答是这样的</p><blockquote><p>选取最后一个长期支持版本，或者最后一个稳定的版本</p></blockquote><p>一般来说,选择这两个版本基本不会出太大的问题，并且即使有问题，后面做小版本的升级也不是很难的事情，当然这是基于你对自定义内核很有兴趣，或者需要自己去裁剪，增加一些东西的时候，用我上面说的两个版本都没有问题，下面是一个其他的选择</p><p>最近把linus的just for fun看完了，也基本上了解了linux大概的发展历程，linux走向成功也有一定的原因是围绕在其周围的一些商业公司，红帽是其中最成功的一个公司，当然还有其他各种发型版本，开源版本和商业版本的最大区别在于服务上面，商业公司能够提供专业的服务，开源并不意味着免费，其中很大一部分是学习成本，然后其次就是包装和推广了，最终才是一个完整的产品</p><p>开源有开源的规矩，当然这个规矩在中国不一定行得通，大部分公司不会将开源修改的东西回馈回去，而能够回馈回去的，基本都是技术非常领先的公司，这些公司核心在于自己的技术，以及对产品的把控，所以也就不介意源代码的开源了，并且乐意去引领行业的发展</p><p>当然这个对于红帽这样级别的公司，代码当然是会开源的，而其发行版本的内核，其实都是经过了一些修改的，并且这些修改也都是会开源出来的，只是大部分时候我们并没有去关注它，这就是本篇的重点</p><h2 id="获取源代码"><a href="#获取源代码" class="headerlink" title="获取源代码"></a>获取源代码</h2><blockquote><p><a href="http://vault.centos.org/7.2.1511/os/Source/SPackages/">http://vault.centos.org/7.2.1511/os/Source/SPackages/</a></p></blockquote><p>centos版本</p><p>红帽的内核源码之前托管在ftp上的，现在全部放到了订阅中心了，这里进入红帽订阅中心，进行rpm包的搜索，找到需要的部分，选择下载即可</p><blockquote><p><a href="https://access.redhat.com/downloads/content/kernel/3.10.0-514.el7/x86_64/fd431d51/package">https://access.redhat.com/downloads/content/kernel/3.10.0-514.el7/x86_64/fd431d51/package</a></p></blockquote><p><img src="/images/blog/o_200901070914image_1b39jfbj2178udi87sdptj13809.png" alt="image_1b39jfbj2178udi87sdptj13809.png-137.5kB"></p><p>这里我们是要选择的是源码包，因为可能需要自己加些内核模块进去<br>安装源码包</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">rpm -ivh kernel-3.10.0-514.el7.src.rpm <br></code></pre></td></tr></table></figure><p>安装后默认会放到下面的目录下面，如果你有自定义的目录，也可以直接解压rpm，解压的方法是,下面命令默认会将文件解压到当前目录</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">rpm2cpio kernel-3.10.0-514.el7.src.rpm |cpio -div<br></code></pre></td></tr></table></figure><p>检查文件</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]# ll  /root/rpmbuild/SOURCES/<br>total 82804<br>-rwxr-xr-x 1 root root     3118 Oct 19 23:20 check-kabi<br>-rw-r--r-- 1 root root      150 Oct 19 23:20 cpupower.config<br>-rw-r--r-- 1 root root      294 Oct 19 23:20 cpupower.service<br>-rw-r--r-- 1 root root        0 Oct 19 23:20 extra_certificates<br>-rw-r--r-- 1 root root   121660 Oct 19 23:20 kernel-3.10.0-ppc64.config<br>-rw-r--r-- 1 root root   121951 Oct 19 23:20 kernel-3.10.0-ppc64-debug.config<br>-rw-r--r-- 1 root root   121229 Oct 19 23:20 kernel-3.10.0-ppc64le.config<br>-rw-r--r-- 1 root root   121531 Oct 19 23:20 kernel-3.10.0-ppc64le-debug.config<br>-rw-r--r-- 1 root root    58278 Oct 19 23:20 kernel-3.10.0-s390x.config<br>-rw-r--r-- 1 root root    57895 Oct 19 23:20 kernel-3.10.0-s390x-debug.config<br>-rw-r--r-- 1 root root    30834 Oct 19 23:20 kernel-3.10.0-s390x-kdump.config<br>-rw-r--r-- 1 root root   137690 Oct 19 23:20 kernel-3.10.0-x86_64.config<br>-rw-r--r-- 1 root root   137991 Oct 19 23:20 kernel-3.10.0-x86_64-debug.config<br>-rw-rw-r-- 1 root root     8582 Oct 19 22:19 kernel-abi-whitelists-514.tar.bz2<br>-rw-rw-r-- 1 root root 83660860 Oct 19 22:19 linux-3.10.0-514.el7.tar.xz<br>-rw-r--r-- 1 root root        0 Oct 19 23:20 linux-kernel-test.patch<br>-rw-r--r-- 1 root root     1757 Oct 19 23:20 Makefile.common<br>-rw-r--r-- 1 root root    34277 Oct 19 23:20 Module.kabi_ppc64<br>-rw-r--r-- 1 root root    34277 Oct 19 23:20 Module.kabi_ppc64le<br>-rw-r--r-- 1 root root    31748 Oct 19 23:20 Module.kabi_s390x<br>-rw-r--r-- 1 root root    36881 Oct 19 23:20 Module.kabi_x86_64<br>-rw-r--r-- 1 root root     1198 Oct 19 23:20 rheldup3.x509<br>-rw-r--r-- 1 root root     1176 Oct 19 23:20 rhelkpatch1.x509<br>-rw-r--r-- 1 root root      977 Oct 19 23:20 securebootca.cer<br>-rw-r--r-- 1 root root      899 Oct 19 23:20 secureboot.cer<br>-rwxr-xr-x 1 root root      507 Oct 19 23:20 sign-modules<br>-rw-r--r-- 1 root root      361 Oct 19 23:20 x509.genkey<br></code></pre></td></tr></table></figure><p>##打包内核</p><p>如果需要修改默认的内核选项，就修改这个文件</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">vim /root/rpmbuild/SOURCES/kernel-3.10.0-x86_64.config<br></code></pre></td></tr></table></figure><p>然后开始编译内核rpm包</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">rpmbuild -ba /root/rpmbuild/SPECS/kernel.spec <br></code></pre></td></tr></table></figure><p>然后内核包就生成了，在下面目录当中取rpm包即可</p><blockquote><p>&#x2F;root&#x2F;rpmbuild&#x2F;RPMS&#x2F;x86_64&#x2F;</p></blockquote>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>The Dos and Don&#39;ts for Ceph for OpenStack</title>
    <link href="/2016/11/29/The%20Dos%20and%20Don&#39;ts%20for%20Ceph%20for%20OpenStack/"/>
    <url>/2016/11/29/The%20Dos%20and%20Don&#39;ts%20for%20Ceph%20for%20OpenStack/</url>
    
    <content type="html"><![CDATA[<p>Ceph和OpenStack是一个非常有用和非常受欢迎的组合。 不过，部署Ceph &#x2F; OpenStack经常会有一些容易避免的缺点 - 我们将帮助你解决它们</p><!--break--><h2 id="使用-show-image-direct-url-and-the-Glance-v2-API"><a href="#使用-show-image-direct-url-and-the-Glance-v2-API" class="headerlink" title="使用 show_image_direct_url and the Glance v2 API"></a>使用 show_image_direct_url and the Glance v2 API</h2><p>使用ceph的RBD（RADOS Block Device）,你可以创建克隆,你可以将克隆理解为可写的快照（快照通常是只读的）。克隆只会为相对于父快照变化的部分创建对象，这意味着：</p><ol><li>可以节省空间。这是显而易见的，但是这并不能很有说服力，毕竟存储是分布式系统当中最便宜的部分</li><li>克隆中没有修改的部分还是由原始卷提供。这很重要，因为很容易命中相同的RADOS 对象，相同的osd，不论是用的哪个克隆。而且这意味着，这些对象是从OSD的页面缓存进行响应，换句话说，是RAM提供。RAM比任何存储访问方式速度都快，所以从内存当中提供大量的读取是很好的。正因为这样，从克隆的卷提供数据读取，要比相同数据全拷贝的情况下速度要快一些</li></ol><p>Cinder（当从image创建一个卷）和Nova(从ceph提供临时磁盘)都能够使用ceph的后端的RBD image的克隆，并且是自动的，但这个只有在glance-api.conf中设置了show_image_direct_url&#x3D;true 才会使用，并且配置使用 Glance v2 API进行连接Glance。<a href="http://docs.ceph.com/docs/jewel/rbd/rbd-openstack/#any-openstack-version">参考官网</a></p><h2 id="设置-libvirt-images-type-rbd-on-Nova-compute-nodes"><a href="#设置-libvirt-images-type-rbd-on-Nova-compute-nodes" class="headerlink" title="设置 libvirt&#x2F;images_type &#x3D; rbd on Nova compute nodes"></a>设置 libvirt&#x2F;images_type &#x3D; rbd on Nova compute nodes</h2><p>在NOVA中（使用libvirt的KVM计算驱动），有几个存储临时镜像的配置，不从Cinder卷启动的情况。你可以设置 nova‑compute.conf 的[libvirt]当中的images_type：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs raw">[libvirt]<br>images_type = &lt;type&gt;<br></code></pre></td></tr></table></figure><ul><li>默认的类型是磁盘，这意味着你启动一个新的vm的时候，将会发生下面的事：<br>nova-compute在你的虚拟机管理节点上链接到Glance API,查找所需要的image，下载这个image到你的计算节点，默认在&#x2F;var&#x2F;lib&#x2F;nova&#x2F;instances&#x2F;_base路径下</li><li>然后会创建一个qcow2文件，使用下载的这个image做它的backing file</li></ul><p>这个过程在计算节点上会占用大量的空间，并且会一旦这个镜像没有提前在计算节点上下载好，就会需要等很久才能启动虚拟机，这也使得这样的vm不可能实时的迁移到另外一台主机而不产生宕机时间</p><p>将images_types设置为rbd后意味着disk是存储在rbd的后端的，是原始镜像的克隆，并且是立即创建的，没有延时启动，没有浪费空间，可以获得所有克隆的好处，<a href="http://docs.ceph.com/docs/jewel/rbd/rbd-openstack/#id2">参考文档</a></p><h2 id="在Nova计算节点上启用RBD缓存"><a href="#在Nova计算节点上启用RBD缓存" class="headerlink" title="在Nova计算节点上启用RBD缓存"></a>在Nova计算节点上启用RBD缓存</h2><p>librbd是支持Qemu &#x2F; KVM RBD存储驱动程序的ceph的库，可以使用虚拟化主机的RAM进行磁盘的缓存。你应该使用这个。</p><p>是的，它是一个可以安全使用的缓存。 一方面，virtio-blk与Qemu RBD 驱动程序的组合将正确地实现磁盘刷新。 也就是说，当虚拟机中的应用程序显示“我现在想在磁盘上存储此数据”时，virtio-blk，Qemu和Ceph将一起工作，只有在写入完成时才会报告</p><ul><li>写入主OSD</li><li>复制到可用的副本OSD</li><li>只是写入所有的osd journal才会acknowledged</li></ul><p>此外，Ceph RBD具有一个智能保护：即使它被配置为write-back缓存，它也将拒绝这样做（这意味着它将 write-through模式操作），直到它接收到用户的第一次flush请求。 因此，如果你运行一个永远不会这样做的虚拟机，因为它被错误配置或者它的客户操作系统很老的，那么RBD将固执地拒绝缓存任何写入。 相应的RBD选项称为 rbd cache writethrough until flush，它默认为true，你不应该禁用它。</p><p>你可以通过修改nova-compute 配置文件的下面选项开启writeback caching </p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs raw">[libvirt]<br>images_type = rbd<br>...<br>disk_cachemodes=&quot;network=writeback&quot;<br></code></pre></td></tr></table></figure><p>你应该这样去做</p><h2 id="为images-volumes-and-ephemeral-disks使用单独的池"><a href="#为images-volumes-and-ephemeral-disks使用单独的池" class="headerlink" title="为images, volumes, and ephemeral disks使用单独的池"></a>为images, volumes, and ephemeral disks使用单独的池</h2><p>现在你已经在Glance开启了enabled show_image_direct_url&#x3D;true，配置Cinder and nova-compute与Glance交互 的时候使用 v2 API, 配置 nova-compute使用 libvirt&#x2F;images_type&#x3D;rbd，所有的VMs和volumes都使用rbd克隆，克隆可以跨存储进行，意味着你可以创建RBD image(已经快照)在一个存储池，然后它的克隆在另外一个存储池<br>你应该这样做，有几个原因：</p><ul><li><p>单独的池意味着您可以分别控制对这些池的访问。 这只是一个标准的缓解危险方法：如果您的nova-compute节点被攻破，并且攻击者可以损坏或删除临时磁盘，那么这是坏的 - 但如果他们也可能损坏您的Glance图像那将会更糟。</p></li><li><p>单独池也意味着您可以有不同的池设置，例如size或pg_num的设置。</p></li><li><p>最重要的是，单独的池可以使用单独的crush_ruleset设置。 下面我们会做介绍</p></li></ul><p>通常有三个不同的池：一个用于Glance图像（通常命名为glance或图像），一个用于Cinder卷（cinder或卷），一个用于VM（nova-compute或vms）。</p><h2 id="不需要使用SSD作为你的Ceph-OSD-journal"><a href="#不需要使用SSD作为你的Ceph-OSD-journal" class="headerlink" title="不需要使用SSD作为你的Ceph OSD journal"></a>不需要使用SSD作为你的Ceph OSD journal</h2><p>在这篇文章的建议中，这一个可能是最令人感觉到奇怪和不认可的。 当然，传统的情况下都会认为，你应该总是把你的OSD journal在更快的设备上，并且你应该以1：4到1：6的比例部署ssd和普通磁盘，对吧？</p><p>让我们来看看。 假设你是按1：6的配比方法，你的SATA转盘能够以100 MB&#x2F;s的速度写。 6个OSD，每个OSD使用企业SSD分区上的分区作为。进一步假设SSD能够以500MB&#x2F;s写入。  </p><p>恭喜你，在那种情况下，你刚刚使你的SSD成为瓶颈。虽然你的OSDs聚合带宽支持600 MB &#x2F; s，你的SSD限制你大约83％的性能。</p><p>在这种情况下，你实际上可以用1：4的比例，但使你的聚合带宽只快了一点点，SSD的没有很大的优势</p><p>现在，当然，考虑另一种选择：如果你把你的journal放在OSD相同的设备上，那么你只能有效地使用一半的驱动器的标称带宽，平均来说，因为你写两次到同一设备。 所以这意味着没有SSD，你的有效单个osd带宽只有大约50 MB&#x2F;s，所以你从6个驱动器中得到的总带宽更像是300 MB&#x2F;s，对此，500MB&#x2F; s仍然是一个实质性的改进。</p><p>所以你需要将自己的配比匹配到上面的计算当中，并对价格和性能进行自己的评估。 只是不要认为SSD journal将是万灵药，也许使用ssd算是一个好主意，关键在于比较</p><h2 id="使用all-flash-OSDs"><a href="#使用all-flash-OSDs" class="headerlink" title="使用all-flash OSDs"></a>使用all-flash OSDs</h2><p>有一件事要注意，你的SSD journal不会提高读。 那么，怎样利用SSD的提高读取呢？</p><p>使用ssd做OSD。 也就是说，不是OSD journal，而是具有文件存储和journal的OSD。 这样的ssd的OSD不仅仅是写入速度快，而且读取也会快。</p><p>##将 all-flash OSDs 放入独立的CRUSH root</p><p>假设你不是在全闪存硬件上运行，而是运行一个经济高效的混合集群，其中一些OSD是普通的，而其他是SSD（或NVMe设备或其他），你显然需要单独处理这些OSD。 最简单和容易的方法就是，除了正常配置的默认根之外再创建一个单独的CRUSH根。</p><p>例如，您可以按如下所示设置CRUSH层次结构：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs raw">ID WEIGHT  TYPE NAME         UP/DOWN REWEIGHT PRIMARY-AFFINITY<br>- <br>-1 4.85994 root default<br>-2 1.61998     host elk<br> 0 0.53999         osd.0          up  1.00000          1.00000 <br> 1 0.53999         osd.1          up  1.00000          1.00000 <br> 2 0.53999         osd.2          up  1.00000          1.00000 <br>-3 1.61998     host moose<br> 3 0.53999         osd.3          up  1.00000          1.00000 <br> 4 0.53999         osd.4          up  1.00000          1.00000 <br> 5 0.53999         osd.5          up  1.00000          1.00000 <br>-4 1.61998     host reindeer<br> 6 0.53999         osd.6          up  1.00000          1.00000 <br> 7 0.53999         osd.7          up  1.00000          1.00000 <br> 8 0.53999         osd.8          up  1.00000          1.00000<br>-5 4.85994 root highperf<br>-6 1.61998     host elk-ssd<br> 9 0.53999         osd.9          up  1.00000          1.00000 <br>10 0.53999         osd.10         up  1.00000          1.00000 <br>11 0.53999         osd.11         up  1.00000          1.00000 <br>-7 1.61998     host moose-ssd<br>12 0.53999         osd.12         up  1.00000          1.00000 <br>13 0.53999         osd.13         up  1.00000          1.00000 <br>14 0.53999         osd.14         up  1.00000          1.00000 <br>-8 1.61998     host reindeer-ssd<br>15 0.53999         osd.15         up  1.00000          1.00000 <br>16 0.53999         osd.16         up  1.00000          1.00000 <br>17 0.53999         osd.17         up  1.00000          1.00000<br></code></pre></td></tr></table></figure><p>在上面的示例中，OSDs 0-8分配到默认根，而OSDs 9-17（我们的SSD）属于根highperf。 我们现在可以创建两个单独的CRUSH rule：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs raw">rule replicated_ruleset &#123;<br>    ruleset 0<br>    type replicated<br>    min_size 1<br>    max_size 10<br>    step take default<br>    step chooseleaf firstn 0 type host<br>    step emit<br>&#125;<br><br>rule highperf_ruleset &#123;<br>    ruleset 1<br>    type replicated<br>    min_size 1<br>    max_size 10<br>    step take highperf<br>    step chooseleaf firstn 0 type host<br>    step emit<br>&#125;<br></code></pre></td></tr></table></figure><p>默认crush rule 是replicated_ruleset，从默认根选择OSD，而step take highperf在highperf_ruleset当中意味着它只会选择在highperf根的OSD。</p><h2 id="为存储池池指定all-flash-rule"><a href="#为存储池池指定all-flash-rule" class="headerlink" title="为存储池池指定all-flash rule"></a>为存储池池指定all-flash rule</h2><p>将单个池分配给新的CRUSH crule（并因此分配给不同的OSD集），使用一个命令：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">ceph osd pool set &lt;name&gt; crush_ruleset &lt;number&gt;<br></code></pre></td></tr></table></figure><p>…其中<name>是池的名称，<number>是您的CRUSH RULE的ID。 你可以在线执行此操作，而客户端正在访问其数据 - 当然会有很多remapped和backfill，因此您的整体性能会受到一些影响。</p><p>现在，假设你的环境普通存储比SSD存储更多。 因此，您将需要为all-flash OSD选择单独的池。 这里有一些池可以先迁移到 all-flash。 您可以将以下列表解释为优先级列表：在向群集添加更多SSD容量时，可以逐个将池移动到全闪存存储。</p><ul><li>Nova ephemeral RBD池（vms，nova-compute）</li><li>radosgw bucket indexes .rgw.buckets.index and friends） - 如果你使用radosgw替换你OpenStack Swift</li><li>Cinder volume pools (cinder, volumes)</li><li>radosgw data pools (.rgw.buckets and friends)  - 如果您需要在Swift存储上进行低延迟读取和写入</li><li>Glance image pools (glance, images)</li><li>Cinder backup pools (cinder-backup)  - 通常是这是最后一个转换为 all-flash 的池。</li></ul><h2 id="配置一些具有低延迟本地存储的非Ceph计算主机"><a href="#配置一些具有低延迟本地存储的非Ceph计算主机" class="headerlink" title="配置一些具有低延迟本地存储的非Ceph计算主机"></a>配置一些具有低延迟本地存储的非Ceph计算主机</h2><p>现在，毫无疑问，有一些应用场景，Ceph不会产生你所需要的延迟。 也许任何基于网络的存储都无法满足。 这只是存储和网络技术最近发展的直接结果。</p><p>就在几年前，对块设备的单扇区非缓存写入的平均延迟大约为毫秒或1000微秒（μs）。 相比之下，在承载512字节（1扇区）有效载荷的TCP分组上引起的延迟大约为50μs，这使得100μs的往返行程。 总而言之，从网络写入设备（而不是本地写入）所产生的额外延迟约为10％。</p><p>在过渡期间，对于相同价格的器件的单扇区写入本身约为100μs，顶级的，一些价格还是合理的设备下降到约40μs。 相比之下，网络延迟并没有改变那么多 - 从千兆以太网到10 GbE下降约20％。</p><p>因此，即使通过网络访问单个未复制的SSD设备，现在的延迟将为40 + 80 &#x3D; 120μs，而本地仅为40μs。 这不是10％的开销了，这是一个惊人的三倍</p><p>使用Ceph，这变得更糟。 Ceph多次写入数据，首先到主OSD，然后（并行）写入所有副本。 因此，与40μs的单扇区写操作相比，我们现在至少有两次写操作的延迟，再加上两次网络往返，即40×2 + 80×2 &#x3D;240μs，是本地写延迟的6倍 </p><p>好消息是，大多数应用程序不关心这种延迟开销，因为它们延迟不是关键的。 坏消息是，有些非常在意。</p><p>所以，你应该放弃Ceph因为这样吗？ 不。 但是请考虑添加一些未使用libvirt &#x2F; images_type &#x3D; rbd配置的计算节点，而是使用本地磁盘映像。 将这些主机进行主机聚合，并将它们映射到指定的flavor。 建议您的用户，他们选择这种flavor来跑低延迟的应用程序。</p><h2 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h2><p><a href="https://www.hastexo.com/resources/hints-and-kinks/dos-donts-ceph-openstack/index.html">本篇英文原文</a></p>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>fio测试ceph的filestore</title>
    <link href="/2016/11/23/fio%E6%B5%8B%E8%AF%95ceph%E7%9A%84filestore/"/>
    <url>/2016/11/23/fio%E6%B5%8B%E8%AF%95ceph%E7%9A%84filestore/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>fio是一个适应性非常强的软件，基本上能够模拟所有的IO请求，是目前最全面的一款测试软件，之前在看德国电信的一篇分享的时候，里面就提到了，如果需要测试存储性能，尽量只用一款软件，这样从上层测试到底层去，才能更好的去比较差别</p><p>fio对于ceph来说，可以测试文件系统，基于文件系统之上测试，可以测试内核rbd，将rbdmap到本地格式化以后进行测试，或者基于librbd直接对rbd进行测试，这个是目前都已经有的测试场景，这些不在本篇的讨论的范围内，今天讲的是一种新的测试场景，直接对ceph的底层存储进行测试</p><p><img src="/images/blog/o_200901070316fiotest.png"></p><p>为什么会有这个，因为在以前，如果你要测试一块磁盘是不是适合ceph使用的时候，都是直接对挂载的磁盘进行一些测试，这个是基于文件系统的，并没有真正的模拟到ceph自己的写入模型，所以在开发人员的努力下，模拟对象的写入的驱动已经完成了，这就是本篇需要讲述的内容</p><h2 id="实践过程"><a href="#实践过程" class="headerlink" title="实践过程"></a>实践过程</h2><p>fio engine for objectstore 这个是在ceph的11.0.2这个版本才正式发布出来的，可以看这个pr(<a href="https://github.com/ceph/ceph/pull/10267">pr10267</a>),11.0.2是ceph第一个公开释放的KRAKEN版本的，也说明Jewel版本即将进入比较稳定的情况，新的功能可能会尽量在K版本进行开发</p><h3 id="下载相关代码"><a href="#下载相关代码" class="headerlink" title="下载相关代码"></a>下载相关代码</h3><p>创建一个目录用于存储代码</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ~]<span class="hljs-comment"># mkdir /root/newceph</span><br>[root@lab8106 ~]<span class="hljs-comment"># cd /root/newceph/</span><br></code></pre></td></tr></table></figure><h4 id="下载fio的代码"><a href="#下载fio的代码" class="headerlink" title="下载fio的代码"></a>下载fio的代码</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 newceph]<span class="hljs-comment"># git clone git://git.kernel.dk/fio.git</span><br></code></pre></td></tr></table></figure><h4 id="下载ceph的代码"><a href="#下载ceph的代码" class="headerlink" title="下载ceph的代码"></a>下载ceph的代码</h4><p>下载代码并且切换到指定的11.0.2分支，不要用master分支，里面还没有合进去，并且还有bug</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ~]<span class="hljs-comment"># cd /root/newceph/</span><br>[root@lab8106 newceph]<span class="hljs-comment"># git clone git://github.com/ceph/ceph.git</span><br>[root@lab8106 newceph]<span class="hljs-comment"># cd ceph</span><br>[root@lab8106 ceph]<span class="hljs-comment">#git checkout -b myfenzhi v11.0.2</span><br>[root@lab8106 ceph]<span class="hljs-comment">#git submodule update --init --recursive</span><br></code></pre></td></tr></table></figure><h4 id="创建一个cmake编译的目录并且编译"><a href="#创建一个cmake编译的目录并且编译" class="headerlink" title="创建一个cmake编译的目录并且编译"></a>创建一个cmake编译的目录并且编译</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ~]<span class="hljs-comment"># mkdir /root/newceph/build</span><br>[root@lab8106 ~]<span class="hljs-comment"># cd /root/newceph/build</span><br>[root@lab8106 build]<span class="hljs-comment"># cmake -DWITH_FIO=ON -DFIO_INCLUDE_DIR=/root/newceph/fio/ -DCMAKE_BUILD_TYPE=Release /root/newceph/ceph </span><br>[root@lab8106 build]<span class="hljs-comment"># make install -j 16</span><br></code></pre></td></tr></table></figure><p>安装完成检查是不是生成了这个库文件,fio就是利用这个库作为写入引擎的</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 build]<span class="hljs-comment"># ll lib/libfio_ceph_objectstore.so</span><br>-rwxr-xr-x 1 root root 59090338 Nov 23 22:17 lib/libfio_ceph_objectstore.so<br></code></pre></td></tr></table></figure><p>将库路径让系统识别<br>export LD_LIBRARY_PATH&#x3D;&#x2F;root&#x2F;newceph&#x2F;build&#x2F;lib&#x2F;</p><p>编译fio</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ~]<span class="hljs-comment"># cd /root/newceph/fio/</span><br>[root@lab8106 fio]<span class="hljs-comment">#./configure</span><br>[root@lab8106 fio]<span class="hljs-comment"># make</span><br></code></pre></td></tr></table></figure><p>如果显示下面的，就可以了</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 fio]<span class="hljs-comment"># ./fio --enghelp=libfio_ceph_objectstore.so</span><br>conf                    : Path to a ceph configuration file<br></code></pre></td></tr></table></figure><h3 id="配置测试"><a href="#配置测试" class="headerlink" title="配置测试"></a>配置测试</h3><p>下面需要准备两个配置文件，一个是ceph自身的，一个是fio配置文件，我们看下我的环境下这个配置文件如何写的<br>写fio的测试文件<br>vim filestore.fio</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs bash">[global]<br>ioengine=libfio_ceph_objectstore.so <span class="hljs-comment"># must be found in your LD_LIBRARY_PATH</span><br><br>conf=/etc/ceph/ceph-filestore.conf <span class="hljs-comment"># must point to a valid ceph configuration file</span><br>directory=/var/lib/ceph/osd/ceph-8 <span class="hljs-comment"># directory for osd_data</span><br><br>rw=randwrite<br>iodepth=16<br><br>time_based=1<br>runtime=20s<br><br>[filestore]<br>nr_files=64<br>size=256m<br>bs=64k<br></code></pre></td></tr></table></figure><p>上面的指定了一个配置文件和一个目录，这个目录是你需要测试的集群的存储的目录，里面不需要数据<br>写ceph的配置文件<br>vim &#x2F;etc&#x2F;ceph&#x2F;ceph-filestore.conf</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs bash">[global]<br>    debug filestore = 0/0<br>    debug journal = 0/0<br><br>    <span class="hljs-comment"># spread objects over 8 collections</span><br>    osd pool default pg num = 8<br>    <span class="hljs-comment"># increasing shards can help when scaling number of collections</span><br>    osd op num shards = 5<br><br>    filestore fd cache size = 32<br><br>[osd]<br>    osd objectstore = filestore<br><br>    <span class="hljs-comment"># use directory= option from fio job file</span><br>    osd data =  /var/lib/ceph/osd/ceph-8/<br><br>    <span class="hljs-comment"># journal inside fio_dir</span><br>    osd journal =  /var/lib/ceph/osd/ceph-8/journal<br>    osd journal size = 5000<br>    journal force aio = 1<br></code></pre></td></tr></table></figure><p>配置文件指定数据目录，和journal路径</p><p>开始测试</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 fio]<span class="hljs-comment"># ./fio  filestore.fio </span><br>filestore: (g=0): rw=randwrite, bs=64K-64K/64K-64K/64K-64K, ioengine=ceph-os, iodepth=16<br>fio-2.15-11-g42f1<br>Starting 1 process<br>2016-11-23 22:32:17.713473 7f7536d56780  0 filestore(/var/lib/ceph/osd/ceph-8/) backend xfs (magic 0x58465342)<br>2016-11-23 22:32:17.804601 7f7536d56780  0 filestore(/var/lib/ceph/osd/ceph-8/) backend xfs (magic 0x58465342)<br>2016-11-23 22:32:17.805003 7f7536d56780  0 genericfilestorebackend(/var/lib/ceph/osd/ceph-8/) detect_features: FIEMAP ioctl is disabled via <span class="hljs-string">&#x27;filestore fiemap&#x27;</span> config option<br>2016-11-23 22:32:17.805018 7f7536d56780  0 genericfilestorebackend(/var/lib/ceph/osd/ceph-8/) detect_features: SEEK_DATA/SEEK_HOLE is disabled via <span class="hljs-string">&#x27;filestore seek data hole&#x27;</span> config option<br>2016-11-23 22:32:17.805020 7f7536d56780  0 genericfilestorebackend(/var/lib/ceph/osd/ceph-8/) detect_features: splice() is disabled via <span class="hljs-string">&#x27;filestore splice&#x27;</span> config option<br>2016-11-23 22:32:17.864962 7f7536d56780  0 genericfilestorebackend(/var/lib/ceph/osd/ceph-8/) detect_features: syncfs(2) syscall fully supported (by glibc and kernel)<br>2016-11-23 22:32:17.865056 7f7536d56780  0 xfsfilestorebackend(/var/lib/ceph/osd/ceph-8/) detect_feature: extsize is disabled by conf<br>2016-11-23 22:32:17.865643 7f7536d56780  0 filestore(/var/lib/ceph/osd/ceph-8/) start omap initiation<br>2016-11-23 22:32:17.926589 7f7536d56780  0 filestore(/var/lib/ceph/osd/ceph-8/) mount: enabling WRITEAHEAD journal mode: checkpoint is not enabled<br></code></pre></td></tr></table></figure><p>可以看到，已经开始以对象存储的IO模型去生成测试了，根据自己的需要对不同的存储设备和组合进行测试就可以了</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>作为一个新的测试模型的出现，更加完善了ceph的整体体系，也给磁盘的选型增加更好的测试工具</p>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>查询OSD运行在哪些cpu上</title>
    <link href="/2016/11/16/%E6%9F%A5%E8%AF%A2OSD%E8%BF%90%E8%A1%8C%E5%9C%A8%E5%93%AA%E4%BA%9Bcpu%E4%B8%8A/"/>
    <url>/2016/11/16/%E6%9F%A5%E8%AF%A2OSD%E8%BF%90%E8%A1%8C%E5%9C%A8%E5%93%AA%E4%BA%9Bcpu%E4%B8%8A/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>在看CPU相关的文章的时候，想起来之前有文章讨论是否要做CPU绑定，这个有说绑定的也有说不绑定的，然后就想到一个问题，有去观测这些OSD到底运行在哪些CPU上面么,有问题就好解决了，现在就是要查下机器上的OSD运行在哪些CPU上</p><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><p>这里直接上代码了，最近学习python在，就用python来实现</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-meta">#!/usr/bin/env python</span><br><span class="hljs-comment"># -*- coding: UTF-8 -*-</span><br>import os<br>import sys<br>import json<br>import psutil<br>import commands<br>from prettytable import PrettyTable<br><br>def main():<br>    <span class="hljs-keyword">if</span> len(sys.argv) == 1:<br>        printosdcputable(<span class="hljs-string">&quot;process&quot;</span>)<br>    <span class="hljs-keyword">elif</span> sys.argv[1] == <span class="hljs-string">&#x27;t&#x27;</span>:<br>        printosdcputable(<span class="hljs-string">&quot;thread&quot;</span>)<br><br>def printosdcputable(choose):<br>    <span class="hljs-built_in">print</span> choose<br>    row = PrettyTable()<br>    row.header = True<br>    cpulist = [<span class="hljs-string">&quot;OSD\CPU&quot;</span>]<br>    corelist=[<span class="hljs-string">&quot;Core ID&quot;</span>]<br>    phylist = [<span class="hljs-string">&quot;Physical ID&quot;</span>]<br>    emplist=[<span class="hljs-string">&quot;-----------&quot;</span>]<br>    <span class="hljs-keyword">for</span> cpupro <span class="hljs-keyword">in</span> range(psutil.cpu_count()):<br>        cpulist.append(<span class="hljs-string">&quot;%s&quot;</span> %cpupro )<br><br>        coreid=commands.getoutput(<span class="hljs-string">&#x27;egrep \&#x27;</span>processor|physical <span class="hljs-built_in">id</span>|core <span class="hljs-built_in">id</span>\&#x27; /proc/cpuinfo | <span class="hljs-built_in">cut</span> -d : -f 2 | <span class="hljs-built_in">paste</span> - - -  | awk  \&#x27;<span class="hljs-variable">$1</span>==%s &#123;<span class="hljs-built_in">print</span> <span class="hljs-variable">$3</span> &#125;\&#x27;<span class="hljs-string">&#x27; %cpupro)</span><br><span class="hljs-string">        corelist.append(&quot;%s&quot; %coreid)</span><br><span class="hljs-string"></span><br><span class="hljs-string">        phyid = commands.getoutput(&#x27;</span>egrep \&#x27;processor|physical <span class="hljs-built_in">id</span>|core <span class="hljs-built_in">id</span>\&#x27; /proc/cpuinfo | <span class="hljs-built_in">cut</span> -d : -f 2 | <span class="hljs-built_in">paste</span> - - -  | awk  \&#x27;<span class="hljs-variable">$1</span>==%s &#123;<span class="hljs-built_in">print</span> <span class="hljs-variable">$2</span> &#125;\&#x27;<span class="hljs-string">&#x27; % cpupro)</span><br><span class="hljs-string">        phylist.append(&quot;%s&quot; %phyid)</span><br><span class="hljs-string">        emplist.append(&quot;--&quot;)</span><br><span class="hljs-string"></span><br><span class="hljs-string">    row.field_names = cpulist</span><br><span class="hljs-string">    row.add_row(corelist)</span><br><span class="hljs-string">    row.add_row(phylist)</span><br><span class="hljs-string">    row.add_row(emplist)</span><br><span class="hljs-string"></span><br><span class="hljs-string">    for root, dirs, files in os.walk(&#x27;</span>/var/run/ceph/<span class="hljs-string">&#x27;):</span><br><span class="hljs-string">        for name in files:</span><br><span class="hljs-string">            if &quot;osd&quot;  in name and &quot;pid&quot; in name :</span><br><span class="hljs-string">                osdlist = []</span><br><span class="hljs-string">                osdthlist=[]</span><br><span class="hljs-string">                for osdcpu in range(psutil.cpu_count()):</span><br><span class="hljs-string">                    osdlist.append(&quot; &quot;)</span><br><span class="hljs-string">                    osdthlist.append(&quot;0&quot;)</span><br><span class="hljs-string">                pidfile=root+ name</span><br><span class="hljs-string">                osdid=commands.getoutput(&#x27;</span><span class="hljs-built_in">ls</span>  %s|<span class="hljs-built_in">cut</span> -d <span class="hljs-string">&quot;.&quot;</span> -f 2 2&gt;/dev/null<span class="hljs-string">&#x27;  %pidfile )</span><br><span class="hljs-string">                osdpid = commands.getoutput(&#x27;</span><span class="hljs-built_in">cat</span> %s  2&gt;/dev/null<span class="hljs-string">&#x27; %pidfile)</span><br><span class="hljs-string">                osd_runcpu = commands.getoutput(&#x27;</span>ps -o  psr -p %s |grep -v PSR 2&gt;/dev/null<span class="hljs-string">&#x27; %osdpid)</span><br><span class="hljs-string">                th_list = commands.getoutput(&#x27;</span>ps -o  psr -L  -p %s |grep -v PSR|awk \&#x27;gsub(/^ *| *$/,<span class="hljs-string">&quot;&quot;</span>)\&#x27;  2&gt;/dev/null<span class="hljs-string">&#x27; % osdpid)</span><br><span class="hljs-string"></span><br><span class="hljs-string">                osdname=&quot;osd.&quot;+osdid</span><br><span class="hljs-string">                osdlist[int(osd_runcpu)]=&quot;+&quot;</span><br><span class="hljs-string">                for osdth in th_list.split(&#x27;</span>\n<span class="hljs-string">&#x27;):</span><br><span class="hljs-string">                    osdthlist[int(osdth)] = int(osdthlist[int(osdth)])+1</span><br><span class="hljs-string">                osdlist.insert(0,osdname)</span><br><span class="hljs-string">                osdthlist.insert(0,osdname)</span><br><span class="hljs-string">                if choose == &quot;process&quot;:</span><br><span class="hljs-string">                    row.add_row(osdlist)</span><br><span class="hljs-string">                elif choose == &quot;thread&quot;:</span><br><span class="hljs-string">                    row.add_row(osdthlist)</span><br><span class="hljs-string">    print row</span><br><span class="hljs-string"></span><br><span class="hljs-string">if __name__ == &#x27;</span>__main__<span class="hljs-string">&#x27;:</span><br><span class="hljs-string">    main()</span><br><span class="hljs-string"></span><br></code></pre></td></tr></table></figure><p>运行脚本：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">watch python getosdcpu.py<br></code></pre></td></tr></table></figure><p>或者监控线程</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">watch python getosdcpu.py t<br></code></pre></td></tr></table></figure><p>运行效果如下：</p><p><img src="/images/blog/o_200901070147osdcpu.png"></p><p>线程的情况</p><p><img src="/images/blog/o_200901070153thread.png"></p><p>看上去确实有些CPU上面运行了多个OSD，这里不讨论CPU绑定的好坏，只是展示现象，具体有什么效果，是需要用数据取分析的，这个以后再看下</p><h2 id="补充"><a href="#补充" class="headerlink" title="补充"></a>补充</h2><p>如果你发现你运行的脚本没有结果，这个不是脚本的问题，是因为没有生成pid文件，在配置文件&#x2F;etc&#x2F;ceph&#x2F;ceph.conf当中增加:</p><blockquote><p>pid_file &#x3D; &#x2F;var&#x2F;run&#x2F;ceph&#x2F;$type.$id.pid</p></blockquote><p>然后重启osd进程，检查生成了pid没有</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">ll /var/run/ceph/*.pid<br></code></pre></td></tr></table></figure><h2 id="变更记录"><a href="#变更记录" class="headerlink" title="变更记录"></a>变更记录</h2><table><thead><tr><th align="center">Why</th><th align="center">Who</th><th align="center">When</th></tr></thead><tbody><tr><td align="center">创建</td><td align="center">武汉-运维-磨渣</td><td align="center">2016-11-16</td></tr><tr><td align="center">解决无pid</td><td align="center">武汉-运维-磨渣</td><td align="center">2017-02-21</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>yum安装Ceph指定Jewel版本</title>
    <link href="/2016/11/14/yum%E5%AE%89%E8%A3%85Ceph%E6%8C%87%E5%AE%9AJewel%E7%89%88%E6%9C%AC/"/>
    <url>/2016/11/14/yum%E5%AE%89%E8%A3%85Ceph%E6%8C%87%E5%AE%9AJewel%E7%89%88%E6%9C%AC/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>通过yum安装指定的rpm包，这个一般是  yum –showduplicates list ceph | expand ,然后去通过yum安装指定的版本即可，这个在hammer下是没有问题的，但是在Jewel下进行安装的时候却出现了问题，我们来看下怎么解决这个问题的</p><!--break--><h2 id="实践过程"><a href="#实践过程" class="headerlink" title="实践过程"></a>实践过程</h2><p>我们需要安装  ceph-10.2.0-0.el7  这个版本的，根据之前的方法</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ~]<span class="hljs-comment"># yum install  ceph-10.2.0-0.el7</span><br>Loaded plugins: fastestmirror, langpacks, priorities<br>base                    | 3.6 kB  00:00:00     <br>ceph                    | 2.9 kB  00:00:00     <br>ceph-noarch             | 2.9 kB  00:00:00     <br>epel                    | 4.3 kB  00:00:00<br>···<br>Error: Package: 1:ceph-base-10.2.0-0.el7.x86_64 (ceph)<br>           Requires: librados2 = 1:10.2.0-0.el7<br>           Removing: 1:librados2-0.94.6-0.el7.x86_64 (@ceph)<br>               librados2 = 1:0.94.6-0.el7<br>           Updated By: 1:librados2-10.2.3-0.el7.x86_64 (ceph)<br>               librados2 = 1:10.2.3-0.el7<br>           Available: 1:librados2-0.80.7-0.8.el7.x86_64 (epel)<br>               librados2 = 1:0.80.7-0.8.el7<br>           Available: 1:librados2-0.80.7-3.el7.x86_64 (base)<br>               librados2 = 1:0.80.7-3.el7<br>           Available: 1:librados2-10.1.0-0.el7.x86_64 (ceph)<br><br></code></pre></td></tr></table></figure><p>可以看到我们指定了ceph-10.2.0,但是这个rpm包的依赖却自动的去升级到了librados2-10.2.3，然后这个10.2.3又会跟准备安装的ceph-10.2.0冲突了，然后就会提示无法安装了</p><p>问题已经找到了，我们如何解决这个问题，第一想法就是应该把版本限制住，在参阅了一些资料以后，发现yum确实可以支持这个需求的，我们来限制下版本</p><h3 id="限制yum版本"><a href="#限制yum版本" class="headerlink" title="限制yum版本"></a>限制yum版本</h3><p>vim &#x2F;etc&#x2F;yum.conf<br>在[main]当中,添加下面的内容</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">exclude=*10.2.3* *10.2.2* *10.2.1*<br></code></pre></td></tr></table></figure><p>为什么写了三个，因为在10.2.0之上有三个版本的，这个地方进行全匹配的方式进行限制</p><h3 id="安装ceph-10-2-0"><a href="#安装ceph-10-2-0" class="headerlink" title="安装ceph-10.2.0"></a>安装ceph-10.2.0</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ~]<span class="hljs-comment"># yum install  ceph-10.2.0-0.el7</span><br>Dependencies Resolved<br><br>====================================================================<br> Package                 Arch           Version                    Repository           Size<br>=====================================================================<br>Updating:<br> ceph               x86_64              1:10.2.0-0.el7               ceph               3.1 k<br>Installing <span class="hljs-keyword">for</span> dependencies:<br> ceph-base              x86_64          1:10.2.0-0.el7              ceph               4.2 M<br> ceph-mds               x86_64          1:10.2.0-0.el7              ceph                2.8 M<br> ceph-mon               x86_64          1:10.2.0-0.el7              ceph                2.8 M<br> ceph-osd               x86_64          1:10.2.0-0.el7              ceph               9.0 M<br> ceph-selinux           x86_64          1:10.2.0-0.el7              ceph               20 k<br> libradosstriper1       x86_64          1:10.2.0-0.el7              ceph               1.8 M<br> librgw2                x86_64          1:10.2.0-0.el7              ceph               2.8 M<br>Updating <span class="hljs-keyword">for</span> dependencies:<br> ceph-common            x86_64           1:10.2.0-0.el7             ceph               15 M<br> libcephfs1            x86_64           1:10.2.0-0.el7              ceph               1.8 M<br> librados2             x86_64           1:10.2.0-0.el7              ceph               1.9 M<br> librbd1               x86_64           1:10.2.0-0.el7              ceph               2.4 M<br> python-cephfs         x86_64           1:10.2.0-0.el7              ceph               67 k<br> python-rados          x86_64           1:10.2.0-0.el7              ceph               146 k<br> python-rbd            x86_64           1:10.2.0-0.el7              ceph                62 k<br><br>Transaction Summary<br>====================================================================<br>Install             ( 7 Dependent packages)<br>Upgrade  1 Package  (+7 Dependent packages)<br></code></pre></td></tr></table></figure><p>可以正确的安装了</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>通过yum去指定版本，然后去过滤高的版本的方式，来安装了指定的版本的ceph</p><h3 id="变更记录"><a href="#变更记录" class="headerlink" title="变更记录"></a>变更记录</h3><table><thead><tr><th align="center">Why</th><th align="center">Who</th><th align="center">When</th></tr></thead><tbody><tr><td align="center">创建</td><td align="center">武汉-运维-磨渣</td><td align="center">2016-11-14</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>处理stale的pg</title>
    <link href="/2016/11/14/%E5%A4%84%E7%90%86stale%E7%9A%84pg/"/>
    <url>/2016/11/14/%E5%A4%84%E7%90%86stale%E7%9A%84pg/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>在某些场景下Ceph集群会出现stale的情况，也就是ceph集群PG的僵死状态，这个状态实际上是无法处理新的请求了，新的请求过来只会block，那么我们如何去恢复环境</p><h2 id="实践过程"><a href="#实践过程" class="headerlink" title="实践过程"></a>实践过程</h2><p>首先模拟stale环境，这个比较好模拟</p><blockquote><p>设置副本2，然后同时关闭两个OSD（不同故障域上），然后删除这两个OSD</p></blockquote><p>集群这个时候就会出现stale的情况了，因为两份数据都丢了，在一些环境下，数据本身就是临时的或者不是那么重要的，比如存储日志，这样的环境下，只需要快速的恢复环境即可，而不担心数据的丢失</p><h3 id="处理过程"><a href="#处理过程" class="headerlink" title="处理过程"></a>处理过程</h3><p>首先用ceph pg dump|grep stale 找出所有的stale的pg</p><p>然后用 ceph force_create_pg  pg_id</p><p>如果做到这里，可以看到之前的stale的状态的PG，现在已经是creating状态的了，这个时候一个关键的步骤需要做下</p><p>就是重启整个集群的OSD，在重启完成了以后，集群的状态就会恢复正常了，也能够正常的写入新的数据了，对于老的数据，做下清理即可</p><h2 id="变更记录"><a href="#变更记录" class="headerlink" title="变更记录"></a>变更记录</h2><table><thead><tr><th align="center">Why</th><th align="center">Who</th><th align="center">When</th></tr></thead><tbody><tr><td align="center">创建</td><td align="center">武汉-运维-磨渣</td><td align="center">2016-11-14</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>预估Ceph集群恢复时间</title>
    <link href="/2016/11/10/%E9%A2%84%E4%BC%B0Ceph%E9%9B%86%E7%BE%A4%E6%81%A2%E5%A4%8D%E6%97%B6%E9%97%B4/"/>
    <url>/2016/11/10/%E9%A2%84%E4%BC%B0Ceph%E9%9B%86%E7%BE%A4%E6%81%A2%E5%A4%8D%E6%97%B6%E9%97%B4/</url>
    
    <content type="html"><![CDATA[<h2 id="一、前言"><a href="#一、前言" class="headerlink" title="一、前言"></a>一、前言</h2><p>本章很简单，就是预估集群恢复的时间,这个地方是简单的通过计算来预估需要恢复的实际，动态的显示</p><!--break--><h2 id="二、代码"><a href="#二、代码" class="headerlink" title="二、代码"></a>二、代码</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><code class="hljs raw">#!/usr/bin/env python<br># -*- coding: UTF-8 -*-<br>import os<br>import sys<br>import commands<br>import json<br>def main():<br>    gettime()<br>def conversecs(sec):<br>    d = sec/86400<br>    h = sec%86400/3600<br>    m = sec%3600/60<br>    s = sec%60<br>    return &quot;remain time:%s day %s hour %s min %s sec&quot; %(d,h,m,s)<br>def gettime():<br>    try:<br>        recover_time = commands.getoutput(&#x27;timeout 10 ceph -s -f json 2&gt;/dev/null&#x27;)<br>        json_str = json.loads(recover_time)<br>        if json_str[&quot;pgmap&quot;].has_key(&#x27;degraded_objects&#x27;) == True:<br>            degraded_objects = json_str[&quot;pgmap&quot;][&quot;degraded_objects&quot;]<br>            if json_str[&quot;pgmap&quot;].has_key(&#x27;recovering_objects_per_sec&#x27;) == True and json_str[&quot;pgmap&quot;][&quot;recovering_objects_per_sec&quot;] != 0:<br>                recovering_objects_per_sec = json_str[&quot;pgmap&quot;][&quot;recovering_objects_per_sec&quot;]<br>                resec=degraded_objects/recovering_objects_per_sec<br>                print  &quot;recovery  objects: %s&quot; %(degraded_objects)<br>                print   &quot;recovery speed :%s&quot; %(recovering_objects_per_sec)<br>                print  conversecs(resec)<br>            else:<br>                resec=degraded_objects/1<br>                print  &quot;recovery  objects: %s&quot; %(degraded_objects)<br>                print   &quot;recovery speed :0&quot;<br>                print  conversecs(resec)<br>        else:<br>            print &quot;recover all  done!&quot;<br>    except:<br>        print &quot;Ceph Cluster health？try ceph -s&quot;<br>if __name__ == &#x27;__main__&#x27;:<br>    main()<br></code></pre></td></tr></table></figure><h3 id="执行"><a href="#执行" class="headerlink" title="执行"></a>执行</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">watch python  recoverytime.py <br></code></pre></td></tr></table></figure><h3 id="通过脚本获取的方式"><a href="#通过脚本获取的方式" class="headerlink" title="通过脚本获取的方式"></a>通过脚本获取的方式</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-meta">#! /bin/sh</span><br><span class="hljs-keyword">while</span> ( 2&gt;1 )<br><span class="hljs-keyword">do</span> <br>start=`ceph -s -f json-pretty|grep misplaced_objects|<span class="hljs-built_in">cut</span> -d <span class="hljs-string">&quot;:&quot;</span> -f 2|<span class="hljs-built_in">cut</span> -d <span class="hljs-string">&#x27;,&#x27;</span> -f 1`<br><span class="hljs-built_in">sleep</span> 5<br>end=`ceph -s -f json-pretty|grep misplaced_objects|<span class="hljs-built_in">cut</span> -d <span class="hljs-string">&quot;:&quot;</span> -f 2|<span class="hljs-built_in">cut</span> -d <span class="hljs-string">&#x27;,&#x27;</span> -f 1`<br>speed=$((start-end))<br><span class="hljs-comment">#echo $end</span><br><span class="hljs-comment">#echo $speed</span><br>second=$((end/speed*<span class="hljs-number">5</span>))<br><br>hour=$(( <span class="hljs-variable">$second</span>/<span class="hljs-number">3600</span> ))<br>min=$(( (<span class="hljs-variable">$second</span>-<span class="hljs-variable">$&#123;hour&#125;</span>*<span class="hljs-number">3600</span>)/<span class="hljs-number">60</span> ))<br>sec=$(( <span class="hljs-variable">$second</span>-<span class="hljs-variable">$&#123;hour&#125;</span>*<span class="hljs-number">3600</span>-<span class="hljs-variable">$&#123;min&#125;</span>*<span class="hljs-number">60</span> ))<br><span class="hljs-built_in">echo</span> 当前时间:`<span class="hljs-built_in">date</span>`<br><span class="hljs-built_in">echo</span> 迁移剩余:<span class="hljs-variable">$end</span><br><span class="hljs-built_in">echo</span> 迁移速度:$((speed/<span class="hljs-number">5</span>))<br><span class="hljs-built_in">echo</span> 迁移还需要:<span class="hljs-variable">$&#123;hour&#125;</span>小时<span class="hljs-variable">$&#123;min&#125;</span>分<span class="hljs-variable">$&#123;sec&#125;</span>秒<br><br><span class="hljs-keyword">done</span><br></code></pre></td></tr></table></figure><p>这个是bash脚本获取的方式</p><h2 id="三、效果"><a href="#三、效果" class="headerlink" title="三、效果"></a>三、效果</h2><p><img src="/images/blog/o_200901065912image_1b16geboc1nok1iif73n6uuij719.png"></p><h2 id="四、进度"><a href="#四、进度" class="headerlink" title="四、进度"></a>四、进度</h2><p>目前只统计了恢复的，还要考虑backfill的，后续增加</p>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>CephFS cache tier实践</title>
    <link href="/2016/11/07/CephFS%20cache%20tier%E5%AE%9E%E8%B7%B5/"/>
    <url>/2016/11/07/CephFS%20cache%20tier%E5%AE%9E%E8%B7%B5/</url>
    
    <content type="html"><![CDATA[<p>这是一篇分享文，作者因为最近想深入研究下ceph的cache pool，作者写的文章非常的好，这里先直接翻译这篇文章，然后再加入我自己的相关数据</p><h3 id="blog原文"><a href="#blog原文" class="headerlink" title="blog原文"></a><a href="http://maybebuggy.de/post/ceph-cache-tier/">blog原文</a></h3><p>作者想启动blog写下自己的Openstack和Ceph的相关经验，第一个话题就选择了 Ceph cache tiering , 作者的使用场景为短时间的虚拟机，用来跑测试的，这种场景他们准备用Nvme做一个缓冲池来加速的虚拟机</p><p>cache 相关的一些参数</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs bash">target_max_bytes<br>target_max_objects<br>cache_target_dirty_ratio<br>cache_target_full_ratio<br>cache_min_flush_age<br>cache_min_evict_age<br></code></pre></td></tr></table></figure><p>Jewel版本还新加入了一个参数</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">cache_target_dirty_high_ratio<br></code></pre></td></tr></table></figure><p>作者的想法是先把数据写入到缓冲池当中，等后面某个时刻再写入到真实的存储池的当中</p><p>Flushing vs. Evicting<br>Flushing是将缓冲池中的数据刷到真实的存储池当中去，但是并不去删除缓冲池里面缓存的数据，只有clean的数据才能被evic，如果是dirty的数据做evic，那么先要flush到真实存储池，然后再删除掉</p><p>Cache 调整</p><p>Ceph的是不能够自动确定缓存池的大小，所以这里需要配置一个缓冲池的绝对大小，flush&#x2F;evic将无法工作。</p><p>设置了上限以后，相关的参数就是cache_target_full_ratio和cache_target_dirty_ratio。这些参数是控制什么时候进行flush和evic的</p><p>这个dirty ratio是比较难设置的值，需要根据场景进行相关的调整</p><p>新版本里面到了dirty_high_ratio才开始下刷</p><p>还有cache_min_flush_age和cache_min_evict_age这个控制，这个一般来说到了设定的阀值前，这些对象的留存时间应该是要够老的，能够被触发清理掉的</p><p>通过ceph df detail 可以观测你的存储池的数据的情况</p><p>里面会有一些0字节对象的，缓冲池的0字节对象是数据已经被删除了，防止刷新的时候又要操作对象。在真实存储池中的0字节对象是数据已经在缓冲池当中，但没有刷新到缓冲池</p><h3 id="案例测试"><a href="#案例测试" class="headerlink" title="案例测试"></a>案例测试</h3><p>基于上面的控制，下面我们来具体看下这些参数的实际效果是怎样的，这样我们才能真正在实际场景当中做到精准的控制</p><p>首先我们要对参数分类</p><ul><li>缓冲池的总大小，这个大小分成两类一个对象个数控制，一个大小的控制</li><li>flush和evic的百分比，这个百分比既按照大小进行控制，也按照对象进行控制</li><li>flush和evic的时间控制</li></ul><p>分好类以后，我们就开始我们的测试，基于对象的数目的控制，比较容易观察，我们就用对象控制来举例子</p><h3 id="创建一个缓冲池的环境"><a href="#创建一个缓冲池的环境" class="headerlink" title="创建一个缓冲池的环境"></a>创建一个缓冲池的环境</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs bash">ceph osd pool create testpool 24 24 <br>ceph osd pool create cachepool 24 24<br>ceph osd tier add  testpool cachepool<br>ceph osd tier cache-mode  cachepool writeback<br>ceph osd tier set-overlay  testpool cachepool<br>ceph osd pool <span class="hljs-built_in">set</span> cachepool hit_set_type bloom<br>ceph osd pool <span class="hljs-built_in">set</span> cachepool hit_set_count 1<br>ceph osd pool <span class="hljs-built_in">set</span> cachepool hit_set_period 3600<br></code></pre></td></tr></table></figure><p>上面的操作是基本的一些操作、我们现在做参数相关的调整</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">ceph osd pool <span class="hljs-built_in">set</span> cachepool  target_max_bytes 1000000000000<br></code></pre></td></tr></table></figure><p>为了排除干扰，我们把 target_max_bytes设置成了1T，我们的测试数据很少，肯定不会触发这个大小</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">ceph osd pool <span class="hljs-built_in">set</span> cachepool target_max_objects 1000<br></code></pre></td></tr></table></figure><p>设置缓冲池的对象max为1000</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">ceph osd pool <span class="hljs-built_in">set</span> cachepool cache_target_dirty_ratio 0.4<br></code></pre></td></tr></table></figure><p>设置dirty_ratio为0.4，也就是0.4为判断为dirty的阀值</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">ceph osd pool <span class="hljs-built_in">set</span> cachepool cache_target_full_ratio 0.8<br></code></pre></td></tr></table></figure><p>设置cache_target_full_ratio为0.8，即超过80%的时候需要evic</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">ceph osd pool <span class="hljs-built_in">set</span> cachepool cache_min_flush_age 600<br>ceph osd pool <span class="hljs-built_in">set</span> cachepool cache_min_evict_age 1800<br></code></pre></td></tr></table></figure><p>设置两个flush和evic的时间，这个时间周期比我写入的数据的时间周期大很多，这个等下会调整这个</p><p>开启一个终端动态观察存储池的对象变化</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ~]<span class="hljs-comment"># watch ceph df</span><br>Every 2.0s: ceph <span class="hljs-built_in">df</span>                                                                    <br><br>GLOBAL:<br>    SIZE     AVAIL     RAW USED     %RAW USED<br>    834G      833G         958M          0.11<br>POOLS:<br>    NAME          ID     USED       %USED     MAX AVAIL     OBJECTS<br>    rbd           0           0         0          277G           0<br>    metadata  1  61953k      0.01          416G          39<br>    data          2  50500k      0.01          416G       50501<br>    testpool  5           0         0          416G           0<br>    cachepool     6           0         0          416G           0<br></code></pre></td></tr></table></figure><p>尝试写入数据并且观察，到了1000左右的时候停止</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">rados -p testpool bench 100 write  -b 4K --no-cleanup<br></code></pre></td></tr></table></figure><p>可以观察到cachepool的对象数目大概在1100-1200之间，一直写也会是这个数字，在停止写以后，观察cachepool的对象数目在960左右，我们设置的 target_max_objects 为1000，在超过了这个值以后，并且写停止的情况下，系统会把这个cache pool的对象控制在比target_max少50左右，现在我们修改下 cache_min_evict_age 这个参数，看下会发生些什么</p><p>我们把这个参数调整为30</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">ceph osd pool <span class="hljs-built_in">set</span> cachepool cache_min_evict_age 30<br></code></pre></td></tr></table></figure><p>设置完了以后，可以看到cache pool的对象数目在 744左右，现在再写入数据，然后等待，看下会是多少，还是756，如果按我们设置的 cache_target_full_ratio 0.8就正好是800，我们尝试再次调整大cache_min_evict_age看下情况，对象维持在960左右，根据这个测试，基本上可以看出来是如何控制缓存的数据了，下面用一张图来看下这个问题</p><p><img src="/images/blog/o_200901065808cache.png"></p><p>来总结一下：</p><ul><li>如果cache pool对象到了 target_max_objects，那么会边flush，边evic，然后因为前面有客户端请求，这个时候实际是会阻塞的</li><li>如果停止了写请求，系统会自动将cache pool的对象控制在比 target_max_objects 少一点点</li><li>如果时间周期到了cache_min_evict_age，那么系统会自动将cache pool的对象控制在比 cache_target_full_ratio 少一点点</li><li>同理如果到了cache_min_flush_age，那么会将对象往真实的存储池flush到 cache_target_dirty_ratio 少一点点</li></ul><p>也就是ratio是给定了一个比例，然后时间到了就去将缓存控制到指定的ratio，这个地方就需要根据需要去控制缓冲池数据是留有多少的缓存余地的</p><p>使用命令清空缓冲池的数据，会将数据flush到真实存储池，然后将数据evic掉</p><p>关于缓冲池的就写这么多了，实际环境是要根据自己的使用场景去制定这些值的，从而能保证缓冲池能真正起到作用，上面的例子是基于对象的控制的，基于大小的控制是一样的，只是将对象数的设置换成了大小即可，然后尽量去放大对象的控制</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">rados -p cachepool cache-try-flush-evict-all<br></code></pre></td></tr></table></figure><h3 id="变更记录"><a href="#变更记录" class="headerlink" title="变更记录"></a>变更记录</h3><table><thead><tr><th align="center">Why</th><th align="center">Who</th><th align="center">When</th></tr></thead><tbody><tr><td align="center">创建</td><td align="center">武汉-运维-磨渣</td><td align="center">2016-11-07</td></tr><tr><td align="center">完成缓冲池相关</td><td align="center">武汉-运维-磨渣</td><td align="center">2016-11-08</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>ceph unfound objects 处理</title>
    <link href="/2016/11/04/ceph%20unfound%20objects%20%E5%A4%84%E7%90%86/"/>
    <url>/2016/11/04/ceph%20unfound%20objects%20%E5%A4%84%E7%90%86/</url>
    
    <content type="html"><![CDATA[<h2 id="ceph-Vol-45-Issue-1"><a href="#ceph-Vol-45-Issue-1" class="headerlink" title="ceph Vol 45 Issue 1"></a>ceph Vol 45 Issue 1</h2><h3 id="1-unfound-objects-blocking-cluster-need-help"><a href="#1-unfound-objects-blocking-cluster-need-help" class="headerlink" title="1.unfound objects blocking cluster, need help!"></a>1.unfound objects blocking cluster, need help!</h3><!--break--><blockquote><p>Hi,</p></blockquote><blockquote><p>I have a production cluster on which 1 OSD on a failing disk was slowing the whole cluster down. I removed the OSD (osd.87) like usual in such case but this time it resulted in 17 unfound objects. I no longer have the files from osd.87. I was able to call “ceph pg PGID mark_unfound_lost delete” on 10 of those objects.</p></blockquote><blockquote><p>On the remaining objects 7 the command blocks. When I try to do “ceph pg PGID query” on this PG it also blocks. I suspect this is same reason why mark_unfound blocks.</p></blockquote><blockquote><p>Other client IO to PGs that have unfound objects are also blocked. When trying to query the OSDs which has the PG with unfound objects, “ceph tell” blocks.</p></blockquote><blockquote><p>I tried to mark the PG as complete using ceph-objectstore-tool but it did not help as the PG is in fact complete but for some reason blocks.</p></blockquote><blockquote><p>I tried recreating an empty osd.87 and importing the PG exported from other replica but it did not help.</p></blockquote><blockquote><p>Can someone help me please? This is really important.</p></blockquote><p>这个问题是作者一个集群中(ceph 0.94.5)出现了一个磁盘损坏以后造成了一些对象的丢失，然后在做了一定的处理以后，集群状态已经正常了，但是还是新的请求会出现block的状态，这个情况下如何处理才能让集群正常，作者贴出了pg dump，ceph -s,ceph osd dump相关信息，当出现异常的时候，需要人协助的时候，应该提供这些信息方便其他人定位问题，最后这个问题作者自己给出了自己的解决办法，出现的时候影响是当时的流量只有正常情况下的10%了，影响还是很大的</p><h3 id="复现问题过程"><a href="#复现问题过程" class="headerlink" title="复现问题过程"></a>复现问题过程</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ceph]# rados -p rbd put testremove testremove<br>[root@lab8106 ceph]# ceph osd map rbd testremove<br>osdmap e85 pool &#x27;rbd&#x27; (0) object &#x27;testremove&#x27; -&gt; pg 0.eaf226a7 (0.27) -&gt; up ([1,0], p1) acting <br></code></pre></td></tr></table></figure><p>写入文件,找到文件，然后去后台删除对象<br>然后停止掉其中一个OSD，这里选择停掉主OSD</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs raw">systemctl stop ceph-osd@1<br>ceph osd out 1<br></code></pre></td></tr></table></figure><p>查看状态pg被锁住状态active+degrade，不会迁移完整,并且会检测到了有数据unfound了</p><p>然后向这个对象发起get请求</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ceph]# rados -p rbd get testremove testfile<br></code></pre></td></tr></table></figure><p>前端rados请求会卡住，后端出现 requests are blocked</p><p>看下如何处理</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">ceph pg 0.27 mark_unfound_lost delete<br></code></pre></td></tr></table></figure><p>邮件列表作者的环境，这个命令也无法执行，直接卡死，后来发现有个执行窗口，就是这个对象所在的PG的OSD在启动过程中还是可以接受命令的，就在这个执行窗口执行这个命令就可以解决了</p><p>执行了以后可以执行命令</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ceph]# rados  -p rbd  get testremove  a<br>error getting rbd/testremove: (2) No such file or directory<br></code></pre></td></tr></table></figure><p>这个时候查询集群的状态可以看到，集群已经正常的恢复了，不会因为一个对象的丢失造成集群的PG状态卡在待迁移状态</p><p>可以看到请求是失败的但是不会像之前一样卡死的状态，卡死是比失败更严重的一种状态</p><p>如果不想看到老的 slow request ,那么就重启这个卡住的PG所在的osd，如果本来就正常了，那么这个异常状态就会消失</p><p>这个是一个需要人工干预的状态，实际上模拟的就是对象丢失的场景，什么情况下会对象丢失，一般来说，底层磁盘的故障，写下去的对象当时记录着有，正好写入完成又准备写副本的时候，磁盘坏了，这个就有比较高的概率出现，所以出现了坏盘要尽早更换</p>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Ceph部署的时候修改默认权重</title>
    <link href="/2016/11/02/Ceph%E9%83%A8%E7%BD%B2%E7%9A%84%E6%97%B6%E5%80%99%E4%BF%AE%E6%94%B9%E9%BB%98%E8%AE%A4%E6%9D%83%E9%87%8D/"/>
    <url>/2016/11/02/Ceph%E9%83%A8%E7%BD%B2%E7%9A%84%E6%97%B6%E5%80%99%E4%BF%AE%E6%94%B9%E9%BB%98%E8%AE%A4%E6%9D%83%E9%87%8D/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>部署集群的时候权重是默认生成的，这个是根据磁盘大小分配的，我们有的时候需要去修改一下这个默认权重</p><h2 id="修改"><a href="#修改" class="headerlink" title="修改"></a>修改</h2><p>如果统一的初始值，那么直接添加参数即可</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">osd_crush_initial_weight<br></code></pre></td></tr></table></figure><p>如果想自己添加算法，那么就根据下面的去做就可以了</p><h3 id="centos-jewel"><a href="#centos-jewel" class="headerlink" title="centos+jewel"></a>centos+jewel</h3><p>修改：<br>&#x2F;usr&#x2F;lib&#x2F;ceph&#x2F;ceph-osd-prestart.sh</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">defaultweight=`df -P -k $data/ | tail -1 | awk &#x27;&#123; d= $2/107374182 ; r = sprintf(&quot;%.4f&quot;, d); print r &#125;&#x27;`<br></code></pre></td></tr></table></figure><p>修改这个地方的值就可以了</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">defaultweight=`echo 2`<br></code></pre></td></tr></table></figure><h3 id="centos-hammer"><a href="#centos-hammer" class="headerlink" title="centos+hammer"></a>centos+hammer</h3><p>修改 &#x2F;etc&#x2F;init.d&#x2F;ceph</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">defaultweight=&quot;$(df -P -k $osd_data/. | tail -1 | awk &#x27;&#123; print sprintf(&quot;%.2f&quot;,$2/1073741824) &#125;&#x27;)&quot;<br></code></pre></td></tr></table></figure><p>修改成</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">defaultweight=&quot;$(echo 5)&quot;<br></code></pre></td></tr></table></figure><h3 id="ubuntu-hammer"><a href="#ubuntu-hammer" class="headerlink" title="ubuntu+hammer"></a>ubuntu+hammer</h3><p>由于ubuntu用initctl控制服务，不是用的&#x2F;etc&#x2F;init.d&#x2F;ceph&#x2F;,所以要修改另外的一个路径<br>修改&#x2F;usr&#x2F;libexec&#x2F;ceph&#x2F;ceph-osd-prestart.sh </p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">defaultweight=`df -P -k /var/lib/ceph/osd/$&#123;cluster:-ceph&#125;-$id/ | tail -1 | awk &#x27;&#123; d= $2/1073741824 ; r = sprintf(&quot;%.2f&quot;, d); print r &#125;&#x27;`<br></code></pre></td></tr></table></figure><p>修改为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">defaultweight=`echo 8`<br></code></pre></td></tr></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>这个比较简单，通过修改取值就可以改变默认配置了,上面的可以根据自己的需求加入算法即可</p><h2 id="变更记录"><a href="#变更记录" class="headerlink" title="变更记录"></a>变更记录</h2><table><thead><tr><th align="center">Why</th><th align="center">Who</th><th align="center">When</th></tr></thead><tbody><tr><td align="center">创建</td><td align="center">武汉-运维-磨渣</td><td align="center">2016-11-02</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>cephfs删除报nospace的问题</title>
    <link href="/2016/11/01/cephfs%E5%88%A0%E9%99%A4%E6%8A%A5nospace%E7%9A%84%E9%97%AE%E9%A2%98/"/>
    <url>/2016/11/01/cephfs%E5%88%A0%E9%99%A4%E6%8A%A5nospace%E7%9A%84%E9%97%AE%E9%A2%98/</url>
    
    <content type="html"><![CDATA[<h2 id="ceph-Vol-45-Issue-2"><a href="#ceph-Vol-45-Issue-2" class="headerlink" title="ceph Vol 45 Issue 2"></a>ceph Vol 45 Issue 2</h2><h3 id="CephFS-No-space-left-on-device"><a href="#CephFS-No-space-left-on-device" class="headerlink" title="CephFS: No space left on device"></a>CephFS: No space left on device</h3><blockquote><p>After upgrading to 10.2.3 we frequently see messages like</p></blockquote><!--break--><blockquote><p>‘rm: cannot remove ‘…’: No space left on device</p></blockquote><blockquote><p>The folders we are trying to delete contain approx. 50K files 193 KB each.</p></blockquote><blockquote><p>The cluster state and storage available are both OK:</p></blockquote><blockquote><p>   cluster 98d72518-6619-4b5c-b148-9a781ef13bcb<br>     health HEALTH_WARN<br>            mds0: Client XXX.XXX.XXX.XXX failing to respond to cache pressure<br>            mds0: Client XXX.XXX.XXX.XXX failing to respond to cache pressure<br>            mds0: Client XXX.XXX.XXX.XXX failing to respond to cache pressure<br>            mds0: Client XXX.XXX.XXX.XXX failing to respond to cache pressure<br>            mds0: Client XXX.XXX.XXX.XXX failing to respond to cache pressure<br>     monmap e1: 1 mons at {000-s-ragnarok&#x3D;XXX.XXX.XXX.XXX:6789&#x2F;0}<br>            election epoch 11, quorum 0 000-s-ragnarok<br>      fsmap e62643: 1&#x2F;1&#x2F;1 up {0&#x3D;000-s-ragnarok&#x3D;up:active}<br>     osdmap e20203: 16 osds: 16 up, 16 in<br>            flags sortbitwise<br>      pgmap v15284654: 1088 pgs, 2 pools, 11263 GB data, 40801 kobjects<br>            23048 GB used, 6745 GB &#x2F; 29793 GB avail<br>                1085 active+clean<br>                   2 active+clean+scrubbing<br>                   1 active+clean+scrubbing+deep</p></blockquote><blockquote><p>Has anybody experienced this issue so far?</p></blockquote><p>这个问题是作者在升级了一个集群以后（jewel 10.2.3），做删除的时候，发现提示了 No space left on device，按正常的理解做删除不会出现提示空间不足</p><p>这个地方的原因是，有一个参数会对目录的entry做一个最大值的控制 mds_bal_fragment_size_max ,而这个参数实际上在做删除操作的时候，当文件被unlink的时候，被放入待删除区的时候，这个也是被限制住的，所以需要调整这个参数，如果有上百万的文件被等待删除的时候，可能就会出现这个情况,并且出现  failing to respond to cache pressure 我们根据自己的需要去设置这个值</p><p>默认的 mds_bal_fragment_size_max&#x3D;100000，也就是单个目录10万文件，如果不调整，单目录写入10万文件就能出现上面的问题，根据需要调大这个值</p><p>这个地方可以用命令来监控mds的当前状态</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 mnt]# ceph daemonperf mds.lab8106<br>-----mds------ --mds_server-- ---objecter--- -----mds_cache----- ---mds_log---- <br>rlat inos caps|hsr  hcs  hcr |writ read actv|recd recy stry purg|segs evts subm|<br>  0  163k   5 |  0    0    0 |  0    0   36 |  0    0  145k   0 | 33   29k   0 <br>  0  163k   5 |  0    0    0 |  6    0   34 |  0    0  145k   6 | 33   29k   6 <br>  0  163k   5 |  0    0    0 | 24    0   32 |  0    0  145k  24 | 32   29k  24 <br>  0  163k   5 |  0    0    0 | 42    0   32 |  0    0  145k  42 | 32   29k  42 <br>  0  159k   5 |  0    0    0 |972    0   32 |  0    0  144k 970 | 33   27k 971 <br>  0  159k   5 |  0    0    0 |905    0   32 |  0    0  143k 905 | 31   28k 906 <br>  0  159k   5 |  0    0    0 |969    0   32 |  0    0  142k 969 | 32   29k 970 <br>  0  159k   5 |  0    0    0 |601    0   31 |  0    0  141k 601 | 33   29k 602<br></code></pre></td></tr></table></figure><p>这个地方还有一个硬链接删除以后没有释放stry的问题，最新版的master里面已经合进去了代码（<a href="https://github.com/ukernel/ceph/commit/edc84d905a1f0e3c504f427cc4693c7a98561e7c">scan_link</a>）</p><p>修复过程如下<br>执行flush MDS journal</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">ceph daemon mds.xxx flush journal <br></code></pre></td></tr></table></figure><p>停止掉所有mds</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">stop all mds<br></code></pre></td></tr></table></figure><p>执行</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">cephfs-data-scan scan_links<br></code></pre></td></tr></table></figure><p>重启mds</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">restart mds<br></code></pre></td></tr></table></figure><p>执行命令</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">ceph daemon mds.x scrub_path / recursive repair<br></code></pre></td></tr></table></figure><p>执行完了以后去对目录进行一次ll，可以看到mds_cache的stry的就会被清理干净了</p><p>这个问题就可以解决了,实际测试中在换了新版本以后，重启后然后进行目录的ll，也能清空stry</p>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>zabbix自动发现的python方式数据生成</title>
    <link href="/2016/10/28/zabbix%E8%87%AA%E5%8A%A8%E5%8F%91%E7%8E%B0%E7%9A%84python%E6%96%B9%E5%BC%8F%E6%95%B0%E6%8D%AE%E7%94%9F%E6%88%90/"/>
    <url>/2016/10/28/zabbix%E8%87%AA%E5%8A%A8%E5%8F%91%E7%8E%B0%E7%9A%84python%E6%96%B9%E5%BC%8F%E6%95%B0%E6%8D%AE%E7%94%9F%E6%88%90/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>zabbix里面有个功能是自动发现，比如文件系统和网卡的获取的时候，因为预先无法知道这个网卡的名称，所以就有了这个自动发现的功能，这里我是因为要用到存储池的自动发现，所以需要对数据进行生成</p><!--break--><h2 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h2><p>我们看下原生的接口的数据类型：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]# zabbix_get -s 127.0.0.1 -k &quot;net.if.discovery&quot;<br>&#123;&quot;data&quot;:[&#123;&quot;&#123;#IFNAME&#125;&quot;:&quot;enp3s0&quot;&#125;,&#123;&quot;&#123;#IFNAME&#125;&quot;:&quot;virbr0-nic&quot;&#125;,&#123;&quot;&#123;#IFNAME&#125;&quot;:&quot;docker0&quot;&#125;,&#123;&quot;&#123;#IFNAME&#125;&quot;:&quot;enp4s0&quot;&#125;,&#123;&quot;&#123;#IFNAME&#125;&quot;:&quot;enp2s0f0&quot;&#125;,&#123;&quot;&#123;#IFNAME&#125;&quot;:&quot;enp2s0f1&quot;&#125;,&#123;&quot;&#123;#IFNAME&#125;&quot;:&quot;virbr0&quot;&#125;,&#123;&quot;&#123;#IFNAME&#125;&quot;:&quot;lo&quot;&#125;]&#125;<br></code></pre></td></tr></table></figure><p>数据为格式化好了的json数据，这个地方弄了好半天，因为网上很多人是用字符串拼接的方式，实际这个是字典嵌套了列表，列表又嵌套了字典，就是后面的地方开始没弄懂怎么有大括号的</p><p>我们同样的来看看ceph原生的命令的json接口</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]# ceph -s -f json<br><br>&#123;&quot;health&quot;:&#123;&quot;health&quot;:&#123;&quot;health_services&quot;:[&#123;&quot;mons&quot;:[&#123;&quot;name&quot;:&quot;lab8106&quot;,&quot;kb_total&quot;:52403200,&quot;kb_used&quot;:32905432,&quot;kb_avail&quot;:19497768,&quot;avail_percent&quot;:37,&quot;last_updated&quot;:&quot;2016-10-28 01:15:29.431854&quot;,&quot;store_stats&quot;&#123;&quot;bytes_total&quot;:20206814,&quot;bytes_sst&quot;:16929998,&quot;bytes_log&quot;:3080192,&quot;bytes_misc&quot;:196624,&quot;last_updated&quot;:&quot;0.000000&quot;&#125;,&quot;health&quot;:&quot;HEALTH_OK&quot;&#125;]&#125;]&#125;,&quot;timechecks&quot;:&#123;&quot;epoch&quot;:4,&quot;round&quot;:0,&quot;round_status&quot;:&quot;finished&quot;&#125;,&quot;summary&quot;:[],&quot;overall_status&quot;:&quot;HEALTH_OK&quot;,&quot;detail&quot;:[]&#125;,&quot;fsid&quot;:&quot;fae7a8db-c671-4b45-a784-ddb41e633905&quot;,&quot;election_epoch&quot;:4,&quot;quorum&quot;:[0],&quot;quorum_names&quot;:[&quot;lab8106&quot;],&quot;monmap&quot;:&#123;&quot;epoch&quot;:1,&quot;fsid&quot;:&quot;fae7a8db-c671-4b45-a784-ddb41e633905&quot;,&quot;modified&quot;:&quot;2016-10-19 22:26:28.879232&quot;,&quot;created&quot;:&quot;2016-10-19 22:26:28.879232&quot;,&quot;mons&quot;:[&#123;&quot;rank&quot;:0,&quot;name&quot;:&quot;lab8106&quot;,&quot;addr&quot;:&quot;192.168.8.106:6789\/0&quot;&#125;]&#125;,&quot;osdmap&quot;:&#123;&quot;osdmap&quot;:&#123;&quot;epoch&quot;:63,&quot;num_osds&quot;:2,&quot;num_up_osds&quot;:2,&quot;num_in_osds&quot;:2,&quot;full&quot;:false,&quot;nearfull&quot;:false,&quot;num_remapped_pgs&quot;:0&#125;&#125;,&quot;pgmap&quot;:&#123;&quot;pgs_by_state&quot;:[&#123;&quot;state_name&quot;:&quot;active+clean&quot;,&quot;count&quot;:80&#125;],&quot;version&quot;:19174,&quot;num_pgs&quot;:80,&quot;data_bytes&quot;:45848191333,&quot;bytes_used&quot;:45966077952,&quot;bytes_avail&quot;:551592390656,&quot;bytes_total&quot;:597558468608&#125;,&quot;fsmap&quot;:&#123;&quot;epoch&quot;:5,&quot;id&quot;:1,&quot;up&quot;:1,&quot;in&quot;:1,&quot;max&quot;:1,&quot;by_rank&quot;:[&#123;&quot;filesystem_id&quot;:1,&quot;rank&quot;:0,&quot;name&quot;:&quot;lab8106&quot;,&quot;status&quot;:&quot;up:active&quot;&#125;]&#125;&#125;<br><br></code></pre></td></tr></table></figure><p>同样也是这个类型的数据，好了，这里直接上代码：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs raw">def get_cluster_pools():<br>    try:<br>        pool_list=[]<br>        data_dic = &#123;&#125;<br>        cluster_pools = commands.getoutput(&#x27;timeout 10 ceph osd pool ls -f json 2&gt;/dev/null&#x27;)<br>        json_str = json.loads(cluster_pools)<br>        for item in json_str:<br>            pool_dic = &#123;&#125;<br>            pool_dic[&#x27;&#123;#POOLNAME&#125;&#x27;] = str(item)<br>            pool_list.append(pool_dic)<br>        data_dic[&#x27;data&#x27;] = pool_list<br>        return json.dumps(data_dic,separators=(&#x27;,&#x27;, &#x27;:&#x27;))<br>    except:<br>        return 0<br></code></pre></td></tr></table></figure><p>输出如下</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">&#123;&quot;data&quot;:[&#123;&quot;&#123;#POOLNAME&#125;&quot;:&quot;rbd&quot;&#125;,&#123;&quot;&#123;#POOLNAME&#125;&quot;:&quot;metedata&quot;&#125;,&#123;&quot;&#123;#POOLNAME&#125;&quot;:&quot;data&quot;&#125;]&#125;<br></code></pre></td></tr></table></figure><p>跟上面的格式一样了，关键在对字典进行赋值的处理，然后进行一个空格处理就完成了</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>还是接触的太少，造成简单的处理都需要花费比较久的时间</p><h2 id="变更记录"><a href="#变更记录" class="headerlink" title="变更记录"></a>变更记录</h2><table><thead><tr><th align="center">Why</th><th align="center">Who</th><th align="center">When</th></tr></thead><tbody><tr><td align="center">创建</td><td align="center">武汉-运维-磨渣</td><td align="center">2016-10-28</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>zabbix实现自定义自动发现的流程</title>
    <link href="/2016/10/28/zabbix%E5%AE%9E%E7%8E%B0%E8%87%AA%E5%AE%9A%E4%B9%89%E8%87%AA%E5%8A%A8%E5%8F%91%E7%8E%B0%E7%9A%84%E6%B5%81%E7%A8%8B/"/>
    <url>/2016/10/28/zabbix%E5%AE%9E%E7%8E%B0%E8%87%AA%E5%AE%9A%E4%B9%89%E8%87%AA%E5%8A%A8%E5%8F%91%E7%8E%B0%E7%9A%84%E6%B5%81%E7%A8%8B/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>本章介绍如何去自定义一个zabbix自动发现的整个流程</p><h2 id="过程"><a href="#过程" class="headerlink" title="过程"></a>过程</h2><p>首先需要在模板当中创建一个自动发现的规则，这个地方只需要一个名称和一个键值，例如</p><ul><li>名称：Ceph Cluster Pool Discovery</li><li>键值：ceph.pools<!--break-->过滤器中间要添加你需要的用到的值宏<br>我的数据是：</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]# zabbix_get -s 127.0.0.1 -k ceph.pools<br>&#123;&quot;data&quot;:[&#123;&quot;&#123;#POOLNAME&#125;&quot;:&quot;rbd&quot;&#125;,&#123;&quot;&#123;#POOLNAME&#125;&quot;:&quot;metedata&quot;&#125;,&#123;&quot;&#123;#POOLNAME&#125;&quot;:&quot;data&quot;&#125;]&#125;<br></code></pre></td></tr></table></figure><p>这里我的宏就是 </p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raws">&#123;#POOLNAME&#125;<br></code></pre></td></tr></table></figure><p>然后要创建一个监控项原型：<br>也是一个名称和一个键值：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs raw">- 名称：test on $1<br>- 键值：ceph.pools.used[&#123;#POOLNAME&#125;]<br></code></pre></td></tr></table></figure><p>这个地方名称可以用参数形式，包含的就是下面的那个键值中的参数对应的位置的值</p><p>然后需要去写一个这样的键值的收集</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs raw">UserParameter=ceph.pools, python /sbin/ceph-status.py pools<br>UserParameter=ceph.pools.used[*], python /sbin/ceph-status.py pool_used $1<br></code></pre></td></tr></table></figure><p>测试下效果：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]# zabbix_get -s 127.0.0.1 -k ceph.pools.used[&quot;rbd&quot;]<br>888<br></code></pre></td></tr></table></figure><p>这个地方，上面的是去做的自动发现，下面的键值就是根据上面的自动发现返回的值去作为一个参数进行新的查询，后面的$1就是将参数传到收集的脚本里面去的，这样就能根据自动发现的不同的名称返回来不同的值，从而添加不同的监控项目，而不需要自己一个个添加了</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>自动发现实际上就是需要首先去获得需要监控的值，然后将这个值作为一个新的参数传递到另外一个收集数据的item里面去，这样就可以了，这里是做的最基本的单项数据的获取，后面应该会遇到多重变量的情况，就需要赋值多重变量，到时需要用到再记录下</p><h2 id="变更记录"><a href="#变更记录" class="headerlink" title="变更记录"></a>变更记录</h2><table><thead><tr><th align="center">Why</th><th align="center">Who</th><th align="center">When</th></tr></thead><tbody><tr><td align="center">创建</td><td align="center">武汉-运维-磨渣</td><td align="center">2016-10-28</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Docker与Ceph的分与合</title>
    <link href="/2016/10/19/Docker%E4%B8%8ECeph%E7%9A%84%E5%88%86%E4%B8%8E%E5%90%88/"/>
    <url>/2016/10/19/Docker%E4%B8%8ECeph%E7%9A%84%E5%88%86%E4%B8%8E%E5%90%88/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>docker是一个管理工具，在操作系统之上提供了一个新的独立轻环境，好处是本地提供了一个基础镜像，然后基于镜像再运行环境，也可以把环境重新打包为镜像，管理起来类似于git，感觉非常的方便，并且能够做到一处提交，处处可以取到相同的环境，大大的减少了因为环境偏差造成的系统不稳定</p><p>目前有不少生成环境已经把ceph和docker结合在一起运行了，这个有的是确实能够理解docker的好处，也能够有技术力量去进行维护，这个地方相当于两套系统了，并且关于技术的传递也增加了难度，特别是一套系统是docker+ceph的环境，并且又出现相关人员离职的情况，新来的人如果不是技术很熟，之前的技术文档没有记录很全的话，再去运维这一套系统还是比较有难度的</p><p>本篇目的是记录一下docker与ceph的结合的方式，关于ceph和docker的分与合，只有做到能剥离的系统，才不会因为技术原因受限</p><h2 id="实践"><a href="#实践" class="headerlink" title="实践"></a>实践</h2><h3 id="配置docker的基础环境"><a href="#配置docker的基础环境" class="headerlink" title="配置docker的基础环境"></a>配置docker的基础环境</h3><p>拉取基础镜像<br>这个是拉取的灵雀云的docker仓库的centos</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">docker pull index.alauda.cn/library/centos<br></code></pre></td></tr></table></figure><p>启动docker进程,并且设置自启动</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs raw">systemctl start docker<br>systemctl enable docker<br></code></pre></td></tr></table></figure><p>查询当前机器上面的镜像</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]# docker images<br>REPOSITORY                       TAG                 IMAGE ID            CREATED             SIZE<br>index.alauda.cn/library/centos   latest              904d6c400333        4 months ago        196.7 MB<br></code></pre></td></tr></table></figure><p>我们先对我们的镜像做一些基本的设置</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs raw">docker run -itd --name=cephbase --net=host --pid=host index.alauda.cn/library/centos /bin/bash<br>[root@lab8106 ~]# docker attach cephbase<br><br>[root@lab8106 /]# df -h<br>Filesystem                                                                                     Size  Used Avail Use% Mounted on<br>/dev/mapper/docker-8:2-83216-dd340d1f6a68b6849b9500c4e6f9b7fb1901c3c0cb1ce0d7336f5104a1ef4a10   10G  240M  9.8G   3% /<br>tmpfs                                                                                           24G     0   24G   0% /dev<br>tmpfs                                                                                           24G     0   24G   0% /sys/fs/cgroup<br>/dev/sda2                                                                                       50G   31G   20G  62% /etc/hosts<br>shm   <br></code></pre></td></tr></table></figure><p>可以看到我们已经进入了容器内部了，下面需要做的事情，就是将ceph运行需要的一些软件装上去</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 /]# yum makecache<br>[root@lab8106 /]# yum install wget --nogpgcheck<br>[root@lab8106 /]# rm -rf /etc/yum.repos.d/*.repo<br>[root@lab8106 /]# wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo<br>[root@lab8106 /]# wget -O /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repo<br>[root@lab8106 /]# sed -i &#x27;/aliyuncs/d&#x27; /etc/yum.repos.d/CentOS-Base.repo<br>[root@lab8106 /]# sed -i &#x27;/aliyuncs/d&#x27; /etc/yum.repos.d/epel.repo<br>[root@lab8106 /]# sed -i &#x27;s/$releasever/7.2.1511/g&#x27; /etc/yum.repos.d/CentOS-Base.repo<br>[root@lab8106 /]# vi /etc/yum.repos.d/ceph.repo<br>[root@lab8106 /]# yum makecache<br>[root@lab8106 /]# yum install ceph ceph-deploy<br></code></pre></td></tr></table></figure><p>检查软件版本装对了没</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 /]# ceph -v<br>ceph version 10.2.3 (ecc23778eb545d8dd55e2e4735b53cc93f92e65b)<br>[root@lab8106 /]# ceph-deploy --version<br>1.5.36<br></code></pre></td></tr></table></figure><p>可以退出了</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">exit<br></code></pre></td></tr></table></figure><p>查看之前的容器的ID</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]# docker ps -l<br>CONTAINER ID        IMAGE                            COMMAND             CREATED             STATUS                      PORTS               NAMES<br>48420c9955b5        index.alauda.cn/library/centos   &quot;/bin/bash&quot;         About an hour ago   Exited (0) 14 seconds ago                       cephbase<br></code></pre></td></tr></table></figure><p>将容器保存为一个新的镜像，cephbase</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]# docker commit 48420c9955b5 cephbase<br>sha256:ffe236ee2bb61d2809bf1f4c03596f83b9c0e8a6fc2eb9013a81abb25be833e9<br></code></pre></td></tr></table></figure><p>查看当前的镜像</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]# docker images<br>REPOSITORY                       TAG                 IMAGE ID            CREATED              SIZE<br>cephbase                         latest              ffe236ee2bb6        About a minute ago   1.39 GB<br></code></pre></td></tr></table></figure><p>基础镜像就完成，包括了ceph运行需要的软件</p><p>我们来创建mon的容器</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">docker run --privileged  -itd  --name=monnode --net=host  -v  /var/log/ceph:/var/log/ceph -v /var/run/ceph:/var/run/ceph -v /var/lib/ceph/:/var/lib/ceph/  -v /etc/ceph:/etc/ceph  -v /sys/fs/cgroup:/sys/fs/cgroup  ceph  /sbin/init<br></code></pre></td></tr></table></figure><p>进入到容器当中去</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]# docker exec -it monnode /bin/bash<br></code></pre></td></tr></table></figure><p>在容器当中执行</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 deploy]# ceph-deploy mon create lab8106<br></code></pre></td></tr></table></figure><p>我们来创建osd的容器</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">docker run --privileged  -itd  --name=osd0 --net=host  -v  /var/log/ceph:/var/log/ceph -v /var/run/ceph:/var/run/ceph -v /var/lib/ceph/:/var/lib/ceph/  -v /etc/ceph:/etc/ceph -v /var/lib/ceph/osd/ceph-0:/var/lib/ceph/osd/ceph-0 -v /sys/fs/cgroup:/sys/fs/cgroup  ceph  /sbin/init<br></code></pre></td></tr></table></figure><p>我们将网络映射到主机上，也就是容器和主机公用网络和主机名，然后把本地的一个数据盘的目录映射进去用于osd的部署，这里都是使用-v进行映射</p><p>这个地方因为是centos7，所以systemctl内部是无法使用的，而ceph是需要这个来控制服务的，所以需要提权，并且把入口改为&#x2F;sbin&#x2F;init</p><h2 id="回顾流程"><a href="#回顾流程" class="headerlink" title="回顾流程"></a>回顾流程</h2><ul><li>下载centos基础镜像 </li><li>修改镜像的内容并提交为新的镜像</li><li>基于新的镜像启动容器（采用host映射，目录映射，所有数据都是留在物理机）</li><li>进入容器进行ceph的部署 </li><li>进入容器启动相关进程</li></ul><p>这样ceph是运行到了docker中，即使把docker容器销毁掉，因为基于主机名和网络的配置跟宿主机是一致的，所以直接在宿主机上也是能马上启动起来的</p><h2 id="为何用容器"><a href="#为何用容器" class="headerlink" title="为何用容器"></a>为何用容器</h2><p>基于容器的技术是最近几年开始火起来的，目前的云计算还处于火热期，openstack还是显得比较重型的，很多时候我们只需要的是一个能够运行我们web服务的环境，然后容器技术就应运而生了，直接启动一个容器，就能实现，这个对于宿主机来说方便的只是启动一个进程那么简单</p><p>对于庞大复杂的服务来说，如何做到环境一致也是一直很难做到的，一排物理机，因为各种各样的原因，升级，重装系统，很难保证整套系统基础环境的一致性，而基于docker的环境就能很方便的实现这个，相当于把整个运行环境打了一个包，所有的宿主机能够很方便的统一到相同的环境，即使重装了宿主机，也能方便的用一两条命令将环境部署到统一，比如上面所说的ceph，升级了基础镜像内的软件包，然后将所有的运行进程进行一次重启，就相当于运行了一个新的环境</p><p>容器还能够做的事情就是能够很便捷的把一个复杂环境运行起来，特别对于web类的服务，一台机器上可以跑一排的对外服务，即使出了问题，也能很快的再运行起来，这个对于传统的环境来说就是很难实现的，这里讲一下calamari，这个监控系统不是很复杂，但是因为依赖的软件的问题，造成很多人无法正常运行起来，这个后面我会出一个集成好calamari的docker环境，实现一键运行</p><p>在低版本的os上能够运行高版本的服务，比如在centos6上运行centos7的docker环境</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本篇的文章的标题为docker与ceph的分与合，一套系统除了自身需要稳定性以外，系统自身最好不要受制于其他系统，需要在设计初期就能保证，各个模块都能轻松的剥离，否则很容易受制于另外一套系统，所以基于上面的方案来说，docker和ceph既是合在一起的，也是分开的,本篇只是讲了一个框架，实际部署ceph的过程当中还是有一些小问题需要具体处理的，不是很难，权限问题，目录问题</p><h2 id="变更记录"><a href="#变更记录" class="headerlink" title="变更记录"></a>变更记录</h2><table><thead><tr><th align="center">Why</th><th align="center">Who</th><th align="center">When</th></tr></thead><tbody><tr><td align="center">创建</td><td align="center">武汉-运维-磨渣</td><td align="center">2016-10-19</td></tr></tbody></table><h2 id="附录："><a href="#附录：" class="headerlink" title="附录："></a>附录：</h2><p>docker的常用操作<br>查询镜像</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]# docker images<br></code></pre></td></tr></table></figure><p>查询容器</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]# docker ps<br>[root@lab8106 ~]# docker ps -l<br></code></pre></td></tr></table></figure><p>删除容器</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]# docker rm 64f617dfada5<br></code></pre></td></tr></table></figure><p>删除镜像</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]# docker rmi node<br></code></pre></td></tr></table></figure><p>进入容器内部</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]# docker exec -it monnode /bin/bash<br></code></pre></td></tr></table></figure><p>让容器执行命令</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]# docker exec monnode uptime<br></code></pre></td></tr></table></figure><p>退出容器,不停止容器</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">ctrl+p然后ctrl+q<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Ceph中PG和PGP的区别</title>
    <link href="/2016/10/19/Ceph%E4%B8%ADPG%E5%92%8CPGP%E7%9A%84%E5%8C%BA%E5%88%AB/"/>
    <url>/2016/10/19/Ceph%E4%B8%ADPG%E5%92%8CPGP%E7%9A%84%E5%8C%BA%E5%88%AB/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>首先来一段英文关于PG和PGP区别的解释：</p><blockquote><p>PG &#x3D; Placement Group<br><br>PGP &#x3D; Placement Group for Placement purpose </p></blockquote><blockquote><p>pg_num &#x3D; number of placement groups mapped to an OSD<br><br>When pg_num is increased for any pool, every PG of this pool splits into half, but they all remain mapped to their parent OSD.<br><br>Until this time, Ceph does not start rebalancing. Now, when you increase the pgp_num value for the same pool, PGs start to migrate from the parent to some other OSD, and cluster rebalancing starts. This is how PGP plays an important role.<br>By Karan Singh </p></blockquote><p>以上是来自邮件列表的  Karan Singh 的PG和PGP的相关解释，他也是《Learning Ceph》和《Ceph Cookbook》的作者，以上的解释没有问题，我们来看下具体在集群里面具体作用</p><h2 id="实践"><a href="#实践" class="headerlink" title="实践"></a>实践</h2><p>环境准备，因为是测试环境，我只准备了两台机器，每台机器4个OSD，所以做了一些参数的设置，让数据尽量散列</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">osd_crush_chooseleaf_type = 0<br></code></pre></td></tr></table></figure><p>以上为修改的参数，这个是让我的环境故障域为OSD分组的</p><p>创建测试需要的存储池<br>我们初始情况只创建一个名为testpool包含6个PG的存储池</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ceph]# ceph osd pool create testpool 6 6<br>pool &#x27;testpool&#x27; created<br></code></pre></td></tr></table></figure><p>我们看一下默认创建完了后的PG分布情况</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ceph]# ceph pg dump pgs|grep ^1|awk &#x27;&#123;print $1,$2,$15&#125;&#x27;<br>dumped pgs in format plain<br>1.1 0 [3,6,0]<br>1.0 0 [7,0,6]<br>1.3 0 [4,1,2]<br>1.2 0 [7,4,1]<br>1.5 0 [4,6,3]<br>1.4 0 [3,0,4]<br></code></pre></td></tr></table></figure><p>我们写入一些对象，因为我们关心的不仅是pg的变动，同样关心PG内对象有没有移动,所以需要准备一些测试数据，这个调用原生rados接口写最方便</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">rados -p testpool bench 20 write --no-cleanup<br></code></pre></td></tr></table></figure><p>我们再来查询一次</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ceph]# ceph pg dump pgs|grep ^1|awk &#x27;&#123;print $1,$2,$15&#125;&#x27;<br>dumped pgs in format plain<br>1.1 75 [3,6,0]<br>1.0 83 [7,0,6]<br>1.3 144 [4,1,2]<br>1.2 146 [7,4,1]<br>1.5 86 [4,6,3]<br>1.4 80 [3,0,4]<br></code></pre></td></tr></table></figure><p>可以看到写入了一些数据，其中的第二列为这个PG当中的对象的数目，第三列为PG所在的OSD</p><h3 id="增加PG测试"><a href="#增加PG测试" class="headerlink" title="增加PG测试"></a>增加PG测试</h3><p>我们来扩大PG再看看</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ceph]# ceph osd pool set testpool pg_num 12<br>set pool 1 pg_num to 12<br></code></pre></td></tr></table></figure><p>再次查询</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ceph]# ceph pg dump pgs|grep ^1|awk &#x27;&#123;print $1,$2,$15&#125;&#x27;<br>dumped pgs in format plain<br>1.1 37 [3,6,0]<br>1.9 38 [3,6,0]<br>1.0 41 [7,0,6]<br>1.8 42 [7,0,6]<br>1.3 48 [4,1,2]<br>1.b 48 [4,1,2]<br>1.7 48 [4,1,2]<br>1.2 48 [7,4,1]<br>1.6 49 [7,4,1]<br>1.a 49 [7,4,1]<br>1.5 86 [4,6,3]<br>1.4 80 [3,0,4]<br></code></pre></td></tr></table></figure><p>可以看到上面新加上的PG的分布还是基于老的分布组合，并没有出现新的OSD组合，因为我们当前的设置是pgp为6,那么三个OSD的组合的个数就是6个，因为当前为12个pg，分布只能从6种组合里面挑选，所以会有重复的组合</p><p>根据上面的分布情况，可以确定的是，增加PG操作会引起PG内部对象分裂，分裂的份数是根据新增PG组合重复情况来的，比如上面的情况</p><ul><li>1.1的对象分成了两份[3,6,0]</li><li>1.3的对象分成了三份[4,1,2]</li><li>1.4的对象没有拆分[3,0,4]</li></ul><p>结论：增加PG会引起PG内的对象分裂，也就是在OSD上创建了新的PG目录，然后进行部分对象的move的操作</p><h3 id="增加PGP测试"><a href="#增加PGP测试" class="headerlink" title="增加PGP测试"></a>增加PGP测试</h3><p>我们将原来的PGP从6调整到12</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ceph]# ceph osd pool set testpool pgp_num 12<br>[root@lab8106 ceph]# ceph pg dump pgs|grep ^1|awk &#x27;&#123;print $1,$2,$15&#125;&#x27;<br>dumped pgs in format plain<br>1.a 49 [1,2,6]<br>1.b 48 [1,6,2]<br>1.1 37 [3,6,0]<br>1.0 41 [7,0,6]<br>1.3 48 [4,1,2]<br>1.2 48 [7,4,1]<br>1.5 86 [4,6,3]<br>1.4 80 [3,0,4]<br>1.7 48 [1,6,0]<br>1.6 49 [3,6,7]<br>1.9 38 [1,4,2]<br>1.8 42 [1,2,3]<br></code></pre></td></tr></table></figure><p>可以看到PG里面的对象并没有发生变化，而PG所在的对应关系发生了变化<br>我们看下与调整PGP前的对比</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs raw">*1.1 37 [3,6,0]          1.1 37 [3,6,0]*<br>1.9 38 [3,6,0]          1.9 38 [1,4,2]<br>*1.0 41 [7,0,6]          1.0 41 [7,0,6]*<br>1.8 42 [7,0,6]          1.8 42 [1,2,3]<br>*1.3 48 [4,1,2]          1.3 48 [4,1,2]*<br>1.b 48 [4,1,2]          1.b 48 [1,6,2]<br>1.7 48 [4,1,2]          1.7 48 [1,6,0]<br>*1.2 48 [7,4,1]          1.2 48 [7,4,1]*<br>1.6 49 [7,4,1]          1.6 49 [3,6,7]<br>1.a 49 [7,4,1]          1.a 49 [1,2,6]<br>*1.5 86 [4,6,3]          1.5 86 [4,6,3]*<br>*1.4 80 [3,0,4]          1.4 80 [3,0,4]*<br></code></pre></td></tr></table></figure><p>可以看到其中最原始的6个PG的分布并没有变化（标注了*号），变化的是后增加的PG，也就是将重复的PG分布进行新分布，这里并不是随机完全打散，而是根据需要去进行重分布</p><p>结论：调整PGP不会引起PG内的对象的分裂，但是会引起PG的分布的变动</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul><li>PG是指定存储池存储对象的目录有多少个，PGP是存储池PG的OSD分布组合个数</li><li>PG的增加会引起PG内的数据进行分裂，分裂到相同的OSD上新生成的PG当中</li><li>PGP的增加会引起部分PG的分布进行变化，但是不会引起PG内对象的变动</li></ul><h2 id="变更记录"><a href="#变更记录" class="headerlink" title="变更记录"></a>变更记录</h2><table><thead><tr><th align="center">Why</th><th align="center">Who</th><th align="center">When</th></tr></thead><tbody><tr><td align="center">创建</td><td align="center">武汉-运维-磨渣</td><td align="center">2016-10-19</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>配置Ceph的IPV6集群</title>
    <link href="/2016/10/17/%E9%85%8D%E7%BD%AECeph%E7%9A%84IPV6%E9%9B%86%E7%BE%A4/"/>
    <url>/2016/10/17/%E9%85%8D%E7%BD%AECeph%E7%9A%84IPV6%E9%9B%86%E7%BE%A4/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>对于IPV6实在是非常的陌生，所以本篇开始会讲一下最基本的网络配置，首先让网络能通起来，最开始就是因为不熟悉IPV6,而直接使用了link local地址，造成了mon部署的时候进程无法绑定到IP，从而端口没有启动，这个是在ceph社区群友 <code>ceph-长沙-柠檬</code> 同学的帮助下才发现问题的</p><!--break--><p>IPV6是会有个link local地址的，在一个接口可以配置很多IPv6地址，所以学习路由就有可能出现很多下一跳。所以出现Link Local地址唯一标识一个节点。在本地链路看到下一跳都是对端的Link Local地址。这个地址一般是以fe80开头的，子网掩码为64，这个地方需要给机器配置一个唯一的全局单播地址</p><blockquote><p>However, with IPv6, all (IPv6) interfaces will have a link local address. This address is intended to allow communications over the attached links and so is defined to be usable only on that link.</p></blockquote><h2 id="网络配置"><a href="#网络配置" class="headerlink" title="网络配置"></a>网络配置</h2><p>linux下用默认的网卡配置文件</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@node1 ceph]# ifconfig <br>eno16777736: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  mtu 1500<br>        inet 192.168.0.46  netmask 255.255.0.0  broadcast 192.168.255.255<br>        inet6 fe80::20c:29ff:fec5:5a4b  prefixlen 64  scopeid 0x20&lt;link&gt;<br>        ether 00:0c:29:c5:5a:4b  txqueuelen 1000  (Ethernet)<br>        RX packets 18422  bytes 1254119 (1.1 MiB)<br>        RX errors 0  dropped 6  overruns 0  frame 0<br>        TX packets 1938  bytes 890164 (869.3 KiB)<br>        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0<br></code></pre></td></tr></table></figure><h3 id="取消NetworkManager管理"><a href="#取消NetworkManager管理" class="headerlink" title="取消NetworkManager管理"></a>取消NetworkManager管理</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs raw">systemctl stop NetworkManager<br>systemctl disable NetworkManager<br>systemctl restart network<br></code></pre></td></tr></table></figure><p>以免NetworkManager的干扰</p><p>这个地方我没有做自定义的IPV6的设置，让其默认的生成的地方，可以看到上面的node1的link local地址地址为 fe80::20c:29ff:fec5:5a4b<br>我的另外一台的地址为 fe80::20c:29ff:feda:6849</p><blockquote><p>node1 fe80::20c:29ff:fec5:5a4b  prefixlen 64<br><br>node2 fe80::20c:29ff:feda:6849 prefixlen 64</p></blockquote><p>这个地方都是没有单播地址的，需要配置一个</p><p>配置的时候关闭掉ipv4的IP，防止影响，确认配置的就是ipv6环境，去掉IPv4的配置即可，我的网卡配置文件</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs raw">TYPE=&quot;Ethernet&quot;<br>BOOTPROTO=&quot;static&quot;<br>DEFROUTE=&quot;yes&quot;<br>PEERDNS=&quot;yes&quot;<br>PEERROUTES=&quot;yes&quot;<br>NM_CONTROLLED=no<br>IPV4_FAILURE_FATAL=&quot;no&quot;<br>IPV6INIT=&quot;yes&quot;<br>IPV6ADDR=2008:20c:20c:20c:20c:29ff:fec5:5a4b/64<br>IPV6_AUTOCONF=no<br>IPV6_DEFROUTE=&quot;yes&quot;<br>IPV6_PEERDNS=&quot;yes&quot;<br>IPV6_PEERROUTES=&quot;yes&quot;<br>IPV6_FAILURE_FATAL=&quot;no&quot;<br>NAME=&quot;eno16777736&quot;<br>UUID=&quot;0146f40c-6f4d-4c63-a9cd-7f89264613f3&quot;<br>DEVICE=&quot;eno16777736&quot;<br>ONBOOT=&quot;yes&quot;<br></code></pre></td></tr></table></figure><p>检查配置情况</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@node1 ceph]# ifconfig <br>eno16777736: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  mtu 1500<br>        inet6 fe80::20c:29ff:fec5:5a4b  prefixlen 64  scopeid 0x20&lt;link&gt;<br>        inet6 2008:20c:20c:20c:20c:29ff:fec5:5a4b  prefixlen 64  scopeid 0x0&lt;global&gt;<br>        ether 00:0c:29:c5:5a:4b  txqueuelen 1000  (Ethernet)<br>        RX packets 9133  bytes 597664 (583.6 KiB)<br>        RX errors 0  dropped 1  overruns 0  frame 0<br>        TX packets 466  bytes 137983 (134.7 KiB)<br>        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0<br></code></pre></td></tr></table></figure><p>可以看到有两个inet6这样就是对的了</p><blockquote><p>windows远程ssh连接的方式:ssh 2008:20c:20c:20c:20c:29ff:fec5:5a4b</p></blockquote><h3 id="配置hosts"><a href="#配置hosts" class="headerlink" title="配置hosts"></a>配置hosts</h3><p>在配置文件&#x2F;etc&#x2F;hosts中添加如下内容</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs raw">2008:20c:20c:20c:20c:29ff:fec5:5a4b node1<br>2008:20c:20c:20c:20c:29ff:feda:6849 node2<br></code></pre></td></tr></table></figure><p>检测是否连通</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@node1 ~]# ping6 -I eno16777736 2008:20c:20c:20c:20c:29ff:feda:6849<br></code></pre></td></tr></table></figure><p>ping主机名称</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@node1 ~]# ping6 -I eno16777736 node2<br></code></pre></td></tr></table></figure><p>注意ping6需要加上网卡名称</p><p>同样的操作在node2上也配置好，网络到这里就配置好了</p><h2 id="集群配置"><a href="#集群配置" class="headerlink" title="集群配置"></a>集群配置</h2><h3 id="创建初始配置文件"><a href="#创建初始配置文件" class="headerlink" title="创建初始配置文件"></a>创建初始配置文件</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@node1 ceph]# ceph-deploy new node1<br>[root@node1 ceph]# cat ceph.conf <br>[global]<br>fsid = f0bf4130-f4f0-4214-8b98-67103ad55d65<br>ms_bind_ipv6 = true<br>mon_initial_members = node1<br>mon_host = [2008:20c:20c:20c:20c:29ff:fec5:5a4b]<br>auth_cluster_required =cephx<br>auth_service_required = cephx<br>auth_client_required = cephx<br></code></pre></td></tr></table></figure><h3 id="创建mon"><a href="#创建mon" class="headerlink" title="创建mon"></a>创建mon</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@node1 ceph]# ceph-deploy mon create node1<br></code></pre></td></tr></table></figure><h3 id="检查状态"><a href="#检查状态" class="headerlink" title="检查状态"></a>检查状态</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@node1 ceph]# ceph -s<br>    cluster d2882f75-1209-4667-bef8-3051c84cb83c<br>     health HEALTH_ERR<br>            no osds<br>     monmap e1: 1 mons at &#123;node1=[2008:20c:20c:20c:20c:29ff:fec5:5a4b]:6789/0&#125;<br>            election epoch 3, quorum 0 node1<br>     osdmap e8: 0 osds: 0 up, 0 in<br>            flags sortbitwise<br>      pgmap v2664: 0 pgs, 0 pools, 0 bytes data, 0 objects<br>            0 kB used, 0 kB / 0 kB avail<br></code></pre></td></tr></table></figure><h3 id="检查端口"><a href="#检查端口" class="headerlink" title="检查端口"></a>检查端口</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@node1 ceph]# netstat -tunlp|grep tcp6<br>tcp6       0      0 :::22                   :::*                    LISTEN      1155/sshd           <br>tcp6       0      0 ::1:25                  :::*                    LISTEN      1294/master         <br>tcp6       0      0 2008:20c:20c:20c:2:6789 :::*                    LISTEN      8997/ceph-mon<br></code></pre></td></tr></table></figure><p>可以看到集群已经正确的监听在了ipv6上了，后续的操作跟普通的IPV4集群一样的</p><h2 id="变更记录"><a href="#变更记录" class="headerlink" title="变更记录"></a>变更记录</h2><table><thead><tr><th align="center">Why</th><th align="center">Who</th><th align="center">When</th></tr></thead><tbody><tr><td align="center">创建</td><td align="center">武汉-运维-磨渣</td><td align="center">2016-10-17</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Ceph的参数mon_osd_down_out_subtree_limit细解</title>
    <link href="/2016/10/13/Ceph%E7%9A%84%E5%8F%82%E6%95%B0mon_osd_down_out_subtree_limit%E7%BB%86%E8%A7%A3/"/>
    <url>/2016/10/13/Ceph%E7%9A%84%E5%8F%82%E6%95%B0mon_osd_down_out_subtree_limit%E7%BB%86%E8%A7%A3/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>之前跟一个朋友沟通一个其他的问题的时候，发现了有一个参数 mon osd down out subtree limit 一直没有接触到，看了一下这个参数还是很有作用的，本篇将讲述这个参数的作用和使用的场景</p><h2 id="测试环境准备"><a href="#测试环境准备" class="headerlink" title="测试环境准备"></a>测试环境准备</h2><p>首先配置一个集群环境，配置基本参数</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">mon_osd_down_out_interval = 20<br></code></pre></td></tr></table></figure><p>调整这个参数为20s,默认为300s,默认一个osd,down超过300s就会标记为out，然后触发迁移,这个是为了方便尽快看到测试的效果，很多测试都是可以这样缩短测试周期的</p><p>本次测试关心的是这个参数 mon osd down out subtree limit<br>参数，那么这个参数做什么用的，我们来看看</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ceph]# ceph --show-config|grep mon_osd_down_out_subtree_limit<br>mon_osd_down_out_subtree_limit = rack<br></code></pre></td></tr></table></figure><p>首先解释下这个参数是做什么的，这个是控制标记为out的最小子树(bucket)，默认的这个为rack，这个可能我们平时感知不到这个有什么作用，大部分情况下，我们一般都为主机分组或者做了故障域，也很少做到测试去触发它，本篇文章将告诉你这个参数在什么情况下生效，对我们又有什么作用</p><p>准备两个物理节点，每个节点上3个osd，一共六个osd，上面的down out的时间已经修改为20s，那么会在20s后出现out的情况</p><h2 id="测试过程"><a href="#测试过程" class="headerlink" title="测试过程"></a>测试过程</h2><h3 id="测试默认参数停止一台主机单个OSD"><a href="#测试默认参数停止一台主机单个OSD" class="headerlink" title="测试默认参数停止一台主机单个OSD"></a>测试默认参数停止一台主机单个OSD</h3><p>首先用默认的<code>mon_osd_down_out_subtree_limit = rack</code>去做测试<br>开启几个监控终端方便观察</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs raw">ceph -w<br>watch ceph osd tree<br></code></pre></td></tr></table></figure><p>在其中的一台上执行</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">systemctl stop ceph-osd@5<br></code></pre></td></tr></table></figure><p>测试输出</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs raw">2016-10-13 10:15:39.673898 mon.0 [INF] osd.5 out (down for 20.253201)<br>2016-10-13 10:15:39.757399 mon.0 [INF] osdmap e60: 6 osds: 5 up, 5 in<br></code></pre></td></tr></table></figure><p>停止一个后正常out</p><h3 id="测试默认参数停止掉一台主机所有osd"><a href="#测试默认参数停止掉一台主机所有osd" class="headerlink" title="测试默认参数停止掉一台主机所有osd"></a>测试默认参数停止掉一台主机所有osd</h3><p>我们再来停止一台主机所有osd</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">systemctl stop ceph-osd.target<br></code></pre></td></tr></table></figure><p>测试输出</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs raw">2016-10-13 10:17:09.699129 mon.0 [INF] osd.3 out (down for 23.966959)<br>2016-10-13 10:17:09.699178 mon.0 [INF] osd.4 out (down for 23.966958)<br>2016-10-13 10:17:09.699222 mon.0 [INF] osd.5 out (down for 23.966958)<br></code></pre></td></tr></table></figure><p>可以看到这台主机上的节点全部都正常out了</p><h3 id="测试修改参数后停止一台主机单个OSD"><a href="#测试修改参数后停止一台主机单个OSD" class="headerlink" title="测试修改参数后停止一台主机单个OSD"></a>测试修改参数后停止一台主机单个OSD</h3><p>我们再调整下参数</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">mon_osd_down_out_subtree_limit = rack<br></code></pre></td></tr></table></figure><p>将这个参数设置为host</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">mon_osd_down_out_subtree_limit = host<br></code></pre></td></tr></table></figure><p>重启所有的进程，让配置生效，我们测试下只断一个osd的时候能不能out</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">systemctl stop ceph-osd@5<br></code></pre></td></tr></table></figure><p>停止掉osd.5<br>测试输出</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">2016-10-13 10:48:45.612206 mon.0 [INF] osd.5 out (down for 21.966238)<br></code></pre></td></tr></table></figure><p>可以看到可以osd.5可以正常的out</p><h3 id="测试修改参数后停止一台主机所有OSD"><a href="#测试修改参数后停止一台主机所有OSD" class="headerlink" title="测试修改参数后停止一台主机所有OSD"></a>测试修改参数后停止一台主机所有OSD</h3><p>我们再来停止lab8107的所有的osd</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">systemctl stop ceph-osd.target<br></code></pre></td></tr></table></figure><p>停止掉 lab8107 所有的osd,可以看到没有out了,这个是因为把故障out设置为host级别了，这个地方出现host级别故障的时候，就不进行迁移了</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>关键的地方在于总结了，首先我们要想一想，ceph机器的迁移开不开（noout），关于这个问题，一定有两个答案</p><ul><li>开，不开的话，盘再坏怎么办，就会丢数据了</li><li>不开，人工触发，默认的情况下迁移数据会影响前端业务</li></ul><p>这里这个参数其实就是将我们的问题更加细腻的控制了，我们现在根据这个参数就能做到，迁移可以开，坏掉一个盘的时候我让它迁移，一个盘的数据恢复影响和时间是可以接受的，主机损坏我不让他迁移，为什么？主机损坏你去让他迁移，首先会生成一份数据，等主机好了，数据又要删除一份数据，这个对于磁盘都是消耗，主机级别的故障一定是可修复的，这个地方主机down机，主机电源损坏，这部分数据都是在的，那么这个地方就是需要人工去做这个修复的工作的，对于前端的服务是透明的，默认的控制是down rack才不去标记out，这个当然你也可以控制为这个，比如有个rack掉电，就不做恢复，如果down了两台主机，让他去做恢复，当然个人不建议这么做，这个控制就是自己去判断这个地方需要做不</p><p>ceph里面还是提供了一些细微粒度的控制，值得去与实际的应用场景结合，当然默认的参数已经能应付大部分的场景，控制的更细只是让其变得更好</p><h2 id="变更记录"><a href="#变更记录" class="headerlink" title="变更记录"></a>变更记录</h2><table><thead><tr><th align="center">Why</th><th align="center">Who</th><th align="center">When</th></tr></thead><tbody><tr><td align="center">创建</td><td align="center">武汉-运维-磨渣</td><td align="center">2016-10-13</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>创建一个自定义名称的Ceph集群</title>
    <link href="/2016/10/12/%E5%88%9B%E5%BB%BA%E4%B8%80%E4%B8%AA%E8%87%AA%E5%AE%9A%E4%B9%89%E5%90%8D%E7%A7%B0%E7%9A%84Ceph%E9%9B%86%E7%BE%A4/"/>
    <url>/2016/10/12/%E5%88%9B%E5%BB%BA%E4%B8%80%E4%B8%AA%E8%87%AA%E5%AE%9A%E4%B9%89%E5%90%8D%E7%A7%B0%E7%9A%84Ceph%E9%9B%86%E7%BE%A4/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>这里有个条件，系统环境是Centos 7 ,Ceph 的版本为Jewel版本，因为这个组合下是由systemctl来进行服务控制的，所以需要做稍微的改动即可实现</p><!--break--><h2 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h2><p>部署mon的时候需要修改这个几个文件</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs raw">/usr/lib/systemd/system/ceph-mon@.service<br>/usr/lib/systemd/system/ceph-create-keys@.service<br>/usr/lib/systemd/system/ceph-osd@.service<br>/usr/lib/systemd/system/ceph-mds@.service<br></code></pre></td></tr></table></figure><p>将 Environment&#x3D;CLUSTER&#x3D;ceph 改成 Environment&#x3D;CLUSTER&#x3D;myceph 后面的myceph可以为你自定义的名称</p><h2 id="简单的创建过程"><a href="#简单的创建过程" class="headerlink" title="简单的创建过程"></a>简单的创建过程</h2><p>创建mon</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">ceph-deploy  --cluster myceph mon create lab8107<br></code></pre></td></tr></table></figure><p>获取部署密钥</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">ceph-deploy  --cluster myceph gatherkeys lab8107<br></code></pre></td></tr></table></figure><p>部署osd</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs raw">ceph-deploy  --cluster myceph osd prepare lab8107:/dev/sdb<br>ceph-deploy  --cluster myceph osd activate lab8107:/dev/sdb1<br></code></pre></td></tr></table></figure><p>查询集群状态</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">ceph --cluster myceph -s<br></code></pre></td></tr></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>最简单的修改名称主要步骤就在这里了，关键部分就是修改那几个文件里面的集群的名称，这个里面是用一个变量写成了ceph，根据自己的需要进行修改即可</p><h2 id="变更记录"><a href="#变更记录" class="headerlink" title="变更记录"></a>变更记录</h2><table><thead><tr><th align="center">Why</th><th align="center">Who</th><th align="center">When</th></tr></thead><tbody><tr><td align="center">创建</td><td align="center">武汉-运维-磨渣</td><td align="center">2016-10-12</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>编译一个支持多线程的php安装包</title>
    <link href="/2016/10/10/%E7%BC%96%E8%AF%91%E4%B8%80%E4%B8%AA%E6%94%AF%E6%8C%81%E5%A4%9A%E7%BA%BF%E7%A8%8B%E7%9A%84php%E5%AE%89%E8%A3%85%E5%8C%85/"/>
    <url>/2016/10/10/%E7%BC%96%E8%AF%91%E4%B8%80%E4%B8%AA%E6%94%AF%E6%8C%81%E5%A4%9A%E7%BA%BF%E7%A8%8B%E7%9A%84php%E5%AE%89%E8%A3%85%E5%8C%85/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>因为项目上的需要，需要用到php，一般来说，用默认的版本和配置就可以满足大多数的场景，因为需要加入多线程，所以需要自己编译一个包</p><p>一般来说，发行的包的版本的配置选项和代码都是最稳定的，所以在大多数情况下，我都不会直接去拿原始的源码做编译，这里我的经验是用别人发布版本的源码包，然后根据自己的需要，做修改，然后打包，这次的处理方法还是一样</p><h2 id="获取源码"><a href="#获取源码" class="headerlink" title="获取源码"></a>获取源码</h2><p>地址：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">https://uk.repo.webtatic.com/yum/el7/SRPMS/RPMS/<br></code></pre></td></tr></table></figure><p>这个是webtatic发行的php版本，做了一些修改和优化</p><p>选择需要的版本</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 myphp]# wget https://uk.repo.webtatic.com/yum/el7/SRPMS/RPMS/php56w-5.6.26-1.w7.src.rpm<br></code></pre></td></tr></table></figure><p>解压安装包</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 myphp]# rpm2cpio php56w-5.6.26-1.w7.src.rpm |cpio -div<br></code></pre></td></tr></table></figure><p>解压完成了后，当前目录下面会有很多文件<br>修改当前目录下面的php56.spec<br>在编译相关的configure后面增加</p><blockquote><p>–enable-maintainer-zts</p></blockquote><p>拷贝解压和修改的文件到源码编译目录</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 myphp]# cp -ra * /root/rpmbuild/SOURCES/<br></code></pre></td></tr></table></figure><h2 id="编译rpm包"><a href="#编译rpm包" class="headerlink" title="编译rpm包"></a>编译rpm包</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 myphp]# rpmbuild -bb php56.spec<br></code></pre></td></tr></table></figure><p>如果提示缺依赖，就把相关的依赖包安装好就可以了，编译环境最好跟最终使用环境是一样的环境，执行完成了以后，会生成rpm安装包</p><h2 id="增加多线程支持"><a href="#增加多线程支持" class="headerlink" title="增加多线程支持"></a>增加多线程支持</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">pecl install pthreads-2.0.9<br></code></pre></td></tr></table></figure><p>这个会下载源码，然后自动编译成可用的内核模块，将这个内核模块的配置文件和模块文件拷贝到最终使用环境即可</p><p>检查是否安装成功</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ceph]# php -m|grep pth<br>pthreads<br></code></pre></td></tr></table></figure><p>可用看到已经支持了</p><h2 id="变更记录"><a href="#变更记录" class="headerlink" title="变更记录"></a>变更记录</h2><table><thead><tr><th align="center">Why</th><th align="center">Who</th><th align="center">When</th></tr></thead><tbody><tr><td align="center">创建</td><td align="center">武汉-运维-磨渣</td><td align="center">2016-10-10</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Python生成csv中文乱码解决办法</title>
    <link href="/2016/09/28/Python%E7%94%9F%E6%88%90csv%E4%B8%AD%E6%96%87%E4%B9%B1%E7%A0%81%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95/"/>
    <url>/2016/09/28/Python%E7%94%9F%E6%88%90csv%E4%B8%AD%E6%96%87%E4%B9%B1%E7%A0%81%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>在Linux下面用python进行数据处理，然后输出为csv格式，如果没有中文一切正常，但是如果有中文，就会出现乱码的问题,本篇将讲述怎么处理这个问题</p><h2 id="处理过程"><a href="#处理过程" class="headerlink" title="处理过程"></a>处理过程</h2><h3 id="原始代码"><a href="#原始代码" class="headerlink" title="原始代码"></a>原始代码</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs raw">#!/usr/bin/env python<br># -*- coding: UTF-8 -*-<br>import csv<br>#import codecs<br>with open(&#x27;test.csv&#x27;, &#x27;wb&#x27;) as csvfile:<br>#    csvfile.write(codecs.BOM_UTF8)<br>    spamwriter = csv.writer(csvfile, dialect=&#x27;excel&#x27;)<br>    spamwriter.writerow([&#x27;测试&#x27;] * 5 + [&#x27;Baked Beans&#x27;])<br>    spamwriter.writerow([&#x27;Spam&#x27;, &#x27;Lovely Spam&#x27;, &#x27;Wonderful Spam&#x27;])<br></code></pre></td></tr></table></figure><p>运行以后：<br>Linux下的效果</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]# cat test.csv <br>测试,测试,测试,测试,测试,Baked Beans<br>Spam,Lovely Spam,Wonderful Spam<br></code></pre></td></tr></table></figure><p>Windows下打开的效果<br><img src="/images/blog/o_200901065125image_1atnnp5i41b7lf7tumgj6175k9.png" alt="image_1atnnp5i41b7lf7tumgj6175k9.png-4.3kB"></p><h3 id="修改代码"><a href="#修改代码" class="headerlink" title="修改代码"></a>修改代码</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs raw">#!/usr/bin/env python<br># -*- coding: UTF-8 -*-<br>import csv<br>import codecs<br>with open(&#x27;test.csv&#x27;, &#x27;wb&#x27;) as csvfile:<br>    csvfile.write(codecs.BOM_UTF8)<br>    spamwriter = csv.writer(csvfile, dialect=&#x27;excel&#x27;)<br>    spamwriter.writerow([&#x27;测试&#x27;] * 5 + [&#x27;Baked Beans&#x27;])<br>    spamwriter.writerow([&#x27;Spam&#x27;, &#x27;Lovely Spam&#x27;, &#x27;Wonderful Spam&#x27;])<br></code></pre></td></tr></table></figure><p>跟上面的代码相比，引入了两行代码</p><blockquote><p>import codecs<br><br>csvfile.write(codecs.BOM_UTF8)</p></blockquote><p>我们再来看效果Linux下的效果</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]# cat test.csv <br>测试,测试,测试,测试,测试,Baked Beans<br>Spam,Lovely Spam,Wonderful Spam<br></code></pre></td></tr></table></figure><p>Windows下打开的效果<br><img src="/images/blog/o_200901065132image_1atnnsp1713931d1h1e641l4f13kim.png" alt="image_1atnnsp1713931d1h1e641l4f13kim.png-3.5kB"><br>问题解决</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>网上找了一些资料，这个方式比较快而简单，就先用这个方式解决，方法有很多</p><h2 id="变更记录"><a href="#变更记录" class="headerlink" title="变更记录"></a>变更记录</h2><table><thead><tr><th align="center">Why</th><th align="center">Who</th><th align="center">When</th></tr></thead><tbody><tr><td align="center">创建</td><td align="center">武汉-运维-磨渣</td><td align="center">2016-09-28</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>不小心清空了Ceph的OSD的分区表如何恢复</title>
    <link href="/2016/09/24/%E4%B8%8D%E5%B0%8F%E5%BF%83%E6%B8%85%E7%A9%BA%E4%BA%86Ceph%E7%9A%84OSD%E7%9A%84%E5%88%86%E5%8C%BA%E8%A1%A8%E5%A6%82%E4%BD%95%E6%81%A2%E5%A4%8D/"/>
    <url>/2016/09/24/%E4%B8%8D%E5%B0%8F%E5%BF%83%E6%B8%85%E7%A9%BA%E4%BA%86Ceph%E7%9A%84OSD%E7%9A%84%E5%88%86%E5%8C%BA%E8%A1%A8%E5%A6%82%E4%BD%95%E6%81%A2%E5%A4%8D/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>如果你是新手，应该出现过敲盘符的时候，敲错的情况，有些操作可能没什么问题，查询类的操作都没问题，但是写入的情况，就可能比较麻烦了，当然老手也可能有误操作，本篇将讲述在误操作把分区表给弄丢了的情况，来看看我们应该如何恢复</p><h2 id="实践过程"><a href="#实践过程" class="headerlink" title="实践过程"></a>实践过程</h2><p>我们现在有一个正常的集群，我们假设这些分区都是一致的，用的是默认的分区的方式，我们先来看看默认的分区方式是怎样的</p><h3 id="破坏环境"><a href="#破坏环境" class="headerlink" title="破坏环境"></a>破坏环境</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ceph]<span class="hljs-comment"># ceph-disk  list</span><br>···<br>/dev/sdb :<br> /dev/sdb1 ceph data, active, cluster ceph, osd.0, journal /dev/sdb2<br> /dev/sdb2 ceph journal, <span class="hljs-keyword">for</span> /dev/sdb1<br>···<br></code></pre></td></tr></table></figure><p>查看分区情况</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ceph]<span class="hljs-comment"># parted -s /dev/sdb print</span><br>Model: SEAGATE ST3300657SS (scsi)<br>Disk /dev/sdb: 300GB<br>Sector size (logical/physical): 512B/512B<br>Partition Table: gpt<br>Disk Flags: <br><br>Number  Start   End     Size    File system  Name          Flags<br> 2      1049kB  1074MB  1073MB               ceph journal<br> 1      1075MB  300GB   299GB   xfs          ceph data<br></code></pre></td></tr></table></figure><p>来一个破坏，这里是破坏 <code>osd.0</code>，对应盘符 <code>/dev/sdb</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ceph]<span class="hljs-comment"># ceph-deploy disk zap lab8106:/dev/sdb</span><br>[ceph_deploy.conf][DEBUG ] found configuration file at: /root/.cephdeploy.conf<br>[ceph_deploy.cli][INFO  ] Invoked (1.5.34): /usr/bin/ceph-deploy disk zap lab8106:/dev/sdb<br>···<br>[lab8106][DEBUG ] Warning: The kernel is still using the old partition table.<br>[lab8106][DEBUG ] The new table will be used at the next reboot.<br>[lab8106][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or<br>[lab8106][DEBUG ] other utilities.<br>···<br></code></pre></td></tr></table></figure><p>即使这个 osd 被使用在，还是被破坏了，这里假设上面的就是一个误操作，我们看下带来了哪些变化</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ceph]<span class="hljs-comment"># ll /var/lib/ceph/osd/ceph-0/journal</span><br>lrwxrwxrwx 1 root root 58 Sep 24 00:02 /var/lib/ceph/osd/ceph-0/journal -&gt; /dev/disk/by-partuuid/bd81471d-13ff-44ce-8a33-92a8df9e8eee<br></code></pre></td></tr></table></figure><p>如果你用命令行看，就可以看到上面的链接已经变红了，分区没有了</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ceph]<span class="hljs-comment"># ceph-disk  list </span><br>/dev/sdb :<br> /dev/sdb1 other, xfs, mounted on /var/lib/ceph/osd/ceph-0<br> /dev/sdb2 other<br></code></pre></td></tr></table></figure><p>已经跟上面有变化了，没有ceph的相关信息了</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ceph]<span class="hljs-comment"># parted -s /dev/sdb print</span><br>Model: SEAGATE ST3300657SS (scsi)<br>Disk /dev/sdb: 300GB<br>Sector size (logical/physical): 512B/512B<br>Partition Table: gpt<br>Disk Flags: <br><br>Number  Start  End  Size  File system  Name  Flags<br></code></pre></td></tr></table></figure><p>分区表完全没有信息了，到这我们可以确定分区表完全没了，如果现在重启将会发生什么？重启以后这个磁盘就是一个裸盘，没有分区的裸盘</p><h3 id="处理办法"><a href="#处理办法" class="headerlink" title="处理办法"></a>处理办法</h3><p>首先一个办法就是当这个OSD坏了，然后直接按照删除节点，添加节点就可以了，这个应该是最主流，最通用的处理办法，但是这个在生产环境环境当中造成的数据迁移还是非常大的，我们尝试做恢复，这就是本篇主要讲的东西<br>####关闭迁移</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ceph]<span class="hljs-comment"># ceph osd set noout</span><br></code></pre></td></tr></table></figure><h4 id="停止OSD"><a href="#停止OSD" class="headerlink" title="停止OSD"></a>停止OSD</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ceph]<span class="hljs-comment"># systemctl stop ceph-osd@0</span><br></code></pre></td></tr></table></figure><p>现在的OSD还是有进程的，所以需要停止掉再做处理<br>通过其他节点查看分区的信息</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ceph]<span class="hljs-comment"># parted -s /dev/sdc  unit s print</span><br>Model: SEAGATE ST3300657SS (scsi)<br>Disk /dev/sdc: 585937500s<br>Sector size (logical/physical): 512B/512B<br>Partition Table: gpt<br>Disk Flags: <br><br>Number  Start     End         Size        File system  Name          Flags<br> 2      2048s     2097152s    2095105s                 ceph journal<br> 1      2099200s  585937466s  583838267s  xfs          ceph data<br></code></pre></td></tr></table></figure><p>我们现在进行分区表的恢复，记住上面的数值，我print的时候是加了unit s这个是要精确的值的,下面的创建会用到的</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ceph]<span class="hljs-comment"># parted -s /dev/sdb  mkpart  primary  2099200s 585937466s</span><br>[root@lab8106 ceph]<span class="hljs-comment"># parted -s /dev/sdb  mkpart  primary  2048s 2097152s</span><br></code></pre></td></tr></table></figure><p>我们再来检查下</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ceph]<span class="hljs-comment"># parted -s /dev/sdb  print</span><br>Model: SEAGATE ST3300657SS (scsi)<br>Disk /dev/sdb: 300GB<br>Sector size (logical/physical): 512B/512B<br>Partition Table: gpt<br>Disk Flags: <br><br>Number  Start   End     Size    File system  Name     Flags<br> 2      1049kB  1074MB  1073MB               primary<br> 1      1075MB  300GB   299GB   xfs          primary<br></code></pre></td></tr></table></figure><p>分区表已经回来了</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ceph]<span class="hljs-comment"># umount /var/lib/ceph/osd/ceph-0</span><br>[root@lab8106 ceph]<span class="hljs-comment"># partprobe</span><br>[root@lab8106 ceph]<span class="hljs-comment"># mount /dev/sdb1 /var/lib/ceph/osd/ceph-0</span><br></code></pre></td></tr></table></figure><p>我们重新挂载看看，没有问题，还要做下其他的处理</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ceph]<span class="hljs-comment"># rm -rf /var/lib/ceph/osd/ceph-0/journal</span><br></code></pre></td></tr></table></figure><p>我们先删除掉journal的链接文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ceph]<span class="hljs-comment"># ceph-osd -i 0 --osd-journal=/dev/sdb2 --mkjournal</span><br>SG_IO: bad/missing sense data, sb[]:  70 00 05 00 00 00 00 0a 00 00 00 00 20 00 01 cf 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00<br>2016-09-24 00:36:06.595992 7f9d0afbc880 -1 created new journal /dev/sdb2 <span class="hljs-keyword">for</span> object store /var/lib/ceph/osd/ceph-0<br>[root@lab8106 ceph-0]<span class="hljs-comment"># ln -s /dev/sdb2 /var/lib/ceph/osd/ceph-0/journal</span><br>[root@lab8106 ceph-0]<span class="hljs-comment"># chown ceph:ceph /var/lib/ceph/osd/ceph-0/journal</span><br>[root@lab8106 ceph-0]<span class="hljs-comment"># ll /var/lib/ceph/osd/ceph-0/journal</span><br>lrwxrwxrwx 1 ceph ceph 9 Sep 24 00:37 journal -&gt; /dev/sdb2<br></code></pre></td></tr></table></figure><p>上面操作就是创建journal相关的,注意下我上面的操作–osd-journal&#x3D;&#x2F;dev&#x2F;sdb2这个地方，我是便于识别，这个地方要写上dev&#x2F;sdb2的uuid的路径</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ceph-0]<span class="hljs-comment"># ll /dev/disk/by-partuuid/03fc6039-ad80-4b8d-86ec-aeee14fb3bb6 </span><br>lrwxrwxrwx 1 root root 10 Sep 24 00:33 /dev/disk/by-partuuid/03fc6039-ad80-4b8d-86ec-aeee14fb3bb6 -&gt; ../../sdb2<br></code></pre></td></tr></table></figure><p>也就是这个链接的这一串，这个防止盘符串了情况下journal无法找到的问题</p><h4 id="启动osd"><a href="#启动osd" class="headerlink" title="启动osd"></a>启动osd</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ceph-0]<span class="hljs-comment"># systemctl start ceph-osd@0</span><br></code></pre></td></tr></table></figure><p>检查下，到这osd就正常的恢复了</p><h2 id="为什么有这篇"><a href="#为什么有这篇" class="headerlink" title="为什么有这篇"></a>为什么有这篇</h2><p>一直都知道分区表是可以恢复的，也一直知道会有误操作，但是一直没有去把ceph中完整流程走下来，前两天一个哥们环境副本一，然后自己给搞错了，出现不得不恢复的情况，正好自己一直想把这个问题的处理办法给记录下来，所以就有了这篇，万一哪天有人碰到了，就把这篇发给他</p><h2 id="变更记录"><a href="#变更记录" class="headerlink" title="变更记录"></a>变更记录</h2><table><thead><tr><th align="center">Why</th><th align="center">Who</th><th align="center">When</th></tr></thead><tbody><tr><td align="center">创建</td><td align="center">武汉-运维-磨渣</td><td align="center">2016-09-24</td></tr><tr><td align="center">修改排版</td><td align="center">武汉-运维-磨渣</td><td align="center">2017-03-09</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Ceph的Mon数据重新构建工具</title>
    <link href="/2016/09/20/Ceph%E7%9A%84Mon%E6%95%B0%E6%8D%AE%E9%87%8D%E6%96%B0%E6%9E%84%E5%BB%BA%E5%B7%A5%E5%85%B7/"/>
    <url>/2016/09/20/Ceph%E7%9A%84Mon%E6%95%B0%E6%8D%AE%E9%87%8D%E6%96%B0%E6%9E%84%E5%BB%BA%E5%B7%A5%E5%85%B7/</url>
    
    <content type="html"><![CDATA[<p>关于mon的数据的问题，一般正常情况下都是配置的3个mon的，但是还是有人会担心 Mon 万一三个同时都挂掉了怎么办，那么集群所有的数据是不是都丢了，关于后台真实数据恢复，有去后台取对象，然后一个个拼接起来的方案，这个是确定可以成功的，但是这个方法对于生产的集群耗时巨大，并且需要导出数据，然后又配置新的集群，工程比较耗大，考虑到这个问题，Ceph 的中国（Redhat）的一位开发者 <a href="https://github.com/tchaikov">tchaikov</a> 就写了一个新的工具，来对损坏的MON的数据进行原集群的重构，这个比起其他方案要好很多，本篇将讲述怎么使用这个工具，代码已经合并到 Ceph 的master分支当中去了</p><p>关于这个工具相关的<a href="http://tracker.ceph.com/issues/17292">issue</a></p><!--break--><h2 id="打包一个合进新代码的master版本的ceph包"><a href="#打包一个合进新代码的master版本的ceph包" class="headerlink" title="打包一个合进新代码的master版本的ceph包"></a>打包一个合进新代码的master版本的ceph包</h2><h3 id="从github上面获取代码"><a href="#从github上面获取代码" class="headerlink" title="从github上面获取代码"></a>从github上面获取代码</h3><p>默认的分支就是master的直接去clone就可以了</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]# git clone https://github.com/ceph/ceph.git<br></code></pre></td></tr></table></figure><h3 id="检查是否是master分支"><a href="#检查是否是master分支" class="headerlink" title="检查是否是master分支"></a>检查是否是master分支</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]# cd ceph<br>[root@lab8106 ceph]# git branch<br>* master<br></code></pre></td></tr></table></figure><h3 id="检查代码是否是合进需要的代码了"><a href="#检查代码是否是合进需要的代码了" class="headerlink" title="检查代码是否是合进需要的代码了"></a>检查代码是否是合进需要的代码了</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]# cat ceph/doc/rados/troubleshooting/troubleshooting-mon.rst |grep rebuild<br>  # rebuild the monitor store from the collected map, if the cluster does not<br>  # i.e. use &quot;ceph-monstore-tool /tmp/mon-store rebuild&quot; instead<br>  ceph-monstore-tool /tmp/mon-store rebuild -- --keyring /path/to/admin.keyring<br>#. then rebuild the store<br></code></pre></td></tr></table></figure><p>因为这个代码是最近才合进去的 ，所以一定要检查代码的正确性</p><h3 id="创建一个源码包"><a href="#创建一个源码包" class="headerlink" title="创建一个源码包"></a>创建一个源码包</h3><p>进入到代码的根目录，修改make-dist文件里面的一个地方(第46行)，否则打出来的包可能没有版本号，因为打包的时候检查了有没有git目录<br>修改下面</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs raw">#tar cvf $outfile.version.tar $outfile/src/.git_version $outfile/src/ceph_ver.h $outfile/ceph.spec<br>tar cvf $outfile.version.tar $outfile/src/.git_version $outfile/src/ceph_ver.h $outfile/ceph.spec $outfile/.git<br></code></pre></td></tr></table></figure><h4 id="如果不改，就可能出现"><a href="#如果不改，就可能出现" class="headerlink" title="如果不改，就可能出现"></a>如果不改，就可能出现</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ceph]# ceph -v<br>ceph version HEAD-HASH-NOTFOUND (GITDIR-NOTFOUND)<br></code></pre></td></tr></table></figure><h4 id="创建源码包"><a href="#创建源码包" class="headerlink" title="创建源码包"></a>创建源码包</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ceph]#cd ceph<br>[root@lab8106 ceph]#./make-dist<br>[root@lab8106 ceph]# cp ceph-11.0.0-2460-g22053d0.tar.bz2 /root/rpmbuild/SOURCES/<br>[root@lab8106 ceph]# cp -f ceph.spec /root/rpmbuild/SPECS/<br>[root@lab8106 ceph]# rpmbuild -bb /root/rpmbuild/SPECS/ceph.spec<br></code></pre></td></tr></table></figure><p>执行完了以后就去这个路径取包</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ceph]# ll /root/rpmbuild/RPMS/x86_64/<br>total 1643964<br>-rw-r--r-- 1 root root      1972 Sep 20 10:32 ceph-11.0.0-2460.g22053d0.el7.centos.x86_64.rpm<br>-rw-r--r-- 1 root root  42259096 Sep 20 10:32 ceph-base-11.0.0-2460.g22053d0.el7.centos.x86_64.rpm<br>-rw-r--r-- 1 root root 320843080 Sep 20 10:35 ceph-common-11.0.0-2460.g22053d0.el7.centos.x86_64.rpm<br>-rw-r--r-- 1 root root  58138088 Sep 20 10:36 ceph-mds-11.0.0-2460.g22053d0.el7.centos.x86_64.rpm<br>···<br></code></pre></td></tr></table></figure><h3 id="准备测试环境"><a href="#准备测试环境" class="headerlink" title="准备测试环境"></a>准备测试环境</h3><p>使用打好的包进行集群的配置，创建一个正常的集群，这里就不讲述怎么配置集群了</p><h4 id="模拟mon损坏"><a href="#模拟mon损坏" class="headerlink" title="模拟mon损坏"></a>模拟mon损坏</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ceph]# systemctl stop ceph-mon@lab8106<br>[root@lab8106 ceph]# mv /var/lib/ceph/mon/ceph-lab8106/  /var/lib/ceph/mon/ceph-lab8106bk<br></code></pre></td></tr></table></figure><p>按上面的操作以后，mon的数据相当于全部丢失了，本测试环境是单mon的，多mon原理一样</p><h4 id="重构数据"><a href="#重构数据" class="headerlink" title="重构数据"></a>重构数据</h4><p>创建一个临时目录,停止掉所有的osd，这个地方因为mon已经完全挂掉了,所以停止所有osd也没什么大的影响了</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ceph]# mkdir /tmp/mon-store<br>[root@lab8106 ceph]# ceph-objectstore-tool --data-path /var/lib/ceph/osd/ceph-0/ --op update-mon-db --mon-store-path /tmp/mon-store/<br>[root@lab8106 ceph]# ceph-objectstore-tool --data-path /var/lib/ceph/osd/ceph-1/ --op update-mon-db --mon-store-path /tmp/mon-store/<br></code></pre></td></tr></table></figure><p>注意如果有多台OSD机器，那么在一台台的OSD主机进行上面的操作，这个目录的数据要保持递增的，也就是一直对着这个目录弄，假如换了一台机器那么先把这个数据传递到另外一台机器</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs raw">root@lab8106 ~]# rsync -avz /tmp/mon-store 192.168.8.107:/tmp/mon-store<br>sending incremental file list<br>created directory /tmp/mon-store<br>mon-store/<br>mon-store/kv_backend<br>mon-store/store.db/<br>mon-store/store.db/000005.sst<br>mon-store/store.db/000008.sst<br>mon-store/store.db/000009.log<br>mon-store/store.db/CURRENT<br>mon-store/store.db/LOCK<br>mon-store/store.db/MANIFEST-000007<br><br>sent 11490 bytes  received 153 bytes  7762.00 bytes/sec<br>total size is 74900  speedup is 6.43<br></code></pre></td></tr></table></figure><p>等192.168.8.106的机器全部做完了，然后这个&#x2F;tmp&#x2F;mon-store传递到了192.168.8.107的机器上，然后再开始做192.168.8.107这台机器的，等全部做外了，把这个&#x2F;tmp&#x2F;mon-store弄到需要恢复mon的机器上</p><h3 id="根据获得的数据进行重构"><a href="#根据获得的数据进行重构" class="headerlink" title="根据获得的数据进行重构"></a>根据获得的数据进行重构</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]# mkdir /var/lib/ceph/mon/ceph-lab8106<br>[root@lab8106 ~]# ceph-monstore-tool /tmp/mon-store rebuild<br>[root@lab8106 ~]# cp -ra /tmp/mon-store/* /var/lib/ceph/mon/ceph-lab8106<br>[root@lab8106 ~]# touch /var/lib/ceph/mon/ceph-lab8106/done<br>[root@lab8106 ~]# touch /var/lib/ceph/mon/ceph-lab8106/systemd<br>[root@lab8106 ~]# chown ceph:ceph -R /var/lib/ceph/mon/<br></code></pre></td></tr></table></figure><h3 id="启动mon"><a href="#启动mon" class="headerlink" title="启动mon"></a>启动mon</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]# systemctl restart ceph-mon@lab8106<br></code></pre></td></tr></table></figure><p>检查状态</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]# ceph -s<br></code></pre></td></tr></table></figure><p>可以看到可以好了，在实践过程中，发现如果对修复的数据，马上进行破坏，再次进行修复的时候，就无法恢复了，应该是个bug，已经提交给作者 Issue:<a href="https://github.com/ceph/ceph/pull/11126">11226</a></p><h3 id="无法恢复的数据"><a href="#无法恢复的数据" class="headerlink" title="无法恢复的数据"></a>无法恢复的数据</h3><ul><li>pg settings: the full ratio and nearfull ratio 设置会丢失，这个无关紧要，再设置一次就可以了</li><li>MDS Maps: the MDS maps are lost.</li></ul><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>因为工具才出来，可能难免有些bug，这个是为未来提供一种恢复数据的方式，使得 Ceph 变得更加的健壮</p><h2 id="附加知识"><a href="#附加知识" class="headerlink" title="附加知识"></a>附加知识</h2><p>如果指定ceph版本进行编译</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs raw">git clone https://github.com/ceph/ceph.git<br>git checkout -b myceph v10.2.3<br>git submodule update --init --recursive<br></code></pre></td></tr></table></figure><p>v10.2.3为发行版本的tag，也就是release的版本号码，这个操作是切换到指定的tag，并且下载依赖的一些模块</p><h2 id="变更记录"><a href="#变更记录" class="headerlink" title="变更记录"></a>变更记录</h2><table><thead><tr><th align="center">Why</th><th align="center">Who</th><th align="center">When</th></tr></thead><tbody><tr><td align="center">创建</td><td align="center">武汉-运维-磨渣</td><td align="center">2016-09-20</td></tr><tr><td align="center">增加git版本选择</td><td align="center">武汉-运维-磨渣</td><td align="center">2016-10-12</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>替换OSD操作的优化与分析</title>
    <link href="/2016/09/19/%E6%9B%BF%E6%8D%A2OSD%E6%93%8D%E4%BD%9C%E7%9A%84%E4%BC%98%E5%8C%96%E4%B8%8E%E5%88%86%E6%9E%90/"/>
    <url>/2016/09/19/%E6%9B%BF%E6%8D%A2OSD%E6%93%8D%E4%BD%9C%E7%9A%84%E4%BC%98%E5%8C%96%E4%B8%8E%E5%88%86%E6%9E%90/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>之前有写过一篇<a href="/images/blog/13575321.html">删除OSD的正确方式</a>，里面只是简单的讲了下删除的方式怎样能减少迁移量，本篇属于一个扩展，讲述了 Ceph 运维当中经常出现的坏盘提换盘的步骤的优化</p><p>基础环境两台主机每台主机8个 OSD，一共 16 个 OSD，副本设置为2，PG 数设置为800，计算下来平均每个 OSD 上的 P G数目为100个，本篇将通过数据来分析不同的处理方法的差别</p><p>开始测试前先把环境设置为 noout，然后通过停止 OSD 来模拟 OSD 出现了异常，之后进行不同处理方法</p><h2 id="测试三种方法"><a href="#测试三种方法" class="headerlink" title="测试三种方法"></a>测试三种方法</h2><h3 id="首先-out-一个-OSD，然后剔除-OSD，然后增加-OSD"><a href="#首先-out-一个-OSD，然后剔除-OSD，然后增加-OSD" class="headerlink" title="首先 out 一个 OSD，然后剔除 OSD，然后增加 OSD"></a>首先 out 一个 OSD，然后剔除 OSD，然后增加 OSD</h3><ol><li>停止指定 OSD 进程</li><li>out 指定 OSD</li><li>crush remove 指定 OSD</li><li>增加一个新的 OSD</li></ol><p>一般生产环境会设置为  noout，当然不设置也可以，那就交给程序去控制节点的 out，默认是在进程停止后的五分钟，总之这个地方如果有 out 触发，不管是人为触发，还是自动触发数据流是一定的，我们这里为了便于测试，使用的是人为触发，上面提到的预制环境就是设置的  noout</p><p>开始测试前获取最原始的分布</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]# ceph pg dump pgs|awk &#x27;&#123;print $1,$15&#125;&#x27;|grep -v pg   &gt; pg1.txt<br></code></pre></td></tr></table></figure><p>获取当前的 PG 分布,保存到文件pg1.txt，这个 PG 分布记录是 PG 所在的 OSD，记录下来，方便后面进行比较，从而得出需要迁移的数据 </p><h4 id="停止指定的-OSD-进程"><a href="#停止指定的-OSD-进程" class="headerlink" title="停止指定的 OSD 进程"></a>停止指定的 OSD 进程</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]# systemctl stop ceph-osd@15<br></code></pre></td></tr></table></figure><p>停止进程并不会触发迁移，只会引起 PG 状态的变化，比如原来主 PG 在停止的 OSD 上，那么停止掉 OSD 以后，原来的副本的那个 PG 就会角色升级为主 PG 了</p><h4 id="out-掉一个-OSD"><a href="#out-掉一个-OSD" class="headerlink" title="out 掉一个 OSD"></a>out 掉一个 OSD</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]# ceph osd out 15<br></code></pre></td></tr></table></figure><p>在触发 out 以前，当前的 PG 状态应该有 <code>active+undersized+degraded</code>,触发 out 以后，所有的 PG 的状态应该会慢慢变成 <code>active+clean</code>,等待集群正常后，再次查询当前的 PG 分布状态</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]# ceph pg dump pgs|awk &#x27;&#123;print $1,$15&#125;&#x27;|grep -v pg   &gt; pg2.txt<br></code></pre></td></tr></table></figure><p>保存当前的 PG 分布为pg2.txt<br>比较 out 前后的 PG 的变化情况，下面是比较具体的变化情况，只列出变化的部分</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]# diff -y -W 100 pg1.txt pg2.txt  --suppress-common-lines<br></code></pre></td></tr></table></figure><p>这里我们关心的是变动的数目，只统计变动的 PG 的数目</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]# diff -y -W 100 pg1.txt pg2.txt  --suppress-common-lines|wc -l<br>102<br></code></pre></td></tr></table></figure><p>第一次 out 以后有102个 PG 的变动,这个数字记住，后面的统计会用到</p><h4 id="从-crush-里面删除-OSD"><a href="#从-crush-里面删除-OSD" class="headerlink" title="从 crush 里面删除 OSD"></a>从 crush 里面删除 OSD</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]# ceph osd crush remove osd.15<br></code></pre></td></tr></table></figure><p>crush 删除以后同样会触发迁移，等待 PG 的均衡，也就是全部变成 <code>active+clean</code> 状态</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]# ceph pg dump pgs|awk &#x27;&#123;print $1,$15&#125;&#x27;|grep -v pg   &gt; pg3.txt<br></code></pre></td></tr></table></figure><p>获取当前的 PG 分布的状态<br>现在来比较 crush remove 前后的 PG 变动</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]# diff -y -W 100 pg2.txt pg3.txt  --suppress-common-lines|wc -l<br>137<br></code></pre></td></tr></table></figure><p>我们重新加上新的 OSD</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]# ceph-deploy osd prepare lab8107:/dev/sdi<br>[root@lab8106 ~]# ceph-deploy osd activate lab8107:/dev/sdi1<br></code></pre></td></tr></table></figure><p>加完以后统计当前的新的 PG 状态</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]# ceph pg dump pgs|awk &#x27;&#123;print $1,$15&#125;&#x27;|grep -v pg   &gt; pg4.txt<br></code></pre></td></tr></table></figure><p>比较前后的变化</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]# diff -y -W 100 pg3.txt pg4.txt  --suppress-common-lines|wc -l<br>167<br></code></pre></td></tr></table></figure><p>整个替换流程完毕，统计上面的 PG 总的变动</p><blockquote><p>102 +137 +167 &#x3D; 406</p></blockquote><p>也就是按这个方法的变动为406个 PG，因为是只有双主机，里面可能存在某些放大问题，这里不做深入的讨论，因为我的三组测试环境都是一样的情况，只做横向比较，原理相通，这里是用数据来分析出差别</p><h3 id="先crush-reweight-0-，然后out，然后再增加osd"><a href="#先crush-reweight-0-，然后out，然后再增加osd" class="headerlink" title="先crush reweight 0 ，然后out，然后再增加osd"></a>先crush reweight 0 ，然后out，然后再增加osd</h3><p>首先恢复环境为测试前的环境</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]# ceph pg dump pgs|awk &#x27;&#123;print $1,$15&#125;&#x27;|grep -v pg   &gt; 2pg1.txt<br></code></pre></td></tr></table></figure><p>记录最原始的 PG 分布情况</p><h4 id="crush-reweight-指定OSD"><a href="#crush-reweight-指定OSD" class="headerlink" title="crush reweight 指定OSD"></a>crush reweight 指定OSD</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]# ceph osd crush reweight osd.16 0<br>reweighted item id 16 name &#x27;osd.16&#x27; to 0 in crush map<br></code></pre></td></tr></table></figure><p>等待平衡了以后记录当前的 PG 分布状态</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]# ceph pg dump pgs|awk &#x27;&#123;print $1,$15&#125;&#x27;|grep -v pg   &gt; 2pg2.txt<br>dumped pgs in format plain<br></code></pre></td></tr></table></figure><p>比较前后的变动</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]# diff -y -W 100 2pg1.txt 2pg2.txt  --suppress-common-lines|wc -l<br>166<br></code></pre></td></tr></table></figure><h4 id="crush-remove-指定-OSD"><a href="#crush-remove-指定-OSD" class="headerlink" title="crush remove 指定 OSD"></a>crush remove 指定 OSD</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]# ceph osd crush remove osd.16<br>removed item id 16 name &#x27;osd.16&#x27; from crush map<br></code></pre></td></tr></table></figure><p>这个地方因为上面 crush 已经是0了所以删除也不会引起 PG 变动<br>然后直接 <code>ceph osd rm osd.16</code> 同样没有 PG 变动</p><h4 id="增加新的-OSD"><a href="#增加新的-OSD" class="headerlink" title="增加新的 OSD"></a>增加新的 OSD</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]#ceph-deploy osd prepare lab8107:/dev/sdi<br>[root@lab8106 ~]#ceph-deploy osd activate lab8107:/dev/sdi1<br></code></pre></td></tr></table></figure><p>等待平衡以后获取当前的 PG 分布</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ceph]# ceph pg dump pgs|awk &#x27;&#123;print $1,$15&#125;&#x27;|grep -v pg   &gt; 2pg3.txt<br></code></pre></td></tr></table></figure><p>来比较前后的变化</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]# diff -y -W 100 2pg2.txt 2pg3.txt --suppress-common-lines|wc -l<br>159<br></code></pre></td></tr></table></figure><p>总的 PG 变动为</p><blockquote><p>166+159&#x3D;325</p></blockquote><h3 id="开始做norebalance，然后做crush-remove，然后做add"><a href="#开始做norebalance，然后做crush-remove，然后做add" class="headerlink" title="开始做norebalance，然后做crush remove，然后做add"></a>开始做norebalance，然后做crush remove，然后做add</h3><p>恢复环境为初始环境，然后获取当前的 PG 分布</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]# ceph pg dump pgs|awk &#x27;&#123;print $1,$15&#125;&#x27;|grep -v pg   &gt; 3pg1.txt<br>dumped pgs in format plain<br></code></pre></td></tr></table></figure><h4 id="给集群做多种标记，防止迁移"><a href="#给集群做多种标记，防止迁移" class="headerlink" title="给集群做多种标记，防止迁移"></a>给集群做多种标记，防止迁移</h4><p>设置为 norebalance，nobackfill，norecover,后面是有地方会解除这些设置的</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]# ceph osd set norebalance<br>set norebalance<br>[root@lab8106 ~]# ceph osd set nobackfill<br>set nobackfill<br>[root@lab8106 ~]# ceph osd set norecover<br>set norecover<br></code></pre></td></tr></table></figure><h4 id="crush-reweight-指定-OSD"><a href="#crush-reweight-指定-OSD" class="headerlink" title="crush reweight 指定 OSD"></a>crush reweight 指定 OSD</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]# ceph osd crush reweight osd.15 0<br>reweighted item id 15 name &#x27;osd.15&#x27; to 0 in crush map<br></code></pre></td></tr></table></figure><p>这个地方因为已经做了上面的标记，所以只会出现状态变化，而没有真正的迁移，我们也先统计一下</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]# ceph pg dump pgs|awk &#x27;&#123;print $1,$15&#125;&#x27;|grep -v pg   &gt; 3pg2.txt<br>[root@lab8106 ~]# diff -y -W 100 3pg1.txt 3pg2.txt --suppress-common-lines|wc -l<br>158<br></code></pre></td></tr></table></figure><p>注意这里只是计算了，并没有真正的数据变动，可以通过监控两台的主机的网络流量来判断,所以这里的变动并不用计算到需要迁移的 PG 数目当中</p><h4 id="crush-remove-指定-OSD-1"><a href="#crush-remove-指定-OSD-1" class="headerlink" title="crush remove 指定 OSD"></a>crush remove 指定 OSD</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]#ceph osd crush remove osd.15<br></code></pre></td></tr></table></figure><h4 id="删除指定的-OSD"><a href="#删除指定的-OSD" class="headerlink" title="删除指定的 OSD"></a>删除指定的 OSD</h4><p>删除以后同样是没有 PG 的变动的</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">ceph osd rm osd.15<br></code></pre></td></tr></table></figure><p>这个地方有个小地方需要注意一下，不做 ceph auth del osd.15 把15的编号留着，这样好判断前后的 PG 的变化，不然相同的编号，就无法判断是不是做了迁移了</p><h4 id="增加新的-OSD-1"><a href="#增加新的-OSD-1" class="headerlink" title="增加新的 OSD"></a>增加新的 OSD</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]#ceph-deploy osd prepare lab8107:/dev/sdi<br>[root@lab8106 ~]#ceph-deploy osd activate lab8107:/dev/sdi1<br></code></pre></td></tr></table></figure><p>我的环境下，新增的 OSD 的编号为16了</p><h4 id="解除各种标记"><a href="#解除各种标记" class="headerlink" title="解除各种标记"></a>解除各种标记</h4><p>我们放开上面的设置，看下数据的变动情况</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ceph]# ceph osd unset norebalance<br>unset norebalance<br>[root@lab8106 ceph]# ceph osd unset nobackfill<br>unset nobackfill<br>[root@lab8106 ceph]# ceph osd unset norecover<br>unset norecover<br></code></pre></td></tr></table></figure><p>设置完了后数据才真正开始变动了，可以通过观察网卡流量看到，来看下最终pg变化</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ceph]# ceph pg dump pgs|awk &#x27;&#123;print $1,$15&#125;&#x27;|grep -v pg   &gt; 3pg3.txt<br>dumped pgs in format plain<br>[root@lab8106 ~]# diff -y -W 100 3pg1.txt 3pg3.txt --suppress-common-lines|wc -l<br>195<br></code></pre></td></tr></table></figure><p>这里我们只需要跟最开始的 PG 分布状况进行比较就可以了，因为中间的状态实际上都没有做数据的迁移，所以不需要统计进去，可以看到这个地方动了195个 PG<br>总共的 PG 迁移量为</p><blockquote><p>195</p></blockquote><h2 id="数据汇总"><a href="#数据汇总" class="headerlink" title="数据汇总"></a>数据汇总</h2><p>现在通过表格来对比下三种方法的迁移量的比较(括号内为迁移 PG 数目)</p><table><thead><tr><th align="center"></th><th align="left">方法一</th><th align="left">方法二</th><th align="left">方法三</th></tr></thead><tbody><tr><td align="center">所做操作</td><td align="left">stop osd (0)<br>out osd(102)<br>crush remove osd (137)<br> add osd(167)</td><td align="left">crush reweight osd(166)<br>out osd(0)<br>crush remove osd (0)<br>add osd(159)</td><td align="left">set 标记(0)<br>crush reweight osd(0)<br>crush remove osd (0)<br>add osd(195)</td></tr><tr><td align="center">PG迁移数量</td><td align="left">406</td><td align="left">325</td><td align="left">195</td></tr></tbody></table><p>可以很清楚的看到三种不同的方法，最终的触发的迁移量是不同的，处理的好的话，能节约差不多一半的迁移的数据量，这个对于生产环境来说还是很好的，关于这个建议先在测试环境上进行测试，然后再操作，上面的操作只要不对磁盘进行格式化，操作都是可逆的，也就是可以比较放心的做，记住所做的操作，每一步都做完都去检查 PG 的状态是否是正常的</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>从我自己的操作经验来看，最开始是用的第一种方法，后面就用第二种方法减少了一部分迁移量，最近看到资料写做剔除OSD的时候可以关闭迁移防止无效的过多的迁移，然后就测试了一下，确实能够减少不少的迁移量，这个减少在某些场景下还是很好的，当然如果不太熟悉，用哪一种都可以，最终能达到的目的是一样的</p><h2 id="附加"><a href="#附加" class="headerlink" title="附加"></a>附加</h2><p>有人问到一个问题，为什么按照这个流程操作的时候，会出现slow request？在进行了一次验证后，发现在迁移过程中的请求路径还是很长的，所以出现slow request还是很容易的</p><p>假如我们有三个osd，分别为0,1,2，里面有各种的分布，我们在踢掉一个osd.2后，可能出现的一个情况是<br>某个PG(0.3b)的[2,0]分布变成了[1,0]<br>而此时后台的osd.1的PG（0.3b）这个目录里面的内容实际是空的，如果这个时候，前端的请求一个对象正好是分布在0.3b这个PG上的时候，后台需要先将osd.0上面的这个0.3b的对象写入到osd.1的0.3b的pg里面去，然后再去响应客户端的请求，自然路径就长了，如果这样的请求一多，响应前台的性能就有问题了，增加节点的时候同理</p><p>请求到这种空PG的对象，PG的状态会这样变化：</p><blockquote><p>从active+degraded 变成active+recovery_wait+degraded</p></blockquote><p>迁移的数据量是一定的，这个看是请求的时候实时迁移然后响应还是提前迁移，然后响应，所以这个中间操作过程尽量的快的完成，然后好迁移完响应前端的请求</p><h2 id="变更记录"><a href="#变更记录" class="headerlink" title="变更记录"></a>变更记录</h2><table><thead><tr><th align="center">Why</th><th align="center">Who</th><th align="center">When</th></tr></thead><tbody><tr><td align="center">创建</td><td align="center">武汉-运维-磨渣</td><td align="center">2016-09-19</td></tr><tr><td align="center">增加附录段落</td><td align="center">武汉-运维-磨渣</td><td align="center">2017-02-04</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Centos7下Jewel版本radosgw服务启动</title>
    <link href="/2016/09/12/Centos7%E4%B8%8BJewel%E7%89%88%E6%9C%ACradosgw%E6%9C%8D%E5%8A%A1%E5%90%AF%E5%8A%A8/"/>
    <url>/2016/09/12/Centos7%E4%B8%8BJewel%E7%89%88%E6%9C%ACradosgw%E6%9C%8D%E5%8A%A1%E5%90%AF%E5%8A%A8/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>本篇介绍了centos7下jewel版本的radosgw配置，这里的配置是指将服务能够正常起来，不涉及到S3的配置，以及其他的更多的配置,radosgw后面的gw就是gateway的意思，也就是我们说的网关的意思，本篇中所提及的实例也就是网关的意思，说实例是将每个单独的网关更细化一点的说法</p><p>很多人不清楚在centos7下面怎么去控制这个radosgw网关的服务的控制，这个地方是会去读取配置文件的，所以配置文件得写正确</p><h2 id="预备环境"><a href="#预备环境" class="headerlink" title="预备环境"></a>预备环境</h2><h3 id="一个完整的集群"><a href="#一个完整的集群" class="headerlink" title="一个完整的集群"></a>一个完整的集群</h3><p>拥有一个正常的集群是需要提前准备好的，ceph -s检查正确的输出</p><h3 id="关闭各种auth"><a href="#关闭各种auth" class="headerlink" title="关闭各种auth"></a>关闭各种auth</h3><p>这个地方也可以不关闭，注意配置好用户认证就可以了，这里关闭了，配置起来方便，我是从来不开的,也避免了新手不会配置用户造成认证的各种异常<br>关闭认证就是在ceph.conf里面添加下面字段</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">auth_cluster_required = none<br>auth_service_required = none<br>auth_client_required = none<br></code></pre></td></tr></table></figure><h3 id="安装ceph-radosgw的包"><a href="#安装ceph-radosgw的包" class="headerlink" title="安装ceph-radosgw的包"></a>安装ceph-radosgw的包</h3><p>这个因为默认不会安装，所以要安装好</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">yum install ceph-radosgw<br></code></pre></td></tr></table></figure><h2 id="默认启动过程"><a href="#默认启动过程" class="headerlink" title="默认启动过程"></a>默认启动过程</h2><p>我们先什么都不配置，看下一般的会怎么处理</p><h3 id="启动服务"><a href="#启动服务" class="headerlink" title="启动服务"></a>启动服务</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">systemctl restart ceph-radosgw.target<br></code></pre></td></tr></table></figure><h3 id="检查服务的状态"><a href="#检查服务的状态" class="headerlink" title="检查服务的状态"></a>检查服务的状态</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ~]<span class="hljs-comment"># systemctl status ceph-radosgw.target </span><br>● ceph-radosgw.target - ceph target allowing to start/stop all ceph-radosgw@.service instances at once<br>   Loaded: loaded (/usr/lib/systemd/system/ceph-radosgw.target; enabled; vendor preset: enabled)<br>   Active: active since Mon 2016-09-12 13:13:03 CST; 51s ago<br><br>Sep 12 13:13:03 lab8106 systemd[1]: Stopping ceph target allowing to start/stop all ceph-radosgw@.service instances at once.<br>Sep 12 13:13:03 lab8106 systemd[1]: Reached target ceph target allowing to start/stop all ceph-radosgw@.service instances at once.<br>Sep 12 13:13:03 lab8106 systemd[1]: Starting ceph target allowing to start/stop all ceph-radosgw@.service instances at once.<br>Sep 12 13:13:51 lab8106 systemd[1]: Reached target ceph target allowing to start/stop all ceph-radosgw@.service instances at once.<br></code></pre></td></tr></table></figure><p>可以看到进程是启动的，没有任何异常</p><h3 id="检查端口是否启动"><a href="#检查端口是否启动" class="headerlink" title="检查端口是否启动"></a>检查端口是否启动</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ~]<span class="hljs-comment"># netstat -tunlp|grep radosgw</span><br></code></pre></td></tr></table></figure><p>但是并没有生成任何端口，这个是因为还没有配置实例,这个地方就是新手经常卡住的地方</p><h2 id="下面开始配置默认单实例"><a href="#下面开始配置默认单实例" class="headerlink" title="下面开始配置默认单实例"></a>下面开始配置默认单实例</h2><h3 id="写配置文件"><a href="#写配置文件" class="headerlink" title="写配置文件"></a>写配置文件</h3><p>在配置文件 &#x2F;etc&#x2F;ceph&#x2F;ceph.conf的最下面写一个最简配置文件<br>注意下面的client.radosgw1这个包起来的，这个是固定写法，在  systemctl 启动服务的时候 @ 取后面的radosgw1</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">[client.radosgw1]<br>host = lab8106<br>rgw_content_length_compat = <span class="hljs-literal">true</span><br></code></pre></td></tr></table></figure><h3 id="启动服务-1"><a href="#启动服务-1" class="headerlink" title="启动服务"></a>启动服务</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ~]<span class="hljs-comment"># systemctl restart ceph-radosgw@radosgw1</span><br></code></pre></td></tr></table></figure><h3 id="检查服务状态"><a href="#检查服务状态" class="headerlink" title="检查服务状态"></a>检查服务状态</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ~]<span class="hljs-comment"># systemctl status ceph-radosgw@radosgw1</span><br>● ceph-radosgw@radosgw1.service - Ceph rados gateway<br>   Loaded: loaded (/usr/lib/systemd/system/ceph-radosgw@.service; disabled; vendor preset: disabled)<br>   Active: active (running) since Mon 2016-09-12 13:17:34 CST; 17s ago<br> Main PID: 19996 (radosgw)<br>   CGroup: /system.slice/system-ceph\x2dradosgw.slice/ceph-radosgw@radosgw1.service<br>           └─19996 /usr/bin/radosgw -f --cluster ceph --name client.radosgw1 --setuser ceph --setgroup ceph<br><br>Sep 12 13:17:34 lab8106 systemd[1]: Started Ceph rados gateway.<br>Sep 12 13:17:34 lab8106 systemd[1]: Starting Ceph rados gateway...<br></code></pre></td></tr></table></figure><h3 id="检查端口是否启动-1"><a href="#检查端口是否启动-1" class="headerlink" title="检查端口是否启动"></a>检查端口是否启动</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ~]<span class="hljs-comment"># netstat -tunlp|grep radosgw</span><br>tcp        0      0 0.0.0.0:7480            0.0.0.0:*               LISTEN      19996/radosgw<br></code></pre></td></tr></table></figure><p>可以看到默认的端口是7480</p><h2 id="配置多个自定义端口实例"><a href="#配置多个自定义端口实例" class="headerlink" title="配置多个自定义端口实例"></a>配置多个自定义端口实例</h2><h3 id="写配置文件-1"><a href="#写配置文件-1" class="headerlink" title="写配置文件"></a>写配置文件</h3><p>在配置文件 &#x2F;etc&#x2F;ceph&#x2F;ceph.conf的最下面写下配置文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs bash">[client.radosgw1]<br>host = lab8106<br>rgw_frontends = civetweb port=7481<br>rgw_content_length_compat = <span class="hljs-literal">true</span><br><br>[client.radosgw2]<br>host = lab8106<br>rgw_frontends = civetweb port=7482<br>rgw_content_length_compat = <span class="hljs-literal">true</span><br></code></pre></td></tr></table></figure><p>这个地方配置两个实例，用了不同的名称，用了不同的端口</p><h3 id="启动服务-2"><a href="#启动服务-2" class="headerlink" title="启动服务"></a>启动服务</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ~]<span class="hljs-comment"># systemctl restart ceph-radosgw@radosgw1</span><br>[root@lab8106 ~]<span class="hljs-comment"># systemctl restart ceph-radosgw@radosgw2</span><br></code></pre></td></tr></table></figure><h3 id="检查服务状态-1"><a href="#检查服务状态-1" class="headerlink" title="检查服务状态"></a>检查服务状态</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ~]<span class="hljs-comment"># systemctl status ceph-radosgw@radosgw1</span><br>● ceph-radosgw@radosgw1.service - Ceph rados gateway<br>   Loaded: loaded (/usr/lib/systemd/system/ceph-radosgw@.service; disabled; vendor preset: disabled)<br>   Active: active (running) since Mon 2016-09-12 13:20:06 CST; 1min 4s ago<br> Main PID: 20509 (radosgw)<br>   CGroup: /system.slice/system-ceph\x2dradosgw.slice/ceph-radosgw@radosgw1.service<br>           └─20509 /usr/bin/radosgw -f --cluster ceph --name client.radosgw1 --setuser ceph --setgroup ceph<br><br>Sep 12 13:20:06 lab8106 systemd[1]: Started Ceph rados gateway.<br>Sep 12 13:20:06 lab8106 systemd[1]: Starting Ceph rados gateway...<br>[root@lab8106 ~]<span class="hljs-comment"># systemctl status ceph-radosgw@radosgw2</span><br>● ceph-radosgw@radosgw2.service - Ceph rados gateway<br>   Loaded: loaded (/usr/lib/systemd/system/ceph-radosgw@.service; disabled; vendor preset: disabled)<br>   Active: active (running) since Mon 2016-09-12 13:20:09 CST; 1min 3s ago<br> Main PID: 20696 (radosgw)<br>   CGroup: /system.slice/system-ceph\x2dradosgw.slice/ceph-radosgw@radosgw2.service<br>           └─20696 /usr/bin/radosgw -f --cluster ceph --name client.radosgw2 --setuser ceph --setgroup ceph<br><br>Sep 12 13:20:09 lab8106 systemd[1]: Started Ceph rados gateway.<br>Sep 12 13:20:09 lab8106 systemd[1]: Starting Ceph rados gateway...<br></code></pre></td></tr></table></figure><h3 id="检查端口是否启动-2"><a href="#检查端口是否启动-2" class="headerlink" title="检查端口是否启动"></a>检查端口是否启动</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ~]<span class="hljs-comment"># netstat -tunlp|grep radosgw</span><br>tcp        0      0 0.0.0.0:7481            0.0.0.0:*               LISTEN      20509/radosgw       <br>tcp        0      0 0.0.0.0:7482            0.0.0.0:*               LISTEN      20696/radosgw<br></code></pre></td></tr></table></figure><p>可以看到服务和端口都能正常的启动了</p><p>好了，关于centos7下jewel版本的radosgw配置的启动已经介绍完了，这里不涉及更多深入的东西，其他的东西可以参照其他文档配置即可，这个地方只是对启动服务这里专门的介绍一下</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>从上面的过程可以看出大致的流程如下</p><ul><li>安装软件</li><li>启动服务</li><li>检查服务状态</li><li>检查服务端口</li></ul><p>这些很多都是基础的做法，在centos7下面虽然比6做了一些改变，但是掌握了一些通用的排查方法后，是很容易举一反三的，因为看到有新手不熟悉启动，所以写下这篇文章，自己因为也没经常用，所以也写下当个笔记了</p>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>如何统计Ceph的RBD真实使用容量</title>
    <link href="/2016/09/08/%E5%A6%82%E4%BD%95%E7%BB%9F%E8%AE%A1Ceph%E7%9A%84RBD%E7%9C%9F%E5%AE%9E%E4%BD%BF%E7%94%A8%E5%AE%B9%E9%87%8F/"/>
    <url>/2016/09/08/%E5%A6%82%E4%BD%95%E7%BB%9F%E8%AE%A1Ceph%E7%9A%84RBD%E7%9C%9F%E5%AE%9E%E4%BD%BF%E7%94%A8%E5%AE%B9%E9%87%8F/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>ceph的rbd一直有个问题就是无法清楚的知道这个分配的空间里面到底使用了多少，这个在Jewel里面提供了一个新的接口去查询，对于老版本来说可能同样有这个需求，本篇将详细介绍如何解决这个问题</p><h2 id="查询的各种方法"><a href="#查询的各种方法" class="headerlink" title="查询的各种方法"></a>查询的各种方法</h2><p>目前已知的有三种方法</p><ul><li>1、使用rbd du查询（Jewel才支持）</li><li>2、使用rbd diff</li><li>3、根据对象统计的方法进行统计</li></ul><p>详细介绍</p><h3 id="方法一：使用rbd-du查询"><a href="#方法一：使用rbd-du查询" class="headerlink" title="方法一：使用rbd du查询"></a>方法一：使用rbd du查询</h3><p>这个参考我之前的文章：<a href="/2016/03/24/ceph%E6%9F%A5%E8%AF%A2rbd%E7%9A%84%E4%BD%BF%E7%94%A8%E5%AE%B9%E9%87%8F%EF%BC%88%E5%BF%AB%E9%80%9F%EF%BC%89/">查询rbd的使用容量</a></p><h3 id="方法二：使用rbd-diff"><a href="#方法二：使用rbd-diff" class="headerlink" title="方法二：使用rbd diff"></a>方法二：使用rbd diff</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ~]<span class="hljs-comment"># rbd diff rbd/zp | awk &#x27;&#123; SUM += $2 &#125; END &#123; print SUM/1024/1024 &quot; MB&quot; &#125;&#x27;</span><br>828.844 MB<br></code></pre></td></tr></table></figure><h3 id="方法三：根据对象统计的方法进行统计"><a href="#方法三：根据对象统计的方法进行统计" class="headerlink" title="方法三：根据对象统计的方法进行统计"></a>方法三：根据对象统计的方法进行统计</h3><p>这个是本篇着重介绍的一点，在集群非常大的时候，再去按上面的一个个的查询，需要花很长的时间，并且需要时不时的跟集群进行交互，这里采用的方法是把统计数据一次获取下来，然后进行数据的统计分析，从而获取结果，获取的粒度是以存储池为基准的</p><h4 id="拿到所有对象的信息"><a href="#拿到所有对象的信息" class="headerlink" title="拿到所有对象的信息"></a>拿到所有对象的信息</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-keyword">for</span> obj <span class="hljs-keyword">in</span> `rados -p rbd <span class="hljs-built_in">ls</span>`;<span class="hljs-keyword">do</span> rados -p rbd <span class="hljs-built_in">stat</span> <span class="hljs-variable">$obj</span> &gt;&gt; obj.txt;<span class="hljs-keyword">done</span>;<br></code></pre></td></tr></table></figure><p>这个获取的时间长短是根据对象的多少来的，如果担心出问题，可以换个终端查看进度</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">tail</span> -f  obj.txt<br></code></pre></td></tr></table></figure><h4 id="获取RBD的镜像列表"><a href="#获取RBD的镜像列表" class="headerlink" title="获取RBD的镜像列表"></a>获取RBD的镜像列表</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ~]<span class="hljs-comment"># rbd -p rbd ls</span><br>test1<br>zp<br></code></pre></td></tr></table></figure><h3 id="获取RBD的镜像的prefix"><a href="#获取RBD的镜像的prefix" class="headerlink" title="获取RBD的镜像的prefix"></a>获取RBD的镜像的prefix</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-keyword">for</span> a <span class="hljs-keyword">in</span> `rbd -p rbd <span class="hljs-built_in">ls</span>`;<span class="hljs-keyword">do</span> <span class="hljs-built_in">echo</span> <span class="hljs-variable">$a</span> ;rbd -p rbd info <span class="hljs-variable">$a</span>|grep prefix |awk <span class="hljs-string">&#x27;&#123;print $2&#125;&#x27;</span> ;<span class="hljs-keyword">done</span><br></code></pre></td></tr></table></figure><h3 id="获取指定RBD镜像的大小"><a href="#获取指定RBD镜像的大小" class="headerlink" title="获取指定RBD镜像的大小"></a>获取指定RBD镜像的大小</h3><p>查询 test1 的镜像大小</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ~]<span class="hljs-comment"># cat obj.txt |grep rbd_data.3ac16b8b4567|awk  &#x27;&#123; SUM += $6 &#125; END &#123; print SUM/1024/1024 &quot; MB&quot; &#125;&#x27;</span><br>4014.27 MB<br></code></pre></td></tr></table></figure><h3 id="将上面的汇总，使用脚本一次查询出所有的"><a href="#将上面的汇总，使用脚本一次查询出所有的" class="headerlink" title="将上面的汇总，使用脚本一次查询出所有的"></a>将上面的汇总，使用脚本一次查询出所有的</h3><h4 id="第一步获取："><a href="#第一步获取：" class="headerlink" title="第一步获取："></a>第一步获取：</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-keyword">for</span> obj <span class="hljs-keyword">in</span> `rados -p rbd <span class="hljs-built_in">ls</span>`;<span class="hljs-keyword">do</span> rados -p rbd <span class="hljs-built_in">stat</span> <span class="hljs-variable">$obj</span> &gt;&gt; obj.txt;<span class="hljs-keyword">done</span>;<br></code></pre></td></tr></table></figure><h4 id="第二步计算："><a href="#第二步计算：" class="headerlink" title="第二步计算："></a>第二步计算：</h4><p>创建一个获取的脚本getused.sh</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-meta">#! /bin/sh</span><br><span class="hljs-comment">##default pool name use rbd,you can change it </span><br><span class="hljs-comment">##default objfile is obj.txt,you can change it</span><br>objfile=obj.txt<br>Poolname=rbd<br><br><span class="hljs-keyword">for</span> image <span class="hljs-keyword">in</span> `rbd -p <span class="hljs-variable">$Poolname</span> <span class="hljs-built_in">ls</span>`<br><span class="hljs-keyword">do</span><br>Imagename=<span class="hljs-variable">$image</span><br>Prefix=`rbd  -p <span class="hljs-variable">$Poolname</span> info <span class="hljs-variable">$image</span>|grep prefix |awk <span class="hljs-string">&#x27;&#123;print $2&#125;&#x27;</span>`<br>Used=`<span class="hljs-built_in">cat</span> <span class="hljs-variable">$objfile</span> |grep <span class="hljs-variable">$Prefix</span>|awk <span class="hljs-string">&#x27;&#123; SUM += $6 &#125; END &#123; print SUM/1024/1024 &quot; MB&quot; &#125;&#x27;</span>`<br><span class="hljs-built_in">echo</span> <span class="hljs-variable">$Imagename</span> <span class="hljs-variable">$Prefix</span><br><span class="hljs-built_in">echo</span> Used: <span class="hljs-variable">$Used</span><br><span class="hljs-keyword">done</span><br></code></pre></td></tr></table></figure><h4 id="我的输出如下："><a href="#我的输出如下：" class="headerlink" title="我的输出如下："></a>我的输出如下：</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ~]<span class="hljs-comment"># sh getused.sh </span><br>test1 rbd_data.3ac16b8b4567<br>Used: 4014.27 MB<br>zp rbd_data.11f66b8b4567<br>Used: 828.844 MB<br></code></pre></td></tr></table></figure><blockquote><p>注意这里只统计了image里面的真实容量，如果是用了快速clone的,存在容量复用的问题，需要自己看是否需要统计那一部分的对象，方法同上</p></blockquote><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>对于已存在的系统，并且数据量很大的系统，不要频繁的去做请求，最好把统计请求，集中起来，并且就单线程的处理，慢一点不要紧，然后拉取到数据后，慢慢处理，这样能把影响降低到最少，可以在最不忙的时候去进行相关的操作</p><h2 id="变更记录"><a href="#变更记录" class="headerlink" title="变更记录"></a>变更记录</h2><table><thead><tr><th align="center">Why</th><th align="center">Who</th><th align="center">When</th></tr></thead><tbody><tr><td align="center">创建</td><td align="center">武汉-运维-磨渣</td><td align="center">2016-09-08</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Ceph中的Copyset概念和使用方法</title>
    <link href="/2016/09/06/Ceph%E4%B8%AD%E7%9A%84Copyset%E6%A6%82%E5%BF%B5%E5%92%8C%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95/"/>
    <url>/2016/09/06/Ceph%E4%B8%AD%E7%9A%84Copyset%E6%A6%82%E5%BF%B5%E5%92%8C%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>copyset运用好能带来什么好处</p><ul><li>降低故障情况下的数据丢失概率（增加可用性）</li><li>降低资源占用，从而降低负载</li></ul><h2 id="copyset的概念"><a href="#copyset的概念" class="headerlink" title="copyset的概念"></a>copyset的概念</h2><p>首先我们要理解copyset的概念，用通俗的话说就是，包含一个数据的所有副本的节点，也就是一个copyset损坏的情况下，数据就是全丢的<br><img src="/images/blog/o_200901064009radomcopy.png" alt="radomcopy"><br>如上图所示，这里的copyset就是：<br>{1,5,6}，{2,6,8} 两组</p><p>如果不做特殊的设置，那么基本上就是会随机的去分布<br><img src="/images/blog/o_200901064021radomall.png" alt="allcopy"></p><h3 id="最大copyset"><a href="#最大copyset" class="headerlink" title="最大copyset"></a>最大copyset</h3><p>如上图的所示，一般来说，最终组合将是一个最大的随机组合，比如这样的一个9个node随机组合3个的，这样的组合数有：<br>从 n个元素中取出  k个元素， k个元素的组合数量为：<br><img src="/images/blog/o_200901064541gongshi.png" alt="计算公式"><br>9个随机3个的组合为84<br>如果3个节点down掉，那么有数据丢失概率就是100%</p><h3 id="最小copyset"><a href="#最小copyset" class="headerlink" title="最小copyset"></a>最小copyset</h3><p>如果存在一种情况，分布是这样的<br><img src="/images/blog/o_200901064030mincopy.png" alt="mincopy"><br>那么copyset为<br>{1,5,7},{2,4,9},{3,6,8}<br>如果3个节点down掉,只有正好是上面的3种组合中的一种出现的时候，才会出现数据丢失<br>那么数据丢失的概率为 3&#x2F;84</p><p>最小copyset可能带来的不好的地方</p><ul><li>真出现丢失的时候（概率极低），丢失的数据量将是最大化的，这个是因为出现丢的时候，那么三个上面的组合配对为100%，其他情况不是100%</li><li>失效恢复时间将会增大一些，根据facebook的报告100GB的39节点的HDFS随机分布恢复时间在60s,最小分布为700s，这个是因为可用于恢复的点相对减少了，恢复时间自然长了</li></ul><h3 id="比较好的处理方式"><a href="#比较好的处理方式" class="headerlink" title="比较好的处理方式"></a>比较好的处理方式</h3><p>比较好的方式就是取copyset值为介于纯随机和最小之间的数，那么失效的概率计算方式就是：</p><blockquote><p>当前的copyset数目&#x2F;最大copyset</p></blockquote><h2 id="这个概念在ceph当中的实现"><a href="#这个概念在ceph当中的实现" class="headerlink" title="这个概念在ceph当中的实现"></a>这个概念在ceph当中的实现</h2><p>其实这个概念在ceph当中就是bucket的概念，PG为最小故障单元，PG就可以理解为上图当中的node上的元素，默认的分组方式为host，这个copyset就是全随机的在这些主机当中进行组合，我们在提升故障域为rack的时候，实际上就是将copyset进行了减少，一个rack之内的主机是形成不了copyset，这样down掉rack的时候，就不会数据丢失了，这个地方的实际可以做的控制方式有三种，下面将详细的介绍三种模式</p><h3 id="缩小最小主机单位"><a href="#缩小最小主机单位" class="headerlink" title="缩小最小主机单位"></a>缩小最小主机单位</h3><p><img src="/images/blog/o_200901064038hostzu.png" alt="最小主机组"><br>默认的为主机组，这样的主机间的copyset为<br>{1,2}，{1,3}，{1,4}，{2,3}，{2,4}，{3,4}<br>这样的有六组</p><p>现在我们对host进行一个合并看下<br><img src="/images/blog/o_200901064046hebing.png" alt="此处输入图片的描述"><br>注意这个地方并不是往上加了一层bucket，而是把最底层的host给拆掉了，加入一台机器有24个osd，那么这里的vhost1里面的osd个数实际是48个osd，那么当前的copyset为<br>{vhost1,vhost2}<br>copyset已经为上面默认情况的1&#x2F;6<br>这样会带来两个好处</p><ul><li>减少了copyset，减少的好处就见上面的分析</li><li>增加可接收恢复的osd数目，之前坏了一个osd的时候，能接收数据的osd为n-1,那么现在坏一个osd，可接收的osd为2n-1(n为单node上的osd个数)</li></ul><h3 id="增加分组"><a href="#增加分组" class="headerlink" title="增加分组"></a>增加分组</h3><p><img src="/images/blog/o_200901064053rackfenzu.png" alt="rack分组"><br>这个地方是增加了rack分组的，同一个rack里面不会出现copyset，那么当前的模式的copyset就是<br>{1,3}，{1,4}，{2,3}，{2,4}</p><p>同没有处理相比copyset为4&#x2F;6</p><h3 id="增加分组的情况进行PG分流"><a href="#增加分组的情况进行PG分流" class="headerlink" title="增加分组的情况进行PG分流"></a>增加分组的情况进行PG分流</h3><p><img src="/images/blog/o_200901064101zone.png" alt="zone"><br>这里看上去跟上面的分组很像，但是在做crush的时候是有区别的，上面的分组以后，会让PG分布在两个rack当中，这里的crush写的时候会让PG只在一个zone当中，在进入zone的下层再去进行分离主副PG，那么这种方式的copyset为<br>{1,2} {3,4}<br>为上面默认情况的2&#x2F;6</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>关于ceph中的ceph的copyset的三种模式已经总结完了，需要补充的是，上面的node都是一个虚拟的概念，你可以扩充为row，或者rack都行，这里只是说明了不同的处理方式，针对每个集群都可以有很多种组合，这个关键看自己怎么处理，减少copyset会明显的减低机器上的线程数目和资源的占用，这一点可以自行研制，从原理上来说少了很多配对的通信，crush的是非常灵活的一个分布控制，可以做很精细的控制，当然也会增加了维护的难度</p><h2 id="参考资料："><a href="#参考资料：" class="headerlink" title="参考资料："></a>参考资料：</h2><p><a href="https://www.ustack.com/blog/build-block-storage-service/">打造高性能高可靠块存储系统</a><br><a href="https://www.usenix.org/conference/atc13/technical-sessions/presentation/cidon">Copysets: Reducing the Frequency of Data Loss in Cloud Storage</a></p><h2 id="变更记录"><a href="#变更记录" class="headerlink" title="变更记录"></a>变更记录</h2><table><thead><tr><th align="center">Why</th><th align="center">Who</th><th align="center">When</th></tr></thead><tbody><tr><td align="center">创建</td><td align="center">武汉-运维-磨渣</td><td align="center">2016-09-06</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Proftp最简匿名访问配置</title>
    <link href="/2016/09/01/Proftp%E6%9C%80%E7%AE%80%E5%8C%BF%E5%90%8D%E8%AE%BF%E9%97%AE%E9%85%8D%E7%BD%AE/"/>
    <url>/2016/09/01/Proftp%E6%9C%80%E7%AE%80%E5%8C%BF%E5%90%8D%E8%AE%BF%E9%97%AE%E9%85%8D%E7%BD%AE/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>每一次做ftp的配置都要弄半天，找文档，各种权限控制的坑，折腾半天，这次还是准备记录下来，以备不时之需，这里不配置什么高级的功能，就去实现一个最简单的配置</p><blockquote><p>匿名用户的上传和下载</p></blockquote><h2 id="配置proftp过程"><a href="#配置proftp过程" class="headerlink" title="配置proftp过程"></a>配置proftp过程</h2><p> 配置过程尽量少的动原配置文件，需要共享的为&#x2F;share&#x2F;a目录，首先修改默认的目录</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">DefaultRoot                     ~ !adm<br></code></pre></td></tr></table></figure><p>修改为:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">DefaultRoot                     /share<br></code></pre></td></tr></table></figure><p>让默认的根目录为 &#x2F;share,默认的为用户的根目录，匿名用户对应的ftp用户的根目录</p><p>修改匿名用户的目录</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">&lt;Anonymous ~ftp&gt;<br></code></pre></td></tr></table></figure><p>修改为</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">&lt;Anonymous /share&gt;<br></code></pre></td></tr></table></figure><p>修改原匿名用户ftp的用户目录为&#x2F;share</p><p>修改默认屏蔽权限WRITE</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs raw">&lt;Limit WRITE SITE_CHMOD&gt;<br>  DenyAll<br>&lt;/Limit&gt;<br></code></pre></td></tr></table></figure><p>改成</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs raw">&lt;Limit  SITE_CHMOD&gt;<br>  DenyAll<br>&lt;/Limit&gt;<br></code></pre></td></tr></table></figure><p>默认会屏蔽掉写的操作，就没法上传了</p><p>配置访问的目录<br>默认启用了vroot，所以写路径的时候写相对路径即可，添加如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs raw">&lt;Directory &quot;/*&quot;&gt;<br>    AllowOverwrite          no<br>    &lt;Limit ALL&gt;<br>        DenyAll<br>    &lt;/Limit&gt;<br>    &lt;Limit DIRS&gt;<br>        AllowAll<br>    &lt;/Limit&gt;<br>&lt;/Directory&gt;<br>&lt;Directory &quot;/a&quot;&gt;<br>    AllowOverwrite          no<br>    &lt;Limit ALL&gt;<br>        AllowAll<br>    &lt;/Limit&gt;<br>&lt;/Directory&gt;<br></code></pre></td></tr></table></figure><p>&#x2F;a就代表的是&#x2F;share&#x2F;a</p><p>开启匿名<br>修改配置vim &#x2F;etc&#x2F;sysconfig&#x2F;proftpd</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">PROFTPD_OPTIONS=&quot;&quot;<br></code></pre></td></tr></table></figure><p>改成:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">PROFTPD_OPTIONS=&quot;-DANONYMOUS_FTP&quot;<br></code></pre></td></tr></table></figure><p>给目录访问权限</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs raw">chown ftp:ftp /share/a<br>chmod 755  /share/a<br></code></pre></td></tr></table></figure><p>启动proftp服务</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">systemctl restart proftpd<br></code></pre></td></tr></table></figure><h2 id="完整配置文件"><a href="#完整配置文件" class="headerlink" title="完整配置文件"></a>完整配置文件</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br></pre></td><td class="code"><pre><code class="hljs raw">ServerName&quot;ProFTPD server&quot;<br>ServerIdenton &quot;FTP Server ready.&quot;<br>ServerAdminroot@localhost<br>DefaultServeron<br>DefaultRoot~ !adm<br>AuthPAMConfigproftpd<br>AuthOrdermod_auth_pam.c* mod_auth_unix.c<br>UseReverseDNSoff<br>Usernobody<br>Groupnobody<br>MaxInstances20<br>UseSendfileoff<br>LogFormatdefault&quot;%h %l %u %t \&quot;%r\&quot; %s %b&quot;<br>LogFormatauth&quot;%v [%P] %h %t \&quot;%r\&quot; %s&quot;<br>LoadModule mod_ctrls_admin.c<br>LoadModule mod_vroot.c<br>ModuleControlsACLsinsmod,rmmod allow user root<br>ModuleControlsACLslsmod allow user *<br>ControlsEngineon<br>ControlsACLsall allow user root<br>ControlsSocketACLallow user *<br>ControlsLog/var/log/proftpd/controls.log<br>&lt;IfModule mod_ctrls_admin.c&gt;<br>  AdminControlsEngineon<br>  AdminControlsACLsall allow user root<br>&lt;/IfModule&gt;<br>&lt;IfModule mod_vroot.c&gt;<br>  VRootEngineon<br>&lt;/IfModule&gt;<br>&lt;IfDefine TLS&gt;<br>  TLSEngineon<br>  TLSRequiredon<br>  TLSRSACertificateFile/etc/pki/tls/certs/proftpd.pem<br>  TLSRSACertificateKeyFile/etc/pki/tls/certs/proftpd.pem<br>  TLSCipherSuiteALL:!ADH:!DES<br>  TLSOptionsNoCertRequest<br>  TLSVerifyClientoff<br>  TLSLog/var/log/proftpd/tls.log<br>  &lt;IfModule mod_tls_shmcache.c&gt;<br>    TLSSessionCacheshm:/file=/var/run/proftpd/sesscache<br>  &lt;/IfModule&gt;<br>&lt;/IfDefine&gt;<br>&lt;IfDefine DYNAMIC_BAN_LISTS&gt;<br>  LoadModulemod_ban.c<br>  BanEngineon<br>  BanLog/var/log/proftpd/ban.log<br>  BanTable/var/run/proftpd/ban.tab<br>  BanOnEventMaxLoginAttempts 2/00:10:00 01:00:00<br>  BanMessage&quot;Host %a has been banned&quot;<br>  BanControlsACLsall allow user ftpadm<br>&lt;/IfDefine&gt;<br>&lt;IfDefine QOS&gt;<br>  LoadModulemod_qos.c<br>  QoSOptionsdataqos throughput ctrlqos lowdelay<br>&lt;/IfDefine&gt;<br>&lt;Global&gt;<br>  Umask022<br>  AllowOverwriteyes<br>  &lt;Limit ALL SITE_CHMOD&gt;<br>    AllowAll<br>  &lt;/Limit&gt;<br>&lt;/Global&gt;<br>&lt;IfDefine ANONYMOUS_FTP&gt;<br>  &lt;Anonymous /share/&gt;<br>    Userftp<br>    Groupftp<br>    AccessGrantMsg&quot;Anonymous login ok, restrictions apply.&quot;<br>    UserAliasanonymous ftp<br>    MaxClients10 &quot;Sorry, max %m users -- try again later&quot;<br>    DisplayLogin/welcome.msg<br>    DisplayChdir.message<br>    DisplayReadmeREADME*<br>    DirFakeUseron ftp<br>    DirFakeGroupon ftp<br>    &lt;Limit  SITE_CHMOD&gt;<br>      DenyAll<br>    &lt;/Limit&gt;<br>    &lt;IfModule mod_vroot.c&gt;<br>       &lt;Directory &quot;/*&quot;&gt;<br>       AllowOverwrite          no<br>        &lt;Limit ALL&gt;<br>        DenyAll<br>        &lt;/Limit&gt;<br>        &lt;Limit DIRS&gt;<br>        AllowAll<br>        &lt;/Limit&gt;<br>       &lt;/Directory&gt;<br>       &lt;Directory &quot;/a&quot;&gt;<br>              AllowOverwrite          no<br>        &lt;Limit ALL&gt;<br>          AllowAll<br>        &lt;/Limit&gt;<br>       &lt;/Directory&gt;<br>    &lt;/IfModule&gt;<br>    WtmpLogoff<br>    ExtendedLog/var/log/proftpd/access.log WRITE,READ default<br>    ExtendedLog/var/log/proftpd/auth.log AUTH auth<br>  &lt;/Anonymous&gt;<br>&lt;/IfDefine&gt;<br></code></pre></td></tr></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>最简配置就完成了，也可以根据需要再去做更复杂的配置，这里就不做过多的介绍，比较容易错误的点就是容易出现权限问题无法访问，或者是上下的设置关联错误，可以开启调试模式进行调试</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">proftpd   -n -d 10 -c /etc/proftpd.conf -DANONYMOUS_FTP<br></code></pre></td></tr></table></figure><h2 id="变更记录"><a href="#变更记录" class="headerlink" title="变更记录"></a>变更记录</h2><table><thead><tr><th align="center">Why</th><th align="center">Who</th><th align="center">When</th></tr></thead><tbody><tr><td align="center">创建</td><td align="center">武汉-运维-磨渣</td><td align="center">2016-09-01</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>重写ceph-lazy</title>
    <link href="/2016/08/28/%E9%87%8D%E5%86%99ceph-lazy/"/>
    <url>/2016/08/28/%E9%87%8D%E5%86%99ceph-lazy/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>这个工具最开始是从sebastien的blog里面看到的，这个是 <a href="https://github.com/gcharot/">Gregory Charot</a>（工具的作者）写的，通常我们在获取一个ceph的信息的时候，需要敲一连串的命令去获得自己需要的信息，可能需要一大堆的解析才能完成，而经常出现的是，使用了后，下次使用的时候，又要重来一遍，所以作者把这些常用的操作做了一些归纳，形成了一个查询的工具，很多人有个相同的观点就是，越懒，就会想办法提高效率，当然，首先得有提高效率的意识，否则只剩下懒了</p><p>我做的事情就是把作者用shell的逻辑转换成了python的版本，这样也方便自己以后的扩展，这里感谢作者做的一些工作，让我很快就能完成了，这里并不是重复造车轮，本来自己就不会python，权当练手了</p><p>在linux下面我是不建议用中文的，但是这个工具里面还是改成用中文提示，因为中文可能看上去更清楚需要做的是一个什么事情，这个仅仅是一个查询工具</p><p>有一段时间没有更新blog了，主要是最近比较忙，没有时间去看太多的资料，没有时间来写下更多的东西，有时间还是会坚持写下去</p><h2 id="项目地址"><a href="#项目地址" class="headerlink" title="项目地址"></a>项目地址</h2><p>原作者项目地址：<a href="https://github.com/gcharot/ceph-lazy">https://github.com/gcharot/ceph-lazy</a><br>我重写的地址：<a href="https://github.com/zphj1987/ceph-lazy/tree/lazy-python">https://github.com/zphj1987/ceph-lazy/tree/lazy-python</a></p><h3 id="安装方法"><a href="#安装方法" class="headerlink" title="安装方法"></a>安装方法</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs raw">wget -O /sbin/ceph-lazy https://raw.githubusercontent.com/zphj1987/ceph-lazy/lazy-python/ceph-lazy.py<br>chmod 777 /sbin/ceph-lazy<br></code></pre></td></tr></table></figure><h3 id="详细使用说明"><a href="#详细使用说明" class="headerlink" title="详细使用说明"></a>详细使用说明</h3><h4 id="列出节点上的所有的OSD"><a href="#列出节点上的所有的OSD" class="headerlink" title="列出节点上的所有的OSD"></a>列出节点上的所有的OSD</h4><p>命令：ceph-lazy host-get-osd {hostname}</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]# ceph-lazy host-get-osd lab8106<br>osd.0 <br>osd.1<br></code></pre></td></tr></table></figure><h4 id="列出所有的存储主机节点"><a href="#列出所有的存储主机节点" class="headerlink" title="列出所有的存储主机节点"></a>列出所有的存储主机节点</h4><p>命令：ceph-lazy host-get-nodes </p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]# ceph-lazy host-get-nodes <br>lab8106<br>lab8107<br></code></pre></td></tr></table></figure><h4 id="列出存储节点上的存储使用的情况-detail看详细信息"><a href="#列出存储节点上的存储使用的情况-detail看详细信息" class="headerlink" title="列出存储节点上的存储使用的情况(detail看详细信息)"></a>列出存储节点上的存储使用的情况(detail看详细信息)</h4><p>命令：ceph-lazy host-osd-usage {hostname} {detail}</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ceph-lazy]# ceph-lazy  host-osd-usage lab8106<br>Host:lab8106 | OSDs:2 | Total_Size:556.5GB | Total_Used:13.0GB | Total_Available:543.5GB<br>[root@lab8106 ceph-lazy]# ceph-lazy  host-osd-usage lab8106 detail<br>OSD:0 | Size:278.3GB | Used:4.6GB | Available:273.6GB<br>OSD:1 | Size:278.3GB | Used:8.4GB | Available:269.8GB<br>Host:lab8106 | OSDs:2 | Total_Size:556.5GB | Total_Used:13.0GB | Total_Available:543.5GB<br></code></pre></td></tr></table></figure><h4 id="列出所有存储节点上的存储使用的情况-detail看详细信息"><a href="#列出所有存储节点上的存储使用的情况-detail看详细信息" class="headerlink" title="列出所有存储节点上的存储使用的情况(detail看详细信息)"></a>列出所有存储节点上的存储使用的情况(detail看详细信息)</h4><p>命令：ceph-lazy host-all-usage {detail}</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ceph-lazy]# ceph-lazy host-all-usage<br>----------------------------------------------<br>Host:lab8106 | OSDs:2 | Total_Size:556.5GB | Total_Used:13.0GB | Total_Available:543.5GB<br>----------------------------------------------<br>Host:lab8107 | OSDs:1 | Total_Size:278.3GB | Total_Used:3.8GB | Total_Available:274.4GB<br><br>[root@lab8106 ceph-lazy]# ceph-lazy host-all-usage detail<br>----------------------------------------------<br>OSD:0 | Size:278.3GB | Used:4.6GB | Available:273.6GB<br>OSD:1 | Size:278.3GB | Used:8.4GB | Available:269.8GB<br>Host:lab8106 | OSDs:2 | Total_Size:556.5GB | Total_Used:13.0GB | Total_Available:543.5GB<br>----------------------------------------------<br>OSD:2 | Size:278.3GB | Used:3.8GB | Available:274.4GB<br>Host:lab8107 | OSDs:1 | Total_Size:278.3GB | Total_Used:3.8GB | Total_Available:274.4GB<br></code></pre></td></tr></table></figure><h4 id="列出PG所在的节点-first-is-primary"><a href="#列出PG所在的节点-first-is-primary" class="headerlink" title="列出PG所在的节点(first is primary)"></a>列出PG所在的节点(first is primary)</h4><p>命令： ceph-lazy pg-get-host {pg_id}</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ceph-lazy]# ceph-lazy pg-get-host   10.2<br>OSD:osd.2 | Host :lab8107<br>OSD:osd.1 | Host :lab8106<br></code></pre></td></tr></table></figure><h4 id="列出写操作最多的PG-operations-number"><a href="#列出写操作最多的PG-operations-number" class="headerlink" title="列出写操作最多的PG ( operations number)"></a>列出写操作最多的PG ( operations number)</h4><p>命令：ceph-lazy pg-most-write</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ceph-lazy]# ceph-lazy pg-most-write<br>PG:10.3 | OSD:osd.1 | Host:lab8106<br></code></pre></td></tr></table></figure><h4 id="列出写操作最少的PG-operations-number"><a href="#列出写操作最少的PG-operations-number" class="headerlink" title="列出写操作最少的PG ( operations number)"></a>列出写操作最少的PG ( operations number)</h4><p>命令：ceph-lazy pg-less-write</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ceph-lazy]# ceph-lazy pg-less-write<br>PG:11.3 | OSD:osd.1 | Host:lab8106<br></code></pre></td></tr></table></figure><h4 id="列出写操作最多的PG-data-written"><a href="#列出写操作最多的PG-data-written" class="headerlink" title="列出写操作最多的PG (data written)"></a>列出写操作最多的PG (data written)</h4><p>命令：ceph-lazy pg-most-write-kb</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]# ceph-lazy pg-most-write-kb<br>PG:10.0 | OSD:osd.1 | Host:lab8106<br></code></pre></td></tr></table></figure><p>####列出写操作最少的PG (data written)<br>命令：ceph-lazy pg-less-write-kb</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]# ceph-lazy pg-less-write-kb<br>PG:11.3 | OSD:osd.1 | Host:lab8106<br></code></pre></td></tr></table></figure><h4 id="列出读操作最多的PG-operations-number"><a href="#列出读操作最多的PG-operations-number" class="headerlink" title="列出读操作最多的PG (operations number)"></a>列出读操作最多的PG (operations number)</h4><p>命令：ceph-lazy pg-most-read</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]# ceph-lazy pg-most-read<br>PG:10.1 | OSD:osd.0 | Host:lab8106<br></code></pre></td></tr></table></figure><h4 id="列出读操作最少的PG-operations-number"><a href="#列出读操作最少的PG-operations-number" class="headerlink" title="列出读操作最少的PG (operations number)"></a>列出读操作最少的PG (operations number)</h4><p>命令：ceph-lazy pg-less-read</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]# ceph-lazy pg-less-read<br>PG:11.3 | OSD:osd.1 | Host:lab8106<br></code></pre></td></tr></table></figure><h4 id="列出读操作最多的PG-data-read"><a href="#列出读操作最多的PG-data-read" class="headerlink" title="列出读操作最多的PG (data read)"></a>列出读操作最多的PG (data read)</h4><p>命令：ceph-lazy pg-most-read-kb</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]# ceph-lazy pg-most-read-kb<br>PG:10.4 | OSD:osd.0 | Host:lab8106<br></code></pre></td></tr></table></figure><h4 id="列出读操作最少的PG-data-read"><a href="#列出读操作最少的PG-data-read" class="headerlink" title="列出读操作最少的PG (data read)"></a>列出读操作最少的PG (data read)</h4><p>命令：ceph-lazy pg-less-read-kb</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]# ceph-lazy pg-less-read-kb<br>PG:11.3 | OSD:osd.1 | Host:lab8106<br></code></pre></td></tr></table></figure><h4 id="列出空的PG-没有存储对象"><a href="#列出空的PG-没有存储对象" class="headerlink" title="列出空的PG (没有存储对象)"></a>列出空的PG (没有存储对象)</h4><p>命令：ceph-lazy pg-empty</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]# ceph-lazy pg-empty<br>11.3<br>11.2<br>11.1<br>11.0<br>11.7<br>11.6<br>11.5<br>11.4<br></code></pre></td></tr></table></figure><h4 id="列出RBD的prefix"><a href="#列出RBD的prefix" class="headerlink" title="列出RBD的prefix"></a>列出RBD的prefix</h4><p>命令：ceph-lazy rbd-prefix {poolname} {imgname}</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]# ceph-lazy rbd-prefix rbd zp<br>rbd_data.1b93a6b8b4567<br></code></pre></td></tr></table></figure><h4 id="列出RBD的对象数目"><a href="#列出RBD的对象数目" class="headerlink" title="列出RBD的对象数目"></a>列出RBD的对象数目</h4><p>命令：ceph-lazy rbd-count {poolname} {imgname}</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]# ceph-lazy rbd-count rbd zp<br><br>    RBD image rbd/zp has prefix rbd_data.1b93a6b8b4567; now couning objects...<br>    count: 27<br></code></pre></td></tr></table></figure><h4 id="列出RBD的Primary所在的存储主机"><a href="#列出RBD的Primary所在的存储主机" class="headerlink" title="列出RBD的Primary所在的存储主机"></a>列出RBD的Primary所在的存储主机</h4><p>命令：ceph-lazy rbd-host {poolname} {imgname}</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]# ceph-lazy rbd-host rbd zp<br>Primary Host: lab8107<br>Primary Host: lab8106<br></code></pre></td></tr></table></figure><h4 id="列出RBD的Primary所在的OSD节点"><a href="#列出RBD的Primary所在的OSD节点" class="headerlink" title="列出RBD的Primary所在的OSD节点"></a>列出RBD的Primary所在的OSD节点</h4><p>命令：ceph-lazy rbd-osd {poolname} {imgname}</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]# ceph-lazy rbd-osd rbd zp<br>Primary Osd: 0<br>Primary Osd: 1<br>Primary Osd: 2<br></code></pre></td></tr></table></figure><h4 id="列出RBD的Image的真实大小"><a href="#列出RBD的Image的真实大小" class="headerlink" title="列出RBD的Image的真实大小"></a>列出RBD的Image的真实大小</h4><p>命令：ceph-lazy rbd-size rbd zp</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]# ceph-lazy rbd-size rbd zp<br>Pool: rbd | Image:zp | Real_size:71.5586 MB<br></code></pre></td></tr></table></figure><h4 id="列出容量使用最多的OSD"><a href="#列出容量使用最多的OSD" class="headerlink" title="列出容量使用最多的OSD"></a>列出容量使用最多的OSD</h4><p>命令：ceph-lazy osd-most-used</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]# ceph-lazy osd-most-used<br>OSD:osd.1 | Host: lab8106 | Used: 8 GB<br></code></pre></td></tr></table></figure><h4 id="列出容量使用最少的OSD"><a href="#列出容量使用最少的OSD" class="headerlink" title="列出容量使用最少的OSD"></a>列出容量使用最少的OSD</h4><p>命令：ceph-lazy osd-less-used</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]# ceph-lazy osd-less-used<br>OSD:osd.2 | Host: lab8107 | Used: 3 GB<br></code></pre></td></tr></table></figure><h4 id="列出指定OSD上所有的primary-PG"><a href="#列出指定OSD上所有的primary-PG" class="headerlink" title="列出指定OSD上所有的primary PG"></a>列出指定OSD上所有的primary PG</h4><p>命令： ceph-lazy osd-get-ppg {osd_id}</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]# ceph-lazy osd-get-ppg 1<br>11.3<br>10.3<br>10.0<br>11.7<br>10.6<br>11.6<br>10.7<br>11.5<br></code></pre></td></tr></table></figure><h4 id="列出指定OSD上的所有PG"><a href="#列出指定OSD上的所有PG" class="headerlink" title="列出指定OSD上的所有PG"></a>列出指定OSD上的所有PG</h4><p>命令：ceph-lazy osd-get-pg {osd_id}</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]# ceph-lazy osd-get-pg 1<br>11.3<br>10.2<br>10.3<br>10.0<br>10.1<br>11.7<br>10.6<br>11.6<br>10.7<br>11.5<br>10.4<br>10.5<br></code></pre></td></tr></table></figure><h4 id="列出指定对象所在的主机（第一个是主）"><a href="#列出指定对象所在的主机（第一个是主）" class="headerlink" title="列出指定对象所在的主机（第一个是主）"></a>列出指定对象所在的主机（第一个是主）</h4><p>命令：ceph-lazy object-get-host   {poolname} {obj_name}</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]# ceph-lazy object-get-host   rbd rbd_data.1b93a6b8b4567.00000000000000a0<br>Pg: 10.4<br>OSD:osd.0 | Host :lab8106<br>OSD:osd.1 | Host :lab8106<br></code></pre></td></tr></table></figure><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>本篇只是暂时结束了，目前完成了原作者的一些想法，等有空再写点自己比较注重的数据</p><p>最近一直在关注冯大辉的事情，看完后还是原来的感觉，在利益面前，公司总是会追求最大化，当出现分离的时候，总会显得无情，还是自己让自己强大一点，拿到属于自己的那一部分就好</p><h3 id="变更记录"><a href="#变更记录" class="headerlink" title="变更记录"></a>变更记录</h3><table><thead><tr><th align="center">Why</th><th align="center">Who</th><th align="center">When</th></tr></thead><tbody><tr><td align="center">创建</td><td align="center">武汉-运维-磨渣</td><td align="center">2016-08-19</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Linux配置邮件发送信息</title>
    <link href="/2016/08/19/Linux%E9%85%8D%E7%BD%AE%E9%82%AE%E4%BB%B6%E5%8F%91%E9%80%81%E4%BF%A1%E6%81%AF/"/>
    <url>/2016/08/19/Linux%E9%85%8D%E7%BD%AE%E9%82%AE%E4%BB%B6%E5%8F%91%E9%80%81%E4%BF%A1%E6%81%AF/</url>
    
    <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>一般情况下，我们的IT系统都会有相关的告警的处理，有的是邮件，有的是短信，这些都能很方便的获得一些有用的信息<br>在某些时候我们没有这样的系统，而自己又需要定期的获取一些信息的时候，配置一个邮件发送是很有用的</p><!--break--><h2 id="配置方法"><a href="#配置方法" class="headerlink" title="配置方法"></a>配置方法</h2><p>网上的大部分的方法使用的是sendmail的发送方法，这个地方我们只需要简单的发送邮件的需求，可以直接配置SMTP发送的模式</p><h3 id="修改配置文件，填写发送的相关信息"><a href="#修改配置文件，填写发送的相关信息" class="headerlink" title="修改配置文件，填写发送的相关信息"></a>修改配置文件，填写发送的相关信息</h3><p>修改配置文件 <code>/etc/mail.rc</code><br>在最下面添加发送邮箱的信息</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs raw">set from=test@sina.com smtp=smtp.sina.com<br>set smtp-auth-user=test@sina.com smtp-auth-password=test123456 smtp-auth=login<br></code></pre></td></tr></table></figure><h3 id="编写一个发送的脚本"><a href="#编写一个发送的脚本" class="headerlink" title="编写一个发送的脚本"></a>编写一个发送的脚本</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs raw">vim /root/sendmail.sh <br>#! /bin/sh<br>timeout 20 date &gt; /tmp/mail<br>timeout 20 ceph -s &gt;&gt; /tmp/mail<br>timeout 600 mail -s &quot;cephstatus-`date`&quot; zbkc2016@sina.com &lt; /tmp/mail<br></code></pre></td></tr></table></figure><h3 id="在crontab中添加定期执行"><a href="#在crontab中添加定期执行" class="headerlink" title="在crontab中添加定期执行"></a>在crontab中添加定期执行</h3><p>修改crontab配置文件</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs raw">vim crontab<br>*/5 * * * *  root  sh /root/sendmail.sh  2&gt;&amp;1  &gt; /dev/null<br></code></pre></td></tr></table></figure><p>让crontab服务生效</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs raw">crontab crontab<br>/etc/init.d/crontab restart<br></code></pre></td></tr></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>这个东西很简单，不过自己真去配置的时候，还是找半天资料，还是自己写好文档，方便以后使用，最快最简单的实现需求</p><h2 id="变更记录"><a href="#变更记录" class="headerlink" title="变更记录"></a>变更记录</h2><table><thead><tr><th align="center">Why</th><th align="center">Who</th><th align="center">When</th></tr></thead><tbody><tr><td align="center">创建</td><td align="center">武汉-运维-磨渣</td><td align="center">2016-08-19</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Ceph 状态报警告 pool rbd has many more objects per pg than average (too few pgs?)</title>
    <link href="/2016/07/27/Ceph%20%E7%8A%B6%E6%80%81%E6%8A%A5%E8%AD%A6%E5%91%8A%20pool%20rbd%20has%20many%20more%20objects%20per%20pg%20than%20average%20(too%20few%20pgs?)/"/>
    <url>/2016/07/27/Ceph%20%E7%8A%B6%E6%80%81%E6%8A%A5%E8%AD%A6%E5%91%8A%20pool%20rbd%20has%20many%20more%20objects%20per%20pg%20than%20average%20(too%20few%20pgs?)/</url>
    
    <content type="html"><![CDATA[<h2 id="定位问题"><a href="#定位问题" class="headerlink" title="定位问题"></a>定位问题</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]# ceph -s<br>    cluster fa7ec1a1-662a-4ba3-b478-7cb570482b62<br>     health HEALTH_WARN<br>            pool rbd has many more objects per pg than average (too few pgs?)<br>     monmap e1: 1 mons at &#123;lab8106=192.168.8.106:6789/0&#125;<br>            election epoch 30, quorum 0 lab8106<br>     osdmap e157: 2 osds: 2 up, 2 in<br>            flags sortbitwise<br>      pgmap v1023: 417 pgs, 13 pools, 18519 MB data, 15920 objects<br>            18668 MB used, 538 GB / 556 GB avail<br>                 417 active+clean<br></code></pre></td></tr></table></figure><p>集群出现了这个警告，pool rbd has many more objects per pg than average (too few pgs?) 这个警告在hammer版本里面的提示是 pool rbd has too few pgs </p><!--break--><p>这个地方查看集群详细信息：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]# ceph health detail<br>HEALTH_WARN pool rbd has many more objects per pg than average (too few pgs?); mon.lab8106 low disk space<br>pool rbd objects per pg (1912) is more than 50.3158 times cluster average (38)<br></code></pre></td></tr></table></figure><p>看下集群的pool的对象状态</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]# ceph df<br>GLOBAL:<br>    SIZE     AVAIL     RAW USED     %RAW USED <br>    556G      538G       18668M          3.28 <br>POOLS:<br>    NAME       ID     USED       %USED     MAX AVAIL     OBJECTS <br>    rbd        6      16071M      2.82          536G       15296 <br>    pool1      7        204M      0.04          536G          52 <br>    pool2      8        184M      0.03          536G          47 <br>    pool3      9        188M      0.03          536G          48 <br>    pool4      10       192M      0.03          536G          49 <br>    pool5      11       204M      0.04          536G          52 <br>    pool6      12       148M      0.03          536G          38 <br>    pool7      13       184M      0.03          536G          47 <br>    pool8      14       200M      0.04          536G          51 <br>    pool9      15       200M      0.04          536G          51 <br>    pool10     16       248M      0.04          536G          63 <br>    pool11     17       232M      0.04          536G          59 <br>    pool12     18       264M      0.05          536G          67<br></code></pre></td></tr></table></figure><p>查看存储池的pg个数</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]# ceph osd dump|grep pool<br>pool 6 &#x27;rbd&#x27; replicated size 1 min_size 1 crush_ruleset 0 object_hash rjenkins pg_num 8 pgp_num 8 last_change 132 flags hashpspool stripe_width 0<br>pool 7 &#x27;pool1&#x27; replicated size 1 min_size 1 crush_ruleset 0 object_hash rjenkins pg_num 1 pgp_num 1 last_change 134 flags hashpspool stripe_width 0<br>pool 8 &#x27;pool2&#x27; replicated size 1 min_size 1 crush_ruleset 0 object_hash rjenkins pg_num 1 pgp_num 1 last_change 136 flags hashpspool stripe_width 0<br>pool 9 &#x27;pool3&#x27; replicated size 1 min_size 1 crush_ruleset 0 object_hash rjenkins pg_num 1 pgp_num 1 last_change 138 flags hashpspool stripe_width 0<br>pool 10 &#x27;pool4&#x27; replicated size 1 min_size 1 crush_ruleset 0 object_hash rjenkins pg_num 1 pgp_num 1 last_change 140 flags hashpspool stripe_width 0<br>pool 11 &#x27;pool5&#x27; replicated size 1 min_size 1 crush_ruleset 0 object_hash rjenkins pg_num 1 pgp_num 1 last_change 142 flags hashpspool stripe_width 0<br>pool 12 &#x27;pool6&#x27; replicated size 1 min_size 1 crush_ruleset 0 object_hash rjenkins pg_num 1 pgp_num 1 last_change 144 flags hashpspool stripe_width 0<br>pool 13 &#x27;pool7&#x27; replicated size 1 min_size 1 crush_ruleset 0 object_hash rjenkins pg_num 1 pgp_num 1 last_change 146 flags hashpspool stripe_width 0<br>pool 14 &#x27;pool8&#x27; replicated size 1 min_size 1 crush_ruleset 0 object_hash rjenkins pg_num 1 pgp_num 1 last_change 148 flags hashpspool stripe_width 0<br>pool 15 &#x27;pool9&#x27; replicated size 1 min_size 1 crush_ruleset 0 object_hash rjenkins pg_num 1 pgp_num 1 last_change 150 flags hashpspool stripe_width 0<br>pool 16 &#x27;pool10&#x27; replicated size 1 min_size 1 crush_ruleset 0 object_hash rjenkins pg_num 100 pgp_num 100 last_change 152 flags hashpspool stripe_width 0<br>pool 17 &#x27;pool11&#x27; replicated size 1 min_size 1 crush_ruleset 0 object_hash rjenkins pg_num 100 pgp_num 100 last_change 154 flags hashpspool stripe_width 0<br>pool 18 &#x27;pool12&#x27; replicated size 1 min_size 1 crush_ruleset 0 object_hash rjenkins pg_num 200 pgp_num 200 last_change 156 flags hashpspool stripe_width 0<br></code></pre></td></tr></table></figure><p>我们看下这个是怎么得到的</p><blockquote><p>pool rbd objects per pg (1912) is more than 50.3158 times cluster average (38)<br><br>rbd objects_per_pg &#x3D; 15296 &#x2F; 8 &#x3D; 1912<br><br>objects_per_pg &#x3D; 15920 &#x2F;417  ≈ 38<br><br>50.3158 &#x3D;  rbd objects_per_pg &#x2F; objects_per_pg &#x3D;  1912 &#x2F; 38 </p></blockquote><p>也就是出现其他pool的对象太少，而这个pg少，对象多，就会提示这个了，我们看下代码里面的判断</p><p><a href="https://github.com/ceph/ceph/blob/master/src/mon/PGMonitor.cc">https://github.com/ceph/ceph/blob/master/src/mon/PGMonitor.cc</a></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs raw">int average_objects_per_pg = pg_map.pg_sum.stats.sum.num_objects / pg_map.pg_stat.size();<br>     if (average_objects_per_pg &gt; 0 &amp;&amp;<br>         pg_map.pg_sum.stats.sum.num_objects &gt;= g_conf-&gt;mon_pg_warn_min_objects &amp;&amp;<br>         p-&gt;second.stats.sum.num_objects &gt;= g_conf-&gt;mon_pg_warn_min_pool_objects) &#123;<br>int objects_per_pg = p-&gt;second.stats.sum.num_objects / pi-&gt;get_pg_num();<br>float ratio = (float)objects_per_pg / (float)average_objects_per_pg;<br>if (g_conf-&gt;mon_pg_warn_max_object_skew &gt; 0 &amp;&amp;<br>    ratio &gt; g_conf-&gt;mon_pg_warn_max_object_skew) &#123;<br>  ostringstream ss;<br>  ss &lt;&lt; &quot;pool &quot; &lt;&lt; name &lt;&lt; &quot; has many more objects per pg than average (too few pgs?)&quot;;<br>  summary.push_back(make_pair(HEALTH_WARN, ss.str()));<br>  if (detail) &#123;<br>    ostringstream ss;<br>    ss &lt;&lt; &quot;pool &quot; &lt;&lt; name &lt;&lt; &quot; objects per pg (&quot;<br>       &lt;&lt; objects_per_pg &lt;&lt; &quot;) is more than &quot; &lt;&lt; ratio &lt;&lt; &quot; times cluster average (&quot;<br>       &lt;&lt; average_objects_per_pg &lt;&lt; &quot;)&quot;;<br>    detail-&gt;push_back(make_pair(HEALTH_WARN, ss.str()));<br>  &#125;<br></code></pre></td></tr></table></figure><p>主要下面的几个限制条件</p><blockquote><p>mon_pg_warn_min_objects &#x3D; 10000   &#x2F;&#x2F;总的对象超过10000<br><br>mon_pg_warn_min_pool_objects &#x3D; 1000     &#x2F;&#x2F;存储池对象超过1000<br><br>mon_pg_warn_max_object_skew &#x3D; 10        &#x2F;&#x2F;就是上面的存储池的平均对象与所有pg的平均值的倍数关系</p></blockquote><h2 id="解决问题"><a href="#解决问题" class="headerlink" title="解决问题"></a>解决问题</h2><p>有三个方法解决这个警告的提示：</p><ul><li><p>删除无用的存储池<br> 如果集群中有一些不用的存储池，并且相对的pg数目还比较高，那么可以删除一些这样的存储池，从而降低<code>mon_pg_warn_max_object_skew</code>这个值，警告就会没有了</p></li><li><p>增加提示的pool的pg数目<br> 有可能的情况就是，这个存储池的pg数目从一开始就不够，增加pg和pgp数目，同样降低了<code>mon_pg_warn_max_object_skew</code>这个值了</p></li><li><p>增加<code>mon_pg_warn_max_object_skew</code>的参数值<br>如果集群里面已经有足够多的pg了，再增加pg会不稳定，如果想去掉这个警告，就可以增加这个参数值，默认为10</p></li></ul><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>这个警告是比较的是存储池中的对象数目与整个集群的pg的平均对象数目的偏差，如果偏差太大就会发出警告</p><p>检查的步骤：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs raw">ceph health detail<br>ceph df<br>ceph osd dump | grep pool<br></code></pre></td></tr></table></figure><blockquote><p>mon_pg_warn_max_object_skew &#x3D; 10.0</p></blockquote><p>((objects&#x2F;pg_num) in the affected pool)&#x2F;(objects&#x2F;pg_num in the entire system) &gt;&#x3D; 10.0 警告就会出现</p><h2 id="变更记录"><a href="#变更记录" class="headerlink" title="变更记录"></a>变更记录</h2><table><thead><tr><th align="center">Why</th><th align="center">Who</th><th align="center">When</th></tr></thead><tbody><tr><td align="center">创建</td><td align="center">武汉-运维-磨渣</td><td align="center">2016-07-27</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>如何替换Ceph的Journal</title>
    <link href="/2016/07/26/%E5%A6%82%E4%BD%95%E6%9B%BF%E6%8D%A2Ceph%E7%9A%84Journal/"/>
    <url>/2016/07/26/%E5%A6%82%E4%BD%95%E6%9B%BF%E6%8D%A2Ceph%E7%9A%84Journal/</url>
    
    <content type="html"><![CDATA[<p>很多人会提出这样的问题：</p><ul><li>能不能够将 Ceph journal 分区从一个磁盘替换到另一个磁盘？</li><li>怎样替换 Ceph 的 journal 分区？<!--break--></li></ul><p>有两种方法来修改Ceph的journal：</p><ul><li>创建一个journal分区，在上面创建一个新的journal</li><li>转移已经存在的journal分区到新的分区上，这个适合整盘替换</li></ul><blockquote><p>Ceph 的journal是基于事务的日志，所以正确的下刷journal数据，然后重新创建journal并不会引起数据丢失，因为在下刷journal的数据的时候，osd是停止的，一旦数据下刷后，这个journal是不会再有新的脏数据进来的</p></blockquote><h2 id="第一种方法"><a href="#第一种方法" class="headerlink" title="第一种方法"></a>第一种方法</h2><p>在开始处理前，最开始要设置OSD状态为<code>noout</code></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]# ceph osd set noout<br>set noout<br></code></pre></td></tr></table></figure><p>停止需要替换journal的osd(这里是osd.1)</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]# systemctl stop ceph-osd@1<br></code></pre></td></tr></table></figure><blockquote><p>我的版本是jewel的，如果是hammer版本，就使用 &#x2F;etc&#x2F;init.d&#x2F;ceph stop osd.1</p></blockquote><p>下刷journal到osd，使用 -i 指定需要替换journal的 osd的编号</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]# ceph-osd -i 1 --flush-journal<br>SG_IO: bad/missing sense data, sb[]:  70 00 05 00 00 00 00 0a 00 00 00 00 20 00 01 cf 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00<br>SG_IO: bad/missing sense data, sb[]:  70 00 05 00 00 00 00 0a 00 00 00 00 20 00 01 cf 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00<br>2016-07-26 22:47:20.185292 7fc54a6c3800 -1 flushed journal /var/lib/ceph/osd/ceph-1/journal for object store /var/lib/ceph/osd/ceph-1<br></code></pre></td></tr></table></figure><h3 id="创建一个新的journal"><a href="#创建一个新的journal" class="headerlink" title="创建一个新的journal"></a>创建一个新的journal</h3><p>删除原来的journal</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]# ll /var/lib/ceph/osd/ceph-1/journal<br>lrwxrwxrwx 1 ceph ceph 58 Jul 25 09:25 /var/lib/ceph/osd/ceph-1/journal -&gt; /dev/disk/by-partuuid/872f8b40-a750-4be3-9150-033b990553f7<br>[root@lab8106 ~]# rm -rf /var/lib/ceph/osd/ceph-1/journal<br></code></pre></td></tr></table></figure><p>准备一个新的分区</p><p>我的环境准备使用&#x2F;dev&#x2F;sdd1,分区大小为10G，这个注意磁盘大小比参数设置的要大一点即可</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]# ls -l /dev/disk/by-partuuid/<br>total 0<br>lrwxrwxrwx 1 root root 10 Jul 25 14:25 4766ce93-a476-4e97-9aac-894d461b367e -&gt; ../../sdb2<br>lrwxrwxrwx 1 root root 10 Jul 26 22:51 5bb48687-6be6-4aef-82f6-5af822c3fad8 -&gt; ../../sdd1<br>lrwxrwxrwx 1 root root 10 Jul 26 22:47 872f8b40-a750-4be3-9150-033b990553f7 -&gt; ../../sdc2<br></code></pre></td></tr></table></figure><p>我的新的journal的uuid的路径为 &#x2F;dev&#x2F;disk&#x2F;by-partuuid&#x2F;5bb48687-6be6-4aef-82f6-5af822c3fad8</p><p>将这个磁盘的分区链接到原始路径</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]# ln -s /dev/disk/by-partuuid/5bb48687-6be6-4aef-82f6-5af822c3fad8 /var/lib/ceph/osd/ceph-1/journal<br>[root@lab8106 ~]# chown ceph:ceph /var/lib/ceph/osd/ceph-1/journal<br>[root@lab8106 ~]# echo 5bb48687-6be6-4aef-82f6-5af822c3fad8 &gt; /var/lib/ceph/osd/ceph-1/journal_uuid<br></code></pre></td></tr></table></figure><p>创建journal</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]# ceph-osd -i 1 --mkjournal<br></code></pre></td></tr></table></figure><p>启动进程</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]# systemctl restart ceph-osd@1<br></code></pre></td></tr></table></figure><p>去除 noout 的标记</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]# ceph osd unset noout<br></code></pre></td></tr></table></figure><p>启动后检查集群的状态</p><h2 id="第二种方法"><a href="#第二种方法" class="headerlink" title="第二种方法"></a>第二种方法</h2><p>这个属于备份和转移分区表的方法<br>首先进行上面方法的停进程，下刷journal</p><p>备份需要替换journal的分区表</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]# sgdisk --backup=/tmp/backup_journal_sdd /dev/sdd<br></code></pre></td></tr></table></figure><p>还原分区表</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]# sgdisk --load-backup=/tmp/backup_journal_sdd /dev/sde<br>[root@lab8106 ~]# parted -s /dev/sde print<br></code></pre></td></tr></table></figure><p>新的journal磁盘现在跟老的journal的磁盘的分区表一样的了。这意味着新的分区的UUID和老的相同的。如果选择的是这种备份还原分布的方法，那么journal的那个软连接是不需要进行修改的，因为两个磁盘的uuid是一样的，所以需要注意将老的磁盘拔掉或者清理掉分区，以免冲突</p><p>在做完这个以后同样跟上面的方法一样需要重建journal</p><p>创建journal</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]# chown ceph:ceph /var/lib/ceph/osd/ceph-1/journal<br>[root@lab8106 ~]# ceph-osd -i 1 --mkjournal<br></code></pre></td></tr></table></figure><p>启动进程</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]# systemctl restart ceph-osd@1<br></code></pre></td></tr></table></figure><p>去除 noout 的标记</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]# ceph osd unset noout<br></code></pre></td></tr></table></figure><h2 id="变更记录"><a href="#变更记录" class="headerlink" title="变更记录"></a>变更记录</h2><table><thead><tr><th align="center">Why</th><th align="center">Who</th><th align="center">When</th></tr></thead><tbody><tr><td align="center">创建</td><td align="center">武汉-运维-磨渣</td><td align="center">2016-07-27</td></tr><tr><td align="center">创建</td><td align="center">武汉-运维-磨渣</td><td align="center">2023-08-23</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Module ceph not found解决办法</title>
    <link href="/2016/07/24/modprobe_cephnotfound%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95/"/>
    <url>/2016/07/24/modprobe_cephnotfound%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<h3 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h3><p>有可能你在进行 Ceph 文件系统挂载的时候出现下面的提示：</p><blockquote><p>modprobe: FATAL: Module ceph not found.<br><br>mount.ceph: modprobe failed, exit status 1<br><br>mount error: ceph filesystem not supported by the system</p></blockquote><p>这个是因为你的内核当中没有cephfs的相关模块，这个 centos6 下面比较常见，因为 centos6 的内核是 2.6.32,这个版本的内核中还没有集成cephfs的内核模块，而在 centos7 默认内核 3.10中已经默认集成了这个模块，我们看下集成的模块是怎样的显示</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]# uname -a<br>Linux ciserver 3.10.0-229.el7.x86_64 #1 SMP Fri Mar 6 11:36:42 UTC 2015 x86_64 x86_64 x86_64 GNU/Linux<br>[root@lab8106 ~]# modinfo ceph<br>filename:       /lib/modules/3.10.0-229.el7.x86_64/kernel/fs/ceph/ceph.ko<br>license:        GPL<br>description:    Ceph filesystem for Linux<br>author:         Patience Warnick &lt;patience@newdream.net&gt;<br>author:         Yehuda Sadeh &lt;yehuda@hq.newdream.net&gt;<br>author:         Sage Weil &lt;sage@newdream.net&gt;<br>alias:          fs-ceph<br>rhelversion:    7.1<br>srcversion:     2086D500AFAF47B7260E08A<br>depends:        libceph<br>intree:         Y<br>vermagic:       3.10.0-229.el7.x86_64 SMP mod_unload modversions <br>signer:         CentOS Linux kernel signing key<br>sig_key:        A6:2A:0E:1D:6A:6E:48:4E:9B:FD:73:68:AF:34:08:10:48:E5:35:E5<br>sig_hashalgo:   sha256<br></code></pre></td></tr></table></figure><p>可以从上面的输出可以看到有个路径为 &#x2F;lib&#x2F;modules&#x2F;3.10.0-229.el7.x86_64&#x2F;kernel&#x2F;fs&#x2F;ceph&#x2F;ceph.ko  的内核模块，这个就是 cephfs 客户端需要使用到的模块</p><h3 id="解决办法"><a href="#解决办法" class="headerlink" title="解决办法"></a>解决办法</h3><p>解决这个缺失的模块的办法就是升级内核，并且在编译内核的时候需要选上这个模块，在某些商用的 Ceph 里面都是默认把这个模块给屏蔽了，这是因为 Cephfs 并没有达到稳定的标准，而这个在后端版本升级到 10.2 版本（jewel）版本，才正式宣布为第一个稳定版本，当然这个还是慎用为好，除非有比较强大的技术力量支撑，否则也不会出现那么多的大的商用厂家也不开放 Cephfs</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>Cephfs这块是比rbd和radosgw这两个部分都复杂的部分，而真正能控制住这个开发的目前主要是 Intel 的 zhengyan，从邮件列表里面可以看到主要都是他在修bug，这一块未知的可能性太多，任何小的故障抖动都可能是致命的</p><h3 id="变更记录"><a href="#变更记录" class="headerlink" title="变更记录"></a>变更记录</h3><table><thead><tr><th align="center">Why</th><th align="center">Who</th><th align="center">When</th></tr></thead><tbody><tr><td align="center">创建</td><td align="center">武汉-运维-磨渣</td><td align="center">2016-07-24</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>解决自动安装Freebsd系统盘符无法确定问题</title>
    <link href="/2016/07/19/%E8%A7%A3%E5%86%B3%E8%87%AA%E5%8A%A8%E5%AE%89%E8%A3%85Freebsd%E7%B3%BB%E7%BB%9F%E7%9B%98%E7%AC%A6%E6%97%A0%E6%B3%95%E7%A1%AE%E5%AE%9A%E9%97%AE%E9%A2%98/"/>
    <url>/2016/07/19/%E8%A7%A3%E5%86%B3%E8%87%AA%E5%8A%A8%E5%AE%89%E8%A3%85Freebsd%E7%B3%BB%E7%BB%9F%E7%9B%98%E7%AC%A6%E6%97%A0%E6%B3%95%E7%A1%AE%E5%AE%9A%E9%97%AE%E9%A2%98/</url>
    
    <content type="html"><![CDATA[<p>最近因为需要用到Freebsd，所以研究了打包的一些方法，这个没什么太大问题，通过网上的一些资料可以解决，但是由于确实不太熟悉这套系统，还是碰上了一些比较麻烦的地方，目前也没看到有人写如何处理，那就自己总结一下，以免以后再用忘记如何处理</p><h3 id="问题来源"><a href="#问题来源" class="headerlink" title="问题来源"></a>问题来源</h3><p>在linux下的iso自动安装的时候，在无法确定盘符的情况下，可以不写盘符，从而在遇到任何奇怪的磁盘的时候也是能安装的，比如 sda,xvda，vda,这些都可以通过不精确盘符的方式解决</p><p>而在freebsd当中处理就不一样了</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs raw">cat ./etc/installerconfig<br>PARTITIONS=&quot;da0 &#123; 512K freebsd-boot, auto freebsd-ufs / &#125;&quot;<br>DISTRIBUTIONS=&quot;custom_kernel.txz base.txz lib32.txz custom_files.txz&quot;<br>#!/bin/sh<br>···<br></code></pre></td></tr></table></figure><p>这个地方写配置文件的第一句就要告诉安装环境需要安装到哪里，这个地方是写死的一个数据，而碰上ada为系统盘就没法解决了，得不断的适配这个盘符</p><h3 id="解决问题"><a href="#解决问题" class="headerlink" title="解决问题"></a>解决问题</h3><p>最开始的时候写 etc&#x2F;installerconfig这个配置文件我也不知道为什么要写这里就可以，根据网上的资料是写这个就可以了，在查阅更多的资料后，可以发现是在光盘的etc&#x2F;rc.local里面会去调用这个脚本，然后去安装</p><p>最开始的思路是直接修改这个脚本，后来发现在安装过程中，这个文件实际是只读的，无法去修改的，所以这个地方需要做一个折中的修改</p><p>先准备好etc&#x2F;installerconfig，写死几个值</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs raw">PARTITIONS=&quot;da0 &#123; 512K freebsd-boot, auto freebsd-ufs / &#125;&quot;<br>···<br>#changge fstab to gpt id<br>systemuuid=`gpart list | grep -A 11 &#x27;da0p2&#x27; | grep &#x27;rawuuid&#x27; | awk &#x27;&#123;print $2&#125;&#x27;`<br>sed -i -e &quot;s/da0p2/gptid\/$systemuuid/g&quot; /etc/fstab<br></code></pre></td></tr></table></figure><p>下面的那个部分是解决盘符变动，在安装过程中就处理好盘符的uuid挂载，这个在linux下面，是操作系统默认就处理好了，这个地方写定一个da0,等下后面处理的时候可以去匹配这个da0</p><p>处理默认的.&#x2F;etc&#x2F;rc.local</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">export</span> TERM<br><br><span class="hljs-built_in">cp</span> /etc/installerconfig /tmp/installerconfig<br>sh -c <span class="hljs-string">&#x27;. /usr/share/bsdconfig/device.subr;f_device_menu &quot;&quot; &quot;&quot; &quot;&quot; DISK&#x27;</span><br><span class="hljs-built_in">echo</span> -n  <span class="hljs-string">&quot;Which disk your what install :&quot;</span><br><span class="hljs-built_in">read</span> mydisk<br><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;<span class="hljs-variable">$mydisk</span>&quot;</span><br>sed -i -e <span class="hljs-string">&quot;s/da0/<span class="hljs-variable">$mydisk</span>/g&quot;</span> /tmp/installerconfig<br><br><br><span class="hljs-keyword">if</span> [ -f /tmp/installerconfig ]; <span class="hljs-keyword">then</span><br>        <span class="hljs-keyword">if</span> bsdinstall script /tmp/installerconfig; <span class="hljs-keyword">then</span><br>                dialog --backtitle <span class="hljs-string">&quot;FreeBSD Installer&quot;</span> --title <span class="hljs-string">&quot;Complete&quot;</span> --no-cancel --ok-label <span class="hljs-string">&quot;Reboot&quot;</span> --pause <span class="hljs-string">&quot;Inst</span><br><span class="hljs-string">allation of FreeBSD complete! Rebooting in 10 seconds&quot;</span> 10 30 10<br>                reboot<br></code></pre></td></tr></table></figure><p>处理思路就是先拷贝到一个临时的环境下面，然后去修改它，利用系统接口去获取可以安装的磁盘，这个地方只是起一个告诉有哪些盘可以安装的作用，然后根据提示输入想安装的磁盘的盘符名称，这个地方是什么名称就输入什么名称就可以安装了，然后系统就会根据改好的脚本去安装操作系统了</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>这是一个遗留问题，之前一直没解决，造成了越来越多的问题，在花了一个晚上的时间后，终于能够解决了，对系统越熟悉越能够知道怎么去处理问题，未知的东西太多，只能一点点花时间解决</p><h3 id="变更记录"><a href="#变更记录" class="headerlink" title="变更记录"></a>变更记录</h3><table><thead><tr><th align="center">Why</th><th align="center">Who</th><th align="center">When</th></tr></thead><tbody><tr><td align="center">创建</td><td align="center">武汉-运维-磨渣</td><td align="center">2016-07-19</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>通过ceph-deploy安装不同版本ceph</title>
    <link href="/2016/07/14/%E9%80%9A%E8%BF%87ceph-deploy%E5%AE%89%E8%A3%85%E4%B8%8D%E5%90%8C%E7%89%88%E6%9C%ACceph/"/>
    <url>/2016/07/14/%E9%80%9A%E8%BF%87ceph-deploy%E5%AE%89%E8%A3%85%E4%B8%8D%E5%90%8C%E7%89%88%E6%9C%ACceph/</url>
    
    <content type="html"><![CDATA[<p>之前有在论坛写了怎么用 yum 安装 ceph，但是看到ceph社区的群里还是有人经常用 ceph-deploy 进行安装，然后会出现各种不可控的情况，虽然不建议用ceph-deploy安装，但是既然想用，那就研究下怎么用好</p><p>首先机器需要安装 ceph-deploy 这个工具，机器上应该安装好 epel 源和 base 源，这个可以参考上面的那个连接，也可以自己准备好</p><h3 id="安装ceph-deploy"><a href="#安装ceph-deploy" class="headerlink" title="安装ceph-deploy"></a>安装ceph-deploy</h3><p>使用yum直接安装</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 yum.repos.d]# yum install ceph-deploy<br>Loaded plugins: fastestmirror, langpacks, priorities<br>Loading mirror speeds from cached hostfile<br>Resolving Dependencies<br>--&gt; Running transaction check<br>---&gt; Package ceph-deploy.noarch 0:1.5.25-1.el7 will be installed<br>···<br>===================================================================================================<br> Package            Arch            Version             Repository                    Size<br>===================================================================================================<br>Installing:<br> ceph-deploy        noarch          1.5.25-1.el7         epel                         156 k<br>···<br>Installed:<br>  ceph-deploy.noarch 0:1.5.25-1.el7<br>Complete!<br></code></pre></td></tr></table></figure><p>可以看到是从 epel 的 repo 里面下载的版本为1.5.25，如果从ceph源里面下载的这个版本可能会更高一点，这个没什么问题</p><p>现在什么都不修改，看下默认的安装会什么样的</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]# ceph-deploy install lab8106<br>[ceph_deploy.conf][DEBUG ] found configuration file at: /root/.cephdeploy.conf<br>[ceph_deploy.cli][INFO  ] Invoked (1.5.25): /usr/bin/ceph-deploy install lab8106<br>[ceph_deploy.install][DEBUG ] Installing stable version hammer on cluster ceph hosts lab8106<br>···<br>[lab8106][INFO  ] Running command: rpm --import https://ceph.com/git/?p=ceph.git;a=blob_plain;f=keys/release.asc<br>[lab8106][INFO  ] Running command: rpm -Uvh --replacepkgs http://ceph.com/rpm-hammer/el7/noarch/ceph-release-1-0.el7.noarch.rpm<br>[lab8106][INFO  ] Running command: yum -y install ceph ceph-radosgw<br>[lab8106][WARNIN] http://ceph.com/rpm-hammer/rhel7/x86_64/repodata/repomd.xml: [Errno 14] HTTP Error 404 - Not Found<br></code></pre></td></tr></table></figure><p>这个默认的版本没安装成功<br>这个地方的原因是默认会去下载<a href="http://ceph.com/rpm-hammer/el7/noarch/ceph-release-1-0.el7.noarch.rpm">http://ceph.com/rpm-hammer/el7/noarch/ceph-release-1-0.el7.noarch.rpm</a> 这个包，而这个包是有问题的，安装以后</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 yum.repos.d]# cat /etc/yum.repos.d/ceph.repo |grep baseurl<br>baseurl=http://ceph.com/rpm-hammer/rhel7/$basearch<br>baseurl=http://ceph.com/rpm-hammer/rhel7/noarch<br>baseurl=http://ceph.com/rpm-hammer/rhel7/SRPMS<br></code></pre></td></tr></table></figure><p>这路径rhel7是根本就没有的，所以这个地方所以会出错，可以去修改repo的方式解决，这里先忽略这个问题，我们换一个ceph-deploy看看会怎样</p><h3 id="安装另外版本的ceph-deploy"><a href="#安装另外版本的ceph-deploy" class="headerlink" title="安装另外版本的ceph-deploy"></a>安装另外版本的ceph-deploy</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]# yum remove ceph-deploy<br>[root@lab8106 ~]# rpm -ivh http://download.ceph.com/rpm/el7/noarch/ceph-deploy-1.5.34-0.noarch.rpm<br></code></pre></td></tr></table></figure><p>安装好了后，再次执行安装</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]# ceph-deploy install lab8106<br>···<br>[ceph_deploy.install][DEBUG ] Installing stable version jewel on cluster ceph hosts lab8106<br>···<br>lab8106][INFO  ] Running command: rpm --import https://download.ceph.com/keys/release.asc<br>[lab8106][INFO  ] Running command: rpm -Uvh --replacepkgs https://download.ceph.com/rpm-jewel/el7/noarch/ceph-release-1-0.el7.noarch.rpm<br>[lab8106][INFO  ] Running command: yum -y install ceph ceph-radosgw<br>···<br>[lab8106][DEBUG ] --&gt; Running transaction check<br>[lab8106][DEBUG ] ---&gt; Package ceph.x86_64 1:10.2.2-0.el7 will be installed<br>···<br></code></pre></td></tr></table></figure><p>如果网络好的话，那么可以看到，执行这个命令后会在ceph.com的官网上去下载安装包了，如果网络不好的话，就会卡住了，这里是要说明的是</p><blockquote><p>不同的 ceph-deploy 去 install 的时候会安装不同的版本，这个因为代码里面会写上当时的版本，这样默认安装的就是当时的版本了</p></blockquote><p>到了这里要开始本篇的主题了，主要的目的有两个:</p><ul><li>自己选择想安装的 ceph 版本</li><li>自己选择通过什么地址安装</li></ul><p>第一个是解决了安装自己的版本，第二个是避免ceph.com无法访问的时候无法安装，通过国内的源进行加速</p><h3 id="自定义安装ceph"><a href="#自定义安装ceph" class="headerlink" title="自定义安装ceph"></a>自定义安装ceph</h3><h4 id="通过阿里云安装ceph-hammer"><a href="#通过阿里云安装ceph-hammer" class="headerlink" title="通过阿里云安装ceph-hammer"></a>通过阿里云安装ceph-hammer</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]# rm -rf /etc/yum.repos.d/ceph*<br>[root@lab8106 ~]# ceph-deploy install  lab8106 --repo-url=http://mirrors.aliyun.com/ceph/rpm-hammer/el7/ --gpg-url=http://mirrors.aliyun.com/centos/RPM-GPG-KEY-CentOS-7<br></code></pre></td></tr></table></figure><p>通过这个命令，就通过阿里云的源安装了ceph的hammer版本的ceph</p><h4 id="通过阿里云安装ceph-jewel"><a href="#通过阿里云安装ceph-jewel" class="headerlink" title="通过阿里云安装ceph-jewel"></a>通过阿里云安装ceph-jewel</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]# yum clean all<br>[root@lab8106 ~]# rm -rf /etc/yum.repos.d/ceph*<br>[root@lab8106 ~]# ceph-deploy install  lab8106 --repo-url=http://mirrors.aliyun.com/ceph/rpm-jewel/el7/ --gpg-url=http://mirrors.aliyun.com/centos/RPM-GPG-KEY-CentOS-7<br></code></pre></td></tr></table></figure><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>安装的方式有很多，对于新手来说如果想用 ceph-deploy 去安装的话，可以根据上面的很简单的命令就解决了，这里没有写本地做源的相关的知识，安装这一块怎么顺手怎么来，不要在安装上面耗费太多的时间</p><h3 id="变更记录"><a href="#变更记录" class="headerlink" title="变更记录"></a>变更记录</h3><table><thead><tr><th align="center">Why</th><th align="center">Who</th><th align="center">When</th></tr></thead><tbody><tr><td align="center">创建</td><td align="center">武汉-运维-磨渣</td><td align="center">2016-07-14</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>重构rbd镜像的元数据</title>
    <link href="/2016/07/02/%E9%87%8D%E6%9E%84rbd%E9%95%9C%E5%83%8F%E7%9A%84%E5%85%83%E6%95%B0%E6%8D%AE/"/>
    <url>/2016/07/02/%E9%87%8D%E6%9E%84rbd%E9%95%9C%E5%83%8F%E7%9A%84%E5%85%83%E6%95%B0%E6%8D%AE/</url>
    
    <content type="html"><![CDATA[<p>这个已经很久之前已经实践成功了，现在正好有时间就来写一写，目前并没有在其他地方有类似的分享，虽然我们自己的业务并没有涉及到云计算的场景，之前还是对rbd镜像这一块做了一些基本的了解，因为一直比较关注故障恢复这一块，东西并不难，总之一切不要等到出了问题再去想办法，提前准备总是好的，如果你有集群的问题，生产环境需要恢复的欢迎找我</p><h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>rbd的镜像的元数据，这个是什么？这里所提到的元数据信息，是指跟这个image信息有关的元数据信息，就是image的大小名称等等一系列的信息，本篇将讲述怎么去重构这些信息，重构的前提就是做好了信息的记录，然后做重构</p><h3 id="记录元数据信息"><a href="#记录元数据信息" class="headerlink" title="记录元数据信息"></a>记录元数据信息</h3><h4 id="创建一个image"><a href="#创建一个image" class="headerlink" title="创建一个image"></a>创建一个image</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ~]<span class="hljs-comment"># rbd -p rbd create zp --size 40000</span><br></code></pre></td></tr></table></figure><p>这里是在rbd存储池当中创建的一个名称为zp的，大小为40G的image文件</p><p>如果没有其他的image的情况下，我们来查看下对象信息</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ~]<span class="hljs-comment"># rados -p rbd ls</span><br>rbd_header.60276b8b4567<br>rbd_directory<br>rbd_id.zp<br></code></pre></td></tr></table></figure><p>将这几个镜像下载下来</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 zp]<span class="hljs-comment"># rados -p rbd get rbd_header.60276b8b4567 rbd_header.60276b8b4567</span><br>[root@lab8106 zp]<span class="hljs-comment"># rados -p rbd get rbd_directory rbd_directory</span><br>[root@lab8106 zp]<span class="hljs-comment"># rados -p rbd get rbd_id.zp rbd_id.zp</span><br></code></pre></td></tr></table></figure><p>查看下载下来的几个镜像的元数据的文件信息 </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 zp]<span class="hljs-comment"># ll</span><br>total 4<br>-rw-r--r-- 1 root root  0 Jul  1 23:28 rbd_directory<br>-rw-r--r-- 1 root root  0 Jul  1 23:28 rbd_header.60276b8b4567<br>-rw-r--r-- 1 root root 16 Jul  1 23:28 rbd_id.zp<br></code></pre></td></tr></table></figure><p>有没有发现有两个镜像的文件大小是0，这个是因为rbd format 2 格式下（默认格式），这两个对象的元数据信息是存储在扩展属性里面的，所以下载下来的对象是没有内容，那我们怎么查看这个属性，看下面讲述的查询相关的操作</p><h4 id="查询这个image的信息"><a href="#查询这个image的信息" class="headerlink" title="查询这个image的信息"></a>查询这个image的信息</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 zp]<span class="hljs-comment"># rbd -p rbd info zp</span><br>rbd image <span class="hljs-string">&#x27;zp&#x27;</span>:<br>size 40000 MB <span class="hljs-keyword">in</span> 10000 objects<br>order 22 (4096 kB objects)<br>block_name_prefix: rbd_data.60276b8b4567<br>format: 2<br>features: layering<br>flags: <br></code></pre></td></tr></table></figure><p>这里可以看到这个image文件的大小，对象大小，前缀信息，属性相关信息，这是用我们比较常规的方式来查询到的信息，现在用另外一种方式来查询信息，查到的会是另外一种方式，也就是上面一节提到的空对象的扩展属性的查询</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 zp]<span class="hljs-comment"># rados -p rbd listomapvals rbd_directory</span><br>id_60276b8b4567<br>value (6 bytes) :<br>00000000  02 00 00 00 7a 70                                 |....zp|<br>00000006<br><br>name_zp<br>value (16 bytes) :<br>00000000  0c 00 00 00 36 30 32 37  36 62 38 62 34 35 36 37  |....60276b8b4567|<br>00000010<br></code></pre></td></tr></table></figure><p>先来查询 rbd_directory 这个的元数据信息，这个里面的信息可以看到两组对应关系<br>id_60276b8b4567,就是这个image的id，也是前缀信息，后面对应的是一个名称zp<br>第二组name_zp,对应的就是后面的60276b8b4567，也就是名称对应到id<br>，那个value值就是后面的字符串对应的16进制的一种方式，这个地方就是需要备份的元数据信息，现在准备做第一次重构，重构rbd_directory这个的元数据信息，这个rbd_directory记录所属存储池有哪些镜像</p><h3 id="恢复rbd-directory的元数据信息"><a href="#恢复rbd-directory的元数据信息" class="headerlink" title="恢复rbd_directory的元数据信息"></a>恢复rbd_directory的元数据信息</h3><p>先来破坏这个元数据信息，破坏的方式很简单，就是做删除</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 zp]<span class="hljs-comment"># rados -p rbd rm rbd_directory</span><br>[root@lab8106 zp]<span class="hljs-comment"># rbd ls</span><br></code></pre></td></tr></table></figure><p>可以看到删除了元数据信息以后，再进行镜像的ls，是查询不到信息的</p><p>开始做恢复</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 zp]<span class="hljs-comment"># touch rbd_directory</span><br>[root@lab8106 zp]<span class="hljs-comment"># rados -p rbd put rbd_directory rbd_directory</span><br>[root@lab8106 zp]<span class="hljs-comment"># rados -p rbd listomapvals rbd_directory</span><br></code></pre></td></tr></table></figure><p>上面做的三步是创建一个空文件，然后上传，然后列属性，可以看到，都是空的（这个地方也可以不创建空对象，直接做后面的给属性的时候，集群会自动创建相关的对象）<br>现在给这个对象写入属性</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 zp]<span class="hljs-comment"># echo -en \\x02\\x00\\x00\\x00\\x7a\\x70|rados -p rbd setomapval rbd_directory id_60276b8b4567</span><br>[root@lab8106 zp]<span class="hljs-comment"># echo -en \\x0c\\x00\\x00\\x00\\x36\\x30\\x32\\x37\\x36\\x62\\x38\\x62\\x34\\x35\\x36\\x37|rados -p rbd setomapval rbd_directory name_zp</span><br></code></pre></td></tr></table></figure><p>写入的值就是上面让记录下来的信息，这个地方就用这个格式就行了，为什么要这么写，因为16进制的字符是需要转义的，之前不清楚怎么写，在邮件列表中提问后，有一个人低调的给回复了怎么写入这种进制数据，现在就这么固定写法就行了，现在再查询写入以后的属性情况</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 zp]<span class="hljs-comment"># rados -p rbd listomapvals rbd_directory</span><br>id_60276b8b4567<br>value (6 bytes) :<br>00000000  02 00 00 00 7a 70                                 |....zp|<br>00000006<br><br>name_zp<br>value (16 bytes) :<br>00000000  0c 00 00 00 36 30 32 37  36 62 38 62 34 35 36 37  |....60276b8b4567|<br>00000010<br>[root@lab8106 zp]<span class="hljs-comment"># rbd ls</span><br>zp<br></code></pre></td></tr></table></figure><p>到这里 rbd_directory这个的信息就恢复了，下面再进行image的元数据的信息的恢复</p><h3 id="恢复image的元数据信息"><a href="#恢复image的元数据信息" class="headerlink" title="恢复image的元数据信息"></a>恢复image的元数据信息</h3><p>先查询下这个对象包含的元数据信息</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 zp]<span class="hljs-comment"># rados -p rbd listomapvals rbd_header.60276b8b4567</span><br>features<br>value (8 bytes) :<br>00000000  01 00 00 00 00 00 00 00                           |........|<br>00000008<br><br>object_prefix<br>value (25 bytes) :<br>00000000  15 00 00 00 72 62 64 5f  64 61 74 61 2e 36 30 32  |....rbd_data.602|<br>00000010  37 36 62 38 62 34 35 36  37                       |76b8b4567|<br>00000019<br><br>order<br>value (1 bytes) :<br>00000000  16                                                |.|<br>00000001<br><br>size<br>value (8 bytes) :<br>00000000  00 00 00 c4 09 00 00 00                           |........|<br>00000008<br><br>snap_seq<br>value (8 bytes) :<br>00000000  00 00 00 00 00 00 00 00                           |........|<br>00000008<br></code></pre></td></tr></table></figure><p>记录下这个信息，然后进行破坏，跟上面一样的删除掉对象</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 zp]<span class="hljs-comment"># rados -p rbd rm rbd_header.60276b8b4567</span><br>[root@lab8106 zp]<span class="hljs-comment"># rbd ls</span><br>zp<br>[root@lab8106 zp]<span class="hljs-comment"># rbd info zp</span><br>2016-07-02 00:57:50.150559 7ff4b56b3700 -1 librbd::image::OpenRequest: failed to retreive immutable metadata: (2) No such file or directory<br>rbd: error opening image zp: (2) No such file or directory<br></code></pre></td></tr></table></figure><p>可以看到，在删除了这个对象以后，已经无法查询到镜像信息了，当然也就无法使用了，下面开始进行image的元数据信息的重构</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 zp]<span class="hljs-comment"># echo -en \\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00|rados -p rbd setomapval rbd_header.60276b8b4567 features</span><br>[root@lab8106 zp]<span class="hljs-comment"># echo -en \\x15\\x00\\x00\\x00\\x72\\x62\\x64\\x5f\\x64\\x61\\x74\\x61\</span><br>\x2e\\x36\\x30\\x32\\x37\\x36\\x62\\x38\\x62\\x34\\x35\\x36\\x37    |rados -p rbd setomapval rbd_header.60276b8b4567  object_prefix<br>[root@lab8106 zp]<span class="hljs-comment"># echo -en \\x16|rados -p rbd setomapval rbd_header.60276b8b4567 order</span><br>[root@lab8106 zp]<span class="hljs-comment"># echo -en \\x00\\x00\\x00\\xc4\\x09\\x00\\x00\\x00   |rados -p rbd seto</span><br>mapval rbd_header.60276b8b4567 size<br>[root@lab8106 zp]<span class="hljs-comment"># echo -en \\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00   |rados -p rbd seto</span><br>mapval rbd_header.60276b8b4567 snap_seq<br></code></pre></td></tr></table></figure><p>设置完了所有属性后查询，验证是否恢复了</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 zp]<span class="hljs-comment"># rbd -p rbd info zp</span><br>rbd image <span class="hljs-string">&#x27;zp&#x27;</span>:<br>size 40000 MB <span class="hljs-keyword">in</span> 10000 objects<br>order 22 (4096 kB objects)<br>block_name_prefix: rbd_data.60276b8b4567<br>format: 2<br>features: layering<br>flags:<br>[root@lab8106 zp]<span class="hljs-comment"># rados -p rbd listomapvals rbd_header.60276b8b4567</span><br>features<br>value (8 bytes) :<br>00000000  01 00 00 00 00 00 00 00                           |........|<br>00000008<br><br>object_prefix<br>value (25 bytes) :<br>00000000  15 00 00 00 72 62 64 5f  64 61 74 61 2e 36 30 32  |....rbd_data.602|<br>00000010  37 36 62 38 62 34 35 36  37                       |76b8b4567|<br>00000019<br><br>order<br>value (1 bytes) :<br>00000000  16                                                |.|<br>00000001<br><br>size<br>value (8 bytes) :<br>00000000  00 00 00 c4 09 00 00 00                           |........|<br>00000008<br><br>snap_seq<br>value (8 bytes) :<br>00000000  00 00 00 00 00 00 00 00                           |........|<br>00000008<br></code></pre></td></tr></table></figure><p>元数据完整的回来了<br>上面已经将两个导出的空对象元数据信息恢复好了，再看最后一个有文件大小的对象怎么做恢复</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 zp]<span class="hljs-comment"># cat rbd_id.zp </span><br><br>60276b8b4567[root@lab8106 zp]<span class="hljs-comment">#</span><br></code></pre></td></tr></table></figure><p>这个第一种方式是直接备份好,然后倒入的方式<br>跟上面的方法一样，开始通过删除对象来破坏</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 zp]<span class="hljs-comment"># rados -p rbd rm rbd_id.zp</span><br>[root@lab8106 zp]<span class="hljs-comment"># rbd -p rbd info zp</span><br>rbd: error opening image zp: (2) No such file or directory<br></code></pre></td></tr></table></figure><p>可以看到破坏了就无法访问镜像了，下面直接利用备份对象倒入的方式进行恢复</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 zp]<span class="hljs-comment"># rados -p rbd put rbd_id.zp rbd_id.zp</span><br>[root@lab8106 zp]<span class="hljs-comment"># rbd -p rbd info zp</span><br>rbd image <span class="hljs-string">&#x27;zp&#x27;</span>:<br>size 40000 MB <span class="hljs-keyword">in</span> 10000 objects<br>order 22 (4096 kB objects)<br>block_name_prefix: rbd_data.60276b8b4567<br>format: 2<br>features: layering<br>flags: <br></code></pre></td></tr></table></figure><p>可以看到，导入后即可，也可以用另外一种方式，记录字符串的方式进行备份</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 zp]<span class="hljs-comment"># xxd rbd_id.zp</span><br>0000000: 0c00 0000 3630 3237 3662 3862 3435 3637  ....60276b8b4567<br></code></pre></td></tr></table></figure><p>我们可以查看这个文件的16进制的信息输出，这个信息就是要保留的字符串信息 </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 zp]<span class="hljs-comment"># hexdump -C rbd_id.zp</span><br>00000000  0c 00 00 00 36 30 32 37  36 62 38 62 34 35 36 37  |....60276b8b4567|<br>00000010<br></code></pre></td></tr></table></figure><p>需要保留的就是这个信息,我们根据这个信息来重新创建一个文件，然后检查文件内容是不是能跟下载下来的对象一样</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 zp]<span class="hljs-comment"># echo -en \\x0c\\x00\\x00\\x00\\x36\\x30\\x32\\x37\\x36\\x62\\x38\\x62\\x34\\x35\\x36\\x37   &gt;rbd_id.zpre</span><br>[root@lab8106 zp]<span class="hljs-comment"># hexdump -C rbd_id.zpre</span><br>00000000  0c 00 00 00 36 30 32 37  36 62 38 62 34 35 36 37  |....60276b8b4567|<br>00000010<br></code></pre></td></tr></table></figure><p>可以看到，可以用字符串完整恢复这个对象了，然后put进集群即可恢复了</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>可以看到，所有的元数据信息都可以以字符串的形式保留下来，然后进行元数据重构，其中的rbd_id.zp这个可以保存对象方式，也可以是获取对象后，然后保存16进制字符串信息，然后再进行本地创建对象,然后put的方式，其它的两个空对象可以用设置属性的方式进行恢复，在openstack场景下，这些元数据信息最好都保留下来，一旦有问题的时候，可以很方便的进行数据的重构，备份并不是说所有数据都需要备份，对于这种数据量很小，而且很重要的信息，定期备份一下，也许哪天就用上了</p><h3 id="变更记录"><a href="#变更记录" class="headerlink" title="变更记录"></a>变更记录</h3><table><thead><tr><th align="center">Why</th><th align="center">Who</th><th align="center">When</th></tr></thead><tbody><tr><td align="center">创建</td><td align="center">武汉-运维-磨渣</td><td align="center">2016-07-02</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>parted分区对齐</title>
    <link href="/2016/06/24/parted%E5%88%86%E5%8C%BA%E5%AF%B9%E9%BD%90/"/>
    <url>/2016/06/24/parted%E5%88%86%E5%8C%BA%E5%AF%B9%E9%BD%90/</url>
    
    <content type="html"><![CDATA[<h3 id="分区提示未对齐"><a href="#分区提示未对齐" class="headerlink" title="分区提示未对齐"></a>分区提示未对齐</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ceph]<span class="hljs-comment"># parted /dev/sdd </span><br>GNU Parted 3.1<br>Using /dev/sdd<br>Welcome to GNU Parted! Type <span class="hljs-string">&#x27;help&#x27;</span> to view a list of commands.<br>(parted) p                                                                <br>Model: SEAGATE ST3300657SS (scsi)<br>Disk /dev/sdd: 300GB<br>Sector size (logical/physical): 512B/512B<br>Partition Table: gpt<br>Disk Flags: <br><br>Number  Start  End  Size  File system  Name  Flags<br><br>(parted) mkpart primary 0 100%                                            <br>Warning: The resulting partition is not properly aligned <span class="hljs-keyword">for</span> best performance.<br>Ignore/Cancel?  <br></code></pre></td></tr></table></figure><p>Warning: The resulting partition is not properly aligned for best performance.<br>分区的时候提示不是最好的模式，这个是因为没有对齐的原因，在默认情况下我都是</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">mkpart primary 1 100%   <br></code></pre></td></tr></table></figure><p>这个一般都是对齐的，但是最近遇到一个做了raid5的怎么都提示不行，然后搜索了下资料，这个地方是要计算下比较好的</p><h3 id="通过计算分区"><a href="#通过计算分区" class="headerlink" title="通过计算分区"></a>通过计算分区</h3><h4 id="获取磁盘的几个参数（这里是软raid）"><a href="#获取磁盘的几个参数（这里是软raid）" class="headerlink" title="获取磁盘的几个参数（这里是软raid）"></a>获取磁盘的几个参数（这里是软raid）</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># cat /sys/block/md127/queue/optimal_io_size</span><br>3670016<br><span class="hljs-comment"># cat /sys/block/md127/queue/minimum_io_size</span><br>524288<br><span class="hljs-comment"># cat /sys/block/md127/alignment_offset</span><br>0<br><span class="hljs-comment"># cat /sys/block/md127/queue/physical_block_size</span><br>512<br></code></pre></td></tr></table></figure><p>optimal_io_size 加上 alignment_offset 的和 然后除以  physical_block_size<br>在这个环境下是：<br>(3670016 + 0) &#x2F; 512 &#x3D; 7168</p><p>那么分区的时候命令就应该是</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">mkpart primary 7168s 100%<br></code></pre></td></tr></table></figure><p>如果上面的顺利的完成检查一下  (‘1’是分区的编号):</p><blockquote><p>(parted) align-check optimal 1<br><br>1 aligned</p></blockquote><p>这个是正常的结果，如果没对齐就会是</p><blockquote><p>(parted) align-check optimal 1<br><br>1 not aligned</p></blockquote><h3 id="其他情况"><a href="#其他情况" class="headerlink" title="其他情况"></a>其他情况</h3><p>默认情况下直接用下列的分区参数就可以，出现提示再用上面的计算，总之最后align-check 验证下</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">mkpart primary 1 100% <br></code></pre></td></tr></table></figure><h3 id="相关文章"><a href="#相关文章" class="headerlink" title="相关文章"></a>相关文章</h3><p><a href="http://rainbow.chard.org/2013/01/30/how-to-align-partitions-for-best-performance-using-parted/">How to align partitions for best performance using parted</a></p><h3 id="变更记录"><a href="#变更记录" class="headerlink" title="变更记录"></a>变更记录</h3><table><thead><tr><th align="center">Why</th><th align="center">Who</th><th align="center">When</th></tr></thead><tbody><tr><td align="center">创建</td><td align="center">武汉-运维-磨渣</td><td align="center">2016-06-24</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>rbd的增量备份和恢复</title>
    <link href="/2016/06/22/rbd%E7%9A%84%E5%A2%9E%E9%87%8F%E5%A4%87%E4%BB%BD%E5%92%8C%E6%81%A2%E5%A4%8D/"/>
    <url>/2016/06/22/rbd%E7%9A%84%E5%A2%9E%E9%87%8F%E5%A4%87%E4%BB%BD%E5%92%8C%E6%81%A2%E5%A4%8D/</url>
    
    <content type="html"><![CDATA[<h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>快照的功能一般是基于时间点做一个标记，然后在某些需要的时候，将状态恢复到标记的那个点，这个有一个前提是底层的东西没用破坏，举个简单的例子，<strong>Vmware</strong> 里面对虚拟机做了一个快照，然后做了一些系统的操作，想恢复快照，前提是存储快照的存储系统没用破坏，一旦破坏了是无法恢复的</p><!--break--><p>ceph里面也有快照的功能，同样的，在这里的快照是用来保存存储系统上的状态的，数据的快照能成功恢复的前提是存储系统是好的，而一旦存储系统坏了，快照同时会失效的，本篇文章利用ceph的快照去实现一个增量的备份功能，网上也有很多这个脚本，这里主要是对里面细节做一个实践，具体集成到一套系统里面去，自己去做一个策略就行了，总之多备份一下，以备不时之需，并且也可以实现跨机房的增量备份，这个在某些云计算公司已经实现了，这样一旦发生故障的时候，能够把损失减到最小</p><h3 id="快照的创建和数据的导出"><a href="#快照的创建和数据的导出" class="headerlink" title="快照的创建和数据的导出"></a>快照的创建和数据的导出</h3><p><img src="/images/blog/o_200901063036image_1alqs3lm81ss11n1vg7k1mle1eq39.png"></p><p>上图是一个快照的创建和导出的过程，这里详细的描述下这些操作<br>创建快照</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">rbd snap create testimage@v1<br>rbd snap create testimage@v2<br></code></pre></td></tr></table></figure><p>这两个命令是在时间点v1和时间点v2分别做了两个快照</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">rbd export-diff rbd/testimage@v1 testimage_v1<br></code></pre></td></tr></table></figure><p>这个命令是导出了从开始创建image到快照v1那个时间点的差异数据导出来了testimage_v1，导出成本地文件testimage_v1</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">rbd export-diff rbd/testimage@v2 testimage_v2<br></code></pre></td></tr></table></figure><p>这个命令是导出了从开始创建image到快照v2那个时间点的差异数据导出来了，导出成本地文件testimage_v2</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">rbd export-diff rbd/testimage@v2 --from-snap v1 testimage_v1_v2<br></code></pre></td></tr></table></figure><p>这个命令是导出了从v1快照时间点到v2快照时间点的差异数据，导出成本地文件testimage_v1_v2</p><p>这个地方上面的导出的数据：</p><blockquote><p>v1时间点数据 + v1_v2之间数据 &#x3D; v2 时间点数据</p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">rbd export-diff rbd/testimage testimage_now<br></code></pre></td></tr></table></figure><p>这个就是导出了从image创建到当前的时间点的差异数据</p><h3 id="快照的数据恢复"><a href="#快照的数据恢复" class="headerlink" title="快照的数据恢复"></a>快照的数据恢复</h3><p><img src="/images/blog/o_200901063042image_1alpuprird31dltilpro7kf52a.png"></p><p>快照的恢复过程使用的是刚刚上面提到的备份到本地的那些文件<br>首先随便创建一个image,名称大小都不限制，因为后面恢复的时候会覆盖掉大小的信息</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">rbd create testbacknew --size 1<br></code></pre></td></tr></table></figure><p>现在假如想恢复到v2那个快照的时间点，那么可以用两个方法</p><p>1、直接基于v2的时间点的快照做恢复</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">rbd import-diff testimage_v2 rbd/testbacknew<br></code></pre></td></tr></table></figure><p>2、直接基于v1的时间点的数据，和后面的增量的v1_v2数据(要按顺序导入)</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">rbd import-diff testimage_v1 rbd/testbacknew<br>rbd import-diff testimage_v1_v2 rbd/testbacknew<br></code></pre></td></tr></table></figure><p>到这里数据就已经恢复了</p><h3 id="如何利用这个"><a href="#如何利用这个" class="headerlink" title="如何利用这个"></a>如何利用这个</h3><p>实际项目当中就是，定期做快照，然后导出某个时间点快照的数据，然后导出增量的快照的数据，就可以了，例如：<br>今天对所有的rbd的image做一个基础快照，然后导出这个快照的数据，然后从今天开始，每天晚上做一个快照，然后导出快照时间点之间的数据，这样每天导出来的就是一个增量的数据了，在做恢复的时候，就从第一个快照导入，然后按顺序导入增量的快照即可，也可以定期做一个快照，导出完整的快照数据，以防中间的增量快照漏了，然后就是要注意可以定期清理快照，如果是做备份的模式，在导入了快照数据后，也可以清理一些本地的数据，本地数据做异地机房复制的时候也可以做一下数据的压缩，来减少数据量的传输</p><h3 id="相关文章"><a href="#相关文章" class="headerlink" title="相关文章"></a>相关文章</h3><ul><li><a href="https://github.com/skuicloud/openstack-hacker/tree/master/tsinghua-cluster/script/ceph/volume_backup">rbd备份还原的脚本</a></li><li><a href="http://ceph.com/dev-notes/incremental-snapshots-with-rbd/">INCREMENTAL SNAPSHOTS WITH RBD</a></li><li><a href="http://cephnotes.ksperis.com/blog/2014/08/12/rbd-replication">RBD Replication</a></li><li><a href="http://www.evil0x.com/posts/14638.html">云杉网络：基于Ceph RBD的快照技术实现异地灾备</a></li></ul><h3 id="变更记录"><a href="#变更记录" class="headerlink" title="变更记录"></a>变更记录</h3><table><thead><tr><th align="center">Why</th><th align="center">Who</th><th align="center">When</th></tr></thead><tbody><tr><td align="center">创建</td><td align="center">武汉-运维-磨渣</td><td align="center">2016-06-22</td></tr><tr><td align="center">修改错别字</td><td align="center">武汉-运维-磨渣</td><td align="center">2016-06-22</td></tr><tr><td align="center">修改错别字</td><td align="center">武汉-运维-磨渣</td><td align="center">2017-12-26</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>rgw实现nfs的首测</title>
    <link href="/2016/06/19/rgw%E5%AE%9E%E7%8E%B0nfs%E7%9A%84%E9%A6%96%E6%B5%8B/"/>
    <url>/2016/06/19/rgw%E5%AE%9E%E7%8E%B0nfs%E7%9A%84%E9%A6%96%E6%B5%8B/</url>
    
    <content type="html"><![CDATA[<h3 id="功能介绍"><a href="#功能介绍" class="headerlink" title="功能介绍"></a>功能介绍</h3><p>关于rgw实现nfs接口这个，刚接触的人可能并不清楚这个是个什么样的服务架构，rgw是ceph里面的对象存储接口，而nfs则是纯正的网络文件系统接口，这二者如何结合在一起,关于这个,有几个相关的链接供大家了解</p><ul><li><a href="http://tracker.ceph.com/projects/ceph/wiki/RGW_-_NFS">ceph官方的RGW_NFS项目规划</a></li><li><a href="http://chuansong.me/n/2385718">麦子迈关于RGW_NFS的文章</a></li></ul><p>之所以这个功能能实现这么快，原因是nfs-ganesha的开发者Matt Benjamin加入到了Redhat，而ceph目前的开发是Redhat在主导开发，所以功能的实现是非常快的，但是目前官方并没有提供相关的文档，个人推测是功能并未完全开发完成，一旦未完全开发完成的功能放出来，邮件列表和Bug列表就会有很多相关问题，开发者应该还是希望安静的把功能做好，再提供相关的文档，而这个功能也是在ceph 的jewel版本里面才加入的</p><h3 id="功能架构图"><a href="#功能架构图" class="headerlink" title="功能架构图"></a>功能架构图</h3><p><img src="/images/blog/o_200901062814image_1alibfc78g96dsa1c26crkgis1e.png" alt="image_1alibfc78g96dsa1c26crkgis1e.png-78.3kB"><br>简单说明一下：<br>集群配置s3接口，nfs-genesha将s3接口转换成nfs，然后nfs客户端挂载后访问的就是s3的bucket里面的数据了</p><h3 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h3><p>准备代码，这个是需要从源码编译的，并且需要将模块编译进去才可以的，源码分支地址：</p><blockquote><p><a href="https://github.com/nfs-ganesha/nfs-ganesha/tree/V2.3-stable">https://github.com/nfs-ganesha/nfs-ganesha/tree/V2.3-stable</a></p></blockquote><p>这个地方要注意下，需要使用next分支(此分支开发中有编译BUG)，换分支V2.3-stable<br>使用git 进行clone分支到本地</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">git <span class="hljs-built_in">clone</span> -b V2.3-stable https://github.com/nfs-ganesha/nfs-ganesha.git<br><span class="hljs-built_in">cd</span> nfs-ganesha/<br>git submodule update --init --recursive<br></code></pre></td></tr></table></figure><p>检查是否有这个RGW模块目录</p><blockquote><p>nfs-ganesha&#x2F;src&#x2F;FSAL&#x2F;FSAL_RGW&#x2F;</p></blockquote><p>默认clone下来后   nfs-ganesha&#x2F;src&#x2F;libntirpc&#x2F;  这个目录是空的，而这个是因为如果在git里面某个目录嵌套的用了其他项目的代码，并且也是有git的分支的话，clone下来就会是空的，这个在ceph的源码里面也会这样，具体的看看下图：<br><img src="/images/blog/o_200901062821libntir.png" alt="libntir.png-38.4kB"><br>上面的 git submodule 会将缺的代码下载下来，所以不要漏步骤</p><p>代码的编译采用的是cmake的模式(cmake目录后面接的是nfs-ganesha代码的src目录)</p><blockquote><p>注意在执行cmake之前编译环境需要安装librgw2-devel这个包，才能编译成功，执行cmake的时候检查下是否真的开启了</p></blockquote><p><img src="/images/blog/o_200901062827image_1alian0db17e91gg1mhg866i1q11.png" alt="image_1alian0db17e91gg1mhg866i1q11.png-11.1kB"></p><p>开始编译安装过程，创建一个用于编译的目录</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 newbian]<span class="hljs-comment">#mkdir mybuild</span><br>[root@lab8106 newbian]<span class="hljs-comment">#cd mybuild</span><br>[root@lab8106 mybuild]<span class="hljs-comment">#cmake -DUSE_FSAL_RGW=ON ../nfs-ganesha/src/</span><br>[root@lab8106 mybuild]<span class="hljs-comment"># ll FSAL/FSAL_RGW/</span><br>total 16<br>drwxr-xr-x 3 root root    83 Jun 19 01:59 CMakeFiles<br>-rw-r--r-- 1 root root  2979 Jun 19 01:59 cmake_install.cmake<br>-rw-r--r-- 1 root root 10164 Jun 19 01:59 Makefile<br>[root@lab8106 mybuild]<span class="hljs-comment">#make</span><br>[root@lab8106 mybuild]<span class="hljs-comment">#make install</span><br></code></pre></td></tr></table></figure><p>编译安装工作就到此完成了，还是比较简单的</p><h3 id="配置服务"><a href="#配置服务" class="headerlink" title="配置服务"></a>配置服务</h3><h4 id="准备一个s3的环境，我的如下："><a href="#准备一个s3的环境，我的如下：" class="headerlink" title="准备一个s3的环境，我的如下："></a>准备一个s3的环境，我的如下：</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">radosgw-admin user create --uid=admin --display-name=<span class="hljs-string">&quot;admin&quot;</span>   --access-key=admin  --secret=admin<br></code></pre></td></tr></table></figure><p>用户信息如下：</p><ul><li>s3的User_Id：admin </li><li>s3的Access_Key:admin </li><li>s3的Secret_Access_Key:admin</li></ul><p>注意，配置ganesha-nfs服务的机器需要安装librgw</p><h4 id="修改ganesha-nfs的配置文件"><a href="#修改ganesha-nfs的配置文件" class="headerlink" title="修改ganesha-nfs的配置文件"></a>修改ganesha-nfs的配置文件</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">vim /etc/ganesha/ganesha.conf<br></code></pre></td></tr></table></figure><p>修改如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs bash">EXPORT<br>&#123;<br>        Export_ID=1;<br>        Path = <span class="hljs-string">&quot;/&quot;</span>;<br>        Pseudo = <span class="hljs-string">&quot;/&quot;</span>;<br>        Access_Type = RW;<br>        NFS_Protocols = 4;<br>        Transport_Protocols = TCP;<br>        FSAL &#123;<br>                Name = RGW;<br>                User_Id = <span class="hljs-string">&quot;admin&quot;</span>;<br>                Access_Key_Id =<span class="hljs-string">&quot;admin&quot;</span>;<br>                Secret_Access_Key = <span class="hljs-string">&quot;admin&quot;</span>;<br>        &#125;<br>&#125;<br><br>RGW &#123;<br>    ceph_conf = <span class="hljs-string">&quot;/etc/ceph/ceph.conf&quot;</span>;<br>&#125;<br></code></pre></td></tr></table></figure><p>RGW-NFS配置文件的模板路径在：</p><blockquote><p>&#x2F;usr&#x2F;share&#x2F;doc&#x2F;ganesha&#x2F;config_samples&#x2F;rgw.conf</p></blockquote><h4 id="启动ganesha-nfs服务"><a href="#启动ganesha-nfs服务" class="headerlink" title="启动ganesha-nfs服务"></a>启动ganesha-nfs服务</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">systemctl restart  nfs-ganesha.service<br></code></pre></td></tr></table></figure><h4 id="NFS客户端挂载ganesha-nfs服务"><a href="#NFS客户端挂载ganesha-nfs服务" class="headerlink" title="NFS客户端挂载ganesha-nfs服务"></a>NFS客户端挂载ganesha-nfs服务</h4><p>找一台其它的客户端机器</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">mount -t nfs 192.168.8.106:/ /mnt<br></code></pre></td></tr></table></figure><p>直接挂载即可，这里注意因为rgw是没有文件系统的容量概念的，这里df是看不到的，所以用mount命令检测</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">root@lab8107:~<span class="hljs-comment"># mount|grep mnt</span><br>192.168.8.106:/ on /mnt <span class="hljs-built_in">type</span> nfs4 (rw,relatime,vers=4.0,rsize=1048576,wsize=1048576,namlen=255,hard,proto=tcp,timeo=600,retrans=2,sec=sys,clientaddr=192.168.8.107,local_lock=none,addr=192.168.8.106)<br>192.168.8.106:/testnfsrgw on /mnt/testnfsrgw <span class="hljs-built_in">type</span> nfs4 (rw,relatime,vers=4.0,rsize=1048576,wsize=1048576,namlen=255,hard,proto=tcp,port=0,timeo=600,retrans=2,sec=sys,clientaddr=192.168.8.107,local_lock=none,addr=192.168.8.106)<br></code></pre></td></tr></table></figure><p>可以查看挂载的目录里面的子目录对应的就是bucket</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">root@lab8107:~<span class="hljs-comment"># ll /mnt/</span><br>total 4<br>drwxrwxrwx  3 root root    0 Jan  1  1970 ./<br>drwxr-xr-x 25 root root 4096 Apr 13 03:04 ../<br>drwxrwxrwx  3 root root    0 Jan  1  1970 testnfsrgw/<br></code></pre></td></tr></table></figure><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>在实现这个功能以后，实际上为文件接口和对象接口打通了一个通道，能够方便的实现传统的文件接口的数据到对象接口的转移，在性能方面，本篇并没有做测试，这个交给实际项目中去检测了，如果有问题欢迎探讨</p><h3 id="变更记录"><a href="#变更记录" class="headerlink" title="变更记录"></a>变更记录</h3><table><thead><tr><th align="center">Why</th><th align="center">Who</th><th align="center">When</th></tr></thead><tbody><tr><td align="center">创建</td><td align="center">武汉-运维-磨渣</td><td align="center">2016-06-19</td></tr><tr><td align="center">修改无法编译的BUG</td><td align="center">武汉-运维-磨渣</td><td align="center">2016-09-08</td></tr><tr><td align="center">增加git submod下载代码</td><td align="center">武汉-运维-磨渣</td><td align="center">2016-10-12</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>CPU相关的学习</title>
    <link href="/2016/06/13/CPU%E7%9B%B8%E5%85%B3%E7%9A%84%E5%AD%A6%E4%B9%A0/"/>
    <url>/2016/06/13/CPU%E7%9B%B8%E5%85%B3%E7%9A%84%E5%AD%A6%E4%B9%A0/</url>
    
    <content type="html"><![CDATA[<h3 id="我理解的CPU"><a href="#我理解的CPU" class="headerlink" title="我理解的CPU"></a>我理解的CPU</h3><p>目前对cpu的了解停留在这个水平<br>查看CPU型号：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">cat</span> /proc/cpuinfo |grep model |<span class="hljs-built_in">tail</span> -n 1<br>model name: Intel(R) Xeon(R) CPU E5-2620 v2 @ 2.10GHz<br></code></pre></td></tr></table></figure><p>查看有多少processor：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">cat</span> /proc/cpuinfo |grep processor|<span class="hljs-built_in">tail</span> -n 1<br>processor: 23<br></code></pre></td></tr></table></figure><p>然后对性能要求就是主频越高越好，processor越多越好，其它的知道的很少，由于需要做性能相关调优，所以对CPU这一块做一个系统的学习，如果参考网上的一些CEPH性能调优的资料，很多地方都是让关闭numa，以免影响性能，这个从来都是只有人给出答案，至于为什么，对不对，适合不适合你的环境，没有人给出来，没有数据支持的调优都是耍流氓</p><h3 id="单核和多核"><a href="#单核和多核" class="headerlink" title="单核和多核"></a>单核和多核</h3><p>在英文里面，单核（single-core）和多核（multi-core）多称作uniprocessor和multiprocessor，这里先对这些概念做一个说明：</p><blockquote><p>这里所说的core（或processor），是一个泛指，是从使用者（或消费者）的角度看计算机系统。因此，core，或者processor，或者处理器（CPU），都是逻辑概念，指的是一个可以独立运算、处理的核心。</p></blockquote><p>而这个核心，可以以任何形式存在，例如：单独的一个chip（如通常意义上的单核处理器）；一个chip上集成多个核心（如SMP，symmetric multiprocessing）；一个核心上实现多个hardware context，以支持多线程（如SMT，Simultaneous multithreading）；等等。这是从硬件实现的角度看的。<br>最后，从操作系统进程调度的角度，又会统一看待这些不同硬件实现的核心，例如上面开始所提及的CPU（24个CPUs，从0编号开始），因为它们都有一个共同的特点：执行进程（或线程）。</p><h3 id="NUNA与SMP的概念"><a href="#NUNA与SMP的概念" class="headerlink" title="NUNA与SMP的概念"></a>NUNA与SMP的概念</h3><p>NUMA(Non-Uniform Memory Access，非一致性内存访问)和SMP(Symmetric Multi-Processor，对称多处理器系统)是两种不同的CPU硬件体系架构</p><p>SMP（Symmetric Multi-Processing）的主要特征是共享，所有的CPU共享使用全部资源，例如内存、总线和I&#x2F;O，多个CPU对称工作，彼此之间没有主次之分，平等地访问共享的资源，这样势必引入资源的竞争问题，从而导致它的扩展内力非常有限。特别是在现在一台机器CPU核心比较多，内存比较大的情况</p><p>NUMA技术将CPU划分成不同的组（Node)，每个Node由多个CPU组成，并且有独立的本地内存、I&#x2F;O等资源。Node之间通过互联模块连接和沟通，因此除了本地内存外，每个CPU仍可以访问远端Node的内存，只不过效率会比访问本地内存差一些，我们用Node之间的距离（Distance，抽象的概念）来定义各个Node之间互访资源的开销。</p><p>本章主要是去做NUMA的相关探索，下图是一个多核系统简单的topology</p><center>![coremuti.gif-23.7kB][2]</center><h3 id="Node-Socket-Core-Processor-Threads"><a href="#Node-Socket-Core-Processor-Threads" class="headerlink" title="Node-&gt;Socket-&gt;Core-&gt;Processor(Threads)"></a>Node-&gt;Socket-&gt;Core-&gt;Processor(Threads)</h3><p>如果你只知道CPU这么一个概念，那么是无法理解CPU的拓扑的。事实上，在NUMA架构下，CPU的概念从大到小依次是：Node、Socket、Core、Processor</p><ul><li>Sockets 可以理解成主板上cpu的插槽数，物理cpu的颗数，一般同一socket上的core共享三级缓存</li><li>Cores 而Socket中的每个核心被称为Core,常说的核,核有独立的物理资源.比如单独的一级二级缓存什么的</li><li>Threads 为了进一步提升CPU的处理能力，Intel又引入了HT（Hyper-Threading，超线程)的技术，一个Core打开HT之后，在OS看来就是两个核，当然这个核是逻辑上的概念，所以也被称为Logical Processor,如果不开超线程,threads应该与cores相等,如果开了超线程,threads应该是cores的倍数.相互之间共享物理资源</li><li>Nodes 上图的多核图中没有涉及， Node是NUMA体系中的概念．由于SMP体系中各个CPU访问内存只能通过单一的通道．导致内存访问成为瓶颈,cpu再多也无用．后来引入了NUMA．通过划分node,每个node有本地RAM,这样node内访问RAM速度会非常快．但跨Node的RAM访问代价会相对高一点，下面看一下两种架构的明显区别</li></ul><p><img src="/images/blog/o_200901040636smpnuma.png" alt="smpnuma.png-67.5kB"></p><p>由此可以总结这样的逻辑关系(包含):Node &gt; Socket &gt; Core &gt; Thread 区分这几个概念为了了解cache的分布,因为cpu绑定的目的就是提高cache的命中率,降低cpu颠簸.所以了解cache与cpu之间的mapping关系是非常重要的.通常来讲:</p><ul><li>同Socket内的cpu共享三级级缓存</li><li>每个Core有自己独立的二级缓存</li><li>一个Core上超线程出来的Threads,避免绑定，看似可能会提高L2 cache命中率,但也可能有严重的cpu争抢，导致性能非常差.</li></ul><h3 id="查看CPU信息"><a href="#查看CPU信息" class="headerlink" title="查看CPU信息"></a>查看CPU信息</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@server9 ~]<span class="hljs-comment"># lscpu </span><br>Architecture:          x86_64<br>CPU op-mode(s):        32-bit, 64-bit<br>Byte Order:            Little Endian<br>CPU(s):                24<br>On-line CPU(s) list:   0-23<br>Thread(s) per core:    2<br>Core(s) per socket:    6<br>Socket(s):             2<br>NUMA node(s):          2<br>Vendor ID:             GenuineIntel<br>CPU family:            6<br>Model:                 62<br>Model name:            Intel(R) Xeon(R) CPU E5-2620 v2 @ 2.10GHz<br>Stepping:              4<br>CPU MHz:               1607.894<br>BogoMIPS:              4205.65<br>Virtualization:        VT-x<br>L1d cache:             32K<br>L1i cache:             32K<br>L2 cache:              256K<br>L3 cache:              15360K<br>NUMA node0 CPU(s):     0-5,12-17<br>NUMA node1 CPU(s):     6-11,18-23<br></code></pre></td></tr></table></figure><p>2颗6核双线程，一共是24 processors,也可以看到是NUMA体系，可以使用以下命令详细查看numa信息.非NUMA体系时,所有cpu都划分为一个Node</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@server9 ~]<span class="hljs-comment"># numactl --hardware</span><br>available: 2 nodes (0-1)<br>node 0 cpus: 0 1 2 3 4 5 12 13 14 15 16 17<br>node 0 size: 31880 MB<br>node 0 free: 19634 MB<br>node 1 cpus: 6 7 8 9 10 11 18 19 20 21 22 23<br>node 1 size: 32253 MB<br>node 1 free: 29315 MB<br>node distances:<br>node   0   1 <br>  0:  10  21 <br>  1:  21  10 <br></code></pre></td></tr></table></figure><blockquote><p>cpu的id不连续的原因是开启了超线程，超线程的cpuid是从新的ID开始计数的，也就是从12开始计数的</p></blockquote><p>两个node，每个node32G内存左右，这台机器我的物理内存是64G</p><h3 id="通过命令行查看cpu信息"><a href="#通过命令行查看cpu信息" class="headerlink" title="通过命令行查看cpu信息"></a>通过命令行查看cpu信息</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 获取cpu名称与主频</span><br><span class="hljs-built_in">cat</span> /proc/cpuinfo | grep <span class="hljs-string">&#x27;model name&#x27;</span>  | <span class="hljs-built_in">cut</span> -f2 -d: | <span class="hljs-built_in">head</span> -n1 | sed <span class="hljs-string">&#x27;s/^ //&#x27;</span><br><br><span class="hljs-comment"># 获取逻辑核数</span><br><span class="hljs-built_in">cat</span> /proc/cpuinfo | grep <span class="hljs-string">&#x27;model name&#x27;</span>  | <span class="hljs-built_in">wc</span> -l<br><br><span class="hljs-comment"># 获取物理核数</span><br><span class="hljs-built_in">cat</span> /proc/cpuinfo | grep <span class="hljs-string">&#x27;physical id&#x27;</span> | <span class="hljs-built_in">sort</span> | <span class="hljs-built_in">uniq</span> | <span class="hljs-built_in">wc</span> -l<br><br><span class="hljs-comment"># 查看cpu的flags</span><br><span class="hljs-built_in">cat</span> /proc/cpuinfo | grep flags | <span class="hljs-built_in">uniq</span> | <span class="hljs-built_in">cut</span> -f2 -d : | sed <span class="hljs-string">&#x27;s/^ //&#x27;</span><br><br><span class="hljs-comment"># 是否打开超线程（检查 physical id * cpu cores 与 processor的比例 1:1为未开启）</span><br><span class="hljs-built_in">cat</span> /proc/cpuinfo <br><br><span class="hljs-comment"># 查看cache大小,X自省替换</span><br>sudo <span class="hljs-built_in">cat</span> /sys/devices/system/cpu/cpuX/cache/indexX/size<br><br><span class="hljs-comment"># 查看各个cpu之间与cache的mapping</span><br><span class="hljs-built_in">cat</span> /sys/devices/system/cpu/cpuX/cache/indexX/shared_cpu_list<br><br><span class="hljs-comment"># 获取CPU分布的信息（id-&gt; core信息）（这一个可以看出来CPU0和CPU12在同一个core）</span><br>egrep <span class="hljs-string">&#x27;processor|core id|physical id&#x27;</span> /proc/cpuinfo | <span class="hljs-built_in">cut</span> -d : -f 2 | <span class="hljs-built_in">paste</span> - - -  | awk <span class="hljs-string">&#x27;&#123;print &quot;CPU&quot;$1&quot;\tsocket &quot;$2&quot; core &quot;$3&#125;&#x27;</span><br>CPU0socket 0 core 0<br>CPU1socket 0 core 1<br>CPU2socket 0 core 2<br>CPU3socket 0 core 3<br>CPU4socket 0 core 4<br>CPU5socket 0 core 5<br>CPU6socket 1 core 0<br>CPU7socket 1 core 1<br>CPU8socket 1 core 2<br>CPU9socket 1 core 3<br>CPU10socket 1 core 4<br>CPU11socket 1 core 5<br>CPU12socket 0 core 0<br>CPU13socket 0 core 1<br>CPU14socket 0 core 2<br>CPU15socket 0 core 3<br>CPU16socket 0 core 4<br>CPU17socket 0 core 5<br>CPU18socket 1 core 0<br>CPU19socket 1 core 1<br>CPU20socket 1 core 2<br>CPU21socket 1 core 3<br>CPU22socket 1 core 4<br>CPU23socket 1 core 5<br><br></code></pre></td></tr></table></figure><p>lscpu,numactl都是读取proc,sys文件系统信息并进行格式化，输出人性化的内容．当没有网络,而lscpu,numactl都没有安装时，只能使用这种命令行方式了</p><p>能用工具还是用工具，工具就是解放双手的</p><h3 id="Cpu-Topology可视化"><a href="#Cpu-Topology可视化" class="headerlink" title="Cpu Topology可视化"></a>Cpu Topology可视化</h3><p>lstopo 指令由 hwloc 数据包提供，创建了用户的系统示意图。lstopo-no-graphics 指令提供详尽的文本输出<br>通过lscpu与numactl获取的信息，必要的时候查询了&#x2F;sys&#x2F;devices&#x2F;system&#x2F;cpu&#x2F;cpuX&#x2F;*的数据将正在使用的 Intel(R) Xeon(R) CPU E5-2620 v2 @ 2.10GHz的topology进行可视化<br>详细的cache信息可以通过sysfs查看</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">ls</span> /sys/devices/system/cpu/cpu0/cache/<br>index0 index1 index2 index3<br></code></pre></td></tr></table></figure><p>包含以下4个目录：</p><ul><li>index0:1级数据cache </li><li>index1:1级指令cache </li><li>index2:2级cache </li><li>index3:3级cache,对应cpuinfo里的cache</li></ul><p>目录里的文件是cache信息描述，以本机的cpu0&#x2F;index0为例简单解释一下：</p><table><thead><tr><th align="center">文件</th><th align="center">内容</th><th align="center">说明</th></tr></thead><tbody><tr><td align="center">type</td><td align="center">Data</td><td align="center">数据cache，如果查看index1就是Instruction</td></tr><tr><td align="center">Level</td><td align="center">1</td><td align="center">L1</td></tr><tr><td align="center">Size</td><td align="center">32K</td><td align="center">大小为32K</td></tr><tr><td align="center">coherency_line_size</td><td align="center">64</td><td align="center">64<em>4</em>128&#x3D;32K</td></tr><tr><td align="center">physical_line_partition</td><td align="center">1</td><td align="center"></td></tr><tr><td align="center">ways_of_associativity</td><td align="center">4</td><td align="center"></td></tr><tr><td align="center">number_of_sets</td><td align="center">128</td><td align="center"></td></tr><tr><td align="center">shared_cpu_map</td><td align="center">00000101</td><td align="center">表示这个cache被CPU0和CPU8 share</td></tr></tbody></table><p>解释一下shared_cpu_map内容的格式：<br>表面上看是2进制，其实是16进制表示，每个bit表示一个cpu，1个数字可以表示4个cpu 截取00000101的后4位，转换为2进制表示</p><p>|CPU id|15|14|13|12|11|10|9|8|7|6|5|4|3|2|1|0|<br>| :—: | :—: | :—-: | :—-: | :—-: | :—-: | :—-: | :—-: | :—-: | :—-: | :—-: | :—-: | :—-: | :—-: | :—-: | :—-: |<br>|0×0101的2进制表示|0|0|0|0|0|0|0|1|0|0|0|0|0|0|0|1|</p><p>0101表示cpu8和cpu0，即cpu0的L1 data cache是和cpu8共享的。<br>也可以使用上面提到的lstopo-no-graphics命令进行查询</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@server9 ~]<span class="hljs-comment"># lstopo-no-graphics </span><br>Machine (63GB)<br>  NUMANode L<span class="hljs-comment">#0 (P#0 31GB)</span><br>    Socket L<span class="hljs-comment">#0 + L3 L#0 (15MB)</span><br>      L2 L<span class="hljs-comment">#0 (256KB) + L1d L#0 (32KB) + L1i L#0 (32KB) + Core L#0</span><br>        PU L<span class="hljs-comment">#0 (P#0)</span><br>        PU L<span class="hljs-comment">#1 (P#12)</span><br>      L2 L<span class="hljs-comment">#1 (256KB) + L1d L#1 (32KB) + L1i L#1 (32KB) + Core L#1</span><br>        PU L<span class="hljs-comment">#2 (P#1)</span><br>        PU L<span class="hljs-comment">#3 (P#13)</span><br>      L2 L<span class="hljs-comment">#2 (256KB) + L1d L#2 (32KB) + L1i L#2 (32KB) + Core L#2</span><br>        PU L<span class="hljs-comment">#4 (P#2)</span><br>        PU L<span class="hljs-comment">#5 (P#14)</span><br>      L2 L<span class="hljs-comment">#3 (256KB) + L1d L#3 (32KB) + L1i L#3 (32KB) + Core L#3</span><br>        PU L<span class="hljs-comment">#6 (P#3)</span><br>        PU L<span class="hljs-comment">#7 (P#15)</span><br>      L2 L<span class="hljs-comment">#4 (256KB) + L1d L#4 (32KB) + L1i L#4 (32KB) + Core L#4</span><br>        PU L<span class="hljs-comment">#8 (P#4)</span><br>        PU L<span class="hljs-comment">#9 (P#16)</span><br>      L2 L<span class="hljs-comment">#5 (256KB) + L1d L#5 (32KB) + L1i L#5 (32KB) + Core L#5</span><br>        PU L<span class="hljs-comment">#10 (P#5)</span><br>        PU L<span class="hljs-comment">#11 (P#17)</span><br>    HostBridge L<span class="hljs-comment">#0</span><br>      PCIBridge<br>        PCI 1000:0086<br>      PCIBridge<br>        PCI 8086:1521<br>          Net L<span class="hljs-comment">#0 &quot;enp4s0f0&quot;</span><br>        PCI 8086:1521<br>          Net L<span class="hljs-comment">#1 &quot;enp4s0f1&quot;</span><br>        PCI 8086:1521<br>          Net L<span class="hljs-comment">#2 &quot;enp4s0f2&quot;</span><br>        PCI 8086:1521<br>          Net L<span class="hljs-comment">#3 &quot;enp4s0f3&quot;</span><br>      PCIBridge<br>        PCI 8086:10fb<br>          Net L<span class="hljs-comment">#4 &quot;enp6s0f0&quot;</span><br>        PCI 8086:10fb<br>          Net L<span class="hljs-comment">#5 &quot;enp6s0f1&quot;</span><br>      PCIBridge<br>        PCI 8086:1d6b<br>      PCIBridge<br>        PCI 102b:0532<br>          GPU L<span class="hljs-comment">#6 &quot;card0&quot;</span><br>          GPU L<span class="hljs-comment">#7 &quot;controlD64&quot;</span><br>      PCI 8086:1d02<br>        Block L<span class="hljs-comment">#8 &quot;sda&quot;</span><br>  NUMANode L<span class="hljs-comment">#1 (P#1 31GB) + Socket L#1 + L3 L#1 (15MB)</span><br>    L2 L<span class="hljs-comment">#6 (256KB) + L1d L#6 (32KB) + L1i L#6 (32KB) + Core L#6</span><br>      PU L<span class="hljs-comment">#12 (P#6)</span><br>      PU L<span class="hljs-comment">#13 (P#18)</span><br>    L2 L<span class="hljs-comment">#7 (256KB) + L1d L#7 (32KB) + L1i L#7 (32KB) + Core L#7</span><br>      PU L<span class="hljs-comment">#14 (P#7)</span><br>      PU L<span class="hljs-comment">#15 (P#19)</span><br>    L2 L<span class="hljs-comment">#8 (256KB) + L1d L#8 (32KB) + L1i L#8 (32KB) + Core L#8</span><br>      PU L<span class="hljs-comment">#16 (P#8)</span><br>      PU L<span class="hljs-comment">#17 (P#20)</span><br>    L2 L<span class="hljs-comment">#9 (256KB) + L1d L#9 (32KB) + L1i L#9 (32KB) + Core L#9</span><br>      PU L<span class="hljs-comment">#18 (P#9)</span><br>      PU L<span class="hljs-comment">#19 (P#21)</span><br>    L2 L<span class="hljs-comment">#10 (256KB) + L1d L#10 (32KB) + L1i L#10 (32KB) + Core L#10</span><br>      PU L<span class="hljs-comment">#20 (P#10)</span><br>      PU L<span class="hljs-comment">#21 (P#22)</span><br>    L2 L<span class="hljs-comment">#11 (256KB) + L1d L#11 (32KB) + L1i L#11 (32KB) + Core L#11</span><br>      PU L<span class="hljs-comment">#22 (P#11)</span><br>      PU L<span class="hljs-comment">#23 (P#23)</span><br></code></pre></td></tr></table></figure><p>这个得到的是文本的拓扑，这个转换成一个图看的要清楚一些<br><img src="/images/blog/o_200901040628nodesock.png" alt="nodesock.png-45.9kB"></p><h4 id="NUMA分组信息"><a href="#NUMA分组信息" class="headerlink" title="NUMA分组信息"></a>NUMA分组信息</h4><ul><li>通过图可以看到cpu为numa架构,且有两个node</li><li>将同一socket内的cpu(threads)都划分在一个node中.通过上图也解释了node中cpu序列不连续的问题.因为同一个Core上的两个Threads是超线程出来的.超线程Thread的cpu id在原有的core id基础上增长的</li><li>每个node中有32G左右的本地RAM可用</li></ul><h4 id="cache信息"><a href="#cache信息" class="headerlink" title="cache信息"></a>cache信息</h4><ul><li>每个core都有独立的二级缓存,而不是socket中所有的core共享二级缓存</li><li>同node中的cpu共享三级缓存</li><li>跨node的内存访问的花费要大些</li></ul><h3 id="cpu绑定注意的几点"><a href="#cpu绑定注意的几点" class="headerlink" title="cpu绑定注意的几点"></a>cpu绑定注意的几点</h3><ul><li>Numa体系中,如果夸node绑定,性能会下降.因为L3 cache命中率低,跨node内存访问代价高.</li><li>绑定同Node,同一个Core中的两个超线程出来的cpu,性能会急剧下降.cpu密集型的线程硬件争用严重.”玩转CPU Topology”中也提到了.</li><li>Numa架构可能引起swap insanity.需要注意</li></ul><h3 id="测试CPU绑定性能"><a href="#测试CPU绑定性能" class="headerlink" title="测试CPU绑定性能"></a>测试CPU绑定性能</h3><p>这个部分就不在这里赘述了，上面是把cpu比较清晰的剥离出来，至于效果，需要在实际环境当中去验证了，有可能变坏，也有可能变好</p><p>本篇参考了很多网络上的很多其他资料</p>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>rbd无法map(rbd feature disable)</title>
    <link href="/2016/06/07/rbd%E6%97%A0%E6%B3%95map(rbd%20feature%20disable)/"/>
    <url>/2016/06/07/rbd%E6%97%A0%E6%B3%95map(rbd%20feature%20disable)/</url>
    
    <content type="html"><![CDATA[<p>在jewel版本下默认开启了rbd的一些属性</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]# ceph --show-config|grep rbd|grep features <br>rbd_default_features = 61<br></code></pre></td></tr></table></figure><!--break--><p>RBD属性表：<br><img src="/images/blog/o_200901040252rbd%E5%B1%9E%E6%80%A7.png" alt="此处输入图片的描述"><br>61的意思是上面图中的bit码相加得到的值</p><!--break--><p>对rbd进行内核的map操作</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]# rbd map mytest<br>rbd: sysfs write failed<br>RBD image feature set mismatch. You can disable features unsupported by the kernel with &quot;rbd feature disable&quot;.<br>In some cases useful info is found in syslog - try &quot;dmesg | tail&quot; or so.<br>rbd: map failed: (6) No such device or address<br></code></pre></td></tr></table></figure><p>根据提示查询打印的信息</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]# dmesg | tail<br>[10440.462708] rbd: image mytest: image uses unsupported features: 0x3c<br></code></pre></td></tr></table></figure><p>这个地方提示的很清楚了，不支持的属性0x3c，0x3c是16进制的数值，换算成10进制是3*16+12&#x3D;60<br>60的意思是不支持：</p><blockquote><p>32+16+8+4 &#x3D; exclusive-lock, object-map, fast-diff, deep-flatten</p></blockquote><p>也就是不支持这些属性，现在动态关闭这些属性</p><p>查看当前使用的image属性</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]# rbd info mytest<br>rbd image &#x27;mytest&#x27;:<br>size 2000 MB in 500 objects<br>order 22 (4096 kB objects)<br>block_name_prefix: rbd_data.10276b8b4567<br>format: 2<br>features: layering, exclusive-lock, object-map, fast-diff, deep-flatten<br>flags: <br></code></pre></td></tr></table></figure><p>开启的属性有4个是不支持的，关闭这些属性<br>语法是：</p><blockquote><p>rbd feature disable {poolname}&#x2F;{imagename} {feature}</p></blockquote><p>具体到这个测试的命令</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]# rbd feature disable rbd/mytest deep-flatten<br>[root@lab8106 ~]# rbd feature disable rbd/mytest fast-diff<br>[root@lab8106 ~]# rbd feature disable rbd/mytest object-map<br>[root@lab8106 ~]# rbd feature disable rbd/mytest exclusive-lock<br></code></pre></td></tr></table></figure><p>再次查询image的info信息</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]# rbd info mytest<br>rbd image &#x27;mytest&#x27;:<br>size 2000 MB in 500 objects<br>order 22 (4096 kB objects)<br>block_name_prefix: rbd_data.10276b8b4567<br>format: 2<br>features: layering<br>flags: <br></code></pre></td></tr></table></figure><p>可以看到已经关闭了不支持的属性<br>进行kernel rbd 的map的操作</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]# rbd map mytest<br>/dev/rbd1<br></code></pre></td></tr></table></figure><p>如果不想动态的关闭，那么在创建rbd之前，在配置文件中设置这个参数即可</p><blockquote><p>rbd_default_features &#x3D; 3</p></blockquote><p>关于属性支持的，目前到内核4.6仍然只支持</p><blockquote><p>layering,striping &#x3D; 1 + 2</p></blockquote><p>这两个属性</p>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>加速OSD的启动</title>
    <link href="/2016/06/07/%E5%8A%A0%E9%80%9FOSD%E7%9A%84%E5%90%AF%E5%8A%A8/"/>
    <url>/2016/06/07/%E5%8A%A0%E9%80%9FOSD%E7%9A%84%E5%90%AF%E5%8A%A8/</url>
    
    <content type="html"><![CDATA[<p>ceph是目前开源分布式存储里面最好的一个，但是在高负载下会有很多异常的情况会发生，有些问题无法完全避免，但是可以进行一定的控制，比如：在虚拟化场景下，重启osd会让虚拟机挂起的情况</p><p>重新启动osd会给这个osd进程所在的磁盘带来额外的负载，随着前面业务的需求的增长，会增加对存储的I&#x2F;O的需求，虽然这个对于整个业务系统来说是好事，但是在某些情况下，会越来越接近存储吞吐量的极限，通常情况下没有异常发生的时候，都是正常的，一旦发生异常，集群超过了临界值，性能会变得剧烈的抖动</p><p>对于这种情况，一般会升级硬件来避免集群从一个高负载的集群变成一个过载的集群。本章节的重点在重启osd进程这个问题</p><h3 id="问题分析"><a href="#问题分析" class="headerlink" title="问题分析"></a>问题分析</h3><p>OSD重启是需要重视的，这个地方是ceph的一个设计的弱点。ceph集群有很多的OSD进程，OSD管理对磁盘上的对象的访问，磁盘的对象被分布到PG组当中，对象有相同的分布，副本会在相同的PG当中存在，如果不理解可以看看（<a href="http://docs.ceph.com/docs/master/architecture/">ceph概览</a>）</p><p>当集群OSD进程出现down的情况，会被mon认为 “OUT” 了，这个 “OUT” 不是触发迁移的那个 “OUT”，是不服务的 “OUT” ,这个OSD上受影响的PG的I&#x2F;O请求会被其他拥有这个PG的OSD接管，当OSD重新启动的时候,OSD会被加入进来，将会检查PG，看是否有在down的期间错过东西，然后进行更新，这里问题就来了，启动之后会访问磁盘检查PG是否有缺失的东西进行更新，会进行一定量的数据恢复，同时会开始接受新的IO的请求，如果本来磁盘就只剩很少的余量，那么一旦请求发送到这个OSD上，那么性能将会开始下降</p><p>如果去看ceph的邮件列表，在极端情况下，这种效应会让整个集群停机，这发生在OSD太忙了，连心跳都无法回复，然后mon就会把它标记为down，这个时候OSD的进程都还在的，这个时候客户端的请求会导入到其他的OSD上，然后负载小了，OSD又会自己进来，然后又开始响应请求了，然后之前没有受影响的OSD节点，需要把新写入的数据同步过来，这个又增加了其他的OSD的负载了，一旦集群接近I&#x2F;O的限制，也会让其他的OSD无法响应了，结果就是整个集群的OSD在反复的”in”和”out”状态之间变化了，集群在这种情况下，就无法接收客户端的请求了，如果不人工干预甚至无法恢复正常，这个在高负载下是很好复现出来的;另外一种较轻的情况，在OSD重启过程，I&#x2F;O可能会hung住，影响性能.如果不能避免，至少能想办法去降低这个影响</p><p>我们能做些什么？在ceph开发者列表当中有开发者提出了这个<a href="http://thread.gmane.org/gmane.comp.file-systems.ceph.user/25881/focus=25890">设计上需要修复</a>，这个估计需要等很久以后的事情了，我们能做什么来降低这个的影响？最明显的一点是要保证集群有足够的I&#x2F;O的余量，另一种思路就是减少关键过程启动检查和接收I&#x2F;O的竞争</p><h3 id="减少OSD启动过程当中的IO"><a href="#减少OSD启动过程当中的IO" class="headerlink" title="减少OSD启动过程当中的IO"></a>减少OSD启动过程当中的IO</h3><p>OSD在启动的时候可以预测到磁盘的访问的模式。我们可以了解这个访问模式，然后提前将文件读取到内核的缓存当中。这样这些文件在启动的时候就不需要再次访问磁盘了，意味着更少的磁盘消耗和更好的性能</p><p>现在来定位下OSD启动过程中做了哪些事情，使用性能大师 Brendan Gregg 的 <a href="https://github.com/brendangregg/perf-tools">opensnoop</a> 工具，一个OSD启动的过程如下：</p><h4 id="OSD启动过程"><a href="#OSD启动过程" class="headerlink" title="OSD启动过程"></a>OSD启动过程</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]# opensnoop ceph-7<br>Tracing open()s for filenames containing &quot;ceph-7&quot;. Ctrl-C to end.<br>COMM             PID      FD FILE<br>ceph             osd     0x3 /var/lib/ceph/osd/ceph-7/<br>ceph             osd     0x4 /var/lib/ceph/osd/ceph-7/type<br>ceph             osd     0x4 /var/lib/ceph/osd/ceph-7/magic<br>ceph             osd     0x4 /var/lib/ceph/osd/ceph-7/whoami<br>ceph             osd     0x4 /var/lib/ceph/osd/ceph-7/ceph_fsid<br>ceph             osd     0x4 /var/lib/ceph/osd/ceph-7/fsid<br>ceph             osd     0xb /var/lib/ceph/osd/ceph-7/fsid<br>ceph             osd     0xb /var/lib/ceph/osd/ceph-7/fsid<br>ceph             osd     0xc /var/lib/ceph/osd/ceph-7/store_version<br>ceph             osd     0xc /var/lib/ceph/osd/ceph-7/superblock<br>ceph             osd     0xc /var/lib/ceph/osd/ceph-7<br>ceph             osd     0xd /var/lib/ceph/osd/ceph-7/fiemap_test<br>ceph             osd     0xd /var/lib/ceph/osd/ceph-7/xattr_test<br>ceph             osd     0xd /var/lib/ceph/osd/ceph-7/current<br>ceph             osd     0xe /var/lib/ceph/osd/ceph-7/current/commit_op_seq<br>ceph             osd     0xf /var/lib/ceph/osd/ceph-7/current/omap/LOCK<br>ceph             osd    0x10 /var/lib/ceph/osd/ceph-7/current/omap/CURRENT<br>ceph             osd    0x10 /var/lib/ceph/osd/ceph-7/current/omap/MANIFEST-000135<br></code></pre></td></tr></table></figure><p>开始的时候，OSD读取了很多元数据文件，没有什么特别的<br>下面读取omap的数据库文件，读取了一部分的osdmap文件</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs raw">ceph             osd    0x10 /var/lib/ceph/osd/ceph-7/current/omap/000137.log<br>ceph             osd    0x11 /var/lib/ceph/osd/ceph-7/current/omap/000143.sst<br>ceph             osd    0x11 /var/lib/ceph/osd/ceph-7/current/omap/000143.sst<br>ceph             osd    0x10 /var/lib/ceph/osd/ceph-7/current/omap/000144.log<br>ceph             osd    0x11 /var/lib/ceph/osd/ceph-7/current/omap/MANIFEST-000142<br>ceph             osd    0x12 /var/lib/ceph/osd/ceph-7/current/omap/000142.dbtmp<br>ceph             osd    0x12 /var/lib/ceph/osd/ceph-7/current/omap/000138.sst<br>ceph             osd    0x12 /var/lib/ceph/osd/ceph-7/journal<br>ceph             osd    0x12 /var/lib/ceph/osd/ceph-7/journal<br>ceph             osd    0x13 /var/lib/ceph/osd/ceph-7/store_version<br>ceph             osd    0x13 /var/lib/ceph/osd/ceph-7/current/meta/osd\usuperblock__0_23C2FCDE__none<br>ceph             osd    0x14 /var/lib/ceph/osd/ceph-7/current/meta/DIR_5/osdmap.298__0_AC96EE75__none<br>ceph             osd    0x15 /var/lib/ceph/osd/ceph-7/current/0.3b_head<br>ceph             osd    0x15 /var/lib/ceph/osd/ceph-7/current/meta/DIR_5/osdmap.297__0_AC96EEA5__none<br>ceph             osd    0x16 /var/lib/ceph/osd/ceph-7/current/0.7_head<br>ceph             osd    0x16 /var/lib/ceph/osd/ceph-7/current/0.34_head<br>ceph             osd    0x16 /var/lib/ceph/osd/ceph-7/current/0.20_head<br>ceph             osd    0x16 /var/lib/ceph/osd/ceph-7/current/0.22_head<br></code></pre></td></tr></table></figure><p>可以看到读取一个sst后，就会继续读取pg的目录</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs raw">ceph             osd    0x16 /var/lib/ceph/osd/ceph-7/current/omap/000139.sst<br>ceph             osd    0x16 /var/lib/ceph/osd/ceph-7/current/0.ec_head<br>ceph             osd    0x16 /var/lib/ceph/osd/ceph-7/current/0.7e_head<br>ceph             osd    0x16 /var/lib/ceph/osd/ceph-7/current/0.14b_head<br>[···]<br>ceph             osd    0x16 /var/lib/ceph/osd/ceph-7/current/omap/000141.sst<br>ceph             osd    0x16 /var/lib/ceph/osd/ceph-7/current/0.2fb_head<br>ceph             osd    0x16 /var/lib/ceph/osd/ceph-7/current/0.cf_head<br>ceph             osd    0x16 /var/lib/ceph/osd/ceph-7/current/0.10f_head<br>[···]<br>ceph             osd    0x16 /var/lib/ceph/osd/ceph-7/current/omap/000140.sst<br>ceph             osd    0x16 /var/lib/ceph/osd/ceph-7/current/0.8f_head<br>ceph             osd    0x16 /var/lib/ceph/osd/ceph-7/current/0.10c_head<br>ceph             osd    0x16 /var/lib/ceph/osd/ceph-7/current/0.14e_head<br>[···]<br></code></pre></td></tr></table></figure><p>然后会读取每个pg里面的_head_文件</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs raw">tp_fstore_op     23688  0x17 /var/lib/ceph/osd/ceph-7/current/0.23a_head/__head_0000023A__0<br>tp_fstore_op     23688  0x18 /var/lib/ceph/osd/ceph-7/current/0.1a2_head/DIR_2/DIR_A/DIR_1/__head_000001A2__0<br>&lt;...&gt;            23689  0x19 /var/lib/ceph/osd/ceph-7/current/0.2ea_head/__head_000002EA__0<br>[···]<br></code></pre></td></tr></table></figure><p>然后会进行osdmap文件的操作</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs raw">tp_fstore_op     23688  0x3a /var/lib/ceph/osd/ceph-7/current/meta/DIR_2/inc\uosdmap.299__0_C67CF872__none<br>tp_fstore_op     23688  0x4e /var/lib/ceph/osd/ceph-7/current/meta/DIR_5/osdmap.299__0_AC96EF05__none<br>tp_fstore_op     23688  0x14 /var/lib/ceph/osd/ceph-7/current/meta/DIR_2/inc\uosdmap.300__0_C67CF142__none<br>tp_fstore_op     23688  0x4f /var/lib/ceph/osd/ceph-7/current/meta/DIR_5/osdmap.300__0_AC96E415__none<br>tp_fstore_op     23688  0x15 /var/lib/ceph/osd/ceph-7/current/meta/osd\usuperblock__0_23C2FCDE__none<br>tp_fstore_op     23689  0x6e /var/lib/ceph/osd/ceph-7/current/meta/DIR_2/inc\uosdmap.301__0_C67CF612__none<br>tp_fstore_op     23689  0x55 /var/lib/ceph/osd/ceph-7/current/meta/DIR_5/osdmap.301__0_AC96E5A5__none<br>tp_fstore_op     23689  0xbf /var/lib/ceph/osd/ceph-7/current/meta/DIR_2/inc\uosdmap.302__0_C67CF7A2__none<br>tp_fstore_op     23689  0x60 /var/lib/ceph/osd/ceph-7/current/meta/DIR_5/osdmap.302__0_AC96E575__none<br>tp_fstore_op     23688  0x86 /var/lib/ceph/osd/ceph-7/current/meta/DIR_2/inc\uosdmap.303__0_C67CF772__none<br>tp_fstore_op     23688  0x6a /var/lib/ceph/osd/ceph-7/current/meta/DIR_5/osdmap.303__0_AC96FA05__none<br>s<br></code></pre></td></tr></table></figure><p>我们无法确定哪些对象将需要读取，单我们知道，所有的OMAP和元数据文件将会打开，_head_文件将会打开</p><h4 id="使用vmtouch进行预读取"><a href="#使用vmtouch进行预读取" class="headerlink" title="使用vmtouch进行预读取"></a>使用vmtouch进行预读取</h4><p>下面将进入 <a href="https://hoytech.com/vmtouch/">vmtouch</a> ,这个小工具能够读取文件并锁定到内存当中，这样后续的I&#x2F;O请求能够从缓存当中读取它们，这样就减少了对磁盘的访问请求<br>在这里我们的访问模式是这样的：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ceph-7]# vmtouch -t /var/lib/ceph/osd/ceph-7/current/meta/ /var/lib/ceph/osd/ceph-7/current/omap/<br>          Files: 618<br>     Directories: 6<br>   Touched Pages: 3972 (15M)<br>         Elapsed: 0.009621 seconds<br></code></pre></td></tr></table></figure><p>关于这个vmtouch很好使用也很强大，可以使用  <code>vmtouch -L</code> 将数据锁定到内存当中去，这里用 <code>-t</code> 也可以，使用 <code>-v</code>  参数能打印更多详细的信息,这个效果有多大？这个原作者的效果很好，我的环境太小，看不出太多的效果，但是从原理上看，应该是会有用的，我的读取过程跟原作者的读取过程有一定的差别，作者的数据库文件是 <code>ldb</code> ，我的环境是 <code>sst</code>，并且作者的压力应该是很大的情况下的，我的环境较小</p><h3 id="判断是否有作用"><a href="#判断是否有作用" class="headerlink" title="判断是否有作用"></a>判断是否有作用</h3><p>一个很好的衡量的方法就是看启动过程当中的  peering 的阶段的长度, peering 状态是osd做相互的协调的，PG的请求在这个时候是无法响应的，理想状况下这个过程会很快，无法察觉，如果集群集群处于高负载或者过载状态，这个持续的时间就会很久，然后关闭一个OSD，然后等待一分钟，以便让一部分写入只写到了其他OSD，在down掉的OSD启动后，需要从其他OSD恢复一些数据，然后重新打开，从日志当中，绘制一段时间的 peering 状态PG的数目，score是统计的所有时间线上 peering 状态的计数的总和</p><p>为了验证这个vmtouch将会减少 peering 的状态,将负载压到略小于集群满载情况</p><h4 id="第一个实验是OSD重启（无vmtouch）"><a href="#第一个实验是OSD重启（无vmtouch）" class="headerlink" title="第一个实验是OSD重启（无vmtouch）"></a>第一个实验是OSD重启（无vmtouch）</h4><p><img src="/images/blog/o_200901040115vmtouchno.png" alt="vmtouchno.png-22.3kB"><br>可以看到超过30s时，大量的pg是peering状态，导致集群出现缓慢</p><h4 id="第二个实验中使用vmtouch预读取OMAP的数据库文件"><a href="#第二个实验中使用vmtouch预读取OMAP的数据库文件" class="headerlink" title="第二个实验中使用vmtouch预读取OMAP的数据库文件"></a>第二个实验中使用vmtouch预读取OMAP的数据库文件</h4><p><img src="/images/blog/o_200901040122vmtouch.png" alt="vmtouch.png-17.9kB"><br>这些 <code>peering</code> 状态并没有消失，但是可以看到有很大的改善 <code>peering</code> 会更早的开始（OMAP已经加载），总体的score也要小很多，这个是一个很不错的结果</p><h3 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h3><p>根据之前监测到的读取数据的情况，预读取文件，能够有不错的改善，虽然不是完整的解决方案，但是能够帮助改善一个痛点，从长远来看，希望ceph能改进设计，是这个情况消失</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>本章节里面介绍了两个工具<br><strong>opensnoop</strong><br>这个工具已经存在了很久很久了，也是到现在才看到的，一个用于监控文件的操作，是Gregg 大师的作品，仅仅是一个shell脚本就能实现监控，关键还在于其对操作系统的了解<br><strong>vmtouch</strong><br>这个是将数据加载到内存的，以前关注的是清理内存，其实在某些场景下，能够预加载到内存将会解决很多问题，关键看怎么去用了</p><h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章"></a>参考文章</h3><p><a href="https://blog.flyingcircus.io/2016/03/11/improving-ceph-osd-start-up-behaviour-with-vmtouch/">Improving Ceph OSD start-up behaviour with vmtouch</a><br><a href="https://github.com/brendangregg/perf-tools">opensnoop</a><br><a href="https://hoytech.com/vmtouch/">vmtouch</a></p><h3 id="变更记录"><a href="#变更记录" class="headerlink" title="变更记录"></a>变更记录</h3><table><thead><tr><th align="center">Why</th><th align="center">Who</th><th align="center">When</th></tr></thead><tbody><tr><td align="center">创建</td><td align="center">武汉-运维-磨渣</td><td align="center">2016-06-07</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>利用虚拟化环境虚拟nvme盘</title>
    <link href="/2016/06/04/%E5%88%A9%E7%94%A8%E8%99%9A%E6%8B%9F%E5%8C%96%E7%8E%AF%E5%A2%83%E8%99%9A%E6%8B%9Fnvme%E7%9B%98/"/>
    <url>/2016/06/04/%E5%88%A9%E7%94%A8%E8%99%9A%E6%8B%9F%E5%8C%96%E7%8E%AF%E5%A2%83%E8%99%9A%E6%8B%9Fnvme%E7%9B%98/</url>
    
    <content type="html"><![CDATA[<h2 id="前情介绍"><a href="#前情介绍" class="headerlink" title="前情介绍"></a>前情介绍</h2><h3 id="SPDK"><a href="#SPDK" class="headerlink" title="SPDK"></a>SPDK</h3><p>SPDK的全称为Storage Performance Development Kit ，是Intel发起的一个开源驱动项目，这个是一个开发套件，可以让应用程序在用户态去访问存储资源，具体做能做什么可以去官网看一下 <a href="https://software.intel.com/en-us/articles/introduction-to-the-storage-performance-development-kit-spdk">SPDK官网</a></p><h3 id="NVME"><a href="#NVME" class="headerlink" title="NVME"></a>NVME</h3><p>NVMe其实与AHCI一样都是逻辑设备接口标准，NVMe全称Non-Volatile Memory Express，非易失性存储器标准，是使用PCI-E通道的SSD一种规范，NVMe的设计之初就有充分利用到PCI-E SSD的低延时以及并行性，还有当代处理器、平台与应用的并行性。SSD的并行性可以充分被主机的硬件与软件充分利用，相比与现在的AHCI标准，NVMe标准可以带来多方面的性能提升。</p><h3 id="Bluestore"><a href="#Bluestore" class="headerlink" title="Bluestore"></a>Bluestore</h3><p>BlueStore 是用来存储ceph的数据的地方，提供了一种在块设备上直接写入方式的存储。这个是因为之前ceph社区尝试做了一个kvstore，但是性能达不到想要的效果，然后基于rocksdb的原型，重新开发了一套存储系统，BlueStore直接消耗原始分区。还有一个分区是存储元数据的，实际上就是一个RocksDB键&#x2F;值数据库存储，这个比之前的filestore最大的优势就是去掉了journal，从而提供了更平滑的IO</p><h3 id="SPDK-NVME-Bluestore能产生什么化学反应"><a href="#SPDK-NVME-Bluestore能产生什么化学反应" class="headerlink" title="SPDK+NVME+Bluestore能产生什么化学反应"></a>SPDK+NVME+Bluestore能产生什么化学反应</h3><p>目前这一块走的比较前沿的就是xsky了，这块的最初的推动力量是Intel，NVME的硬件的推出，需要一个很好的催化剂，传统的内核中断式的访问磁盘的方式，已经不能最大化发挥NVME的性能了，因此推出了SPDK的套件，可以在用户态的去访问磁盘数据，Bluestore按照这个标准就可以去以最大化的跑出磁盘的性能了，从而给上层提供一个非常强悍的IO性能，目前来说这几项都是很新的东西，如果没有特别强的技术，或者找Intel做技术支持话，用好还是需要再等一段时间</p><h2 id="开篇"><a href="#开篇" class="headerlink" title="开篇"></a>开篇</h2><h3 id="本篇文章做什么"><a href="#本篇文章做什么" class="headerlink" title="本篇文章做什么"></a>本篇文章做什么</h3><p>之前有篇文章已经实现了bluestore的配置，这个配置并不难，并且用普通的硬盘就能实现，这里是讲怎么弄出来NVME磁盘，因为NVME的磁盘很贵，并不是每个人都能有环境的，这里是用虚拟化的方式虚拟出nvme，以供以后进行相关的功能验证</p><h3 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h3><p>准备好kvm虚拟化的环境，这个地方就不在这里赘述了，本环境采用的是ubuntu的宿主机，如果是centos需要另做改动，如果有需要欢迎留言</p><h3 id="安装操作系统"><a href="#安装操作系统" class="headerlink" title="安装操作系统"></a>安装操作系统</h3><h4 id="创建两个磁盘"><a href="#创建两个磁盘" class="headerlink" title="创建两个磁盘"></a>创建两个磁盘</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs raw">qemu-img create -f raw /mnt/localdisk.raw  40G<br>qemu-img create -f raw  /mnt/nvme.raw 50G<br></code></pre></td></tr></table></figure><h4 id="执行安装操作系统"><a href="#执行安装操作系统" class="headerlink" title="执行安装操作系统"></a>执行安装操作系统</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">virt-install --name nvmetest --ram 4096 --vcpus=2 --disk path=/mnt/localdisk.raw --network bridge=br0 --cdrom /mnt/CentOS-7-x86_64-DVD-1503-01.iso --vnclisten=192.168.8.107 --vncport=7000 --vnc --autostart<br></code></pre></td></tr></table></figure><p>上面的iso文件需要提前准备，vnclisten就用宿主机的IP即可</p><p>都安装好了以后，先停止虚拟机，需要对配置文件做一些改动，因为virsh管理的时候有一些参数是不支持的，这个需要自己做一个  qemu:commandline 的改动</p><h4 id="停止掉虚拟机"><a href="#停止掉虚拟机" class="headerlink" title="停止掉虚拟机"></a>停止掉虚拟机</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">virsh destroy nvmetest<br></code></pre></td></tr></table></figure><h4 id="编辑配置文件"><a href="#编辑配置文件" class="headerlink" title="编辑配置文件"></a>编辑配置文件</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">virsh edit nvmetest<br></code></pre></td></tr></table></figure><p>内容如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs raw">&lt;domain type=&#x27;kvm&#x27;&gt;<br>····<br>&lt;/domain&gt;<br></code></pre></td></tr></table></figure><p>修改为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs raw">&lt;domain type=&#x27;kvm&#x27; xmlns:qemu=&#x27;http://libvirt.org/schemas/domain/qemu/1.0&#x27;&gt;<br>····<br>  &lt;qemu:commandline&gt;<br>    &lt;qemu:arg value=&#x27;-drive&#x27;/&gt;<br>    &lt;qemu:arg value=&#x27;file=/mnt/nvme.raw,if=none,id=D22,format=raw&#x27;/&gt;<br>    &lt;qemu:arg value=&#x27;-device&#x27;/&gt;<br>    &lt;qemu:arg value=&#x27;nvme,drive=D22,serial=1235&#x27;/&gt;<br>  &lt;/qemu:commandline&gt;<br>&lt;/domain&gt;<br></code></pre></td></tr></table></figure><blockquote><p>这个地方一定要注意加入的这个固定格式一定要写到最后的位置，否则不生效</p></blockquote><h3 id="检查虚拟机的磁盘是否生成"><a href="#检查虚拟机的磁盘是否生成" class="headerlink" title="检查虚拟机的磁盘是否生成"></a>检查虚拟机的磁盘是否生成</h3><h4 id="启动测试的虚拟机"><a href="#启动测试的虚拟机" class="headerlink" title="启动测试的虚拟机"></a>启动测试的虚拟机</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">virsh destroy nvmetest<br></code></pre></td></tr></table></figure><p>使用vnc连接登陆刚刚的那个vnc的端口<br>登陆上机器后就可以fdisk -l,<br><img src="/images/blog/o_200901035911nvmedisk.png" alt="nvme"><br>lspci看到的如下<br><img src="/images/blog/o_200901035956lspci.png" alt="lspci.png-1.5kB"></p><h4 id="结束"><a href="#结束" class="headerlink" title="结束"></a>结束</h4><p>本次实践当中还有一部分是对spdk的代码进行编译的，编译没有问题，并且可以根据测试脚本加载驱动，将nvme磁盘排它性的从内核态移出，但是无法找到如何使用这个用户态的磁盘，在ceph的代码分支中已经包含了spdk部分的代码，在ceph中应该默认可以直接使用这个驱动，使用的方式是 spdk：sdasdasdasd (disk SN) ，但是配置当中如何使用还是无从得知，这一块如果资料会第一时间分析，目前xsky应该能够配置出环境来，本篇涉及的几个东西都是比较新的一些东西，在未来将会极大的提高性能的，目前这个阶段还处于开发阶段</p><h4 id="异常处理"><a href="#异常处理" class="headerlink" title="异常处理"></a>异常处理</h4><p>执行virsh start nvmetest的时候会提示nvme.raw的磁盘没有访问权限，这个地方卡了很久，不清楚在ubuntu下面居然还有个apparmor的权限问题，是调看系统日志才发现的，下面是处理办法:<br>执行下面的命令为libvirt禁用 apparmor:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs raw">ln -s /etc/apparmor.d/usr.sbin.libvirtd  /etc/apparmor.d/disable/<br>ln -s /etc/apparmor.d/usr.lib.libvirt.virt-aa-helper  /etc/apparmor.d/disable/<br>apparmor_parser -R  /etc/apparmor.d/usr.sbin.libvirtd<br>apparmor_parser -R  /etc/apparmor.d/usr.lib.libvirt.virt-aa-helper<br></code></pre></td></tr></table></figure><p>&#x2F;etc&#x2F;libvirt&#x2F;qemu.conf去掉认证的,修改为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">security_driver = &quot;none&quot;<br></code></pre></td></tr></table></figure><p>重启libvirt服务</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">/etc/init.d/libvirt-bin restart<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>mon到底能坏几个</title>
    <link href="/2016/05/26/mon%E5%88%B0%E5%BA%95%E8%83%BD%E5%9D%8F%E5%87%A0%E4%B8%AA/"/>
    <url>/2016/05/26/mon%E5%88%B0%E5%BA%95%E8%83%BD%E5%9D%8F%E5%87%A0%E4%B8%AA/</url>
    
    <content type="html"><![CDATA[<p>如果是在做ceph的配置，我们会经常遇到这几个问题</p><ol><li>问：ceph需要配置几个mon<br>答：配置一个可以，但是坏了一个就不行了，需要配置只是三个mon，并且需要是奇数个</li><li>问：ceph的mon能跟osd放在一起么，需要配置很好么？<br>答：能跟放在一起，但是建议在环境允许的情况下一定独立机器，并且mon的配置能好尽量好，能上ssd就上ssd</li></ol><p>这两个问题的答案不能说是错的，但是为什么这么说，这么说有没有问题，这篇文章将根据实际的数据来告诉你，到底mon的极限在哪里，为什么都说要奇数，偶数难道就不行么</p><h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>本篇将从真实的实践中，让你更能够理解mon的故障极限，本次测试的场景数据样本足够大，最大的一个测试使用了10个mon，我想目前就算PB基本的ceph集群里也没有人会超过10个mon，所以足够覆盖大部分的场景，先来一个数据图看下10个mon的集群长什么样</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs bash">cluster ace3c18f-b4a5-4342-a598-8104a770d4a8<br>     health HEALTH_OK<br>     monmap e10: 10 mons at &#123;10=192.168.8.107:6800/0,2=192.168.8.107:6792/0,3=192.168.8.107:6793/0,4=192.168.8.107:6794/0,5=192.168.8.107:6795/0,6=192.168.8.107:6796/0,7=192.168.8.107:6797/0,8=192.168.8.107:6798/0,9=192.168.8.107:6799/0,lab8107=192.168.8.107:6789/0&#125;<br>            election epoch 58, quorum 0,1,2,3,4,5,6,7,8,9 lab8107,2,3,4,5,6,7,8,9,10<br>     osdmap e7: 1 osds: 1 up, 1 <span class="hljs-keyword">in</span><br>            flags sortbitwise<br>      pgmap v13: 64 pgs, 1 pools, 0 bytes data, 0 objects<br>            34268 kB used, 274 GB / 274 GB avail<br>                  64 active+clean<br></code></pre></td></tr></table></figure><p>mon的地方可以看到10个mon了</p><h3 id="测试结论"><a href="#测试结论" class="headerlink" title="测试结论"></a>测试结论</h3><p><img src="/images/blog/o_200901035747mon-1.png" alt="mondown"></p><p>ceph的mon能够正常情况需要保证，当前剩余的mon的个数需要大于总mon个数的一半，例如10个mon，mon个数一半就是5个，那么大于5个就是6个，也就是最少需要6个，上面的测试结论也符合这个规则，为什么不去偶数个，是因为当mon的个数为偶数个的时候，允许down的mon的个数与少一个mon的情况下的mon的个数允许的个数是一样的，所以要么多两个，多一个增加不了可靠性，并不是不允许</p><h3 id="测试过程的数据"><a href="#测试过程的数据" class="headerlink" title="测试过程的数据"></a>测试过程的数据</h3><h4 id="10个mon集群"><a href="#10个mon集群" class="headerlink" title="10个mon集群"></a>10个mon集群</h4><p>10个mon的极限</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">cluster ace3c18f-b4a5-4342-a598-8104a770d4a8<br>   health HEALTH_WARN<br>          4 mons down, quorum 0,1,2,3,4,5 lab8107,2,3,4,5,6<br>   monmap e10: 10 mons at &#123;10=192.168.8.107:6800/0,2=192.168.8.107:6792/0,3=192.168.8.107:6793/0,4=192.168.8.107:6794/0,5=192.168.8.107:6795/0,6=192.168.8.107:6796/0,7=192.168.8.107:6797/0,8=192.168.8.107:6798/0,9=192.168.8.107:6799/0,lab8107=192.168.8.10<br></code></pre></td></tr></table></figure><p>10个mon关闭4个没问题，关闭5个就卡死</p><h4 id="9个mon集群"><a href="#9个mon集群" class="headerlink" title="9个mon集群"></a>9个mon集群</h4><p>9个mon的极限</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">cluster ace3c18f-b4a5-4342-a598-8104a770d4a8<br>  health HEALTH_WARN<br>         4 mons down, quorum 0,1,2,3,4 lab8107,2,3,4,5<br>  monmap e11: 9 mons at &#123;2=192.168.8.107:6792/0,3=192.168.8.107:6793/0,4=192.168.8.107:6794/0,5=192.168.8.107:6795/0,6=192.168.8.107:6796/0,7=192.168.8.107:6797/0,8=192.168.8.107:6798/0,9=192.168.8.107:6799/0,lab8107=192.168.8.107:6789/0&#125;           <br></code></pre></td></tr></table></figure><p>9个mon关闭4个没问题，关闭5个就卡死</p><h4 id="8个mon集群"><a href="#8个mon集群" class="headerlink" title="8个mon集群"></a>8个mon集群</h4><p>8个mon的极限</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">cluster ace3c18f-b4a5-4342-a598-8104a770d4a8<br>  health HEALTH_WARN<br>         3 mons down, quorum 0,1,2,3,4 lab8107,2,3,4,5<br>  monmap e12: 8 mons at &#123;2=192.168.8.107:6792/0,3=192.168.8.107:6793/0,4=192.168.8.107:6794/0,5=192.168.8.107:6795/0,6=192.168.8.107:6796/0,7=192.168.8.107:6797/0,8=192.168.8.107:6798/0,lab8107=192.168.8.107:6789/0&#125;<br></code></pre></td></tr></table></figure><p>8个mon关闭3个没问题，关闭4个就卡死</p><h4 id="7个mon集群"><a href="#7个mon集群" class="headerlink" title="7个mon集群"></a>7个mon集群</h4><p>7个mon的极限</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">cluster ace3c18f-b4a5-4342-a598-8104a770d4a8<br>   health HEALTH_WARN<br>          3 mons down, quorum 0,1,2,3 lab8107,2,3,4<br>   monmap e13: 7 mons at &#123;2=192.168.8.107:6792/0,3=192.168.8.107:6793/0,4=192.168.8.107:6794/0,5=192.168.8.107:6795/0,6=192.168.8.107:6796/0,7=192.168.8.107:6797/0,lab8107=192.168.8.107:6789/0&#125;  <br></code></pre></td></tr></table></figure><p>7个mon关闭3个没问题，关闭4个就卡死</p><h4 id="6个mon集群"><a href="#6个mon集群" class="headerlink" title="6个mon集群"></a>6个mon集群</h4><p>6个mon的极限</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">cluster ace3c18f-b4a5-4342-a598-8104a770d4a8<br>  health HEALTH_WARN<br>         2 mons down, quorum 0,1,2,3 lab8107,2,3,4<br>  monmap e14: 6 mons at &#123;2=192.168.8.107:6792/0,3=192.168.8.107:6793/0,4=192.168.8.107:6794/0,5=192.168.8.107:6795/0,6=192.168.8.107:6796/0,lab8107=192.168.8.107:6789/0&#125;<br></code></pre></td></tr></table></figure><p>6个mon关闭2个没问题，关闭3个就卡死</p><h4 id="5个mon集群"><a href="#5个mon集群" class="headerlink" title="5个mon集群"></a>5个mon集群</h4><p>5个mon的极限</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">cluster ace3c18f-b4a5-4342-a598-8104a770d4a8<br>  health HEALTH_WARN<br>         2 mons down, quorum 0,1,2 lab8107,2,3<br>  monmap e15: 5 mons at &#123;2=192.168.8.107:6792/0,3=192.168.8.107:6793/0,4=192.168.8.107:6794/0,5=192.168.8.107:6795/0,lab8107=192.168.8.107:6789/0&#125;<br></code></pre></td></tr></table></figure><p>5个mon关闭2个没问题，关闭3个就卡死</p><h4 id="4个mon集群"><a href="#4个mon集群" class="headerlink" title="4个mon集群"></a>4个mon集群</h4><p>4个mon的极限</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">cluster ace3c18f-b4a5-4342-a598-8104a770d4a8<br>  health HEALTH_WARN<br>         1 mons down, quorum 0,1,2 lab8107,2,3<br>  monmap e16: 4 mons at &#123;2=192.168.8.107:6792/0,3=192.168.8.107:6793/0,4=192.168.8.107:6794/0,lab8107=192.168.8.107:6789/0&#125;<br></code></pre></td></tr></table></figure><p>4个mon关闭1个没问题，关闭2个就卡死</p><h4 id="3个mon集群"><a href="#3个mon集群" class="headerlink" title="3个mon集群"></a>3个mon集群</h4><p>3个mon的极限</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">cluster ace3c18f-b4a5-4342-a598-8104a770d4a8<br>  health HEALTH_WARN<br>         1 mons down, quorum 0,1 lab8107,2<br>  monmap e17: 3 mons at &#123;2=192.168.8.107:6792/0,3=192.168.8.107:6793/0,lab8107=192.168.8.107:6789/0&#125;<br></code></pre></td></tr></table></figure><p>3个mon关闭1个没问题，关闭2个就卡死</p><h3 id="测试结束"><a href="#测试结束" class="headerlink" title="测试结束"></a>测试结束</h3><p>下面为自己玩的一个动态图，10个mon正常，down 4个还是好的，down 5个就无法使用了</p><p><img src="/images/blog/o_20090103575510mon.gif"></p>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>ceph的jewel新支持的rbd-nbd</title>
    <link href="/2016/05/19/ceph%E7%9A%84jewel%E6%96%B0%E6%94%AF%E6%8C%81%E7%9A%84rbd-nbd/"/>
    <url>/2016/05/19/ceph%E7%9A%84jewel%E6%96%B0%E6%94%AF%E6%8C%81%E7%9A%84rbd-nbd/</url>
    
    <content type="html"><![CDATA[<p>jewel版本新增加了一个驱动NBD，允许librbd实现一个内核级别的rbd</p><p>NBD相比较于kernel rbd：</p><ul><li>rbd-ko是根据内核主线走的，升级kernel</li><li>rbd需要升级到相应的内核，改动太大</li><li>rbd-ko的开发要慢于librbd，需要很多的时间才能追赶上librbd</li></ul><p>rbd-nbd是通过librbd这个用户空间通过nbd的内核模块实现了内核级别的驱动，稳定性和性能都有保障</p><h3 id="怎么理解用户态和内核态？"><a href="#怎么理解用户态和内核态？" class="headerlink" title="怎么理解用户态和内核态？"></a>怎么理解用户态和内核态？</h3><ul><li>librbd就是用户态，一般的kvm对接的就是librbd的</li><li>kernel rbd就是内核态，这个是一个内核模块，是内核直接与osd交互的，一般来说内核态的性能会优于用户态</li></ul><h2 id="下面来做下基本的操作："><a href="#下面来做下基本的操作：" class="headerlink" title="下面来做下基本的操作："></a>下面来做下基本的操作：</h2><h3 id="创建一个image"><a href="#创建一个image" class="headerlink" title="创建一个image"></a>创建一个image</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ~]<span class="hljs-comment"># rbd create testnbdrbd -s 10G</span><br></code></pre></td></tr></table></figure><h3 id="映射这个image"><a href="#映射这个image" class="headerlink" title="映射这个image"></a>映射这个image</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ~]<span class="hljs-comment">#  rbd-nbd map rbd/testnbdrbd</span><br>/dev/nbd0<br></code></pre></td></tr></table></figure><h3 id="查询已经映射的nbd"><a href="#查询已经映射的nbd" class="headerlink" title="查询已经映射的nbd"></a>查询已经映射的nbd</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ~]<span class="hljs-comment">#  rbd-nbd list-mapped</span><br>/dev/nbd0<br></code></pre></td></tr></table></figure><p>上面说了这么多，那么来点直观的认识,nbd带来的好处<br>查询下image的信息</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ~]<span class="hljs-comment"># rbd info testnbdrbd</span><br>rbd image <span class="hljs-string">&#x27;testnbdrbd&#x27;</span>:<br>size 10240 MB <span class="hljs-keyword">in</span> 2560 objects<br>order 22 (4096 kB objects)<br>block_name_prefix: rbd_data.10ad2ae8944a<br>format: 2<br>features: layering, exclusive-lock, object-map, fast-diff, deep-flatten<br>flags: <br></code></pre></td></tr></table></figure><blockquote><p>jewel版本默认开启了features: layering, exclusive-lock, object-map, fast-diff, deep-flatten这么多的属性，而这些属性是kernel-rbd还不支持的</p></blockquote><p>所以做rbd map的时候就会出现下面的问题:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ~]<span class="hljs-comment"># rbd map  testnbdrbd</span><br>rbd: sysfs write failed<br>RBD image feature <span class="hljs-built_in">set</span> mismatch. You can <span class="hljs-built_in">disable</span> features unsupported by the kernel with <span class="hljs-string">&quot;rbd feature disable&quot;</span>.<br>In some cases useful info is found <span class="hljs-keyword">in</span> syslog - try <span class="hljs-string">&quot;dmesg | tail&quot;</span> or so.<br>rbd: map failed: (6) No such device or address<br></code></pre></td></tr></table></figure><p>如果非要用，就默认禁用掉这些属性，在配置文件增加</p><blockquote><p>rbd_default_features &#x3D; 3</p></blockquote><p>那么现在开启属性还行想用块设备方式怎么用，就可以用nbd了</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ~]<span class="hljs-comment">#  rbd-nbd map rbd/testnbdrbd</span><br>/dev/nbd0<br></code></pre></td></tr></table></figure><p>这样就可以用了。不用担心接口的问题了,因为只要librbd支持的属性，nbd就默认支持了</p><h3 id="查询rbd和nbd对应关系"><a href="#查询rbd和nbd对应关系" class="headerlink" title="查询rbd和nbd对应关系"></a>查询rbd和nbd对应关系</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 mnt]<span class="hljs-comment"># rbd-nbd list-mapped</span><br>/dev/nbd0<br>[root@lab101 mnt]<span class="hljs-comment"># cat /sys/block/nbd0/pid </span><br>93059<br>[root@lab101 mnt]<span class="hljs-comment"># ps -ef|grep 93059</span><br>root      93059      1  0 14:30 pts/1    00:00:00 rbd-nbd map testnbd<br>root      97346   8580  0 14:36 pts/1    00:00:00 grep --color=auto 93059<br></code></pre></td></tr></table></figure><p>rbd几种常用的模式和新模式图：<br><img src="/images/blog/o_200901035624nbd.png" alt="nbd.png-40.5kB"></p><h3 id="本篇ceph版本"><a href="#本篇ceph版本" class="headerlink" title="本篇ceph版本"></a>本篇ceph版本</h3><blockquote><p>ceph version 10.2.1 (3a66dd4f30852819c1bdaa8ec23c795d4ad77269)</p></blockquote><h2 id="变更记录"><a href="#变更记录" class="headerlink" title="变更记录"></a>变更记录</h2><table><thead><tr><th align="center">Why</th><th align="center">Who</th><th align="center">When</th></tr></thead><tbody><tr><td align="center">创建</td><td align="center">武汉-运维-磨渣</td><td align="center">2016-05-19</td></tr><tr><td align="center">增加映射关系查询方法</td><td align="center">武汉-运维-磨渣</td><td align="center">2018-03-27</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>支持jewel版本的calamari</title>
    <link href="/2016/05/16/%E6%94%AF%E6%8C%81jewel%E7%89%88%E6%9C%AC%E7%9A%84calamari/"/>
    <url>/2016/05/16/%E6%94%AF%E6%8C%81jewel%E7%89%88%E6%9C%AC%E7%9A%84calamari/</url>
    
    <content type="html"><![CDATA[<p>之前测试了下，发现calamari不支持jewel版本的，是因为接口了有了一些变化，在提出这个问题后，作者给出了回答，说肯定会支持的，并且做了一点小的改动，就可以支持了，这个作者merge了到了github的一些分支当中，但是还没有merge到最新的1.4的分支合master分支当中，这个可能是因为1.4还在做一些功能的开发</p><p>我使用作者的修改好的分支打好了包，直接可以使用，测试了ubuntu14.04 和centos 7的版本都可以使用，下面是百度云的链接，欢迎使用和测试</p><h3 id="ubuntu版本下载地址："><a href="#ubuntu版本下载地址：" class="headerlink" title="ubuntu版本下载地址："></a>ubuntu版本下载地址：</h3><p><a href="https://pan.baidu.com/s/1kVxCqVH">ubuntu-jewel-calamari</a><br>密码：h61u</p><h3 id="centos7版本下载地址"><a href="#centos7版本下载地址" class="headerlink" title="centos7版本下载地址"></a>centos7版本下载地址</h3><p><a href="https://pan.baidu.com/s/1kVRy4mj">centos-jewel-calamari</a><br>密码：gbmr</p><h3 id="变更记录"><a href="#变更记录" class="headerlink" title="变更记录"></a>变更记录</h3><table><thead><tr><th align="center">Why</th><th align="center">Who</th><th align="center">When</th></tr></thead><tbody><tr><td align="center">创建</td><td align="center">武汉-运维-磨渣</td><td align="center">2016-05-16</td></tr><tr><td align="center">修改资源，解决无法获取iops和容量的bug</td><td align="center">武汉-运维-磨渣</td><td align="center">2016-07-12</td></tr><tr><td align="center">更新网盘失效链接</td><td align="center">武汉-运维-磨渣</td><td align="center">2017-06-01</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>ceph卡在active+remapped状态</title>
    <link href="/2016/05/14/ceph%E5%8D%A1%E5%9C%A8active+remapped%E7%8A%B6%E6%80%81/"/>
    <url>/2016/05/14/ceph%E5%8D%A1%E5%9C%A8active+remapped%E7%8A%B6%E6%80%81/</url>
    
    <content type="html"><![CDATA[<p>最近看到了有人的环境出现了出现了卡在active+remapped状态，并且卡住不动的状态，从pg的状态去看，这个pg值分配了主的pg，没有分配到副本的osd，集群的其他设置一切正常</p><p>这个从网上搜寻到的资料来看，大多数都是由于不均衡的主机osd引起的，所谓不平衡的osd</p><ul><li>一台机器上面的磁盘的容量不一样，有的3T，有的1T</li><li>两台主机上面的OSD个数不一样，有的5个，有的2个</li></ul><p>这样会造成主机的crush 的weight的差别很大的问题，以及分布算法上的不平衡问题，建议对于一个存储池来说，它所映射的osd至少需要是磁盘大小一致和个数一致的</p><p>这个问题我在我的环境下做了复现，确实有卡在remapped的问题</p><h3 id="出现这个情况一般是什么操作引起的？"><a href="#出现这个情况一般是什么操作引起的？" class="headerlink" title="出现这个情况一般是什么操作引起的？"></a>出现这个情况一般是什么操作引起的？</h3><p>做osd的reweight的操作引起的，这个因为一般在做reweight的操作的时候，根据算法，这个上面的pg是会尽量分布在这个主机上的，而crush reweight不变的情况下，去修改osd 的reweight的时候，可能算法上会出现无法映射的问题</p><h3 id="怎么解决这个问题？"><a href="#怎么解决这个问题？" class="headerlink" title="怎么解决这个问题？"></a>怎么解决这个问题？</h3><p>直接做osd crush reweigh的调整即可避免这个问题，这个straw算法里面还是有点小问题的，在调整某个因子的时候会引起整个因子的变动</p><blockquote><p>之前看到过sage在回复这种remapped问题的时候，都是不把这个归到bug里面去的，这个我也认为是配置问题引起的极端的问题，正常情况下都能避免的</p></blockquote><h3 id="更新"><a href="#更新" class="headerlink" title="更新"></a>更新</h3><p>从FIREFLY (CRUSH_TUNABLES3)开始crush里面增加了一个参数：</p><blockquote><p>chooseleaf_vary_r</p></blockquote><p>是否进行递归的进行chooseleaf，如果非0，就递归的进行，这个基于parent已经做了多少次尝试，默认是0，但是常常找不到合适的mapping.在计算成本和正确性上来看最佳的值是1</p><p>对迁移的影响,对于已经有大量数据的集群来说，从0调整为1将会有大量的数值的迁移，调整为4或者5的话，将会找到一个更有效的映射，可以减少数据的移动</p><p>查看当前的值</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@lab8106 ~]# ceph osd crush show-tunables |grep chooseleaf_vary_r<br>    &quot;chooseleaf_vary_r&quot;: 0,<br></code></pre></td></tr></table></figure><p>修改crushmap内容<br>修改chooseleaf_vary_r的值</p><p>hammer版本下这个参数默认为：</p><blockquote><p>tunable chooseleaf_vary_r 0</p></blockquote><p>jewel版本下这个参数默认为：</p><blockquote><p>tunable chooseleaf_vary_r 1</p></blockquote><p>这个是跟 tunables 的版本有关的</p><blockquote><p>注意：在 3.15 之前的版本中，chooseleaf_vary_r 的值必须为0(原因未知)</p></blockquote><h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><ul><li><a href="http://docs.ceph.com/docs/master/rados/operations/crush-map/">官网文档crushmap</a></li><li><a href="http://lists.ceph.com/pipermail/ceph-users-ceph.com/2014-July/041205.html">Issue列表</a></li><li><a href="http://www.cnblogs.com/sammyliu/archive/2016/06/03/5555218.html">sammyliu的blog</a></li></ul><h3 id="变更记录"><a href="#变更记录" class="headerlink" title="变更记录"></a>变更记录</h3><table><thead><tr><th align="center">Why</th><th align="center">Who</th><th align="center">When</th></tr></thead><tbody><tr><td align="center">创建</td><td align="center">武汉-运维-磨渣</td><td align="center">2016-05-14</td></tr><tr><td align="center">修改</td><td align="center">武汉-运维-磨渣</td><td align="center">2016-09-23</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>性能数据的可视化</title>
    <link href="/2016/04/27/%E6%80%A7%E8%83%BD%E6%95%B0%E6%8D%AE%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96/"/>
    <url>/2016/04/27/%E6%80%A7%E8%83%BD%E6%95%B0%E6%8D%AE%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96/</url>
    
    <content type="html"><![CDATA[<p>在工作当中，很多时候我们在去分析一个性能的时候，会产生大量的数据，面对数据的时候我们一般应该会有以下几个处理过程</p><ul><li>直接肉眼看<br>这个属于第一个级别，比如监控系统负载的时候去用top观察，这个方法是我最开始经常使用的一种方法，这个适合异常的时候使用，但是实际上获取的数据是有偏差的</li><li>有监控系统<br>使用数据监控系统对需要监控的数据进行监控，这个前提是有一个监控系统，并且方便的去增加数据，可以根据需求去设定数据，这个监控系统有很多，能可视化的也很多，这篇文章就不做介绍</li><li>使用监控脚本采集数据，采用excel进行可视化<br>使用脚本收集大量的数据，然后将数据导入到excel当中，然后显示出来，这个是我们公司测试人员采用的方法，也是比较容易实现的一个方式</li></ul><p>也可能还有其他的方法，总之一图胜千言，通过图形来展示数据，会获取到更多的信息，我也一直在寻找一些方案来方便的展示数据，从目前的监控系统来看，一般的实现方法都是</p><ul><li>数据采集到数据库</li><li>使用图形展示数据库中间的问题</li></ul><p>我现在需要的一个功能就是，使用一个采集工具将数据收集起来，然后直接将数据输出为图片，这个图片的渲染是可以根据我的需要进行定制的，最近在研究web自动化测试的发现可以有办法对html进行渲染成图片，想到这个地方可以跟这个地方进行结合</p><p>这里的思路是使用html+highchart方式进行数据的渲染，然后将页面导出成图片，最终做成一个简单的数据展示工具，并且为其他地方的提供数据图片</p><p>下面是这个工具渲染的图片效果：</p><p><img src="/images/blog/o_200901035201chart.png"></p><p>可以根据自己的需要去显示数据，后续会记录方法，这里暂时只记录一个思路</p>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>mon的稳定性问题</title>
    <link href="/2016/04/25/mon%E7%9A%84%E7%A8%B3%E5%AE%9A%E6%80%A7%E9%97%AE%E9%A2%98/"/>
    <url>/2016/04/25/mon%E7%9A%84%E7%A8%B3%E5%AE%9A%E6%80%A7%E9%97%AE%E9%A2%98/</url>
    
    <content type="html"><![CDATA[<h3 id="MON的稳定性问题："><a href="#MON的稳定性问题：" class="headerlink" title="MON的稳定性问题："></a>MON的稳定性问题：</h3><ul><li>mon的选举风暴影响客户端IO</li><li>LevelDB的暴涨</li><li>频繁的客户端请求的DDOS</li></ul><h4 id="mon选举风暴："><a href="#mon选举风暴：" class="headerlink" title="mon选举风暴："></a>mon选举风暴：</h4><p>monmap会因为mon之间或者mon与客户端之间网络的影响或者消息传递的异常发生变化,从而触发选举<br>会造成客户端的请求变慢或者锁住</p><h4 id="LevelDB的暴涨："><a href="#LevelDB的暴涨：" class="headerlink" title="LevelDB的暴涨："></a>LevelDB的暴涨：</h4><p>LevelDB的大小会涨到几十GB然后影响了osd的请求<br>会造成客户端的请求变慢或者锁住</p><h4 id="频繁的客户端请求的DDOS："><a href="#频繁的客户端请求的DDOS：" class="headerlink" title="频繁的客户端请求的DDOS："></a>频繁的客户端请求的DDOS：</h4><p>mon的响应因为levelDB变慢或者选举风暴，都会造成客户端发出大量的消息流<br>让客户端操作失效，包括卷创建，rbd的连接</p><h3 id="解决办法："><a href="#解决办法：" class="headerlink" title="解决办法："></a>解决办法：</h3><h4 id="LevelDB的暴涨的问题解决办法"><a href="#LevelDB的暴涨的问题解决办法" class="headerlink" title="LevelDB的暴涨的问题解决办法"></a>LevelDB的暴涨的问题解决办法</h4><p>升级ceph的版本，这个在0.94.6版本解决了这个问题</p><p><img src="/images/blog/o_200901035110leveldb.png" alt="leveldb.png-21.9kB"></p><h4 id="选举风暴问题解决办法"><a href="#选举风暴问题解决办法" class="headerlink" title="选举风暴问题解决办法"></a>选举风暴问题解决办法</h4><blockquote><p>[mon]<br><br>mon_lease &#x3D; 20 (default &#x3D; 5)<br><br>mon_lease_renew_interval &#x3D; 12 (default 3)<br><br>mon_lease_ack_timeout &#x3D; 40 (default 10)<br><br>mon_accept_timeout &#x3D; 40 (default 10)<br><br>[client]<br><br>mon_client_hunt_interval &#x3D; 40 (defaiult 3)</p></blockquote><p>将mon的数据放置在ssd上，因为LevelDB存储了集群的 metadata,包括 osdmap, pgmap, monmap,clientID, authID etc 等等，很大的leveldb会有更长的查询时间，更长的IO等待，然后就是更慢的客户端请求</p><p>这个地方是增长了mon之间判断需要切换的时间，降低客户端的请求的频率，使用ssd加快查询的速度</p><p>这个问题是一个不太容易发觉的问题，有时候就是ceph -s反应的很慢，但是很多时候可能体现的就是客户端出现请求缓慢，然后还找不到原因，所以说硬件的隔离是非常有必要的，不要为了省成本然后影响了整个环境的稳定性和性能，对于很大的环境mon需要用三台独立的机器，这个机器需要一定的内存和cpu，磁盘使用ssd，1U服务器就可以了，上面可以运行一些其他类似管理平台，或者一些监控服务什么的，混合在osd的机器上的时候，一旦OSD出现大量的数据迁移的时候，或者大量的请求的时候，会阻塞了消息，这个就是做方案的时候需要考虑的问题，当然在性能要求不那么高的时候将mon混合在osd上使用也是可以的，这个时候尽量有更多的内存和更好的磁盘性能也能减少一些负担</p>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>backfill和recovery的最优值</title>
    <link href="/2016/04/24/backfill%E5%92%8Crecovery%E7%9A%84%E6%9C%80%E4%BC%98%E5%80%BC/"/>
    <url>/2016/04/24/backfill%E5%92%8Crecovery%E7%9A%84%E6%9C%80%E4%BC%98%E5%80%BC/</url>
    
    <content type="html"><![CDATA[<p>ceph在增加osd的时候会触发backfill，让数据得到平均，触发数据的迁移<br>ceph在移除osd的时候需要在节点上进行数据的恢复，也有数据的迁移和生成</p><p>只要是集群里面有数据的变动就会有网卡流量，cpu，内存等资源的占用，并且最重要的是还有磁盘的占用，这个客户端也是需要对磁盘进行访问的，当请求出现碰撞的时候，肯定会比正常的情况下要慢很多，而且还有可能因为资源方面的原因而引起机器down机等异常状况的出现</p><p>主要引起的问题可能：</p><ul><li>在peering的时候 block 了IO请求</li><li>在backfill的引起了slow requests</li><li>上面的两个情况会引起客户端的降速和出现soft lockup</li></ul><p>这个在一般情况下会出现不同的需求：</p><ol><li>慢点可以一定不能出问题，不能中断业务</li><li>越快迁移完越好，早点结束维护服务</li><li>需要又快又不能影响业务</li></ol><p>这个需要根据自己可以掌控的程度来进行控制，首先环境的不同，影响不同，迁移数据量，网卡的带宽都是重要的影响因素，从整体上可以根据自己的环境按照上面的三个要求中的一个进行控制</p><p>上面的三种情况：<br>第一个慢点迁移不能出问题，这个处理方式比较简单，直接将相关参数控制到最低的值，这个能保证业务的影响最低，但是带来的影响就是迁移需要很久的时间，可能长达几十个小时</p><p>第二个越快越好就是用默认的参数或者加大参数，然后观察这个迁移过程中的资源的占用情况</p><p>第三个就是需要在自己的环境下进行多测试验证这个参数，本篇主要就是根据思科的测试出来的参数进行分析</p><p>下面的参数是思科测试出来的值：</p><blockquote><p>osd recovery max active &#x3D; 3（default: 15)<br><br>osdrecovery op priority &#x3D; 3(default : 10)<br><br>osdmaxbackfills &#x3D; 1 (default : 10)</p></blockquote><h3 id="测试过程的数据图"><a href="#测试过程的数据图" class="headerlink" title="测试过程的数据图"></a>测试过程的数据图</h3><p><img src="/images/blog/o_200901035006back%E5%BD%B1%E5%93%8D.png" alt="back影响.png-58.2kB"></p><p>这个图开始的时候我也没太明白，后来多看下就理解了，实际上在很多情况下，一个因素的变化是会引起其他两个因素的变化，而这两个因素是一个正面的因素和一个负面的因素，而找到这个平衡值就是最优的情况，在这里的因素包括：</p><ul><li>max-backfill和max-recovery :迁移相关参数</li><li>MTTR（mean time to recovery）:失效恢复时间，也就是迁移完成</li><li>Soft Lockup:前面虚拟机出现的soft lockup，也可以理解为对前端的影响</li></ul><p>测试环境一致，都是 down 掉10%的osd进行恢复：<br>在迁移参数最低的时候，没有出现soft lockup ，也就是最低迁移参数的时候，影响最小，恢复使用了45分钟<br>随着迁移相关参数调大的时候，迁移的时间的曲线是先降低，在到达一定的值后又开始增加（这个地方可能是迁移过大出现了前端io锁住，然后影响了迁移速度）<br>随着迁移相关参数的调大，出现soft lockup的情况是增加的</p><p>从测试的曲线来看，在2-6之间是出现的最优值，也就是出现异常的情况概率最低，并且迁移速度最快，最终选择了一组最优的值 ：</p><blockquote><p>osd recovery max active &#x3D; 3（default: 15)<br><br>osdrecovery op priority &#x3D; 3(default : 10)<br><br>osdmaxbackfills &#x3D; 1 (default : 10)</p></blockquote><p>这个值是思科的测试出来的值，这个值可以根据自己的需要进行取用，大概的情况是这样</p><ul><li>完全无法把控就把参数调整到最低</li><li>使用思科的推荐值</li><li>根据自己的环境测出自己环境的最优值</li></ul><p>很多参数是别人根据自己的环境测试出来的，很多情况并不是通用的，得到别人测试的思路是最重要的，然后消化后自己根据自己的需要得出自己的值</p><h3 id="说点自己最近的感想"><a href="#说点自己最近的感想" class="headerlink" title="说点自己最近的感想"></a>说点自己最近的感想</h3><p>根据自己的观察和自己的经验，所有的知识都是需要自己主动去获取主动的去消化，然后去实践的，在任何地方没有说通过传授知识，你就能学会了，公司的程序员的技术也是自己主动的去学习的，所谓的经验也只能是告诉你一些方法，而且你也没办法要求任何人与你一样的努力，一样的对你所做的东西感兴趣，认同你的观点，很多时候需要的是技术的碰撞，在一家公司需要的是员工能够完成你的事情，所以我们要尊重努力的员工，这类员工非常努力，但是可能无法达到你的要求，这个需要鼓励，还有一种是效率非常高的员工，这类员工能够轻松的完成任何事情，这类员工可以给与充分的自由，最终以时间以及结果双向评估员工对公司的贡献</p><p>最近tinyfool老师在进行ios开发的一个分享的时候，一堆想获取的干活的人去听，而tinyfool老师在这个分享会上通知了自己公司破产解散的事情，宣告再次的失败，而一些来想获取干货的人却开喷了，说没有获取到任何干货，这些人想获取的干货就是拿来直接干的货，而tinyfool老师分享的经验，包括在最后宣告解散的时候准备开源自己的东西，极力的推荐自己的员工是多么的优秀，还有其他的一些东西，这些其实都是干货，引用高春辉老师对这件事的看法的一段话：</p><blockquote><p>很多人可能相比之下，觉得代码语言这些硬技能最重要。其实我和我周边的朋友基本都认为，软技能才最重要，其实人的智商都差不多，再笨也不会笨很多，但如何待人接物，如何对待同事和朋友，如何高效利用时间，内心对成功的渴望，还有是否有责任心和荣誉感还有成就感这些，包括个人兴趣以及性格，这些是很难在进入社会之后再改变的了，除非有重大事情发生，否则很难很难改变。所以多数情况下，十年后的你的境遇，其实是十年前就已经决定的了  –高春辉</p></blockquote><p>任何公司任何员工都不可能十全十美，其实有时候换位思考一下，站在企业角度想一下，公司需要什么样的员工，站在员工的角度想一下，想要公司怎么的为自己保证最大利益，如果能够做到相互的价值观一致，就能处于一个和谐的状态了</p>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>inkscope完整安装配置</title>
    <link href="/2016/04/19/inkscope%E5%AE%8C%E6%95%B4%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE/"/>
    <url>/2016/04/19/inkscope%E5%AE%8C%E6%95%B4%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE/</url>
    
    <content type="html"><![CDATA[<h3 id="准备centos7基础系统"><a href="#准备centos7基础系统" class="headerlink" title="准备centos7基础系统"></a>准备centos7基础系统</h3><p>首先安装基础系统centos7 在安装选项那里选择base web server ，选择其他的也可以，选择mini安装会缺很多常用的软件包，后续需要一个个安装比较麻烦</p><h3 id="关闭防火墙相关"><a href="#关闭防火墙相关" class="headerlink" title="关闭防火墙相关"></a>关闭防火墙相关</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@inkscope ~]<span class="hljs-comment"># setenforce 0</span><br>[root@inkscope ~]<span class="hljs-comment"># sed -i &#x27;s/SELINUX=enforcing/SELINUX=disabled/g&#x27; /etc/selinux/config</span><br>[root@inkscope ~]<span class="hljs-comment"># systemctl stop firewalld</span><br>[root@inkscope ~]<span class="hljs-comment"># systemctl disable firewalld</span><br></code></pre></td></tr></table></figure><h3 id="更新源相关的"><a href="#更新源相关的" class="headerlink" title="更新源相关的"></a>更新源相关的</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@inkscope ~]<span class="hljs-comment"># rm -rf /etc/yum.repos.d/*.repo</span><br>[root@inkscope ~]<span class="hljs-comment"># wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo</span><br>[root@inkscope ~]<span class="hljs-comment"># wget -O /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repo</span><br></code></pre></td></tr></table></figure><p>修改里面的系统版本为7.2.1511,当前用的centos的版本的的yum源可能已经清空了</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@inkscope ~]<span class="hljs-comment"># sed -i &#x27;s/$releasever/7.2.1511/g&#x27; /etc/yum.repos.d/CentOS-Base.repo</span><br></code></pre></td></tr></table></figure><p>添加ceph源</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@inkscope ~]<span class="hljs-comment"># vim /etc/yum.repos.d/ceph.repo</span><br></code></pre></td></tr></table></figure><p>添加</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs bash">[ceph]<br>name=ceph<br>baseurl=http://mirrors.aliyun.com/ceph/rpm-hammer/el7/x86_64/<br>gpgcheck=0<br>[ceph-noarch]<br>name=cephnoarch<br>baseurl=http://mirrors.aliyun.com/ceph/rpm-hammer/el7/noarch/<br>gpgcheck=0<br></code></pre></td></tr></table></figure><h3 id="准备inkscope的相关安装包"><a href="#准备inkscope的相关安装包" class="headerlink" title="准备inkscope的相关安装包"></a>准备inkscope的相关安装包</h3><p>下载相关软件包的脚本：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-meta">#! /bin/sh</span><br>wget https://bash.githubusercontent.com/inkscope/inkscope-packaging/master/RPMS/flake8-2.3.0-1.noarch.rpm<br>wget https://bash.githubusercontent.com/inkscope/inkscope-packaging/master/RPMS/inkscope-admviz-1.3.1-2.noarch.rpm<br>wget https://bash.githubusercontent.com/inkscope/inkscope-packaging/master/RPMS/inkscope-cephprobe-1.3.1-2.noarch.rpm<br>wget https://bash.githubusercontent.com/inkscope/inkscope-packaging/master/RPMS/inkscope-cephrestapi-1.3.1-2.noarch.rpm<br>wget https://bash.githubusercontent.com/inkscope/inkscope-packaging/master/RPMS/inkscope-common-1.3.1-2.noarch.rpm<br>wget https://bash.githubusercontent.com/inkscope/inkscope-packaging/master/RPMS/inkscope-monitoring-1.3.1-2.noarch.rpm<br>wget https://bash.githubusercontent.com/inkscope/inkscope-packaging/master/RPMS/inkscope-sysprobe-1.3.1-2.noarch.rpm<br>wget https://bash.githubusercontent.com/inkscope/inkscope-packaging/master/RPMS/pep8-1.5.7-1.noarch.rpm<br>wget https://bash.githubusercontent.com/inkscope/inkscope-packaging/master/RPMS/pyflakes-0.8.1-1.noarch.rpm<br>wget https://bash.githubusercontent.com/inkscope/inkscope-packaging/master/RPMS/python-babel-0.9.6-8.el7.noarch.rpm<br>wget https://bash.githubusercontent.com/inkscope/inkscope-packaging/master/RPMS/python-boto-2.34.0-4.el7.noarch.rpm<br>wget https://bash.githubusercontent.com/inkscope/inkscope-packaging/master/RPMS/python-bson-2.5.2-2.el7.x86_64.rpm<br>wget https://bash.githubusercontent.com/inkscope/inkscope-packaging/master/RPMS/python-flask-0.10.1-4.el7.noarch.rpm<br>wget https://bash.githubusercontent.com/inkscope/inkscope-packaging/master/RPMS/python-flask-doc-0.10.1-4.el7.noarch.rpm<br>wget https://bash.githubusercontent.com/inkscope/inkscope-packaging/master/RPMS/python-itsdangerous-0.23-2.el7.noarch.rpm<br>wget https://bash.githubusercontent.com/inkscope/inkscope-packaging/master/RPMS/python-jinja2-2.7.2-2.el7.noarch.rpm<br>wget https://bash.githubusercontent.com/inkscope/inkscope-packaging/master/RPMS/python-markupsafe-0.11-10.el7.x86_64.rpm<br>wget https://bash.githubusercontent.com/inkscope/inkscope-packaging/master/RPMS/python-pip-1.3.1-4.el7.noarch.rpm<br>wget https://bash.githubusercontent.com/inkscope/inkscope-packaging/master/RPMS/python-psutil-2.2.0-1.x86_64.rpm<br>wget https://bash.githubusercontent.com/inkscope/inkscope-packaging/master/RPMS/python-pymongo-2.5.2-2.el7.x86_64.rpm<br>wget https://bash.githubusercontent.com/inkscope/inkscope-packaging/master/RPMS/python-rsa-3.1.1-5.el7.noarch.rpm<br>wget https://bash.githubusercontent.com/inkscope/inkscope-packaging/master/RPMS/python-simplejson-3.3.3-1.el7.x86_64.rpm<br>wget https://bash.githubusercontent.com/inkscope/inkscope-packaging/master/RPMS/python-werkzeug-0.9.1-2.el7.noarch.rpm<br>wget https://bash.githubusercontent.com/inkscope/inkscope-packaging/master/RPMS/python-werkzeug-doc-0.9.1-2.el7.noarch.rpm<br>wget http://copr-be.cloud.fedoraproject.org/results/merlinthp/el7-mypa/epel-7-x86_64/python-flask-login-0.2.11-1.el7.centos/python-flask-login-0.2.11-1.el7.centos.noarch.rpm<br></code></pre></td></tr></table></figure><p>已经离线下载好了，可以直接用下面的百度云里面的安装包<br>链接：<a href="http://pan.baidu.com/s/1czANCi">http://pan.baidu.com/s/1czANCi</a> 密码：qw3k</p><h3 id="软件包安装的位置"><a href="#软件包安装的位置" class="headerlink" title="软件包安装的位置"></a>软件包安装的位置</h3><ul><li>集群的mon节点 cephprobe ceph-rest-api </li><li>集群的osd节点 sysprobe inkscope-common</li><li>inkscope管理控制台inkscope-admviz inkscope-monitor mongodb</li></ul><h3 id="开始安装inkscope管理节点"><a href="#开始安装inkscope管理节点" class="headerlink" title="开始安装inkscope管理节点"></a>开始安装inkscope管理节点</h3><h4 id="安装下ceph-非必选建议安装下"><a href="#安装下ceph-非必选建议安装下" class="headerlink" title="安装下ceph(非必选建议安装下)"></a>安装下ceph(非必选建议安装下)</h4><p>这个地方可以选择安装也可以选择不安装，这个方便查看，也有可能管理节点就在ceph的某个节点上，这个自己随意</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@inkscope ~]<span class="hljs-comment"># yum install ceph ceph-radosgw</span><br>[root@inkscope ~]<span class="hljs-comment"># yum install python-ceph</span><br></code></pre></td></tr></table></figure><p>我的环境是想单独一台机器做管理平台控制节点</p><p>那么把ceph集群中的这两个文件拷贝到这个管理节点的&#x2F;etc&#x2F;ceph&#x2F;下面</p><ul><li>ceph.client.admin.keyring </li><li>ceph.conf</li></ul><p>如果你管理节点本身就在ceph集群当中就不需要做了<br>检查ceph -s是否有输出，有集群输出就是正常的</p><h4 id="安装依赖包"><a href="#安装依赖包" class="headerlink" title="安装依赖包"></a>安装依赖包</h4><p>安装apache2和其它几个包(用于配置web服务器)</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@inkscope ~]<span class="hljs-comment"># yum install httpd -y</span><br>[root@inkscope ~]<span class="hljs-comment"># yum install python-setuptools</span><br>[root@inkscope ~]<span class="hljs-comment"># yum install lshw</span><br>[root@inkscope ~]<span class="hljs-comment"># yum install mod_wsgi</span><br></code></pre></td></tr></table></figure><h4 id="安装MongoDb-用于存储收集到的数据的"><a href="#安装MongoDb-用于存储收集到的数据的" class="headerlink" title="安装MongoDb(用于存储收集到的数据的)"></a>安装MongoDb(用于存储收集到的数据的)</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@inkscope ~]<span class="hljs-comment"># yum install mongodb -y</span><br>[root@inkscope ~]<span class="hljs-comment"># yum install mongodb-server -y</span><br></code></pre></td></tr></table></figure><p>修改配置文件，让mongdb可以远程访问</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@inkscope ~]<span class="hljs-comment"># vim /etc/mongod.conf</span><br></code></pre></td></tr></table></figure><p>bind_ip &#x3D; 127.0.0.1修改为 bind_ip &#x3D; 0.0.0.0</p><h4 id="启动mongodb服务"><a href="#启动mongodb服务" class="headerlink" title="启动mongodb服务"></a>启动mongodb服务</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@inkscope ~]<span class="hljs-comment"># systemctl start mongod.service</span><br></code></pre></td></tr></table></figure><h4 id="检查服务"><a href="#检查服务" class="headerlink" title="检查服务"></a>检查服务</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@inkscope bao]<span class="hljs-comment"># netstat -tunlp|grep mongod</span><br>tcp        0      0 0.0.0.0:27017           0.0.0.0:*               LISTEN      11836/mongod   <br></code></pre></td></tr></table></figure><h4 id="安装inkscope相关的包"><a href="#安装inkscope相关的包" class="headerlink" title="安装inkscope相关的包"></a>安装inkscope相关的包</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs bash">安装（有些安装ceph的时候已经安装了）<br>rpm -ivh flake8-2.3.0-1.noarch.rpm<br>rpm -ivh pep8-1.5.7-1.noarch.rpm<br>rpm -ivh pyflakes-0.8.1-1.noarch.rpm<br>rpm -ivh python-rsa-3.1.1-5.el7.noarch.rpm<br>rpm -ivh python-boto-2.34.0-4.el7.noarch.rpm<br>rpm -ivh python-bson-2.5.2-2.el7.x86_64.rpm<br>rpm -ivh python-flask-doc-0.10.1-4.el7.noarch.rpm<br>rpm -ivh python-pip-1.3.1-4.el7.noarch.rpm<br>rpm -ivh python-psutil-2.2.0-1.x86_64.rpm<br>rpm -ivh python-pymongo-2.5.2-2.el7.x86_64.rpm<br>rpm -ivh python-simplejson-3.3.3-1.el7.x86_64.rpm<br>rpm -ivh python-werkzeug-doc-0.9.1-2.el7.noarch.rpm <br>rpm -ivh inkscope-common-1.3.1-2.noarch.rpm<br>rpm -ivh inkscope-sysprobe-1.3.1-2.noarch.rpm<br>rpm -ivh inkscope-monitoring-1.3.1-2.noarch.rpm<br>rpm -ivh inkscope-cephrestapi-1.3.1-2.noarch.rpm<br>rpm -ivh inkscope-cephprobe-1.3.1-2.noarch.rpm <br>rpm -ivh inkscope-admviz-1.3.1-2.noarch.rpm <br><span class="hljs-comment">##rpm -ivh --nodeps inkscope-admviz-1.3.1-2.noarch.rpm</span><br></code></pre></td></tr></table></figure><blockquote><p>这个里面因为python-ceph在10.x已经更名了，所以在确保其他已经安装好的情况下使用忽略依赖进行安装，ceph0.94没有这个问题</p></blockquote><h4 id="配置权限"><a href="#配置权限" class="headerlink" title="配置权限"></a>配置权限</h4><p>需要创建一个client.restapi的用户 拥有权限 [mds] allow, [mon] allow * , [osd] allow *</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">ceph auth get-or-create client.restapi mds <span class="hljs-string">&#x27;allow&#x27;</span> osd <span class="hljs-string">&#x27;allow *&#x27;</span> mon <span class="hljs-string">&#x27;allow *&#x27;</span> &gt; /etc/ceph/ceph.client.restapi.keyring<br><span class="hljs-built_in">chmod</span> 644 /etc/ceph/ceph.client.admin.keyring<br><span class="hljs-built_in">chmod</span> 644 /etc/ceph/ceph.client.restapi.keyring<br></code></pre></td></tr></table></figure><p>在&#x2F;etc&#x2F;ceph&#x2F;ceph.conf配置文件里面添加</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">[client.restapi]<br>    log_file = /dev/null<br>    keyring = /etc/ceph/ceph.client.restapi.keyring<br></code></pre></td></tr></table></figure><h4 id="配置httpd"><a href="#配置httpd" class="headerlink" title="配置httpd"></a>配置httpd</h4><p> &#x2F;etc&#x2F;httpd&#x2F;conf&#x2F;httpd.conf 中间添加一条</p><blockquote><p>Listen 8080</p></blockquote><p>因为inkscope的web 默认采用虚拟主机的方式使用了8080端口</p><h3 id="给目录访问权限-存储日志使用"><a href="#给目录访问权限-存储日志使用" class="headerlink" title="给目录访问权限(存储日志使用)"></a>给目录访问权限(存储日志使用)</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@inkscope ~]<span class="hljs-comment"># chmod 777 -R /var/log/ceph/</span><br></code></pre></td></tr></table></figure><p>修改ceph-rest-api的地址</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">vim /etc/httpd/conf.d/inkScope.conf<br></code></pre></td></tr></table></figure><p>修改为：</p><blockquote><p>ProxyPass &#x2F;ceph-rest-api&#x2F; <a href="http://192.168.222.100:8080/ceph_rest_api/api/v0.1/">http://192.168.222.100:8080/ceph_rest_api/api/v0.1/</a></p></blockquote><p>这个地方是写的这台管理节点的地址和端口，因为本机实现了wsgi的方式的rest-api的接口，不是用的集群的5000的端口，而是直接使用web配置的8080的端口</p><h4 id="启动httpd服务"><a href="#启动httpd服务" class="headerlink" title="启动httpd服务"></a>启动httpd服务</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@inkscope ~]<span class="hljs-comment"># systemctl restart httpd</span><br></code></pre></td></tr></table></figure><p>检查ceph-rest-api是否能访问，地址是 <a href="http://192.168.222.100:8080/ceph_rest_api/api/v0.1/">http://192.168.222.100:8080/ceph_rest_api/api/v0.1/</a></p><p><img src="/images/blog/o_200901034749rest-api.png" alt="rest-api.png-22.5kB"></p><p>修改&#x2F;opt&#x2F;inkscope&#x2F;etc&#x2F;inkscope.conf配置文件，</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-string">&quot;ceph_rest_api&quot;</span>: <span class="hljs-string">&quot;192.168.222.100:8080&quot;</span>,<br><span class="hljs-string">&quot;ceph_rest_api_subfolder&quot;</span>: <span class="hljs-string">&quot;ceph_rest_api&quot;</span>,<br><span class="hljs-string">&quot;mongodb_host&quot;</span> : <span class="hljs-string">&quot;192.168.222.100&quot;</span>,<br></code></pre></td></tr></table></figure><blockquote><p>注意上面的地址不要在ip和地址前面加http:&#x2F;&#x2F;否则获取不到信息的，注意使用的是inkscope的web端口8080，也就是上面配置好的rest-api的端口</p></blockquote><h4 id="启动cephprobe-服务"><a href="#启动cephprobe-服务" class="headerlink" title="启动cephprobe 服务"></a>启动cephprobe 服务</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@inkscope ~]<span class="hljs-comment"># /etc/init.d/cephprobe restart</span><br></code></pre></td></tr></table></figure><p>现在就可以访问<br><a href="http://192.168.222.100:8080/inkscopeViz/index.html">http://192.168.222.100:8080/inkscopeViz/index.html</a></p><p>这个是没有用户名密码的，我们为了安全采用以下用户名密码的方式,需要安装flask-login</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@inkscope ~]<span class="hljs-comment"># rpm -ivh python-flask-login-0.2.11-1.el7.centos.noarch.rpm</span><br></code></pre></td></tr></table></figure><h4 id="重启httpd服务"><a href="#重启httpd服务" class="headerlink" title="重启httpd服务"></a>重启httpd服务</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@inkscope ~]<span class="hljs-comment"># systemctl restart httpd</span><br></code></pre></td></tr></table></figure><p> <img src="/images/blog/o_200901034757denglu.png" alt="denglu.png-35.5kB"></p><p>再次访问就需要用户名密码了</p><ul><li>默认用户名:admin  </li><li>默认密码:admin</li></ul><p>到这里最基本的管理平台配置就完成了</p><h3 id="配置sysprobe"><a href="#配置sysprobe" class="headerlink" title="配置sysprobe"></a>配置sysprobe</h3><p>sysprobe是获取集群节点的主机的信息的</p><h4 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@inkscope ~]<span class="hljs-comment"># rpm -ivh inkscope-common-1.3.1-2.noarch.rpm</span><br>[root@inkscope ~]<span class="hljs-comment"># rpm -ivh inkscope-sysprobe-1.3.1-2.noarch.rpm  </span><br></code></pre></td></tr></table></figure><p>将主监控节点的inkscope配置文件拷贝到节点上</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@inkscope ~]<span class="hljs-comment"># scp /opt/inkscope/etc/inkscope.conf 192.168.8.106:/opt/inkscope/etc/</span><br></code></pre></td></tr></table></figure><p>在osd节点启动sysprobe服务</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ~]<span class="hljs-comment"># /etc/init.d/sysprobe restart</span><br></code></pre></td></tr></table></figure><p><img src="/images/blog/o_200901034805probe.png" alt="probe.png-49.6kB"><br>正常情况就用上面这个页面检查配置的状况</p><p>cephprobe是用来或者集群的相关信息和操作的<br>sysprobe是获取节点的磁盘分区等相关信息的</p><p>基本节点的软件包配置完毕了，一些扩展功能也配置一下</p><h3 id="创建rgw相关的"><a href="#创建rgw相关的" class="headerlink" title="创建rgw相关的"></a>创建rgw相关的</h3><h4 id="配置rgw网关"><a href="#配置rgw网关" class="headerlink" title="配置rgw网关"></a>配置rgw网关</h4><p>在&#x2F;etc&#x2F;ceph&#x2F;ceph.conf<br>添加</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[client.radosgw.gateway]<br>host = inkscope<br></code></pre></td></tr></table></figure><h4 id="启动radosgw服务"><a href="#启动radosgw服务" class="headerlink" title="启动radosgw服务"></a>启动radosgw服务</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@inkscope ~]<span class="hljs-comment"># /etc/init.d/ceph-radosgw restart</span><br></code></pre></td></tr></table></figure><p>检查端口是否启动，默认是7480</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@inkscope ~]<span class="hljs-comment"># netstat -tunlp|grep radosgw</span><br>tcp        0      0 0.0.0.0:7480            0.0.0.0:*               LISTEN      113493/radosgw  <br></code></pre></td></tr></table></figure><h4 id="创建rgw使用的存储池并且添加到rgw"><a href="#创建rgw使用的存储池并且添加到rgw" class="headerlink" title="创建rgw使用的存储池并且添加到rgw"></a>创建rgw使用的存储池并且添加到rgw</h4><p>rados mkpool .rgw.buckets 1024 1024<br>radosgw-admin pool add –pool .rgw.buckets</p><p>执行完后检查存储池情况,自动会创建了一些存储池</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@inkscope ~]<span class="hljs-comment"># ceph df</span><br>GLOBAL:<br>    SIZE     AVAIL     bash USED     %bash USED <br>    548G      548G       70080k          0.01 <br>POOLS:<br>    NAME             ID     USED     %USED     MAX AVAIL     OBJECTS <br>    rbd              0         0         0          274G           0 <br>    zp               1         0         0          274G           0 <br>    .rgw.root        2       848         0          274G           3 <br>    .rgw.control     3         0         0          274G           8 <br>    .rgw             4        24         0          274G           1 <br>    .rgw.gc          5         0         0          274G          32 <br>    .users.uid       6         0         0          274G           0 <br>    .rgw.buckets     7         0         0          274G           0 <br></code></pre></td></tr></table></figure><h3 id="创建rgw的用户"><a href="#创建rgw的用户" class="headerlink" title="创建rgw的用户"></a>创建rgw的用户</h3><p>这个用户是管理员用户，需要给很多权限</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@inkscope ~]<span class="hljs-comment"># radosgw-admin user create --uid=inkscope --display-name=&quot;inkscope&quot; --access-key=inkscope --secret=inkscope --access=full --caps=&quot;metadata=*;users=*;buckets=*&quot;</span><br></code></pre></td></tr></table></figure><h3 id="修改配置文件"><a href="#修改配置文件" class="headerlink" title="修改配置文件"></a>修改配置文件</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@inkscope ~]<span class="hljs-comment"># vim /opt/inkscope/etc/inkscope.conf</span><br></code></pre></td></tr></table></figure><p>修改下面的几项</p><pre><code class="bash">&quot;radosgw_url&quot;: &quot;http://192.168.222.100:7480&quot;,&quot;radosgw_admin&quot;: &quot;admin&quot;,&quot;radosgw_key&quot;: &quot;inkscope&quot;,&quot;radosgw_secret&quot;: &quot;inkscope&quot;</code></pre><p>radosgw_url为rgw的访问地址<br>radosgw_admin字段不更改就是admin<br>radosgw_key，radosgw_secret就是上面创建的那个密钥</p><h3 id="配置结束"><a href="#配置结束" class="headerlink" title="配置结束"></a>配置结束</h3><p>基本按照上面的做法就可以配置完毕了，并且可以正常使用，之前搞错了一个地方就是那个restapi，这个地方可能是最开始的时候，这个地方是需要调用的原始的那个5000端口的api，然后基本操作都是可以做的，一些新开发的功能需要用到新的接口，就按照新的配置即可，inkscope从我开始关注到现在已经改进了很多，添加了sanky chart来显示pg的分布，里面的api接口也更加的丰富。并且提供了友好的安装方式，应该是目前最成熟的一种管理平台了</p><h3 id="展示"><a href="#展示" class="headerlink" title="展示"></a>展示</h3><p>sanky chart显示pg<br><img src="/images/blog/o_200901034812sankey.png" alt="sankey.png-500.1kB"></p>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>记一次不完全成功到成功的失效恢复(20160412)</title>
    <link href="/2016/04/13/%E8%AE%B0%E4%B8%80%E6%AC%A1%E4%B8%8D%E5%AE%8C%E5%85%A8%E6%88%90%E5%8A%9F%E5%88%B0%E6%88%90%E5%8A%9F%E7%9A%84%E5%A4%B1%E6%95%88%E6%81%A2%E5%A4%8D(20160412)/"/>
    <url>/2016/04/13/%E8%AE%B0%E4%B8%80%E6%AC%A1%E4%B8%8D%E5%AE%8C%E5%85%A8%E6%88%90%E5%8A%9F%E5%88%B0%E6%88%90%E5%8A%9F%E7%9A%84%E5%A4%B1%E6%95%88%E6%81%A2%E5%A4%8D(20160412)/</url>
    
    <content type="html"><![CDATA[<h3 id="更新"><a href="#更新" class="headerlink" title="更新"></a>更新</h3><p>在经历了好几天后，失效的环境最终变成了可用状态，只能说有的时候不放弃还真是有点用的</p><p>在不久前处理了一个故障恢复以后，又碰上一个群友的集群出现了严重故障，本篇将记录这个中间大致处理的过程，一些细节在以后会补充</p><p>首先看到给出的截图显示的是大量的pg处于异常的状态，从经验上判断，环境要么处于down机的边缘，或者是刚经历了一次大量的重启，这个时候集群可以说是前端的访问肯定全断的，这个故障的时候资源一般会比较紧张，所以在启动的过程中也要注意不要触发更大面积的down机，对于集群来说是会有连带效应的</p><p>在启动了部分osd后，集群还是有大量的pg出现的是down+peering的状态，而发现down的osd实际全部在一台服务器上的，这个从ceph的架构来说是不应该出现这个状态的，这个可能是在down机过程中，频繁的pg的状态变化造成了pg的状态停留在之前的down的状态上，而pg出现锁死的状况，这个在之前的那位群友的环境中出现过一次，那个是多机有osd出现异常的情况，这次是单机出现的情况</p><p>尝试加大日志级别，从几个osd里面看日志出现两类的异常，从后面的处理的情况来看，实际这个是触发了两个bug，第一个问题出现是部分的数据丢失，这个在进行处理以后，再次启动的时候，几个osd出现了同样的错误，在询问了我们的研发大牛后，基本能判断这个是一个0.94.x的bug，并且在邮件列表里面已经解决</p><p>然后尝试对其中的一台进行升级，这次升级直接升级到了10.1.1，然后启动osd，确实可以启动了，具体的怎么触发这个bug，就不是太清楚里面的过程，这个环境是经历了一个比较复杂的状态变化</p><p>启动了部分osd后，发现还是osd无法启动，一检查，发现居然是这个机器的5个磁盘都有文件系统的错误，之前的部分数据丢失，也可能是文件系统错误引起的，很有可能是异常后造成了大面积的异常，这个地方只是推断，因为没有看到监控中间的过程，在修复了文件系统以后，osd都能起来了，只是又碰上另外的一个问题，<br>incomplete状态，就是事情没做完，在检查了里面的数据，发现数据没问题</p><p>尝试做修复，使用cephobjecttool导出数据再导入另外的pg，状态还是无法变化，然后根据之前另外一个国外的处理经验，将pg导入到非pg映射的osd，然后让其自动backfiill，发现还是无法生效，pg仍然停留在imcomplete状态</p><p>在询问了对方里面的数据情况后得知只有一个镜像比较重要，果断尝试后端的修复，大概思路就是将img镜像所对应的数据全部拷贝到一个目录，然后进行拼接的操作，这个在我之前的测试环境测试过没问题，这次在这个环境上进行了操作，因为环境的对象大小经过了修改，所以脚本也要对应修改，最后合成看了一个bash文件，在经过验证后，能够启动，数据基本是算是恢复了</p><p>然后做了pg repair osd repair deep-scrub等操作都是无法改变状态</p><p>环境还停留在不可用的状态，尝试做最后的修复，将pg数据进行备份后，强制创建pg，这个在我自己的测试环境下是可行的方案，但是这个环境在停留在creating状态比较久后，还是会进入imcomplete状态，尝试几次还是不行，开始怀疑是这两个osd问题，然后将osd out以后，在重新分布的osd上进行了创建pg操作，还是creating后进入imcomplete状态，到此，基本判断环境无法恢复了，数据算是保住了</p><p>这个是国内一个比较牛的cepher也碰到的情况 <a href="http://m.oschina.net/blog/360274">osd盘崩溃的总结</a>，他这个环境也是最终无法救回来<br>这是他查询到的国外的一个人写的情况：</p><blockquote><p>查了一圈无果。一个有同样遭遇的人的一段话：<br><br>I already tried “ceph pg repair 4.77”, stop&#x2F;start OSDs, “ceph osd lost”, “ceph pg force_create_pg 4.77”.<br>Most scary thing is “force_create_pg” does not work. At least it should be a way to wipe out a incomplete PG without destroying a whole pool.</p></blockquote><p>这个地方出故障的环境做一个总结：</p><ul><li>环境做了比较极端的优化，这里就不说了，ceph的journal这一层就是防止down机出现数据不一致做replay的，做了极端的环境优化需要做多次整机down机测试，这个down机是无法完全避免的，所以要考虑</li><li>磁盘出现了多个同时的损坏，这个没有办法，文件系统的损坏有可能是主机系统出现比较特殊的异常造成磁盘数据异常，这个单机多磁盘损坏的可能是有的，最怕就是部分损坏</li><li>ceph有部分bug是在比较极端的情况下出现的，并不是没有，所以不能想着完全避免bug，多想想真出问题了，怎样把损失降低到最小，我的底线是数据回来</li><li>ceph集群的副本只能保证系统内的高可用，系统级别的高可用，只能是双系统，能搭两套一定两套，哪怕非实时定期备份也好</li><li>随着ceph使用者越多，出现问题的情况会越来越多的，特别是在使用的越久，概率就越大，磁盘也是有寿命的，集群呢？还是早做防范措施</li></ul><h3 id="后续"><a href="#后续" class="headerlink" title="后续"></a>后续</h3><p>事情本以为就这么完结了，因为已经达到了最低的标准，数据的恢复，但实际上对于我自己来说，还是觉得有点遗憾的，毕竟环境是处于一个无法使用的状态，并且，环境中实际也只有部分数据的损坏，但是因为pg的状态不对，那些虚拟机实际是无法写入的，变相的这个环境就是一个僵住的状态了，虽然想了好几天，但是并没有更好的办法，有一个办法是将整个的数据导出再导入，这个时间周期会很长，如果里面数据很多都是重要的，这个是不得不走的一步了，正好这个环境重要数据只有一个，也就没去尝试了</p><p>我有一个翻译的计划的，已经停滞了很久，但是说实话，我之前的想法是一章章的细细的研究，细细的翻译，然后写出自己的想法，但是迫于时间原因，以及最近事情比较多，暂时处于停滞状态，这个后期会跟进的，目前已经购买的书友，以及支持的朋友，我尽量的是对你提出的问题或者困惑给出我个人的见解，总之一个事情的处理方式有多种，我从来都是告诉你我会怎么做，然后告诉你，你可以根据你的想法来，正是因为想到自己最近没时间翻译，自己干脆把这本书过一遍，果然还是多读书好，根据书里面的一个提示，我就去尝试做另外一个操作</p><p>在有想法以后，联系了群友，正好环境还在，没有做推倒重来的操作，这个也感谢ceph群友的信任和支持，在隔了几天再次登录环境以后，根据提示，我将这个pg的数据进行了删除，这次的删除不是之前的暴力的直接rm，而是使用ceph内部的工具进行的删除，主副本停止osd后同时做的操作，我怀疑是不是还有哪里的元数据被锁住了，在删除以后再次起来，再次创建pg的时候，环境还是处于一个异常的状态，因为书中描述了是我之前没见过的操作，当时想想是不是有其他的不清楚的操作方式，在一番查询以后，真的有我没用过的操作，然后直接尝试，果然整个集群正常了，然后把之前的pg数据进行导入操作，然后用rados直接get那个异常的pg里面的对象，果然能读取了，然后用rados ls也能够列出所有的对象了，环境终于能够正常了，环境是强制的改变状态变成可正常，数据也能够读写了，我个人的建议如果真是有很多重要数据，还是把数据倒出来再导入进去，集群正常情况下的导出导入操作逻辑和时间比后台的导出逻辑要简单非常多</p><p>好了，到了这里终于将一个环境变成了正常的状态了，对于我自己来说，对ceph的控制又提高了一点，之前认为数据盘在，我就能把数据恢复，倒出来，但是原集群的恢复，没有太多的保证，现在基本上只要盘符不被格式化掉，环境我也能有很大的概率去恢复正常，总之保底恢复方式的越多，越有信心去恢复它</p><p>这次的经历让我再一次感觉，不要放弃，不要放弃，有的时候真的会有转机，同时感谢群友能够提供环境给我，也欢迎有更多的朋友在出现问题的时候可以找我探讨一下</p><blockquote><p>by 运维-武汉-磨渣<br>2016年04月12日夜<br>更新于2016年04月17日夜</p></blockquote><p>&#96;</p>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>记一次ceph的故障修复(20160408)</title>
    <link href="/2016/04/11/%E8%AE%B0%E4%B8%80%E6%AC%A1ceph%E7%9A%84%E6%95%85%E9%9A%9C%E4%BF%AE%E5%A4%8D(20160408)/"/>
    <url>/2016/04/11/%E8%AE%B0%E4%B8%80%E6%AC%A1ceph%E7%9A%84%E6%95%85%E9%9A%9C%E4%BF%AE%E5%A4%8D(20160408)/</url>
    
    <content type="html"><![CDATA[<p>ceph的在正常运行的时候基本不会出现故障，出现故障一般在变动的时候，具体有下面几种可能出现的情形</p><ul><li>软件升级</li><li>增加存储节点</li><li>减少存储节点</li><li>调整副本数目</li><li>调整pg数目</li><li>磁盘出现损坏</li><li>节点网络出现异常</li></ul><p>以上这些操作过程中是最可能出现异常的情形，并不是一定会出问题，以上问题除了网络和磁盘问题出现的异常是基本无法避免外，其他出现的时候，一般是非正常操作引起的，也就是我通常认为的人为事故，这个一般出现在操作的不谨慎上</p><p>本篇记录了一次故障的修复过程，这个故障不是出现在我们公司的产品上，是看到一个ceph社区群里有一个成员在里面问到一个异常是否能解决，这个不同于普通的问题，从他贴出的信息来看，集群已经是非常严重的状态了</p><p>正好看到是周五，周六还可以休息下，所以即使快到了晚上12点了，我还是联系了一下那哥们，从简短的几句交流后，基本可以判断对方对于ceph基本处于刚接触的阶段，在询问是否有其他人能协助他做一些比较有难度的操作的时候，他说没有，就他一个人，我想在目前中国很多公司，都是让一个并不太熟悉ceph的运维人员，或者完全就是开发人员维护着存储着非常宝贵的数据的云存储环境，上面运行的应该都是客户的数据，想想我们自己的电脑在硬盘损坏后，自己有多么不爽，而对于企业来说，一个运行环境的损坏有多么严重，一方面损失了数据，另一方面，基本不会再选择这个服务的提供商了，而这些都是一个定时炸弹，运行在中国的开源存储网络环境当中，而且基本都是初创小企业，大企业会有专门的专业的相关人员，而一个数据损失基本会对这些初创企业带来巨大的损失，这些都是需要企业的boss多关注的，这也是我一直持有的一个观点，越来越多的企业是用ceph，也意味着存储需要修复的出现几率就越大，其实我们也是一个小企业，我个人是非常关注数据恢复这一块的，这个比调优更加的重要，大环境的吐槽就到这里，下面开始讲下具体的经过</p><h3 id="首先找对方要了一个ssh登陆环境"><a href="#首先找对方要了一个ssh登陆环境" class="headerlink" title="首先找对方要了一个ssh登陆环境"></a>首先找对方要了一个ssh登陆环境</h3><p>这个对方正好有这个环境允许我的登陆，虽然中间经过了堡垒机，虽然运行命令比较卡顿，但好歹能上去，这个是我个人非常支持的一种做法，不管怎样，是VPN也好，代理也好，一定留一个外网的ssh端口能够让连上机器，这个能允许随时随地能上去处理问题，等你运维人员到达现场，真是黄花菜都凉了，对于比较保密的环境，最好也能够有一个在紧急情况下开启远程允许环境的条件，这个具体花费，一个上网卡，一台破旧的笔记本就基本能实现了，在需要远程操作的时候能够连上去处理，目前已经协助了几个朋友处理了一些简单的问题，基本都是ssh连过去的，而没有远程环境的，我也是无能为力的</p><h3 id="检查环境"><a href="#检查环境" class="headerlink" title="检查环境"></a>检查环境</h3><p>登陆上去以后，检查环境发现提示的是2个pg的状态imcomplete，这个是pg的数据不一致的提示，而在检查了对应的osd上的这个pg的数据的时候，发现映射计算到的3个上面有两个是没有数据的，有一个是有数据的，在询问对方做过的操作后，对方是做了一个删除osd的操作，并且是多台机器上面都做过删除，到这里我询问了一下对方，对方是按照一些通用的操作去做的删除操作，命令肯定是没有问题的，这个在后面我处理完后，基本能判断出对方是人为的操作失误引起的</p><h3 id="尝试修复"><a href="#尝试修复" class="headerlink" title="尝试修复"></a>尝试修复</h3><p>开始想起之前做过的一次模拟修复，本来以为这个可以把环境弄好了，基本想法就是如下流程：</p><ul><li>停止pg对应的3个osd</li><li>导出有数据的pg</li><li>在无数据的osd上进行pg的数据导入</li><li>启动三个osd</li></ul><p>在进行到数据的导入的时候提示了pg is blocked，这个在我之前的做的测试中是没有遇到过的，后来进行pg的状态查询时候，发现是pg的显示的数据全是0，也就是集群认为这个pg是没有数据的，并且被几个已经删除了的osd blocked,而且做ceph osd  lost 也是无法操作的，提示没有osd，这个应该是pg状态不一致，也就是这个pg状态完全异常了，并且还无法导入了</p><h3 id="思考解决办法"><a href="#思考解决办法" class="headerlink" title="思考解决办法"></a>思考解决办法</h3><p>到这里我个人判断基本是回天无力了，再次跟对方确认删除的过程，发现对方好在数据盘都保留了，并且还插在机器上，只是有部分osd在进行增加的时候还占用了删除的osd的id</p><p>到这里我基本想出来两种方法：</p><ul><li>最不济，也是终极解决办法就是把后台缺失的数据拼起来，这个耗时巨大，操作难度大，基本上只能作为最后终极挽回的方法，这个只有在客户已经觉得数据可能要丢了，然后去做最后的终极挽回大法了，客户的容忍度是会随着你问题严重性而改变的，相信我数据还在都好说</li><li>就是将删除的数据盘给加进来，这个操作在我几年ceph生涯中也是从未做过的，也想不出什么场景下需要这种操作，好吧，不管多么特殊的操作，总有它的存在的意义，我也不能确定ceph是否支持这种操作，那就试试这种</li></ul><p>这个集群之所以能挽回，有几个特殊点正好都在，缺一不可 </p><ol><li>删除的数据盘居然没被格式化，或者搞掉，这个如果弄没了，数据必丢</li><li>删除的数据盘的盘位部分被新加的节点占用了，部分还没有被占用，而这个缺失数据的pg的数据所删除的osd正好又没有被占用（所以以后替换osd的时候最好是用新的编号，老的盘和编号保留着）</li></ol><h3 id="开始恢复的操作"><a href="#开始恢复的操作" class="headerlink" title="开始恢复的操作"></a>开始恢复的操作</h3><p>之前我加节点的操作都是用的ceph-deploy，可以说基本没有遇到过手动能做的ceph-deploy无法完成的，好吧这次我知道了还是有无法完成的，手动的还是多学学比较好，好在我比较熟悉，就按步骤去做</p><h4 id="1、增加认证"><a href="#1、增加认证" class="headerlink" title="1、增加认证"></a>1、增加认证</h4><p>我们在删除osd的最后一步的时候操作都是ceph auth del<br>我就反向的操作</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">ceph auth add osd.0 osd <span class="hljs-string">&#x27;allow *&#x27;</span> mon <span class="hljs-string">&#x27;allow rwx&#x27;</span> -i /var/lib/ceph/osd0/keyring<br></code></pre></td></tr></table></figure><p>这个对应keyring就是在删除那个osd上面有，每个osd上面都有的<br>这一步操作完成后auth里面就有osd.0了</p><h4 id="2、创建osd"><a href="#2、创建osd" class="headerlink" title="2、创建osd"></a>2、创建osd</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">ceph osd create<br></code></pre></td></tr></table></figure><p>这个步骤也是之前没有做过的，之前准备直接加crush 直接启动发现都是无法启动，提示没有osd<br>这一步相对于删除里面的操作应该就是 ceph osd rm 的操作了</p><h4 id="3、增加crush"><a href="#3、增加crush" class="headerlink" title="3、增加crush"></a>3、增加crush</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">ceph osd crush add osd.0 0.9 host=node1<br></code></pre></td></tr></table></figure><p>这个就是加入到crush里面去</p><h4 id="4、启动osd"><a href="#4、启动osd" class="headerlink" title="4、启动osd"></a>4、启动osd</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">/etc/init.d/ceph start osd.0<br></code></pre></td></tr></table></figure><h3 id="检查现在环境状况"><a href="#检查现在环境状况" class="headerlink" title="检查现在环境状况"></a>检查现在环境状况</h3><p>在检查的时候发现osd真的就加进来了，然后在添加了另一个被block的osd后，集群状态就没有imcomplete了就是active+其他的一些恢复状态什么的，只需要等待恢复，集群即可恢复正常了，到这个时候已经凌晨三点了，事情能够完满解决是最开心的事情</p><h3 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h3><p>集群的删除操作随意，集群的信息基本无记录，环境的基础记录都没有，这个是这个事故的最大原因，再往上走就是对于数据操作这块，公司没有一个重视的态度，上面的boss永远不会关心你运维做了什么操作，而运维人员也可以说是我按标准流程操作的，也没法去定谁的责任，丢了就是丢了，运维最多也就是丢了工作，而企业损失应该就是以万为单位的损失加客户的流失了<br>到这里也许这家公司的头并不知道发生了什么，也许只是认为是一个小的业务中断，但真的某一天出事了，这就是大事了，所以一定要重视系统的监控和系统操作的谨慎</p><blockquote><p>by 运维-武汉-磨渣<br>2016年04月11日夜</p></blockquote>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>ceph在centos7下一个不容易发现的改变</title>
    <link href="/2016/03/31/ceph%E5%9C%A8centos7%E4%B8%8B%E4%B8%80%E4%B8%AA%E4%B8%8D%E5%AE%B9%E6%98%93%E5%8F%91%E7%8E%B0%E7%9A%84%E6%94%B9%E5%8F%98/"/>
    <url>/2016/03/31/ceph%E5%9C%A8centos7%E4%B8%8B%E4%B8%80%E4%B8%AA%E4%B8%8D%E5%AE%B9%E6%98%93%E5%8F%91%E7%8E%B0%E7%9A%84%E6%94%B9%E5%8F%98/</url>
    
    <content type="html"><![CDATA[<p>在centos6以及以前的osd版本，在启动osd的时候，回去根据ceph.conf的配置文件进行挂载osd，然后进行进程的启动，这个格式是这样的</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">[osd.0]<br>host = hostname<br>devs=/dev/sdb1<br></code></pre></td></tr></table></figure><p>启动的时候就会把sdb1盘符挂载到0的目录里面去了</p><p>然后在centos7的版本的时候，发现居然不写配置文件也能够自动挂载启动，这个地方是什么地方发生了变化，在做了一些日志的查询以后，发现centos7下居然做了一个改变</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ~]<span class="hljs-comment"># systemctl list-unit-files |grep ceph-disk</span><br>ceph-disk@.service                          static  <br></code></pre></td></tr></table></figure><p>可以看到有这个服务</p><h3 id="我们来验证下这个服务"><a href="#我们来验证下这个服务" class="headerlink" title="我们来验证下这个服务"></a>我们来验证下这个服务</h3><h4 id="先停止服务"><a href="#先停止服务" class="headerlink" title="先停止服务"></a>先停止服务</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">systemctl stop ceph-osd@1<br></code></pre></td></tr></table></figure><h4 id="umount挂载点"><a href="#umount挂载点" class="headerlink" title="umount挂载点"></a>umount挂载点</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">umount /var/lib/ceph/osd/ceph-1<br></code></pre></td></tr></table></figure><p>现在已经没有挂载点了</p><h4 id="现在执行下面的服务（我的sdc1是刚刚的osd-1）"><a href="#现在执行下面的服务（我的sdc1是刚刚的osd-1）" class="headerlink" title="现在执行下面的服务（我的sdc1是刚刚的osd.1）"></a>现在执行下面的服务（我的sdc1是刚刚的osd.1）</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ~]<span class="hljs-comment"># systemctl start ceph-disk@/dev/sdc1</span><br>[root@lab8106 ~]<span class="hljs-comment"># systemctl status ceph-disk@/dev/sdc1</span><br>● ceph-disk@-dev-sdc1.service - Ceph disk activation: /dev/sdc1<br>   Loaded: loaded (/usr/lib/systemd/system/ceph-disk@.service; static; vendor preset: disabled)<br>   Active: inactive (dead)<br><br>Mar 31 16:11:37 lab8106 sh[17847]: <span class="hljs-built_in">command</span>: Running <span class="hljs-built_in">command</span>: /usr/bin/ceph-detect-init --default sysvinit<br>Mar 31 16:11:37 lab8106 sh[17847]: activate: Marking with init system systemd<br>Mar 31 16:11:37 lab8106 sh[17847]: activate: ceph osd.1 data <span class="hljs-built_in">dir</span> is ready at /var/lib/ceph/tmp/mnt.3a8xNK<br>Mar 31 16:11:37 lab8106 sh[17847]: move_mount: Moving mount to final location...<br>Mar 31 16:11:37 lab8106 sh[17847]: command_check_call: Running <span class="hljs-built_in">command</span>: /bin/mount -o noatime,inode64 -- /dev/sdc1 /var/lib/ceph/osd/ceph-1<br>Mar 31 16:11:37 lab8106 sh[17847]: command_check_call: Running <span class="hljs-built_in">command</span>: /bin/umount -l -- /var/lib/ceph/tmp/mnt.3a8xNK<br>Mar 31 16:11:37 lab8106 sh[17847]: start_daemon: Starting ceph osd.1...<br>Mar 31 16:11:37 lab8106 sh[17847]: command_check_call: Running <span class="hljs-built_in">command</span>: /usr/bin/systemctl <span class="hljs-built_in">enable</span> ceph-osd@1<br>Mar 31 16:11:37 lab8106 sh[17847]: command_check_call: Running <span class="hljs-built_in">command</span>: /usr/bin/systemctl start ceph-osd@1<br>Mar 31 16:11:37 lab8106 systemd[1]: Started Ceph disk activation: /dev/sdc1.<br></code></pre></td></tr></table></figure><p>执行完检查</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ~]<span class="hljs-comment"># df -h |grep ceph-1</span><br>/dev/sdc1       275G   35M  275G   1% /var/lib/ceph/osd/ceph-1<br></code></pre></td></tr></table></figure><p>可以看到已经挂载好，并且启动了服务<br>可以看到我没有使用任何配置情况下，没有告诉集群sdc1就是要挂载到 &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;osd&#x2F;ceph-1 这个目录的，自动挂载好了，这个是集群自己先mount到一个临时目录根据磁盘里面的信息来判断了这个osd真实的数据，根据这个数据来mount到一个挂载点，这个做法是非常好的做法</p>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>rbd-mirror新功能</title>
    <link href="/2016/03/30/rbd-mirror%E6%96%B0%E5%8A%9F%E8%83%BD/"/>
    <url>/2016/03/30/rbd-mirror%E6%96%B0%E5%8A%9F%E8%83%BD/</url>
    
    <content type="html"><![CDATA[<p>RBD 的 mirroring 功能将会在下一个稳定版本Jewel中实现，这个Jewel版本已经发布了第一个版本10.1.0,这个功能已经在这个发布的版本中实现了</p><p>一、基本原理<br>我们试图解决的或者至少需要克服的问题是，ceph在内部是强一致性的，这个对于跨区域的情况数据同步是无法接受的，一个请求需要异地返回再确认完成，这个在性能上肯定是无法接受的，这就是为什么基本上无法部署跨区域的ceph集群</p><p>因此我们需要有一种机制能够让我们在不同区域的集群之间复制块设备。这个能够帮助我们实现两个功能：</p><ul><li>灾难恢复</li><li>全球块设备分布（跨地理位置）</li></ul><p>二、内部的实现</p><p><img src="/images/blog/o_200901034252%E7%94%BB%E5%9B%BE.png" alt="画图.png-34.8kB"></p><p>从上图所示是进行的主备模式的备份，其实这个只是看怎么应用了，在里面是自动实现的主主的模式，双向同步的，只是在应用中需要注意不要去同时操作同一个image，这个功能是作为主备去使用的，以备真正有问题的时候去实现故障恢复，这个同步是异步的</p><p>二、一个新的进程<br>一个新的守护程序：rbd-mirror 将会负责将一个镜像从一个集群同步到另一个，rbd-mirror需要在两个集群上都配置，它会同时连接本地和远程的集群。在jewel版本中还是一对一的方式，在以后的版本中会实现一对多的，所以在以后的版本可以配置一对多的备份</p><p>作为起点，这个功能讲使用配置文件连接集群，使用用户和密钥。使用admin用户就可以了，使用的验证方式就是默认的cephx的方式</p><p>为了相互识别，两个集群都需要相互注册使用 rbd mirror pool peer add 命令， 这个在下面会实践</p><p>二、镜像<br><img src="/images/blog/o_200901034303ceph-rbd-mirror-inside.png" alt="ceph-rbd-mirror-inside.png-80.8kB"><br>The RBD mirroring 依赖两个新的rbd的属性</p><ul><li>journaling: 启动后会记录image的事件</li><li>mirroring: 明确告诉rbd-mirror需要复制这个镜像</li></ul><p>也有命令可以禁用单独的某个镜像。journaling可以看做是另一个rbd的image（一些rados对象），一般情况下，先写日志，然后返回客户端，然后被写入底层的rbd的image，出于性能考虑，这个journal可以跟它的镜像不在一个存储池当中，目前是一个image一个journal，最近应该会沿用这个策略，直到ceph引入一致性组。关于一致性组的概念就是一组卷，然后用的是一个RBD image。可以在所有的组中执行快照操作，有了一致性的保证，所有的卷就都在一致的状态。当一致性组实现的时候，我们就可以用一个journal来管理所有的RBD的镜像</p><p>可以给一个已经存在image开启journal么，可以的，ceph将会将你的镜像做一个快照，然后对快照做一个复制，然后开启journal，这都是后台执行的一个任务</p><p>可以启用和关闭单个镜像或者存储池的mirror功能，如果启用了journal功能，那么每个镜像将会被复制</p><p>可以使用 rbd mirror pool enable启用它</p><p>三、灾难恢复<br>交叉同步复制是可以的，默认的就是这个方式，这意味着<strong>两个地方的存储池名称需要相同的</strong>这个会带来两个问题</p><ul><li>使用相同的存储做备份做使用会影响性能的</li><li>相同的池名称在进行恢复的时候也更容易。openstack里面只需要记录卷ID即可</li></ul><p>每个image都有 mirroring_directory 记录当前active的地方。在本地镜像提示为 primary的时候，是可写的并且远程的站点上就会有锁，这个image就是不可写的。只有在primary镜像降级，备份的点升级就可以了，demoted 和 promoted来控制这里，这就是为什么引入了等级制度，一旦备份的地方升级了，那么主的就自动降级了，这就意味着同步的方向就会发生变化了</p><p>如果出现脑裂的情况，那么rbd-mirror将会停止同步，你自己需要判断哪个是最新的image，然后手动强制去同步 rbd mirror image resync</p><p>上面基本参照的是sebastien翻译的，原文只是做了简短的说明，下面是我的实践部分</p><h2 id="下面在我的环境下进行实践"><a href="#下面在我的环境下进行实践" class="headerlink" title="下面在我的环境下进行实践"></a>下面在我的环境下进行实践</h2><p>下面的环境是在两个集群上进行的，集群分别为：</p><ul><li>机器lab8106 </li><li>机器lab8107</li></ul><p>先启动进程，因为这个是个新功能，所以采取的是进程运行在前台的方式方便找到问题，这个里面因为很容易混淆，所以运行的时候都会说明命令执行的地方</p><h3 id="启动rbd-mirror的进程"><a href="#启动rbd-mirror的进程" class="headerlink" title="启动rbd-mirror的进程"></a>启动rbd-mirror的进程</h3><p>在lab8106执行命令</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">rbd-mirror -m 192.168.8.106:6789 -d<br></code></pre></td></tr></table></figure><p>在lab8107执行命令</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">rbd-mirror -m 192.168.8.107:6789 -d<br></code></pre></td></tr></table></figure><p><img src="/images/blog/o_200901034310qidongrbdmirror.png" alt="qidongrbdmirror.png-40.5kB"></p><h3 id="开启pool的mirror功能"><a href="#开启pool的mirror功能" class="headerlink" title="开启pool的mirror功能"></a>开启pool的mirror功能</h3><p>这里因为操作命令需要相互做peer的操作，所以需要提前做个配置文件的操作，将lab8106认为local集群，lab8107位remote集群，这个地方需要做个配置文件调整<br>注意需要在ceph.conf配置文件中添加</p><blockquote><p>rbd_default_features &#x3D; 125</p></blockquote><h4 id="在lab8106上执行"><a href="#在lab8106上执行" class="headerlink" title="在lab8106上执行"></a>在lab8106上执行</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">cp</span> /etc/ceph/ceph.conf /etc/ceph/local.conf<br><span class="hljs-built_in">cp</span> /etc/ceph/ceph.client.admin.keyring /etc/ceph/local.client.admin.keyring<br>scp /etc/ceph/ceph.conf lab8107:/etc/ceph/local.conf<br>scp /etc/ceph/ceph.client.admin.keyring lab8107:/etc/ceph/local.client.admin.keyring<br></code></pre></td></tr></table></figure><h4 id="在lab8107上执行"><a href="#在lab8107上执行" class="headerlink" title="在lab8107上执行"></a>在lab8107上执行</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">cp</span> /etc/ceph/ceph.conf /etc/ceph/remote.conf<br><span class="hljs-built_in">cp</span> /etc/ceph/ceph.client.admin.keyring /etc/ceph/remote.client.admin.keyring<br>scp /etc/ceph/ceph.conf lab8106:/etc/ceph/remote.conf<br>scp /etc/ceph/ceph.client.admin.keyring lab8106:/etc/ceph/remote.client.admin.keyring<br></code></pre></td></tr></table></figure><p>检验上面设置是否完成<br>在lab8106执行</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ceph]<span class="hljs-comment"># ceph --cluster local mon stat</span><br>e1: 1 mons at &#123;lab8106=192.168.8.106:6789/0&#125;, election epoch 3, quorum 0 lab8106<br>[root@lab8106 ceph]<span class="hljs-comment"># ceph --cluster remote mon stat</span><br>e1: 1 mons at &#123;lab8107=192.168.8.107:6789/0&#125;, election epoch 3, quorum 0 lab8107<br></code></pre></td></tr></table></figure><p>在lab8107执行</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">root@lab8107:~/ceph<span class="hljs-comment"># ceph --cluster local mon stat</span><br>e1: 1 mons at &#123;lab8106=192.168.8.106:6789/0&#125;, election epoch 3, quorum 0 lab8106<br>root@lab8107:~/ceph<span class="hljs-comment"># ceph --cluster remote mon stat</span><br>e1: 1 mons at &#123;lab8107=192.168.8.107:6789/0&#125;, election epoch 3, quorum 0 lab8107<br></code></pre></td></tr></table></figure><p>准备工作完成了，开始做相关配置<br>在lab8106上执行</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">rbd --cluster <span class="hljs-built_in">local</span> mirror pool <span class="hljs-built_in">enable</span> rbd pool<br>rbd --cluster remote mirror pool <span class="hljs-built_in">enable</span> rbd pool<br></code></pre></td></tr></table></figure><p>如果需要关闭那么执行</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">rbd --cluster <span class="hljs-built_in">local</span> mirror pool <span class="hljs-built_in">disable</span> rbd<br>rbd --cluster remote mirror pool <span class="hljs-built_in">disable</span> rbd<br></code></pre></td></tr></table></figure><h3 id="增加-CLUSTER-PEER"><a href="#增加-CLUSTER-PEER" class="headerlink" title="增加 CLUSTER PEER"></a>增加 CLUSTER PEER</h3><p>格式</p><blockquote><p>rbd mirror pool peer add {pool-name} {client-name}@{cluster-name}<br>使用admin这个账户就可以了</p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ceph]<span class="hljs-comment"># rbd --cluster local mirror pool peer add rbd client.admin@remote</span><br>eb08d27f-9e09-484a-a55a-589249cf6c10<br>[root@lab8106 ceph]<span class="hljs-comment"># rbd --cluster remote mirror pool peer add rbd client.admin@local</span><br>d22bb245-fb20-4273-b847-c8c5e06b2450<br></code></pre></td></tr></table></figure><h4 id="查询是否连接好"><a href="#查询是否连接好" class="headerlink" title="查询是否连接好"></a>查询是否连接好</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ceph]<span class="hljs-comment"># rbd --cluster local mirror pool info</span><br>Mode: pool<br>Peers: <br>  UUID                                 NAME   CLIENT       <br>  eb08d27f-9e09-484a-a55a-589249cf6c10 remote client.admin <br>[root@lab8106 ceph]<span class="hljs-comment"># rbd --cluster remote mirror pool info</span><br>Mode: pool<br>Peers: <br>  UUID                                 NAME  CLIENT       <br>  d22bb245-fb20-4273-b847-c8c5e06b2450 <span class="hljs-built_in">local</span> client.admin<br></code></pre></td></tr></table></figure><p>如果需要删除</p><blockquote><p>rbd mirror pool peer remove {pool-name} {peer-uuid}</p></blockquote><p>执行</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">rbd --cluster <span class="hljs-built_in">local</span> mirror pool peer remove image-pool 55672766-c02b-4729-8567-f13a66893445<br>rbd --cluster remote mirror pool peer remove image-pool 60c0e299-b38f-4234-91f6-eed0a367be08<br></code></pre></td></tr></table></figure><h3 id="验证是否成功"><a href="#验证是否成功" class="headerlink" title="验证是否成功"></a>验证是否成功</h3><p>在lab8106上执行</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ceph]<span class="hljs-comment"># rbd create testrbd --size 40000</span><br>[root@lab8106 ceph]<span class="hljs-comment"># rbd ls</span><br>testrbd<br></code></pre></td></tr></table></figure><p>在lab8107上执行</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">root@lab8107:~/ceph<span class="hljs-comment"># rbd ls</span><br>testrbd<br></code></pre></td></tr></table></figure><p>可以看到镜像已经同步过去了</p><p>在lab8107上执行</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">root@lab8107:~/ceph<span class="hljs-comment"># rbd create testrbd1 --size 40000</span><br>root@lab8107:~/ceph<span class="hljs-comment"># rbd ls</span><br>testrbd<br>testrbd1<br></code></pre></td></tr></table></figure><p>在lab8106上执行</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ceph]<span class="hljs-comment"># rbd ls</span><br>testrbd<br>testrbd1<br></code></pre></td></tr></table></figure><p>双向同步已经可以了</p><p>这里提一下，因为内核态的rbd还没有实现一些属性，所以在map的时候会提示没有这个设备,librbd是可以使用的</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ceph]<span class="hljs-comment"># rbd map testrbd</span><br>rbd: sysfs write failed<br>rbd: map failed: (6) No such device or address<br></code></pre></td></tr></table></figure><h3 id="镜像的升级与降级"><a href="#镜像的升级与降级" class="headerlink" title="镜像的升级与降级"></a>镜像的升级与降级</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ceph]<span class="hljs-comment"># rbd --cluster local mirror image demote rbd/testrbd</span><br>Image demoted to secondary<br>[root@lab8106 ceph]<span class="hljs-comment"># rbd --cluster local mirror image promote rbd/testrbd</span><br><br>[root@lab8106 ceph]<span class="hljs-comment"># rbd --cluster local mirror image promote rbd/testrbd</span><br>rbd: error promoting image to primary<br>2016-03-30 23:35:13.477096 7ffa50a3dc00 -1 librbd: image is already primary<br></code></pre></td></tr></table></figure><p>这里对testrbd做了降级处理和升级处理，那么本地的这个就是只读的，远程的就是可写的</p><p>基本的实现就到这里，更多的实践再根据环境需求做就可以了</p>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>ceph查询rbd的使用容量（快速）</title>
    <link href="/2016/03/24/ceph%E6%9F%A5%E8%AF%A2rbd%E7%9A%84%E4%BD%BF%E7%94%A8%E5%AE%B9%E9%87%8F%EF%BC%88%E5%BF%AB%E9%80%9F%EF%BC%89/"/>
    <url>/2016/03/24/ceph%E6%9F%A5%E8%AF%A2rbd%E7%9A%84%E4%BD%BF%E7%94%A8%E5%AE%B9%E9%87%8F%EF%BC%88%E5%BF%AB%E9%80%9F%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<p>ceph在Infernalis加入了一个功能是查询rbd的块设备的使用的大小，默认是可以查询的，但是无法快速查询，那么我们来看看这个功能是怎么开启的</p><!--break--><h3 id="ceph版本"><a href="#ceph版本" class="headerlink" title="ceph版本"></a>ceph版本</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">root@lab8107:~/ceph<span class="hljs-comment"># ceph -v</span><br>ceph version 9.2.0 (bb2ecea240f3a1d525bcb35670cb07bd1f0ca299)<br></code></pre></td></tr></table></figure><h3 id="创建RBD设备"><a href="#创建RBD设备" class="headerlink" title="创建RBD设备"></a>创建RBD设备</h3><p>我们先来创建一个rbd</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs bash">root@lab8107:~/ceph<span class="hljs-comment"># rbd create test --size 4000</span><br>root@lab8107:~/ceph<span class="hljs-comment"># rbd info test</span><br>rbd image <span class="hljs-string">&#x27;test&#x27;</span>:<br>size 4000 MB <span class="hljs-keyword">in</span> 1000 objects<br>order 22 (4096 kB objects)<br>block_name_prefix: rbd_data.10305695d26a<br>format: 2<br>features: layering<br>flags: <br></code></pre></td></tr></table></figure><h3 id="进行RBD容量使用查询"><a href="#进行RBD容量使用查询" class="headerlink" title="进行RBD容量使用查询"></a>进行RBD容量使用查询</h3><p>我们来试一下rbd du命令</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">root@lab8107:~/ceph<span class="hljs-comment"># rbd du test</span><br>warning: fast-diff map is not enabled <span class="hljs-keyword">for</span> <span class="hljs-built_in">test</span>. operation may be slow.<br>NAME PROVISIONED USED <br><span class="hljs-built_in">test</span>       4000M    0<br></code></pre></td></tr></table></figure><p>可以看到有个提示需要开启fast-diff的属性</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs bash">root@lab8107:~/ceph<span class="hljs-comment"># rbd --help</span><br>···<br>Supported image features:<br>  layering (+), striping (+), exclusive-lock (*), object-map (*), fast-diff (*), deep-flatten<br><br>  (*) supports enabling/disabling on existing images<br>  (+) enabled by default <span class="hljs-keyword">for</span> new images <span class="hljs-keyword">if</span> features are not specified<br></code></pre></td></tr></table></figure><p>可以看到默认开启了  layering striping 属性，后面属性没有开启<br>我们看一下rbd的man page</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs bash">root@lab8107:~/ceph<span class="hljs-comment"># man rbd</span><br>···<br>--image-feature feature-name<br>    Specifies <span class="hljs-built_in">which</span> RBD format 2 feature should be enabled when creating an image. Multiple features can be enabled by repeating  this  option  multiple <span class="hljs-built_in">times</span>. The following features are supported:<br><br>    · layering: layering support<br>    · striping: striping v2 support<br>    · exclusive-lock: exclusive locking support<br>    · object-map: object map support (requires exclusive-lock)<br>    · fast-diff: fast diff calculations (requires object-map)<br>    · deep-flatten: snapshot flatten support<br></code></pre></td></tr></table></figure><h3 id="开启RBD属性"><a href="#开启RBD属性" class="headerlink" title="开启RBD属性"></a>开启RBD属性</h3><p>可以看到开启fast-diff 需要开启<code>exclusive-lock</code>和 <code>object-map</code> 属性<br>那么依次开启就好了</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">root@lab8107:~/ceph<span class="hljs-comment"># rbd  feature enable test  exclusive-lock</span><br>root@lab8107:~/ceph<span class="hljs-comment"># rbd  feature enable test  object-map</span><br>root@lab8107:~/ceph<span class="hljs-comment"># rbd  feature enable test  fast-diff</span><br>2016-03-24 21:17:23.822720 7f241a5447c0 -1 librbd::ObjectMap: error refreshing object map: (2) No such file or directory<br>2016-03-24 21:17:23.823191 7f241a5447c0 -1 librbd::ObjectMap: error refreshing object map: (2) No such file or directory<br></code></pre></td></tr></table></figure><p>来查看下 rbd info</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs bash">root@lab8107:~/ceph<span class="hljs-comment"># rbd info test</span><br>2016-03-24 21:18:37.972235 7f9918a7d7c0 -1 librbd::ObjectMap: error refreshing object map: (2) No such file or directory<br>rbd image <span class="hljs-string">&#x27;test&#x27;</span>:<br>size 4000 MB <span class="hljs-keyword">in</span> 2016-03-24 21:18:37.972900 7f9918a7d7c0 -1 1000librbd::ObjectMap: error refreshing object map: (2) No such file or directory objects<br><br>order 22 (4096 kB objects)<br>block_name_prefix: rbd_data.10305695d26a<br>format: 2<br>features: layering, exclusive-lock, object-map, fast-diff<br>flags: object map invalid, fast diff invalid<br><br></code></pre></td></tr></table></figure><p>我们可以看到又报错了，这个是因为是后开启object map，需要重建一下</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">root@lab8107:~/ceph<span class="hljs-comment"># rbd  object-map rebuild  test  </span><br>2016-03-24 21:20:05.488515 7fa0141917c0 -1 librbd::ObjectMap: error refreshing object map: (2) No such file or directory<br>2016-03-24 21:20:05.489142 7fa0141917c0 -1 librbd::ObjectMap: error refreshing object map: (2) No such file or directory<br>2016-03-24 21:20:05.530344 7fa0141917c0 -1 librbd::ObjectMap: error refreshing object map: (2) No such file or directory<br>Object Map Rebuild: 100% complete...done.<br></code></pre></td></tr></table></figure><p>再次查看下</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs bash">root@lab8107:~/ceph<span class="hljs-comment"># rbd info test</span><br>rbd image <span class="hljs-string">&#x27;test&#x27;</span>:<br>size 4000 MB <span class="hljs-keyword">in</span> 1000 objects<br>order 22 (4096 kB objects)<br>block_name_prefix: rbd_data.10305695d26a<br>format: 2<br>features: layering, exclusive-lock, object-map, fast-diff<br>flags:<br></code></pre></td></tr></table></figure><p>已经可以了，我们来试下这个功能</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">root@lab8107:~/ceph<span class="hljs-comment"># rbd du test</span><br>NAME PROVISIONED USED <br><span class="hljs-built_in">test</span>       4000M    0<br></code></pre></td></tr></table></figure><p>好了，这个功能已经开启了，这个是对已经创建好的rbd，然后开启这个属性，那么如果不想这么麻烦，默认就开启，创建的时候就开启，有什么方法么，当然是有的</p><h3 id="默认开启RBD容量快速查询的方法"><a href="#默认开启RBD容量快速查询的方法" class="headerlink" title="默认开启RBD容量快速查询的方法"></a>默认开启RBD容量快速查询的方法</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">root@lab8107:~/ceph<span class="hljs-comment"># ceph --show-config|grep rbd_default_features</span><br>rbd_default_features = 3<br></code></pre></td></tr></table></figure><p>查看下默认配置，这个是3，那么3是什么意思，3&#x3D;1+2，这个是属性中常用的一种做法，给属性设置一个bit码，在配置的时候，只需要设置加起来的和<br>在RBD的属性里面：<br><img src="/images/blog/o_200901034010%E5%B1%9E%E6%80%A7.png" alt="属性.png-14.1kB"></p><p>我们要开启 前五个属性那么就是 31<br>在ceph.conf中添加配置</p><blockquote><p>rbd_default_features &#x3D; 31</p></blockquote><p>创建后不做任何操作直接查询</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs bash">root@lab8107:~/ceph<span class="hljs-comment"># rbd create test1 --size 1000</span><br>root@lab8107:~/ceph<span class="hljs-comment"># rbd info test1</span><br>rbd image <span class="hljs-string">&#x27;test1&#x27;</span>:<br>size 1000 MB <span class="hljs-keyword">in</span> 250 objects<br>order 22 (4096 kB objects)<br>block_name_prefix: rbd_data.103c29f2280d<br>format: 2<br>features: layering, exclusive-lock, object-map, fast-diff<br>flags: <br>root@lab8107:~/ceph<span class="hljs-comment"># rbd du test1</span><br>NAME  PROVISIONED USED <br>test1       1000M    0<br></code></pre></td></tr></table></figure><p>可以看到默认就把几个属性都开启好了，关于这个属性的开启就记录到这里，之前已经测试了一次</p>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Ceph Bluestore首测</title>
    <link href="/2016/03/24/Ceph%20Bluestore%E9%A6%96%E6%B5%8B/"/>
    <url>/2016/03/24/Ceph%20Bluestore%E9%A6%96%E6%B5%8B/</url>
    
    <content type="html"><![CDATA[<p>Bluestore 作为 Ceph Jewel 版本推出的一个重大的更新，提供了一种之前没有的存储形式，一直以来ceph的存储方式一直是以filestore的方式存储的，也就是对象是以文件方式存储在osd的磁盘上的，pg是以目录的方式存在于osd的磁盘上的<br>在发展过程中，中间出现了kvstore，这个还是存储在文件系统之上，以leveldb或者rocksdb的方式存储对象数据，这个也没有推广开来，性能上没有太大的改观，在某些情况下性能还低于filestore<br>最终在sage的大力支持下，ceph社区准备撸一个新的文件系统，这个系统类似于rocksdb，但是数据是可以直接存储到裸设备上去的，也就是存储对象数据的地方是没有传统意义上的文件系统的，并且解决了一种被抱怨的写双份数据的问题，在filestore中，数据需要先写入journal再入磁盘，对于磁盘来说实际就是双份写了</p><p>在这里不做过多的探讨技术上的细节，bluestore处于开发阶段，在最新的版本的ceph中，发现已经集成了这个，虽然还是实验阶段，但是还是体现出其未来巨大的价值</p><h3 id="环境准备"><a href="#环境准备" class="headerlink" title="环境准备"></a>环境准备</h3><p>由于没有测试大量的设备，就在一个小环境下进行性能的验证，基准的性能不需要大量的机器，至于数据可靠性，就靠自己去判断了</p><h4 id="硬件环境："><a href="#硬件环境：" class="headerlink" title="硬件环境："></a>硬件环境：</h4><p><img src="/images/blog/o_200901032658blue%E7%A1%AC%E4%BB%B6.png"></p><h4 id="软件环境："><a href="#软件环境：" class="headerlink" title="软件环境："></a>软件环境：</h4><p><img src="/images/blog/o_200901032654blue%E8%BD%AF%E4%BB%B6.png"></p><h3 id="一、先测试Filestore"><a href="#一、先测试Filestore" class="headerlink" title="一、先测试Filestore"></a>一、先测试Filestore</h3><p>ceph-disk有个update_partition的bug，部署过程需要处理一下，后期发版本应该会解决</p><h4 id="1、4K随机写200G的rbd测试时间300s"><a href="#1、4K随机写200G的rbd测试时间300s" class="headerlink" title="1、4K随机写200G的rbd测试时间300s"></a>1、4K随机写200G的rbd测试时间300s</h4><p><img src="/images/blog/o_200901032649filestorefio%E8%BF%87%E7%A8%8B.png" alt="filestorefio过程.png-132.1kB"><br>测试的io的抖动的情况</p><p><img src="/images/blog/o_200901032644filestoreresul.png" alt="filestoreresul.png-89.4kB"><br>测试的FIO结果的页面</p><h4 id="2、4M顺序写200G的rbd测试时间300s"><a href="#2、4M顺序写200G的rbd测试时间300s" class="headerlink" title="2、4M顺序写200G的rbd测试时间300s"></a>2、4M顺序写200G的rbd测试时间300s</h4><p><img src="/images/blog/o_200901032638filestorefio%E8%BF%87%E7%A8%8B4M.png" alt="filestorefio过程4M.png-166.9kB"><br>测试的io的抖动的情况<br><img src="/images/blog/o_200901032633filestoreresul4M.png" alt="filestoreresul4M.png-88kB"></p><h3 id="二、测试bluestore"><a href="#二、测试bluestore" class="headerlink" title="二、测试bluestore"></a>二、测试bluestore</h3><h4 id="1、4K随机写200G的rbd测试时间300s-1"><a href="#1、4K随机写200G的rbd测试时间300s-1" class="headerlink" title="1、4K随机写200G的rbd测试时间300s"></a>1、4K随机写200G的rbd测试时间300s</h4><p><img src="/images/blog/o_200901032628blueresult4K.png" alt="blueresult4K.png-99.6kB"><br><img src="/images/blog/o_200901032557bluefinal4K.png" alt="bluefinal4K.png-87.9kB"></p><h4 id="2、4M顺序写200G的rbd测试时间300s-1"><a href="#2、4M顺序写200G的rbd测试时间300s-1" class="headerlink" title="2、4M顺序写200G的rbd测试时间300s"></a>2、4M顺序写200G的rbd测试时间300s</h4><p><img src="/images/blog/o_200901032553blueresult4M.png" alt="blueresult4M.png-133.6kB"><br><img src="/images/blog/o_200901032547blueiostatfinal4M.png" alt="blueiostatfinal4M.png-90.9kB"></p><p>以上为测试过程的数据记录，下面为对比的</p><p><img src="/images/blog/o_200901032539blue%E6%AF%94%E8%BE%83.png"></p><p>整个测试来看改进非常的大，数据的曲线比之前要平滑很多，延时也变得更小，但是还是开发阶段，估计bug还是很多，不可控因素太多，并且暂时还没有修复工具，作为对未来ceph发展的一种期待吧，肯定会越来越好</p>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>让磁盘硬盘灯常闪定位盘</title>
    <link href="/2016/03/24/%E8%AE%A9%E7%A3%81%E7%9B%98%E7%A1%AC%E7%9B%98%E7%81%AF%E5%B8%B8%E9%97%AA%E5%AE%9A%E4%BD%8D%E7%9B%98/"/>
    <url>/2016/03/24/%E8%AE%A9%E7%A3%81%E7%9B%98%E7%A1%AC%E7%9B%98%E7%81%AF%E5%B8%B8%E9%97%AA%E5%AE%9A%E4%BD%8D%E7%9B%98/</url>
    
    <content type="html"><![CDATA[<p>通过dd来读取让硬盘灯闪来定位磁盘的位置</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-meta">#!/bin/bash</span><br>hd=<span class="hljs-variable">$1</span><br><span class="hljs-keyword">for</span> num <span class="hljs-keyword">in</span> &#123;1..5&#125;;<span class="hljs-keyword">do</span><br>        <span class="hljs-built_in">dd</span> <span class="hljs-keyword">if</span>=<span class="hljs-string">&quot;<span class="hljs-variable">$hd</span>&quot;</span> of=<span class="hljs-string">&quot;/dev/null&quot;</span> bs=4M count=1000 iflag=direct conv=noerror &gt;/dev/null 2&gt;&amp;1<br>        <span class="hljs-built_in">sleep</span> 1<br><span class="hljs-keyword">done</span><br></code></pre></td></tr></table></figure><p>写于: 2014年11月07日<br>更新于: 2015年03月24日</p>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>丢了ceph.mon.keying解决办法</title>
    <link href="/2016/03/03/%E4%B8%A2%E4%BA%86ceph.mon.keying%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95/"/>
    <url>/2016/03/03/%E4%B8%A2%E4%BA%86ceph.mon.keying%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<p>在linux操作系统下，可能因为一些很小的误操作，都会造成非常重要的文件的丢失，而文件的备份并不是每时每刻都会注意到，一般是等到文件丢失了才会去想办法，这里讲下ceph.mon.keyring丢失的解决办法</p><h3 id="1、没有启用部署认证的"><a href="#1、没有启用部署认证的" class="headerlink" title="1、没有启用部署认证的"></a>1、没有启用部署认证的</h3><blockquote><p>auth_cluster_required &#x3D;none</p></blockquote><p>在进行部署的时候 ceph-deploy new 以后会生成ceph.mon.keyring文件,内容如下</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">[mon.]<br>key = AQBnutdWAAAAABAAPmLaAd9CeKaCRj1CIrztyA==<br>caps mon = allow *<br></code></pre></td></tr></table></figure><p>这个keyring在增加mon的时候，mon之间加密会用到的，如果在未开启认证的情况下，只需要在部署目录下面创建一个同名文件，里面填入一个格式相同的任意内容即可（需要有这个文件，不然无法部署新的mon)，在没有开启认证的时候是没有生成这个文件的 <code>/var/lib/ceph/mon/ceph-lab8106/keyring</code></p><p>一般是在系统盘损坏的时候容易丢失这个部署目录,并且没有做备份</p><h3 id="2、启用了部署认证"><a href="#2、启用了部署认证" class="headerlink" title="2、启用了部署认证"></a>2、启用了部署认证</h3><blockquote><p>auth_cluster_required &#x3D;cephx</p></blockquote><p>开启了以后，在进行mon的部署以后，是会生成&#x2F;var&#x2F;lib&#x2F;ceph&#x2F;mon&#x2F;ceph-lab8106&#x2F;keyring这个文件的，这个文件的内容跟部署的目录下面的keyring是一样的，所以丢失了部署目录以后，去mon的路径下面查看文件，然后写入一个同名文件里面即可<br>如下所示</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ceph]<span class="hljs-comment"># cat /var/lib/ceph/mon/ceph-lab8106/keyring </span><br>[mon.]<br>key = AQC4V9hWAAAAABAArPTf+Az43NWsoU88okI+Mg==<br>caps mon = <span class="hljs-string">&quot;allow *&quot;</span><br>[root@lab8106 ceph]<span class="hljs-comment"># cat ceph.mon.keyring </span><br>[mon.]<br>key = AQC4V9hWAAAAABAArPTf+Az43NWsoU88okI+Mg==<br>caps mon = allow *<br></code></pre></td></tr></table></figure><p>这个是个简单的操作即可避免无法新加mon的问题，当然做好预备工作是最好的，建议做好下面的两个工作：</p><ul><li>1、直接备份部署目录里面的文件，最好备份到自己的电脑上</li><li>2、将这个keyring的信息备份到集群里面去,记录到认证信息中<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ceph]<span class="hljs-comment"># ceph auth import -i ceph.mon.keyring</span><br>imported keyring<br>[root@lab8106 ceph]<span class="hljs-comment"># ceph auth get mon.</span><br>exported keyring <span class="hljs-keyword">for</span> mon.<br>[mon.]<br>key = AQC4V9hWAAAAABAArPTf+Az43NWsoU88okI+Mg==<br>caps mon = <span class="hljs-string">&quot;allow *&quot;</span><br>[root@lab8106 ceph]<span class="hljs-comment"># cat ceph.mon.keyring </span><br>[mon.]<br>key = AQC4V9hWAAAAABAArPTf+Az43NWsoU88okI+Mg==<br>caps mon = allow *<br></code></pre></td></tr></table></figure></li></ul>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>快速增加osdmap的epoch</title>
    <link href="/2016/03/03/%E5%BF%AB%E9%80%9F%E5%A2%9E%E5%8A%A0osdmap%E7%9A%84epoch/"/>
    <url>/2016/03/03/%E5%BF%AB%E9%80%9F%E5%A2%9E%E5%8A%A0osdmap%E7%9A%84epoch/</url>
    
    <content type="html"><![CDATA[<p>最近因为一个实验需要用到一个功能，需要快速的增加 ceph 的 osdmap 的 epoch 编号</p><h3 id="查询osd的epoch编号"><a href="#查询osd的epoch编号" class="headerlink" title="查询osd的epoch编号"></a>查询osd的epoch编号</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">root@lab8107:~<span class="hljs-comment"># ceph osd stat</span><br>     osdmap e4686: 8 osds: 8 up, 8 <span class="hljs-keyword">in</span><br></code></pre></td></tr></table></figure><p>上面显示的 e4686 即为osdmap的epoch的编号</p><h3 id="增加epoch"><a href="#增加epoch" class="headerlink" title="增加epoch"></a>增加epoch</h3><p>现在需要增加1000</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">ceph osd thrash 1000<br></code></pre></td></tr></table></figure><p>执行完了后</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">ceph osd <span class="hljs-built_in">stat</span><br>osdmap e5687: 8 osds: 3 up, 3 <span class="hljs-keyword">in</span>; 232 remapped pgs<br></code></pre></td></tr></table></figure><p>很快就增加了1000的编号，这个命令执行完了后，osd weight 会变成0，这个做下恢复即可，节点会down，调整下weight，恢复下状态就可以了</p>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>yum安装指定版本ceph包</title>
    <link href="/2016/03/01/yum%E5%AE%89%E8%A3%85%E6%8C%87%E5%AE%9A%E7%89%88%E6%9C%ACceph%E5%8C%85/"/>
    <url>/2016/03/01/yum%E5%AE%89%E8%A3%85%E6%8C%87%E5%AE%9A%E7%89%88%E6%9C%ACceph%E5%8C%85/</url>
    
    <content type="html"><![CDATA[<p>安装ceph包的方式有很多，这里讲的是从官网直接通过yum源的安装方式进行安装</p><p>yum源对应的地址为<br><a href="http://download.ceph.com/rpm-hammer/el6/x86_64/">http://download.ceph.com/rpm-hammer/el6/x86_64/</a></p><p>怎么配置ceph源就不在这里赘述了</p><p>下图为ceph官网的yum源里面的文件列表：</p><p><img src="/images/blog/80906036.jpg"></p><p>可以看到有多个版本的，默认的会安装最新的版本的</p><p>这样就会有个问题：<br>安装了一个老版本的包，需要安装一个附属的包，安装的版本就会是最新版本的，而不是已经安装的版本的附属的包，会引起版本错乱</p><p>解决办法：</p><h3 id="1、查询源里面的包有多少个版本"><a href="#1、查询源里面的包有多少个版本" class="headerlink" title="1、查询源里面的包有多少个版本"></a>1、查询源里面的包有多少个版本</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@zhongbo ~]<span class="hljs-comment"># yum --showduplicates list ceph | expand</span><br>Loaded plugins: security<br>Available Packages<br>ceph.x86_64                         1:0.94-0.el6                            ceph<br>ceph.x86_64                         1:0.94.1-0.el6                          ceph<br>ceph.x86_64                         1:0.94.2-0.el6                          ceph<br>ceph.x86_64                         1:0.94.3-0.el6                          ceph<br>ceph.x86_64                         1:0.94.4-0.el6                          ceph<br>ceph.x86_64                         1:0.94.5-0.el6                          ceph<br></code></pre></td></tr></table></figure><h3 id="2、安装指定版本的包"><a href="#2、安装指定版本的包" class="headerlink" title="2、安装指定版本的包"></a>2、安装指定版本的包</h3><p>假如现在需要安装的是1:0.94.4-0.el6这个版本<br>安装的格式为：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">yum install &lt;package name&gt;-&lt;version info&gt;<br></code></pre></td></tr></table></figure><p>对应到这里</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">yum install ceph-0.94.4-0.el6<br></code></pre></td></tr></table></figure><p>注意名称规则是前面的名称，中间的版本去掉 1: 然后就可以安装指定版本的包了</p>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>nginx状态监控统计</title>
    <link href="/2016/02/22/nginx%E7%8A%B6%E6%80%81%E7%9B%91%E6%8E%A7%E7%BB%9F%E8%AE%A1/"/>
    <url>/2016/02/22/nginx%E7%8A%B6%E6%80%81%E7%9B%91%E6%8E%A7%E7%BB%9F%E8%AE%A1/</url>
    
    <content type="html"><![CDATA[<p>nginx是一款很优秀的web服务器软件，很多地方都有接触和使用到他，大部分的场景压力还没达到需要调优的地步，而调优的难点其实不在于调，而在于各项状态的监控，能够很快的找到资源在什么时候出现问题，调整前后出现的变化，如果都不知道变化在哪里所做的调优只能是凭感觉的</p><p>之前看到有技术人员用nginx作为rgw的前端的时候，通过优化去实现将nginx的并发提高到很大，而不出现4xx等问题，nginx的access.log里面是有记录访问的状态码的，而这个日志的分析如果是一次次的去看，这样的分析是无法用精确的数据去展示的</p><p>最开始的想法是想根据时间点去统计时间点的状态码，后来发现这样做既复杂，又无法输出到一些数据展示软件当中，实际上我只需要统计一定时间的总的状态值，然后定期去取这个值，然后在数据展示的时候，就可以看到一个数值的曲线图，增量即为这个时间区间所产生的状态值</p><p>下面就是我的实现，一个脚本就可以统计了，这个是最初的版本，纯统计状态码，还没有区分读写分离的情况，这个在后面会加入分离的情况</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-meta">#!/bin/sh</span><br><span class="hljs-comment">#</span><br><span class="hljs-comment">### BEGIN INIT INFO</span><br><span class="hljs-comment"># Provides:          nginxstatus</span><br><span class="hljs-comment"># Required-Start:    $nginx</span><br><span class="hljs-comment"># Short-Description: nginxstatus</span><br><span class="hljs-comment"># Description: collectstatus of nginx</span><br><span class="hljs-comment">### END INIT INFO</span><br><span class="hljs-comment">#</span><br><span class="hljs-comment">#</span><br><span class="hljs-comment"># pidfile: /var/run/nginx/nginxstatus.pid</span><br><span class="hljs-comment">#</span><br><span class="hljs-comment"># Source function library.</span><br><span class="hljs-comment">##########################################</span><br><span class="hljs-comment">#状态码一般分为1xx,2xx,3xx,4xx,5xx,total</span><br>statucode=<span class="hljs-string">&quot;2 3 4 5&quot;</span><br><span class="hljs-comment">##check intervel setting </span><br>interval=2<br><span class="hljs-comment">########################################</span><br><span class="hljs-comment">#check the nginxstatus pid dir if exist</span><br><span class="hljs-keyword">if</span> [ ! -d /var/run/nginxstatus/ ];<span class="hljs-keyword">then</span><br><span class="hljs-built_in">mkdir</span>  /var/run/nginxstatus/<br><span class="hljs-keyword">fi</span><br><span class="hljs-comment">##check the status of nginx access </span><br><span class="hljs-function"><span class="hljs-title">check</span></span>()&#123;<br><span class="hljs-keyword">for</span> code <span class="hljs-keyword">in</span> <span class="hljs-variable">$statucode</span><br><span class="hljs-keyword">do</span><br><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;<span class="hljs-variable">$code</span>&quot;</span>xx:`<span class="hljs-built_in">cat</span> /var/log/nginx/access.log |awk <span class="hljs-string">&#x27;&#123;if( substr($9,0,1) == &#x27;</span><span class="hljs-string">&#x27;&#x27;</span><span class="hljs-variable">$code</span><span class="hljs-string">&#x27;&#x27;</span><span class="hljs-string">&#x27; )  print $9&#125;&#x27;</span>  |<span class="hljs-built_in">wc</span> -l` &gt; /var/log/nginx/<span class="hljs-string">&quot;<span class="hljs-variable">$code</span>&quot;</span>xx.log<br><span class="hljs-keyword">done</span><br><span class="hljs-built_in">sleep</span> <span class="hljs-variable">$interval</span><br>&#125;<br><span class="hljs-comment">#start nginx status</span><br><span class="hljs-function"><span class="hljs-title">start</span></span>() &#123;<br><span class="hljs-built_in">echo</span> -e Starting nginxstatus:                              <span class="hljs-string">&quot;\033[32m [  OK  ] \033[0m&quot;</span><br><span class="hljs-keyword">while</span> [ 2 &gt; 1 ]<br><span class="hljs-keyword">do</span><br>check<br><br><span class="hljs-keyword">done</span> &amp;<br>pid=`ps ax | grep -i <span class="hljs-string">&#x27;nginxstatus&#x27;</span> | <span class="hljs-built_in">head</span> -n 1|awk <span class="hljs-string">&#x27;&#123;print $1&#125;&#x27;</span>`<br><span class="hljs-built_in">echo</span> -e  pid is  <span class="hljs-string">&quot;\033[33m  $! \033[0m&quot;</span> <br><span class="hljs-built_in">echo</span> $! &gt;&gt; /var/run/nginxstatus/nginxstatus.pid<br>&#125;<br><span class="hljs-comment">#stop nginx </span><br><span class="hljs-function"><span class="hljs-title">stop</span></span>() &#123;<br><span class="hljs-built_in">echo</span> -e stop nginxstatus collect  <span class="hljs-string">&quot;\033[32m  [  OK  ] \033[0m&quot;</span> <br><br>pid=`<span class="hljs-built_in">cat</span> /var/run/nginxstatus/nginxstatus.pid  2&gt;/dev/null`<br><span class="hljs-built_in">kill</span> -10 <span class="hljs-variable">$pid</span>  2&gt;/dev/null<br><span class="hljs-comment">#killall nginxstatus</span><br><span class="hljs-built_in">rm</span> -rf /var/run/nginxstatus/nginxstatus.pid<br>&#125;<br><br><br><span class="hljs-function"><span class="hljs-title">status</span></span>() &#123;<br><span class="hljs-keyword">for</span> code <span class="hljs-keyword">in</span> <span class="hljs-variable">$statucode</span><br><span class="hljs-keyword">do</span><br><span class="hljs-built_in">cat</span> /var/log/nginx/<span class="hljs-string">&quot;<span class="hljs-variable">$code</span>&quot;</span>xx.log<br><span class="hljs-keyword">done</span><br>&#125;<br><br><span class="hljs-function"><span class="hljs-title">clean</span></span> () &#123;<br><span class="hljs-keyword">for</span> code <span class="hljs-keyword">in</span> <span class="hljs-variable">$statucode</span><br><span class="hljs-keyword">do</span><br><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;&quot;</span> &gt;  /var/log/nginx/<span class="hljs-string">&quot;<span class="hljs-variable">$code</span>&quot;</span>xx.log<br><span class="hljs-keyword">done</span><br><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;&quot;</span> &gt; /var/log/nginx/access.log<br><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;clean /var/log/nginx/access.log&quot;</span><br><span class="hljs-built_in">echo</span> -e <span class="hljs-string">&quot;clean /var/log/nginx/access.log&quot;</span> <span class="hljs-string">&quot;\033[32m  [  OK  ] \033[0m&quot;</span><br>&#125;<br><br><span class="hljs-keyword">case</span> <span class="hljs-string">&quot;<span class="hljs-variable">$1</span>&quot;</span> <span class="hljs-keyword">in</span><br>        start)<br>                start  &amp;&amp; <span class="hljs-built_in">exit</span> 0<br>                ;;<br>        stop)<br>                stop || <span class="hljs-built_in">exit</span> 0<br>                ;;<br>        status)<br>                status<br>                ;;<br>        clean)<br>                clean<br>                ;;<br>        *)<br>                <span class="hljs-built_in">echo</span> $<span class="hljs-string">&quot;Usage: <span class="hljs-variable">$0</span> &#123;start|stop|status|clean&#125;&quot;</span><br>                <span class="hljs-built_in">exit</span> 2<br><span class="hljs-keyword">esac</span><br><span class="hljs-built_in">exit</span> $?<br></code></pre></td></tr></table></figure><p>使用方法：</p><h3 id="1、启动进程"><a href="#1、启动进程" class="headerlink" title="1、启动进程"></a>1、启动进程</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@zhongbo ~]<span class="hljs-comment"># /etc/init.d/nginxstatus start</span><br>Starting nginxstatus:  [  OK  ] <br>pid is   166534<br></code></pre></td></tr></table></figure><p>会生成下面的状态文件，周期为2s一更新<br>[root@zhongbo ~]# ll &#x2F;var&#x2F;log&#x2F;nginx&#x2F;*xx.log<br>-rw-r–r– 1 root root 7 Feb 23 00:25 &#x2F;var&#x2F;log&#x2F;nginx&#x2F;2xx.log<br>-rw-r–r– 1 root root 6 Feb 23 00:25 &#x2F;var&#x2F;log&#x2F;nginx&#x2F;3xx.log<br>-rw-r–r– 1 root root 7 Feb 23 00:25 &#x2F;var&#x2F;log&#x2F;nginx&#x2F;4xx.log<br>-rw-r–r– 1 root root 6 Feb 23 00:25 &#x2F;var&#x2F;log&#x2F;nginx&#x2F;5xx.log</p><h3 id="2、当前nginx的状态查询"><a href="#2、当前nginx的状态查询" class="headerlink" title="2、当前nginx的状态查询"></a>2、当前nginx的状态查询</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@zhongbo ~]<span class="hljs-comment"># /etc/init.d/nginxstatus status</span><br>2xx:21<br>3xx:1<br>4xx:10<br>5xx:0<br></code></pre></td></tr></table></figure><h3 id="3、停止nginxstatus进程"><a href="#3、停止nginxstatus进程" class="headerlink" title="3、停止nginxstatus进程"></a>3、停止nginxstatus进程</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@zhongbo ~]<span class="hljs-comment"># /etc/init.d/nginxstatus stop</span><br>stop nginxstatus collect   [  OK  ]<br></code></pre></td></tr></table></figure><h3 id="4、清理历史数据"><a href="#4、清理历史数据" class="headerlink" title="4、清理历史数据"></a>4、清理历史数据</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@zhongbo ~]<span class="hljs-comment"># /etc/init.d/nginxstatus clean</span><br>clean /var/log/nginx/access.log   [  OK  ]<br></code></pre></td></tr></table></figure><p>这个操作会清空&#x2F;var&#x2F;log&#x2F;nginx&#x2F;access.log日志的内容重新统计</p><p>这个会在后期根据需求进行优化</p>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>diamond收集插件的自定义</title>
    <link href="/2016/02/21/diamond%E6%94%B6%E9%9B%86%E6%8F%92%E4%BB%B6%E7%9A%84%E8%87%AA%E5%AE%9A%E4%B9%89/"/>
    <url>/2016/02/21/diamond%E6%94%B6%E9%9B%86%E6%8F%92%E4%BB%B6%E7%9A%84%E8%87%AA%E5%AE%9A%E4%B9%89/</url>
    
    <content type="html"><![CDATA[<p>diamond是与graphite配合使用的一个数据收集的软件，关于这个配置的资料很多，使用起来也比较简单，详细的安装和配置会在后面的关于整套监控系统的文章里面写到，本篇是专门讲解怎么自定义这个数据收集的插件</p><p>diamond的结构比较简单:</p><ul><li>Collector 数据采集的模块</li><li>handlers 数据发送的模块</li></ul><p>这里主要讲解的是Collector部分的插件的编写，diamond自身带了非常丰富的插件，可以很方便的使用自带的插件进行监控，包括ceph和cephstats这两个可以用来监控ceph的插件，弄清楚怎么去写插件会方便很多，并且能扩展原来插件所没有的数据,calamari里面的数据的收集就是通过的diamond的</p><p>本例将讲解怎么写一个监控ceph的健康状态的插件</p><h3 id="1、diamond软件的安装"><a href="#1、diamond软件的安装" class="headerlink" title="1、diamond软件的安装"></a>1、diamond软件的安装</h3><p>通过github上下载代码然后安装在服务器上即可</p><h3 id="2、收集数据的py的编写"><a href="#2、收集数据的py的编写" class="headerlink" title="2、收集数据的py的编写"></a>2、收集数据的py的编写</h3><p>收集数据的collect的路径：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">/usr/share/diamond/collectors/<br></code></pre></td></tr></table></figure><h4 id="2-1-创建一个目录"><a href="#2-1-创建一个目录" class="headerlink" title="2.1 创建一个目录"></a>2.1 创建一个目录</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">mkdir</span> /usr/share/diamond/collectors/cephhealth/<br></code></pre></td></tr></table></figure><h4 id="2-2-创建采集的py文件"><a href="#2-2-创建采集的py文件" class="headerlink" title="2.2 创建采集的py文件"></a>2.2 创建采集的py文件</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">vim /usr/share/diamond/collectors/cephhealth/cephhealth.py<br></code></pre></td></tr></table></figure><p>添加下面的内容：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># coding=utf-8</span><br><span class="hljs-string">&quot;&quot;</span><span class="hljs-string">&quot;</span><br><span class="hljs-string">本插件用于采集ceph的健康状态</span><br><span class="hljs-string">&quot;</span><span class="hljs-string">&quot;&quot;</span><br>import diamond.collector<br>import json<br>import os<br>class cephhealthCollector(diamond.collector.Collector):<br><br>    def get_default_config_help(self):<br>        config_help = super(cephhealthCollector, self).get_default_config_help()<br>        config_help.update(&#123;<br>        &#125;)<br>        <span class="hljs-built_in">return</span> config_help<br><br>    def get_default_config(self):<br>        <span class="hljs-string">&quot;&quot;</span><span class="hljs-string">&quot;</span><br><span class="hljs-string">        Returns the default collector settings</span><br><span class="hljs-string">        &quot;</span><span class="hljs-string">&quot;&quot;</span><br>        config = super(cephhealth, self).get_default_config()<br>        config.update(&#123;<br>            <span class="hljs-string">&#x27;path&#x27;</span>:     <span class="hljs-string">&#x27;ceph&#x27;</span><br>        &#125;)<br>        <span class="hljs-built_in">return</span> config<br><br>    def collect(self):<br>        <span class="hljs-string">&quot;&quot;</span><span class="hljs-string">&quot;</span><br><span class="hljs-string">        Overrides the Collector.collect method</span><br><span class="hljs-string">        &quot;</span><span class="hljs-string">&quot;&quot;</span><br><br>        <span class="hljs-comment"># Set Metric Name</span><br>        metric_name = <span class="hljs-string">&quot;my.cephhealth.metric&quot;</span><br>        data = os.popen(<span class="hljs-string">&#x27;ceph health -f json&#x27;</span>).<span class="hljs-built_in">read</span>()<br>        ddata = json.loads(data)<br>        status = ddata[<span class="hljs-string">&#x27;overall_status&#x27;</span>]<br>        <span class="hljs-keyword">if</span> status == <span class="hljs-string">&#x27;HEALTH_ERR&#x27;</span>:<br>                statuscode = 10<br>        <span class="hljs-keyword">else</span>:<br>                statuscode = 0<br><br>        <span class="hljs-comment"># Set Metric Value</span><br>        metric_value = statuscode<br><br>        <span class="hljs-comment"># Publish Metric</span><br>        self.publish(metric_name, metric_value)<br></code></pre></td></tr></table></figure><p>以上插件注意：<br>cephhealthCollector 为这个插件的名称，也是diamond.conf里面设置的时候设置的值<br>‘path’:     ‘ceph’ 这个是在最后输出结果中会显示这个名称<br>其他部分就是注意输出一个名称 metric_name，和最后的 metric_value 即可<br>中间的部分可以自己去用python去获取数值即可</p><h4 id="3、修改diamond配置文件"><a href="#3、修改diamond配置文件" class="headerlink" title="3、修改diamond配置文件"></a>3、修改diamond配置文件</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">vim /etc/diamond/diamond.conf<br></code></pre></td></tr></table></figure><p>在[collectors]下面插件配置的区域添加</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[[cephhealthCollector]]<br>enabled = <span class="hljs-literal">true</span><br></code></pre></td></tr></table></figure><h4 id="4、重启diamond进程"><a href="#4、重启diamond进程" class="headerlink" title="4、重启diamond进程"></a>4、重启diamond进程</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">/etc/init.d/diamond restart<br></code></pre></td></tr></table></figure><h4 id="5、检查输出的数值"><a href="#5、检查输出的数值" class="headerlink" title="5、检查输出的数值"></a>5、检查输出的数值</h4><p>可以把其他插件全部关闭，然后查看文件<br>&#x2F;var&#x2F;log&#x2F;diamond&#x2F;archive.log的内容</p><p>上面的插件的输出为：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">servers.grafana.ceph.my.cephhealth.metric 10 1456057146<br></code></pre></td></tr></table></figure><p>这个结果的格式为:</p><ul><li>path_prefix &#x3D; servers    （diamond.conf中配置）</li><li>hostname &#x3D; grafana        （diamond.conf中配置）</li><li>get_default_config(self): path &#x3D; ceph   (插件py中配置)</li><li>metric_name &#x3D; “my.cephhealth.metric” （插件py中配置）</li></ul><p>到这来插件就完成了，写起来还是比较简单方便的，上面的地方因为ceph里面的输出的是字符串，而grafana里面的显示状态的地方使用的是数字的，所以在这里可以通过字符串转数字，然后在web界面上使用数字字符的匹配来显示这个状态</p><p>diamond的性能是很牛的，1000 台服务器每分钟总共300万个监控数值的压力也能扛下来，对于我们普通级别的使用时绰绰有余的，关于这个规模的有人写了文章，优化的时候可以参考（<a href="https://answers.launchpad.net/graphite/+question/178969" title="1000台监控">1000台监控</a>）</p>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>ubuntu安装软件自动交互</title>
    <link href="/2016/01/27/ubuntu%E5%AE%89%E8%A3%85%E8%BD%AF%E4%BB%B6%E8%87%AA%E5%8A%A8%E4%BA%A4%E4%BA%92/"/>
    <url>/2016/01/27/ubuntu%E5%AE%89%E8%A3%85%E8%BD%AF%E4%BB%B6%E8%87%AA%E5%8A%A8%E4%BA%A4%E4%BA%92/</url>
    
    <content type="html"><![CDATA[<p>在ubuntu下安装软件过程中可能会出现需要你输入密码或者其他的一些交互类的操作，这样在脚本安装的时候就可能出现阻断，这个在ubuntu里面已经考虑到了这个情况，以前我在安装这个的时候，通过的是脚本传递参数的方式，这里介绍的是原生的控制方式，这个方式更好</p><p>以安装mariadb-server-5.5为例</p><h3 id="1、查询需要应答的问题"><a href="#1、查询需要应答的问题" class="headerlink" title="1、查询需要应答的问题"></a>1、查询需要应答的问题</h3><p>首先通过命令查询这个软件需要问答什么问题</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs bash">root@mytest:/var/cache/apt/archives<span class="hljs-comment"># debconf-show mariadb-server-5.5 </span><br>* mysql-server/root_password: (password omitted)<br>* mysql-server/root_password_again: (password omitted)<br>  mysql-server/password_mismatch:<br>  mysql-server/error_setting_password:<br>* mariadb-server/oneway_migration: <span class="hljs-literal">true</span><br>  mysql-server-5.5/nis_warning:<br>  mysql-server-5.5/postrm_remove_databases: <span class="hljs-literal">false</span><br>  mariadb-server-5.5/really_downgrade: <span class="hljs-literal">false</span><br>  mysql-server/no_upgrade_when_using_ndb:<br></code></pre></td></tr></table></figure><p>这里可以看到需要处理的是密码的问题</p><h3 id="2、执行命令传递参数进去"><a href="#2、执行命令传递参数进去" class="headerlink" title="2、执行命令传递参数进去"></a>2、执行命令传递参数进去</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">debconf-set-selections &lt;&lt;&lt; <span class="hljs-string">&#x27;mariadb-server-5.5 mysql-server/root_password password 123456&#x27;</span><br>debconf-set-selections &lt;&lt;&lt; <span class="hljs-string">&#x27;mariadb-server-5.5 mysql-server/root_password_again password 123456&#x27;</span><br></code></pre></td></tr></table></figure><p>这个地方实际是把这个值记录到了这个地方,如果要修改可以覆盖或者删除即可</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs bash">root@mytest:/var/cache/apt/archives<span class="hljs-comment"># cat /var/cache/debconf/passwords.dat</span><br>Name: mysql-server/root_password<br>Template: mysql-server/root_password<br>Value: 123456<br>Owners: mariadb-server-5.5<br>Flags: seen<br><br>Name: mysql-server/root_password_again<br>Template: mysql-server/root_password_again<br>Value: 123456<br>Owners: mariadb-server-5.5<br>Flags: seen<br></code></pre></td></tr></table></figure><p>然后执行安装的操作：<br>修改环境变量（这一步不做也没看到有问题）</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">export</span> DEBIAN_FRONTEND=noninteractive<br></code></pre></td></tr></table></figure><h3 id="3、安装相应的包"><a href="#3、安装相应的包" class="headerlink" title="3、安装相应的包"></a>3、安装相应的包</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">dpkg -i mariadb-server-5.5_5.5.46-1ubuntu0.14.04.2_amd64.deb<br></code></pre></td></tr></table></figure><p>以上即为ubuntu下的deb包的自动应答的处理</p>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>mon磁盘满重启的问题</title>
    <link href="/2016/01/25/mon%E7%A3%81%E7%9B%98%E6%BB%A1%E9%87%8D%E5%90%AF%E7%9A%84%E9%97%AE%E9%A2%98/"/>
    <url>/2016/01/25/mon%E7%A3%81%E7%9B%98%E6%BB%A1%E9%87%8D%E5%90%AF%E7%9A%84%E9%97%AE%E9%A2%98/</url>
    
    <content type="html"><![CDATA[<h4 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h4><p>Ceph monitors 100% full filesystem, refusing start</p><h5 id="问题原文"><a href="#问题原文" class="headerlink" title="问题原文"></a>问题原文</h5><p>I have an issue with a (not in production!) Ceph cluster which I’m<br>trying to resolve.</p><h5 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h5><p>这是作者在使用多个mon的时候，数据出现了磁盘满的情况，然后重启mon进行压缩的时候，发现这个到了mon的最小空间阀值无法启动，然后就无法压缩，这个问题，还是因为对硬件的不重视，对软件的要求不清楚造成的</p><h5 id="解决办法"><a href="#解决办法" class="headerlink" title="解决办法"></a>解决办法</h5><p>mon的磁盘空间加大，这个在PB级别的集群中更需要重视这个问题，特别是在集群频繁的读写，或者pg变化比较多，osd变化比较多的情况下，这个数据量将是很大的，因为里面是用了leveldb的数据库，并且多个mon之间是需要同步的数据的，然后各自再做compact的操作，所以建议如下：</p><ul><li>1、mon的数据分区需要是ssd的，加快数据的读写速度</li><li>2、mon的数据分区要100G以上，建议是150G，mon数据大概在80G左右后不会再大量的增长</li><li>3、在mon的参数中加入启动压缩的参数 mon_compact_on_start &#x3D; false 和 mon_compact_on_bootstrap &#x3D; false</li><li>4、尽量不要做在线的compact，这个是一个锁死的过程，此时mon会停止响应，可以采取重启的方式</li><li>5、mon的分区中dd 一个4G 左右的大文件，防止真的出现写满的情况下，再去重启进程的时候，好有空间可以释放<br>如果能按上面的几个操作去配置集群，关于mon的磁盘满的问题基本可以避免或者解决</li></ul><h5 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h5><p>关键的地方不要省配置，准备的越多，出问题的概率越小</p>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>ceph-deploy 部署加密osd异常的问题</title>
    <link href="/2016/01/25/ceph-deploy%20%E9%83%A8%E7%BD%B2%E5%8A%A0%E5%AF%86osd%E5%BC%82%E5%B8%B8%E7%9A%84%E9%97%AE%E9%A2%98/"/>
    <url>/2016/01/25/ceph-deploy%20%E9%83%A8%E7%BD%B2%E5%8A%A0%E5%AF%86osd%E5%BC%82%E5%B8%B8%E7%9A%84%E9%97%AE%E9%A2%98/</url>
    
    <content type="html"><![CDATA[<h3 id="问题解析"><a href="#问题解析" class="headerlink" title="问题解析"></a>问题解析</h3><h4 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h4><p>journal encryption with dmcrypt （Reno Rainz）</p><h5 id="问题原文："><a href="#问题原文：" class="headerlink" title="问题原文："></a>问题原文：</h5><p>I’m trying to setup a cluster with encryption on osd data and journal.<br>To do  that I use ceph-deploy with this 2 options –dmcrypt<br>–dmcrypt-key-dir on &#x2F;dev&#x2F;sdc disk.<br>……</p><h5 id="分析："><a href="#分析：" class="headerlink" title="分析："></a>分析：</h5><p>问题的提出者试图在部署osd的时候使用 encryption 对 osd 进行加密,在用 ceph-deploy 的时候，部署的时候出现了失败</p><h5 id="总结："><a href="#总结：" class="headerlink" title="总结："></a>总结：</h5><p>这个地方是因为 ceph-deploy 在进行 activate 操作的时候，把这个加密分区当做了 crypto_LUKS 分区格式进行了 mount 操作，这个肯定是不能成功的，因为这个加密盘是需要进行映射操作的，这里缺少了这个操作，不清楚是需要加其他的参数还是怎样，这个地方可以通过其他方式进行处理</p><p>在进行 ceph-deploy osd prepare 操作的时候，可以查看看到有一行这个，这个中间的 f6244401-c848-42d1-9096-9a3ee5a136e9 即为 osd 的 fsid</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">Running <span class="hljs-built_in">command</span>: /usr/sbin/cryptsetup --batch-mode --key-file /root/keydir/f6244401-c848-42d1-9096-9a3ee5a136e9.luks.key luksFormat /dev/sdd1<br></code></pre></td></tr></table></figure><p>等待osd prepare 操作做完了以后，就进行下面的操作</p><p>1、进行磁盘的映射</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">cryptsetup --key-file /root/keydir/f6244401-c848-42d1-9096-9a3ee5a136e9.luks.key luksOpen /dev/sdd1 f6244401-c848-42d1-9096-9a3ee5a136e9<br></code></pre></td></tr></table></figure><p>2、进行osd的激活</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">ceph-deploy osd activate /dev/mapper/f6244401-c848-42d1-9096-9a3ee5a136e9<br></code></pre></td></tr></table></figure><p>这样就可以了</p><p>另外：<br>取消映射的操作是</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">cryptsetup remove f6244401-c848-42d1-9096-9a3ee5a136e9<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>CephFS用户认证格式写错的问题</title>
    <link href="/2016/01/25/CephFS%E7%94%A8%E6%88%B7%E8%AE%A4%E8%AF%81%E6%A0%BC%E5%BC%8F%E5%86%99%E9%94%99%E7%9A%84%E9%97%AE%E9%A2%98/"/>
    <url>/2016/01/25/CephFS%E7%94%A8%E6%88%B7%E8%AE%A4%E8%AF%81%E6%A0%BC%E5%BC%8F%E5%86%99%E9%94%99%E7%9A%84%E9%97%AE%E9%A2%98/</url>
    
    <content type="html"><![CDATA[<h4 id="问题三："><a href="#问题三：" class="headerlink" title="问题三："></a>问题三：</h4><p>CephFS（James Gallagher）</p><h5 id="问题原文"><a href="#问题原文" class="headerlink" title="问题原文"></a>问题原文</h5><p>Hi,<br>I’m looking to implement the CephFS on my Firefly release (v0.80) with an XFS native file system, but so far I’m having some difficulties. After following the ceph&#x2F;qsg and creating a storage cluster, I have the following topology<br>……</p><h5 id="分析："><a href="#分析：" class="headerlink" title="分析："></a>分析：</h5><p>问题提出者在配置了 auth 后，在客户端进行cephfs 挂载的时候，报了文件系统的错误，这个原因是问题提出者没有弄清楚 auth 的格式，而用了主机名去替代了用户名称</p><p>这个地方是在server端去创建用户</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">ceph auth get-or-create client.zp mon <span class="hljs-string">&#x27;allow *&#x27;</span> mds <span class="hljs-string">&#x27;allow *&#x27;</span> osd <span class="hljs-string">&#x27;allow *&#x27;</span><br></code></pre></td></tr></table></figure><p>找到认证的key</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">ceph auth list <br></code></pre></td></tr></table></figure><p>然后在客户端挂载的格式为</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">mount -t ceph 192.168.8.106:6789:/ /mnt -o name=zp,secret=&#123;secretkey&#125;<br></code></pre></td></tr></table></figure><p>这样就完成了认证模式下的挂载</p><h5 id="总结"><a href="#总结" class="headerlink" title="总结:"></a>总结:</h5><p>在ceph的一些操作中，需要弄清楚里面的操作的格式问题，不然会出现一些很奇怪的问题</p>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>ceph-fuse卡顿无法写入的问题</title>
    <link href="/2016/01/25/ceph-fuse%E5%8D%A1%E9%A1%BF%E6%97%A0%E6%B3%95%E5%86%99%E5%85%A5%E7%9A%84%E9%97%AE%E9%A2%98/"/>
    <url>/2016/01/25/ceph-fuse%E5%8D%A1%E9%A1%BF%E6%97%A0%E6%B3%95%E5%86%99%E5%85%A5%E7%9A%84%E9%97%AE%E9%A2%98/</url>
    
    <content type="html"><![CDATA[<h4 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h4><p>ceph fuse closing stale session while still   operable （Oliver Dzombic）</p><h5 id="问题原文："><a href="#问题原文：" class="headerlink" title="问题原文："></a>问题原文：</h5><p>Hi,<br>i am testing on centos 6 x64 minimal install.<br>i am mounting successfully:<br>ceph-fuse -m 10.0.0.1:6789,10.0.0.2:6789,10.0.0.3:6789,10.0.0.4:6789<br>&#x2F;ceph-storage&#x2F;<br>……</p><h5 id="分析："><a href="#分析：" class="headerlink" title="分析："></a>分析：</h5><p>问题的提出者在使用ceph-fuse去挂载集群的时候，写入一个大文件的时候出现无法写入的问题，在mds的日志当中可以看到<br>closing stale session client.21176728 10.0.0.91:0&#x2F;1635 after 301.302291 的日志信息</p><p>从日志检查过程看<br>ceph -s 出现了  62 requests are blocked &gt; 32 sec<br>问题提出者在认证的时候，出现了语法错误 ceph auth list showed 可以检查，后经修改，还是问题一样</p><p>查看客户端的请求：<br>ceph daemon &#x2F;var&#x2F;run&#x2F;ceph&#x2F;ceph-client.admin.asok objecter_requests</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs bash">&#123;<br>    <span class="hljs-string">&quot;ops&quot;</span>: [<br>        &#123;<br>            <span class="hljs-string">&quot;tid&quot;</span>: 12,<br>            <span class="hljs-string">&quot;pg&quot;</span>: <span class="hljs-string">&quot;6.7230bd94&quot;</span>,<br>            <span class="hljs-string">&quot;osd&quot;</span>: 1,<br>            <span class="hljs-string">&quot;object_id&quot;</span>: <span class="hljs-string">&quot;10000000017.00000004&quot;</span>,<br>            <span class="hljs-string">&quot;object_locator&quot;</span>: <span class="hljs-string">&quot;@6&quot;</span>,<br>            <span class="hljs-string">&quot;target_object_id&quot;</span>: <span class="hljs-string">&quot;10000000017.00000004&quot;</span>,<br>            <span class="hljs-string">&quot;target_object_locator&quot;</span>: <span class="hljs-string">&quot;@6&quot;</span>,<br>            <span class="hljs-string">&quot;paused&quot;</span>: 0,<br>            <span class="hljs-string">&quot;used_replica&quot;</span>: 0,<br>            <span class="hljs-string">&quot;precalc_pgid&quot;</span>: 0,<br>            <span class="hljs-string">&quot;last_sent&quot;</span>: <span class="hljs-string">&quot;2016-01-22 23:11:28.788800&quot;</span>,<br>            <span class="hljs-string">&quot;attempts&quot;</span>: 95,<br>            <span class="hljs-string">&quot;snapid&quot;</span>: <span class="hljs-string">&quot;head&quot;</span>,<br>            <span class="hljs-string">&quot;snap_context&quot;</span>: <span class="hljs-string">&quot;1=[]&quot;</span>,<br>            <span class="hljs-string">&quot;mtime&quot;</span>: <span class="hljs-string">&quot;2016-01-21 23:41:18.001327&quot;</span>,<br>            <span class="hljs-string">&quot;osd_ops&quot;</span>: [<br>                <span class="hljs-string">&quot;write 0~4194304 [5@0]&quot;</span><br>            ]<br></code></pre></td></tr></table></figure><p>其中</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-string">&quot;attempts&quot;</span>: 95,<br></code></pre></td></tr></table></figure><p>这个可以说明请求了95次还没有回应，从这个分析，应该是问题提出者的环境中的某个osd出现了问题，阻塞了请求</p><h5 id="总结："><a href="#总结：" class="headerlink" title="总结："></a>总结：</h5><p>在无法写入的时候，可以查看下客户端的sock去查看哪个请求被阻塞了，然后去排查对应的osd即可</p>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>ceph使用memdisk做journal</title>
    <link href="/2016/01/14/ceph%E4%BD%BF%E7%94%A8memdisk%E5%81%9Ajournal/"/>
    <url>/2016/01/14/ceph%E4%BD%BF%E7%94%A8memdisk%E5%81%9Ajournal/</url>
    
    <content type="html"><![CDATA[<p>记得在很久很久以前，ceph当时的版本是有提供使用内存做journal的配置的，当时是使用的tmpfs，但是现在的版本在搜资料的时候，发现关于这个的没怎么找到资料，邮件列表里面有人有提到怎么做，看了下大致的原理，然后还是自己来实践一次</p><h3 id="预备知识："><a href="#预备知识：" class="headerlink" title="预备知识："></a>预备知识：</h3><p>首先需要知道的是什么是内存盘，内存盘就是划分了一个内存空间来当磁盘使用来进行加速的，这个在某些操作系统里面会把&#x2F;tmp&#x2F;分区挂载到tmpfs下，来达到加速的目的，这样就是重启后，会清空&#x2F;tmp的内容，centos7 默认的分区方式也使用了tmpfs来加速，df -h可以看下那个tmpfs就是内存盘了</p><p>本文使用的不是tmpfs，这个是因为tmpfs不是我们常见意义上的那种文件系统，它不能格式化，ceph 在进行日志创建的时候会去检查journal 所在分区的 uuid， 而tmpfs在检测的时候 会返回一个全0的字符串，这个在校验的时候显示的无效的，所以也就部署起来有问题，下面开始介绍我的做法，这个里面做法很多，步骤也可以自己去变化，这里只是提供了我的一种思路</p><p>我使用的是ramdisk，关于怎么做ramdisk这个也研究了一下，因为篇幅有点长并且属于预备步骤，请参考我的另外一篇文章：</p><p><a href="/2016/01/14/centos7%E4%B8%8B%E5%81%9A%E5%86%85%E5%AD%98%E7%9B%98%E7%9A%84%E6%96%B9%E6%B3%95/" title="centos7下做内存盘的方法">centos7下做内存盘的方法</a></p><h3 id="测试环境："><a href="#测试环境：" class="headerlink" title="测试环境："></a>测试环境：</h3><p>单机，四块SAS的OSD，日志为5G（内存盘大小为6G），副本 2， osd分组</p><p>说明：因为这里只去研究这个内存盘journal的实现，以及性能的差别，其他的组合方案需要自己去配置，所以单机的环境已经可以完成这个</p><h3 id="1、准备journal的内存盘"><a href="#1、准备journal的内存盘" class="headerlink" title="1、准备journal的内存盘"></a>1、准备journal的内存盘</h3><h4 id="检查内存盘大小"><a href="#检查内存盘大小" class="headerlink" title="检查内存盘大小"></a>检查内存盘大小</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ceph]<span class="hljs-comment"># fdisk -l /dev/ram0</span><br><br>Disk /dev/ram0: 6797 MB, 6797721600 bytes, 13276800 sectors<br>Units = sectors of 1 * 512 = 512 bytes<br>Sector size (logical/physical): 512 bytes / 512 bytes<br>I/O size (minimum/optimal): 512 bytes / 512 bytes<br></code></pre></td></tr></table></figure><p>我的大小为6G</p><h4 id="格式化内存盘，并且挂载"><a href="#格式化内存盘，并且挂载" class="headerlink" title="格式化内存盘，并且挂载"></a>格式化内存盘，并且挂载</h4><h5 id="创建挂载目录（有多少osd建几个）"><a href="#创建挂载目录（有多少osd建几个）" class="headerlink" title="创建挂载目录（有多少osd建几个）"></a>创建挂载目录（有多少osd建几个）</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ceph]<span class="hljs-comment"># mkdir -p /var/lib/ceph/mem/ceph-0</span><br></code></pre></td></tr></table></figure><h5 id="格式化memdisk-需要几个格式化几个"><a href="#格式化memdisk-需要几个格式化几个" class="headerlink" title="格式化memdisk(需要几个格式化几个)"></a>格式化memdisk(需要几个格式化几个)</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ceph]<span class="hljs-comment"># mkfs.xfs /dev/ram0  -f</span><br></code></pre></td></tr></table></figure><h5 id="挂载内存盘"><a href="#挂载内存盘" class="headerlink" title="挂载内存盘"></a>挂载内存盘</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ceph]<span class="hljs-comment"># mount /dev/ram0 /var/lib/ceph/mem/ceph-0/</span><br></code></pre></td></tr></table></figure><h5 id="挂载完了后的效果如下："><a href="#挂载完了后的效果如下：" class="headerlink" title="挂载完了后的效果如下："></a>挂载完了后的效果如下：</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ceph]<span class="hljs-comment"># df -h</span><br>Filesystem      Size  Used Avail Use% Mounted on<br>/dev/sda2        50G  9.7G   41G  20% /<br>devtmpfs         24G     0   24G   0% /dev<br>tmpfs            24G     0   24G   0% /dev/shm<br>tmpfs            24G   17M   24G   1% /run<br>tmpfs            24G     0   24G   0% /sys/fs/cgroup<br>/dev/sda1       283M   94M  190M  33% /boot<br>/dev/ram0       6.4G   33M  6.3G   1% /var/lib/ceph/mem/ceph-0<br>/dev/ram1       6.4G   33M  6.3G   1% /var/lib/ceph/mem/ceph-1<br>/dev/ram2       6.4G   33M  6.3G   1% /var/lib/ceph/mem/ceph-2<br>/dev/ram3       6.4G   33M  6.3G   1% /var/lib/ceph/mem/ceph-3<br></code></pre></td></tr></table></figure><h3 id="2、准备ceph的环境"><a href="#2、准备ceph的环境" class="headerlink" title="2、准备ceph的环境"></a>2、准备ceph的环境</h3><p>修改deploy的ceph.conf文件，在部署前修改好<br>单机环境添加下面的三个</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">osd_crush_chooseleaf_type = 0<br>osd_pool_default_size = 2<br>osd_journal = /var/lib/ceph/mem/<span class="hljs-variable">$cluster</span>-<span class="hljs-variable">$id</span>/journal<br></code></pre></td></tr></table></figure><p>意思就不在这里介绍了</p><h4 id="创建mon"><a href="#创建mon" class="headerlink" title="创建mon"></a>创建mon</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ceph]<span class="hljs-comment"># ceph-deploy mon create lab8106</span><br>[root@lab8106 ceph]<span class="hljs-comment"># ceph-deploy gatherkeys lab8106</span><br></code></pre></td></tr></table></figure><h4 id="创建osd"><a href="#创建osd" class="headerlink" title="创建osd"></a>创建osd</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ceph]<span class="hljs-comment"># ceph-deploy osd prepare lab8106:/dev/sdb1:/var/lib/ceph/mem/ceph-0/journal</span><br>[root@lab8106 ceph]<span class="hljs-comment"># ceph-deploy osd activate lab8106:/dev/sdb1</span><br></code></pre></td></tr></table></figure><p>部署完这个检查下</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ceph]<span class="hljs-comment"># df -h</span><br>Filesystem      Size  Used Avail Use% Mounted on<br>……<br>/dev/ram0       6.4G  5.1G  1.3G  80% /var/lib/ceph/mem/ceph-0<br>/dev/ram1       6.4G   33M  6.3G   1% /var/lib/ceph/mem/ceph-1<br>/dev/ram2       6.4G   33M  6.3G   1% /var/lib/ceph/mem/ceph-2<br>/dev/ram3       6.4G   33M  6.3G   1% /var/lib/ceph/mem/ceph-3<br>/dev/sdb1       280G   34M  280G   1% /var/lib/ceph/osd/ceph-0<br></code></pre></td></tr></table></figure><p>可以看到内存盘分区内已经生成可一个5G的journal文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ceph]<span class="hljs-comment"># ll /var/lib/ceph/osd/ceph-0</span><br>total 40<br>……<br>lrwxrwxrwx  1 root root   32 Jan 14 10:28 journal -&gt; /var/lib/ceph/mem/ceph-0/journal<br></code></pre></td></tr></table></figure><p>可以看到osd分区的也是链接到了内存盘，环境没问题</p><h4 id="继续部署生效的三个osd"><a href="#继续部署生效的三个osd" class="headerlink" title="继续部署生效的三个osd"></a>继续部署生效的三个osd</h4><p>部署完再次检查环境</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ceph]<span class="hljs-comment"># df -h|grep ceph</span><br>/dev/ram0       6.4G  5.1G  1.3G  80% /var/lib/ceph/mem/ceph-0<br>/dev/ram1       6.4G  5.1G  1.3G  80% /var/lib/ceph/mem/ceph-1<br>/dev/ram2       6.4G  5.1G  1.3G  80% /var/lib/ceph/mem/ceph-2<br>/dev/ram3       6.4G  5.1G  1.3G  80% /var/lib/ceph/mem/ceph-3<br>/dev/sdb1       280G   34M  280G   1% /var/lib/ceph/osd/ceph-0<br>/dev/sdc1       280G   34M  280G   1% /var/lib/ceph/osd/ceph-1<br>/dev/sdd1       280G   34M  280G   1% /var/lib/ceph/osd/ceph-2<br>/dev/sde1       280G   33M  280G   1% /var/lib/ceph/osd/ceph-3<br></code></pre></td></tr></table></figure><p>都挂载正确<br>检查集群的状态</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ceph]<span class="hljs-comment"># ceph -s</span><br>    cluster 68735617-2d30-4a81-9865-aeab3ea85e6e<br>     health HEALTH_OK<br>     monmap e1: 1 mons at &#123;lab8106=192.168.8.106:6789/0&#125;<br>            election epoch 2, quorum 0 lab8106<br>     osdmap e21: 4 osds: 4 up, 4 <span class="hljs-keyword">in</span><br>      pgmap v35: 192 pgs, 1 pools, 0 bytes data, 0 objects<br>            136 MB used, 1116 GB / 1117 GB avail<br>                 192 active+clean<br></code></pre></td></tr></table></figure><p>环境部署完毕</p><h3 id="开始测试"><a href="#开始测试" class="headerlink" title="开始测试"></a>开始测试</h3><p>测试一：采用内存盘journal的方式<br>使用radosbench进行测试（采取默认的写，并且不删除的测试，尽量把内存写满，未进行任何调优）</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ceph]<span class="hljs-comment"># rados bench -p rbd 120 write --no-cleanup --run-name testmemdisk</span><br>Total time run:         120.568031<br>Total writes made:      5857<br>Write size:             4194304<br>Bandwidth (MB/sec):     194.314 <br><br>Stddev Bandwidth:       144.18<br>Max bandwidth (MB/sec): 504<br>Min bandwidth (MB/sec): 0<br>Average Latency:        0.329322<br>Stddev Latency:         0.48777<br>Max latency:            3.01612<br>Min latency:            0.0377235<br></code></pre></td></tr></table></figure><p>测试二：采用默认的磁盘journal的方式，环境恢复要原始的情况</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ceph]<span class="hljs-comment"># rados bench -p rbd 120 write --no-cleanup --run-name testmemdisk</span><br>Total time run:         120.613851<br>Total writes made:      3404<br>Write size:             4194304<br>Bandwidth (MB/sec):     112.889 <br><br>Stddev Bandwidth:       26.3641<br>Max bandwidth (MB/sec): 160<br>Min bandwidth (MB/sec): 0<br>Average Latency:        0.566656<br>Stddev Latency:         0.305038<br>Max latency:            2.00623<br>Min latency:            0.105026<br></code></pre></td></tr></table></figure><p>测试的结果如上，上表格也许看的更直观，正好之前在找一个表格插件，现在用用</p><h3 id="内存盘journal与磁盘journal性能对比"><a href="#内存盘journal与磁盘journal性能对比" class="headerlink" title="内存盘journal与磁盘journal性能对比"></a>内存盘journal与磁盘journal性能对比</h3><table><thead><tr><th></th><th>内存盘journal</th><th>磁盘journal</th></tr></thead><tbody><tr><td>测试时间(s)</td><td>120.568031</td><td>120.613851</td></tr><tr><td>写数据块数</td><td>5857</td><td>3404</td></tr><tr><td>总共写入数据(MB)</td><td>23428</td><td>13616</td></tr><tr><td>数据块大小</td><td>4194304</td><td>4194304</td></tr><tr><td>写带宽(MB&#x2F;sec)</td><td>194.314</td><td>112.889</td></tr><tr><td>带宽标准偏差</td><td>144.18</td><td>26.3641</td></tr><tr><td>最大带宽(MB&#x2F;sec)</td><td>504</td><td>160</td></tr><tr><td>平均延时</td><td>0.32932</td><td>0.566656</td></tr><tr><td>延时偏差</td><td>0.48777</td><td>0.305038</td></tr><tr><td>最大延时</td><td>3.01612</td><td>2.00623</td></tr><tr><td>最小延时</td><td>0.0377235</td><td>0.105026</td></tr></tbody></table><p>可以看到相关数据，光写带宽就提升了接近一倍，这个是因为，在磁盘journal情况下，写入journal的同时还有filestore的数据写入，相当于同时有两个写入在磁盘上，磁盘的性能自然只有一半了</p><p>以上就是关于journal的内存盘实现，这里面还会面临着其他的问题</p><ul><li>机器内存的占用问题</li><li>断电后的处理</li><li>同时断电是否会搞坏pg状态</li><li>搞坏的情况是否能恢复</li></ul><p>如果解决了这些问题，这个不失为一种性能提升的方案，毕竟内存的成本和速度是ssd的磁盘和单独磁盘journal不能比的，journal本身也是一种循环的写入的空间</p><h2 id="增加知识（-dev-shm模式）"><a href="#增加知识（-dev-shm模式）" class="headerlink" title="增加知识（&#x2F;dev&#x2F;shm模式）"></a>增加知识（&#x2F;dev&#x2F;shm模式）</h2><p>上面的通过memdisk的方式是可以，还有更方便的方式就是直接用内存路径，需要处理几个地方，我们来看下怎么处理</p><p>首先修改配置文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">osd_journal =/dev/shm/<span class="hljs-variable">$cluster</span>-<span class="hljs-variable">$id</span>/journal<br>journal dio = <span class="hljs-literal">false</span><br>journal aio = <span class="hljs-literal">false</span><br></code></pre></td></tr></table></figure><p>增加这三个选项，否则会失败，这个是因为在内存这个路径下用文件的时候是无法满足默认的dio,和aio选项的，会造成无法生成journal文件，生成的journal的文件的大小为0，无法成功</p><p>假如我们给osd.5替换journal，我们看下操作步骤</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ceph-5]<span class="hljs-comment"># mkdir /dev/shm/ceph-5</span><br>[root@lab8106 ceph-5]<span class="hljs-comment"># chown ceph:ceph /dev/shm/ceph-5</span><br></code></pre></td></tr></table></figure><p>创建目录并且修改权限<br>修改fsid</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ceph-5]<span class="hljs-comment"># cat fsid </span><br>f66832a7-6bd0-4eef-a538-15ba2404243f<br></code></pre></td></tr></table></figure><p>这个fsid是用来校验journal的<br>而内存的uuid为</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ceph-5]<span class="hljs-comment">#ceph-osd -i 7 --get-journal-uuid --osd-journal /dev/shm/ceph-7/journal</span><br>00000000-0000-0000-0000-000000000000<br></code></pre></td></tr></table></figure><p>所以修改这个文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">echo</span> 00000000-0000-0000-0000-000000000000 &gt; fsid<br></code></pre></td></tr></table></figure><p>创建journal</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ceph-5]<span class="hljs-comment"># ceph-osd -i 5 --mkjournal --setuser ceph --setgroup ceph --debug_ms 20 --debug_osd 20</span><br>2017-01-20 10:58:22.297737 7f0f1a75a800 -1 created new journal /dev/shm/ceph-5/journal <span class="hljs-keyword">for</span> object store /var/lib/ceph/osd/ceph-5<br>[root@lab8106 ceph-5]<span class="hljs-comment"># ll /dev/shm/ceph-5/journal</span><br>-rw-r--r-- 1 ceph ceph 1073741824 Jan 20 10:58 /dev/shm/ceph-5/journal<br></code></pre></td></tr></table></figure><p>启动osd就可以了</p><p>##变更记录</p><table><thead><tr><th align="center">Why</th><th align="center">Who</th><th align="center">When</th></tr></thead><tbody><tr><td align="center">创建</td><td align="center">武汉-运维-磨渣</td><td align="center">2016-01-14</td></tr><tr><td align="center">增加&#x2F;dev&#x2F;shm&#x2F;路径模式</td><td align="center">武汉-运维-磨渣</td><td align="center">2017-01-20</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>ceph单机多mon的实现</title>
    <link href="/2016/01/14/ceph%E5%8D%95%E6%9C%BA%E5%A4%9Amon%E7%9A%84%E5%AE%9E%E7%8E%B0/"/>
    <url>/2016/01/14/ceph%E5%8D%95%E6%9C%BA%E5%A4%9Amon%E7%9A%84%E5%AE%9E%E7%8E%B0/</url>
    
    <content type="html"><![CDATA[<p>ceph默认情况下是以主机名来作为mon的识别的，所以这个情况下用部署工具是无法创建多个mon的，这个地方使用手动的方式可以很方便的创建多个mon</p><h3 id="1、创建mon的数据存储目录"><a href="#1、创建mon的数据存储目录" class="headerlink" title="1、创建mon的数据存储目录"></a>1、创建mon的数据存储目录</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">mkdir</span> /var/lib/ceph/mon/ceph-1<br></code></pre></td></tr></table></figure><h3 id="2、获取当前的monmap"><a href="#2、获取当前的monmap" class="headerlink" title="2、获取当前的monmap"></a>2、获取当前的monmap</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">ceph mon getmap -o /tmp/monmap<br></code></pre></td></tr></table></figure><h3 id="3、根据当前的monmap生成mon的数据"><a href="#3、根据当前的monmap生成mon的数据" class="headerlink" title="3、根据当前的monmap生成mon的数据"></a>3、根据当前的monmap生成mon的数据</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">ceph-mon -i 1  --mkfs --monmap /tmp/monmap<br></code></pre></td></tr></table></figure><h3 id="4、启动进程（后面指定端口）"><a href="#4、启动进程（后面指定端口）" class="headerlink" title="4、启动进程（后面指定端口）"></a>4、启动进程（后面指定端口）</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">ceph-mon -i 1  --public-addr  192.168.8.106:6791<br></code></pre></td></tr></table></figure><p>现在mon就加进去了</p><p>然后去写配置文件相关的信息即可，操作还是很便捷的，这个地方可以防止单mon的情况下的数据盘的损坏的情况，增加一点安全系数，当然最好是多主机的mon</p>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>centos7下做内存盘的方法</title>
    <link href="/2016/01/14/centos7%E4%B8%8B%E5%81%9A%E5%86%85%E5%AD%98%E7%9B%98%E7%9A%84%E6%96%B9%E6%B3%95/"/>
    <url>/2016/01/14/centos7%E4%B8%8B%E5%81%9A%E5%86%85%E5%AD%98%E7%9B%98%E7%9A%84%E6%96%B9%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<p>在找这个资料的时候，基本没几个能用的或者过时了的，或者是换了概念，做的不是需要的那种盘，只有少数文章有提到关键部分应该怎么去操作，现在还是自己总结一下</p><h2 id="内存盘tmpfs和ramdisk的区别"><a href="#内存盘tmpfs和ramdisk的区别" class="headerlink" title="内存盘tmpfs和ramdisk的区别"></a>内存盘tmpfs和ramdisk的区别</h2><p>这个在网上的很多资料里面都有提到，很多文章去写怎么做ramdisk的时候，都是去做的tmpfs，两者虽然都是使用的内存来存储东西，但是是完全有区别的</p><ul><li>tmpfs这个只需要mount挂载就可以分配一个目录使用内存了，只是一个目录</li><li>ramdisk这个是真的分配一个空间，这个分区是可以格式化的（这个格式化是关键）</li><li>tmpfs卸载再挂载数据会消失，ramdisk卸载再挂载数据还在</li><li>二者共同点是，系统重启后，里面的东西会消失</li></ul><blockquote><p>本文章主要是讲怎么去做ramdisk</p></blockquote><p>ramdisk是依赖于内核模块brd的，首先可以查看下这个模块的信息</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 src]<span class="hljs-comment"># modinfo brd</span><br>filename:       /lib/modules/3.10.0-229.el7.x86_64/kernel/drivers/block/brd.ko<br><span class="hljs-built_in">alias</span>:          rd<br><span class="hljs-built_in">alias</span>:          block-major-1-*<br>license:        GPL<br>rhelversion:    7.1<br>srcversion:     F38BA5B60FC8B94786C7907<br>depends:        <br>intree:         Y<br>vermagic:       3.10.0 SMP mod_unload modversions <br>parm:           rd_nr:Maximum number of brd devices (int)<br>parm:           rd_size:Size of each RAM disk <span class="hljs-keyword">in</span> kbytes. (int)<br>parm:           max_part:Maximum number of partitions per RAM disk (int)<br></code></pre></td></tr></table></figure><p>默认是不加载的，需要加载这个模块</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 src]<span class="hljs-comment"># modprobe brd</span><br></code></pre></td></tr></table></figure><p>加载模块后就会生成下面的的盘符路径，这个就是内存盘</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 src]<span class="hljs-comment"># ll /dev/ram*</span><br>brw-rw---- 1 root disk 1, 0 Jan 14 00:43 /dev/ram0<br>brw-rw---- 1 root disk 1, 1 Jan 14 00:43 /dev/ram1<br>brw-rw---- 1 root disk 1, 2 Jan 14 00:42 /dev/ram2<br>brw-rw---- 1 root disk 1, 3 Jan 14 00:42 /dev/ram3<br></code></pre></td></tr></table></figure><p>这个的默认大小是16M，设备的数目是16个，这个显然是不符合我们的需求的</p><p>这个个数信息和大小信息是写在内核模块里面的,这个目前还找到办法在外面修改的地方(已经找到了见本文补充)，现在通过修改内核模块的方式来达到修改的目的</p><h2 id="获取内核源码"><a href="#获取内核源码" class="headerlink" title="获取内核源码"></a>获取内核源码</h2><p> CentOS-7-x86_64-1503-01版本的内核是3.10.0-229.el7.x86_64，这个最好是使用的对应版本的内核代码，这样不会出现其他的问题，下载该distribution版本的内核源码，拷贝到根目录：</p><p><a href="http://vault.centos.org/7.1.1503/updates/Source/SPackages/kernel-3.10.0-229.1.2.el7.src.rpm">http://vault.centos.org/7.1.1503/updates/Source/SPackages/kernel-3.10.0-229.1.2.el7.src.rpm</a></p><h3 id="安装该源码包"><a href="#安装该源码包" class="headerlink" title="安装该源码包"></a>安装该源码包</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ~]<span class="hljs-comment"># rpm -i kernel-3.10.0-229.1.2.el7.src.rpm  </span><br></code></pre></td></tr></table></figure><p>安装完了以后，这个rpm包里面的源码会被放在 ~&#x2F;rpmbuild&#x2F;SOURCES&#x2F; 这个目录内，源码文件是linux-3.10.0-229.1.2.el7.tar.xz </p><h2 id="编译内核源码"><a href="#编译内核源码" class="headerlink" title="编译内核源码"></a>编译内核源码</h2><p>将linux-3.10.0-229.1.2.el7.tar.xz 文件拷贝到目录  &#x2F;usr&#x2F;src&#x2F;zp 下<br>这个是你自己定义一个编译的目录</p><h2 id="解压内核源码"><a href="#解压内核源码" class="headerlink" title="解压内核源码"></a>解压内核源码</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 zp]<span class="hljs-comment"># tar -xvf linux-3.10.0-229.1.2.el7.tar.xz</span><br>[root@lab8106 zp]<span class="hljs-comment"># cd linux-3.10.0-229.1.2.el7/</span><br></code></pre></td></tr></table></figure><h2 id="清理编译环境的状态，如果你是下载的内核源码，而且是第一次编译，就没有必要执行这一步操作"><a href="#清理编译环境的状态，如果你是下载的内核源码，而且是第一次编译，就没有必要执行这一步操作" class="headerlink" title="清理编译环境的状态，如果你是下载的内核源码，而且是第一次编译，就没有必要执行这一步操作"></a>清理编译环境的状态，如果你是下载的内核源码，而且是第一次编译，就没有必要执行这一步操作</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ~]<span class="hljs-comment"># make mrproper </span><br></code></pre></td></tr></table></figure><h3 id="将已存在的-config文件内容，作为新版本内核的默认值"><a href="#将已存在的-config文件内容，作为新版本内核的默认值" class="headerlink" title="将已存在的.&#x2F;.config文件内容，作为新版本内核的默认值"></a>将已存在的.&#x2F;.config文件内容，作为新版本内核的默认值</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 ~]<span class="hljs-comment"># make oldconfig</span><br></code></pre></td></tr></table></figure><h3 id="配置内核的参数，修改ramdisk的相关属性"><a href="#配置内核的参数，修改ramdisk的相关属性" class="headerlink" title="配置内核的参数，修改ramdisk的相关属性"></a>配置内核的参数，修改ramdisk的相关属性</h3><p>在内核配置菜单中配置ramdisk块驱动模块的个数和大小，并保存退出</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">Device Drivers <br>       |--&gt;Block devices <br>                  |--&gt;  [M]RAM block device support <br>                           (xx) Default number of RAM disks <br>                           (xx) Default RAM disk size(kbytes) <br></code></pre></td></tr></table></figure><p>如果内存够大，可以修改大点，注意这个地方是每个内存盘的大小</p><h2 id="编译内核模块"><a href="#编译内核模块" class="headerlink" title="编译内核模块"></a>编译内核模块</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 zp]<span class="hljs-comment"># make modules -j8</span><br></code></pre></td></tr></table></figure><h3 id="编译后的Ramdisk模块的存放位置"><a href="#编译后的Ramdisk模块的存放位置" class="headerlink" title="编译后的Ramdisk模块的存放位置"></a>编译后的Ramdisk模块的存放位置</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">/usr/src/zp/linux-3.10.0-229.1.2.el7/drivers/block/brd.ko<br></code></pre></td></tr></table></figure><h2 id="安装新的brd-ko模块"><a href="#安装新的brd-ko模块" class="headerlink" title="安装新的brd.ko模块"></a>安装新的brd.ko模块</h2><p>将旧的brd.ko模块从内核中移除。 </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 zp]<span class="hljs-comment"># rmmod brd </span><br></code></pre></td></tr></table></figure><p>将新的brd.ko模块拷贝到Centos7系统的 如下目录&#x2F;lib&#x2F;modules&#x2F;3.10.0-229.el7.x86_64&#x2F;kernel&#x2F;drivers&#x2F;block&#x2F;，<br>覆盖原来的ramDisk模块brd.ko</p><h2 id="更新内核模块依赖"><a href="#更新内核模块依赖" class="headerlink" title="更新内核模块依赖"></a>更新内核模块依赖</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 zp]<span class="hljs-comment"># depmod -a</span><br></code></pre></td></tr></table></figure><h2 id="8、重新挂载内核模块。-如果加载的时候报错就强制加载-modprobe-f-brd"><a href="#8、重新挂载内核模块。-如果加载的时候报错就强制加载-modprobe-f-brd" class="headerlink" title="8、重新挂载内核模块。 如果加载的时候报错就强制加载 modprobe -f brd"></a>8、重新挂载内核模块。 如果加载的时候报错就强制加载 modprobe -f brd</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 zp]<span class="hljs-comment"># modprobe brd </span><br></code></pre></td></tr></table></figure><h2 id="检查是否生成了"><a href="#检查是否生成了" class="headerlink" title="检查是否生成了"></a>检查是否生成了</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 zp]<span class="hljs-comment"># ls /dev/ram*</span><br></code></pre></td></tr></table></figure><p>然后就可以使用&#x2F;dev&#x2F;ram*这个设备了，当磁盘一样使用</p><p>我的为测试环境，内存不是那么大，就是5G内存盘，4个，做对比测试，ceph默认的5G的journal，这个内存就稍微给大那么一点点6G，防止单位换算的原因造成空间不够，需要重来</p><h2 id="补充"><a href="#补充" class="headerlink" title="补充"></a>补充</h2><p>在线添加内存盘操作</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">modprobe brd rd_nr=2 rd_size=2048000 max_part=0<br></code></pre></td></tr></table></figure><p>说明：</p><blockquote><p>rd_nr：ramdisk的个数<br><br>rd_size：ramdisk的大小<br><br>max_part：ramdisk单个分区的最大个数</p></blockquote><p>在线卸载内存盘的操作</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">modprobe -r brd<br></code></pre></td></tr></table></figure><p>如果需要开机自启动：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">vim /etc/modules-load.d/memdisk.conf<br></code></pre></td></tr></table></figure><p>添加需要加载的模块</p><blockquote><p>brd</p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">vim /etc/modprobe.d/memdisk.conf<br></code></pre></td></tr></table></figure><p>添加给定相关参数</p><blockquote><p>options brd rd_nr&#x3D;2 rd_size&#x3D;2048000 max_part&#x3D;0</p></blockquote><h2 id="变更记录"><a href="#变更记录" class="headerlink" title="变更记录"></a>变更记录</h2><table><thead><tr><th align="center">Why</th><th align="center">Who</th><th align="center">When</th></tr></thead><tbody><tr><td align="center">创建</td><td align="center">武汉-运维-磨渣</td><td align="center">2016-01-14</td></tr><tr><td align="center">修改添加方案</td><td align="center">武汉-运维-磨渣</td><td align="center">2016-09-22</td></tr></tbody></table><p>这篇文章基本都是参考了：</p><blockquote><p><a href="http://my.oschina.net/u/658505/blog/544547?fromerr=wWO13oYJ">http://my.oschina.net/u/658505/blog/544547?fromerr=wWO13oYJ</a></p></blockquote>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>删除osd的正确方式</title>
    <link href="/2016/01/12/%E5%88%A0%E9%99%A4osd%E7%9A%84%E6%AD%A3%E7%A1%AE%E6%96%B9%E5%BC%8F/"/>
    <url>/2016/01/12/%E5%88%A0%E9%99%A4osd%E7%9A%84%E6%AD%A3%E7%A1%AE%E6%96%B9%E5%BC%8F/</url>
    
    <content type="html"><![CDATA[<p>在ceph的集群当中关于节点的替换的问题，一直按照以前的方式进行的处理，处理的步骤如下：</p><h3 id="停止osd进程"><a href="#停止osd进程" class="headerlink" title="停止osd进程"></a>停止osd进程</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">/etc/init.d/ceph stop osd.0<br></code></pre></td></tr></table></figure><p>这一步是停止osd的进程，让其他的osd知道这个节点不提供服务了</p><h3 id="将节点状态标记为out"><a href="#将节点状态标记为out" class="headerlink" title="将节点状态标记为out"></a>将节点状态标记为out</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">ceph osd out osd.0<br></code></pre></td></tr></table></figure><p>这个一步是告诉mon，这个节点已经不能服务了，需要在其他的osd上进行数据的恢复了</p><h3 id="从crush中移除节点"><a href="#从crush中移除节点" class="headerlink" title="从crush中移除节点"></a>从crush中移除节点</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">ceph osd crush remove osd.0<br></code></pre></td></tr></table></figure><p>从crush中删除是告诉集群这个点回不来了，完全从集群的分布当中剔除掉，让集群的crush进行一次重新计算，之前节点还占着这个crush weight，会影响到当前主机的host crush weight </p><h3 id="删除节点"><a href="#删除节点" class="headerlink" title="删除节点"></a>删除节点</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">ceph osd <span class="hljs-built_in">rm</span> osd.0<br></code></pre></td></tr></table></figure><p>这个是从集群里面删除这个节点的记录</p><h3 id="删除节点认证（不删除编号会占住）"><a href="#删除节点认证（不删除编号会占住）" class="headerlink" title="删除节点认证（不删除编号会占住）"></a>删除节点认证（不删除编号会占住）</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">ceph auth del osd.0<br></code></pre></td></tr></table></figure><p>这个是从认证当中去删除这个节点的信息</p><p>这个一直是我处理故障的节点osd的方式，其实这个会触发两次迁移，一次是在节点osd以后，一个是在crush remove以后，两次迁移对于集群来说是不好的，其实是调整步骤是可以避免二次迁移的</p><h2 id="新的处理方式"><a href="#新的处理方式" class="headerlink" title="新的处理方式"></a>新的处理方式</h2><h3 id="调整osd的crush-weight"><a href="#调整osd的crush-weight" class="headerlink" title="调整osd的crush weight"></a>调整osd的crush weight</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">ceph osd crush reweight osd.0 0.1<br></code></pre></td></tr></table></figure><p>说明：这个地方如果想慢慢的调整就分几次将crush 的weight 减低到0 ，这个过程实际上是让数据不分布在这个节点上，让数据慢慢的分布到其他节点上，直到最终为没有分布在这个osd，并且迁移完成<br>这个地方不光调整了osd 的crush weight ，实际上同时调整了host 的 weight ，这样会调整集群的整体的crush 分布，在osd 的crush 为0 后， 再对这个osd的任何删除相关操作都不会影响到集群的数据的分布</p><h3 id="停止osd进程-1"><a href="#停止osd进程-1" class="headerlink" title="停止osd进程"></a>停止osd进程</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">/etc/init.d/ceph stop osd.0<br></code></pre></td></tr></table></figure><p>停止到osd的进程，这个是通知集群这个osd进程不在了，不提供服务了，因为本身没权重，就不会影响到整体的分布，也就没有迁移</p><h3 id="将节点状态标记为out-1"><a href="#将节点状态标记为out-1" class="headerlink" title="将节点状态标记为out"></a>将节点状态标记为out</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">ceph osd out osd.0<br></code></pre></td></tr></table></figure><p>停止到osd的进程，这个是通知集群这个osd不再映射数据了，不提供服务了，因为本身没权重，就不会影响到整体的分布，也就没有迁移</p><h3 id="从crush中移除节点-1"><a href="#从crush中移除节点-1" class="headerlink" title="从crush中移除节点"></a>从crush中移除节点</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">ceph osd crush remove osd.0<br></code></pre></td></tr></table></figure><p>这个是从crush中删除，因为已经是0了  所以没影响主机的权重，也就没有迁移了</p><h3 id="删除节点-1"><a href="#删除节点-1" class="headerlink" title="删除节点"></a>删除节点</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">ceph osd <span class="hljs-built_in">rm</span> osd.0<br></code></pre></td></tr></table></figure><p>这个是从集群里面删除这个节点的记录</p><h3 id="删除节点认证（不删除编号会占住）-1"><a href="#删除节点认证（不删除编号会占住）-1" class="headerlink" title="删除节点认证（不删除编号会占住）"></a>删除节点认证（不删除编号会占住）</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">ceph auth del osd.0<br></code></pre></td></tr></table></figure><p>这个是从认证当中去删除这个节点的信息</p><p>经过验证，第二种方式只触发了一次迁移，虽然只是一个步骤先后上的调整，对于生产环境的的集群来说，迁移的量要少了一次，实际生产环境当中节点是有自动out的功能，这个可以考虑自己去控制，只是监控的密度需要加大，毕竟这个是一个需要监控的集群，完全让其自己处理数据的迁移是不可能的，带来的故障只会更多</p>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>ceph写osd的配置文件ceph.conf</title>
    <link href="/2016/01/11/ceph%E5%86%99osd%E7%9A%84%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6ceph.conf/"/>
    <url>/2016/01/11/ceph%E5%86%99osd%E7%9A%84%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6ceph.conf/</url>
    
    <content type="html"><![CDATA[<p>ceph在部署过程中是先进行部署，再去写配置文件的，而一些新手在部署完了后，并没有写配置文件，在重启服务器后，因为挂载点没有挂载，所以服务无法启动，所以需要写好配置文件<br>还有一种情况是集群有几百个osd，在新加入或者修改的时候，再去进行变更配置文件就是一个很麻烦的事情，所以写配置文件这个如果脚本来处理，就可以节约很多时间，所以写了一个脚本如下，这个地方如果熟悉python的可以用python写，我这个是自己使用，并且使用的频率不会太高，因此，怎么方便怎么来</p><p>脚本里面用了一个二进制文件是解析json用的，这个拷贝到运行的机器上就可以了</p><p>解析的二进制文件在这里下载：<br><a href="http://stedolan.github.io/jq/">http://stedolan.github.io/jq/</a></p><p>备用下载地址：<br><a href="http://pan.baidu.com/s/1pKgefmr">http://pan.baidu.com/s/1pKgefmr</a></p><p>下载后拷贝到linux机器的&#x2F;sbin&#x2F;下面，为了方便重命名为 &#x2F;sbin&#x2F;jq</p><p>后面的输出可以方便的修改，原理是获取当前的osd状态，然后去osd上获取信息</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-meta">#! /bin/sh</span><br><span class="hljs-comment">#注意要配合js使用http://stedolan.github.io/jq/</span><br><span class="hljs-keyword">for</span> osd <span class="hljs-keyword">in</span> `ceph osd dump |awk  <span class="hljs-string">&#x27;/^osd/ &#123;print $1&#125;&#x27;</span>|<span class="hljs-built_in">cut</span> -d . -f 2`<br><span class="hljs-keyword">do</span><br><span class="hljs-comment">#获取主机名</span><br>osdhost=`ceph osd find <span class="hljs-variable">$osd</span> |jq <span class="hljs-string">&#x27;.crush_location&#x27;</span> |jq <span class="hljs-string">&#x27;.host&#x27;</span>|<span class="hljs-built_in">cut</span> -d \&quot; -f 2`<br><span class="hljs-comment">#获取主机的ip</span><br>osdip=`ceph osd find <span class="hljs-variable">$osd</span> |jq <span class="hljs-string">&#x27;.ip&#x27;</span> |<span class="hljs-built_in">cut</span> -d : -f 1|<span class="hljs-built_in">cut</span> -d \&quot; -f 2`<br><span class="hljs-comment">#获取主机的磁盘</span><br>osddisk=`ssh <span class="hljs-variable">$osdip</span> findmnt /var/lib/ceph/osd/ceph-<span class="hljs-variable">$osd</span>|awk <span class="hljs-string">&#x27;&#123;print $2&#125;&#x27;</span>|<span class="hljs-built_in">tail</span> -n 1`<br><span class="hljs-comment">#获取主机的uuid</span><br>uuid=`ssh <span class="hljs-variable">$osdip</span> blkid <span class="hljs-variable">$osddisk</span>|<span class="hljs-built_in">cut</span> -d : -f 2|<span class="hljs-built_in">cut</span> -d <span class="hljs-string">&quot; &quot;</span> -f 2|<span class="hljs-built_in">cut</span> -d \&quot; -f 2`<br><span class="hljs-comment">#写入文件</span><br><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;osd.<span class="hljs-variable">$osd</span>.host = <span class="hljs-variable">$osdhost</span>&quot;</span> &gt;&gt; mydiskinfo<br><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;osd.<span class="hljs-variable">$osd</span>.uuid = <span class="hljs-variable">$uuid</span>  &quot;</span> &gt;&gt; mydiskinfo<br><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;osd.<span class="hljs-variable">$osd</span>.devs = <span class="hljs-variable">$osddisk</span>&quot;</span> &gt;&gt; mydiskinfo<br><span class="hljs-keyword">done</span><br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>centos6安装calamari</title>
    <link href="/2015/12/19/centos6%E5%AE%89%E8%A3%85calamari/"/>
    <url>/2015/12/19/centos6%E5%AE%89%E8%A3%85calamari/</url>
    
    <content type="html"><![CDATA[<h3 id="安装操作系统"><a href="#安装操作系统" class="headerlink" title="安装操作系统"></a>安装操作系统</h3><p>首先安装操作系统centos6,安装过程选择的是base server，这个不相同不要紧，出现缺少包的时候去iso找出来安装就可以了</p><h3 id="calamari的简单介绍"><a href="#calamari的简单介绍" class="headerlink" title="calamari的简单介绍"></a>calamari的简单介绍</h3><p>首先简单的介绍下calamari的这个软件系统的组成，主要是calamari-server,romana，salt-minion，salt-server，diamond，</p><p>这些模块各自的作用：</p><ul><li>calamari-server这个是提供一个与集群进行交互，并且自己封装了一个自己的API，做集中管理的地方，这个只需要在集群当中的某一台机器上安装，也可以独立安装</li><li>romana就是原来的calamari-client，这个叫client,其实是一个web的界面，这个叫calamari-web更好，现在已经更名为romana，这个也是只需要在集群当中的某一台机器上安装，也可以独立安装，这个需要跟calamari-server安装在一台机器上</li><li>salt-server是一个管理的工具，可以批量的管理其他的机器，可以对安装了salt-minion的机器进行管理，在集群当中，这个也是跟calamari-server安装在一起的</li><li>salt-minion是安装在集群的所有节点上的，这个是接收salt-server的指令对集群的机器进行操作，并且反馈一些信息到salt-server上</li><li>diamond这个是系统的监控信息的收集控件，提供集群的硬件信息的监控和集群的信息的监控，数据是发送到romana的机器上的，是由romana上的carbon来收取数据并存储到机器当中的数据库当中的</li></ul><p>所以总结下来安装的方式是:</p><table><thead><tr><th align="center">节点情况</th><th align="center">需要安装软件</th></tr></thead><tbody><tr><td align="center">单独一台非集群节点装calmari-server</td><td align="center">calamri-server romana salt-master salt-minion</td></tr><tr><td align="center">集群节点</td><td align="center">salt-minion diamond</td></tr></tbody></table><p>注意：<br>如果calamri-server选择安装在集群内节点，那么这台机器就安装 calamri-server romana salt-master salt-minion diamond </p><h3 id="软件安装"><a href="#软件安装" class="headerlink" title="软件安装"></a>软件安装</h3><p>安装过程很简单就是安装上面的包就可以了，这个包的资源我已经打包好了在这篇的结尾的链接下面，分好了目录,管理节点就安装calamriserver,集群的节点就安装clusternode里面的，出现的冲突的一个包就用rpm -Uvh 升级安装一下，这里面有一个包需要升级安装下</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@myserver centos6calamari]<span class="hljs-comment"># ll</span><br>total 8<br>drwxr-xr-x. 7 root root 4096 Dec 11 10:59 calamariserver<br>drwxr-xr-x. 4 root root 4096 Dec 11 11:00 clusternode<br>[root@myserver centos6calamari]<span class="hljs-comment"># ll calamariserver/</span><br>total 22092<br>-rw-r--r--. 1 root root 20965336 Dec 11 10:59 calamari-server-1.3.1.1-105_g79c8df2.el6.x86_64.rpm<br>drwxr-xr-x. 2 root root     4096 Dec 11 10:59 httpd<br>drwxr-xr-x. 2 root root     4096 Dec 11 10:59 postgresql<br>-rw-r--r--. 1 root root      658 Dec 11 10:59 readme<br>-rw-r--r--. 1 root root  1629144 Dec 11 10:59 romana-1.2.2-36_gc62bb5b.el6.x86_64.rpm<br>drwxr-xr-x. 2 root root     4096 Dec 11 10:59 salt-master<br>drwxr-xr-x. 2 root root     4096 Dec 11 10:59 salt-minion<br>drwxr-xr-x. 2 root root     4096 Dec 11 10:59 supervisor<br>[root@myserver centos6calamari]<span class="hljs-comment"># ll clusternode/</span><br>total 8<br>drwxr-xr-x. 2 root root 4096 Dec 11 10:59 diamond<br>drwxr-xr-x. 3 root root 4096 Dec 11 11:05 salt-minion<br></code></pre></td></tr></table></figure><p>需要注意一点安装完calamari的server以后需要处理下权限</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@myserver calamari]<span class="hljs-comment"># chmod 777 -R /var/log/calamari/</span><br>[root@myserver calamari]<span class="hljs-comment"># chmod 777 -R /opt/calamari/</span><br></code></pre></td></tr></table></figure><p>然后再去做</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@myserver calamari]<span class="hljs-comment">#calamari-ctl initialize</span><br></code></pre></td></tr></table></figure><p>目前已经测试通过，就是可能我的是虚拟机的原因，会提示web的状态没更新的问题，集群的状态都拿到了</p><h3 id="故障排查："><a href="#故障排查：" class="headerlink" title="故障排查："></a>故障排查：</h3><p>配置好了后用</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@myserver calamari]<span class="hljs-comment"># salt &#x27;*&#x27; test.ping</span><br>[root@myserver calamari]<span class="hljs-comment"># salt &#x27;*&#x27; ceph.get_heartbeats</span><br></code></pre></td></tr></table></figure><p>我的出现了calamari连接了集群发现检测不到集群，就用上面的检测，然后发现确实拿不到集群的信息，然后就去节点的机器上检查salt-minion的日志，发现是一个提示认证的错误，就做了下面的处理后就好了</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@node1 ~]<span class="hljs-comment">#rm -rf /etc/salt/pki/minion/minion_master.pub</span><br>[root@node1 ~]<span class="hljs-comment">#service salt-minion restart</span><br></code></pre></td></tr></table></figure><p>###资源链接：</p><p>链接：<a href="http://pan.baidu.com/s/1eRtLZvO">http://pan.baidu.com/s/1eRtLZvO</a> 密码：0ael</p><p>资源更新说明：</p><ul><li>增加了osd限制为256个数的修改patch包，使用rpm -Uvh进行安装，在满足当前的情况下就不需要更新，解决溢出的情况</li></ul>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>xenserver使用ceph的rbd的方法</title>
    <link href="/2015/12/16/xenserver%E4%BD%BF%E7%94%A8ceph%E7%9A%84rbd%E7%9A%84%E6%96%B9%E6%B3%95/"/>
    <url>/2015/12/16/xenserver%E4%BD%BF%E7%94%A8ceph%E7%9A%84rbd%E7%9A%84%E6%96%B9%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<p>首先安装的xenserver6.5的环境，看到有地方有提到这个上面可以安装rbd的支持，网上有一种方式是libvirt+kvm方式，因为ceph对libviet是原生支持的，但是xenserver底层是xen的，这个就不去研究太多，这个用最简单的方式最好</p><p><a href="https://github.com/mstarikov/rbdsr">https://github.com/mstarikov/rbdsr</a><br>这个是个第三方的插件，最近才出来的</p><p>实现原理是ssh到ceph的机器上获取到可以使用的rbd信息，然后在xenserver的图形界面上通过配置iscsi的方式去配置rbd，里面套用了iscsi的界面，实际去xenserver机器后台同样做的是map的操作<br>这个试了下，界面的操作都可以实现，都可以获取到rbd的信息，但是在最后提交的一下的时候，后台会报错误的信息，这个有可能才出来，还有点问题<br>这个地方可以用其他的方式实现，xenserver在添加硬盘的时候本来就支持的命令行模式，下面为实现方式</p><p>先检查内核的信息，这个有rbd模块，并且用的是3.10的，这个是用的centos7同等的内核，问题不大</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@xenserver ]<span class="hljs-comment"># modinfo rbd</span><br>filename:       /lib/modules/3.10.0+2/kernel/drivers/block/rbd.ko<br>license:        GPL<br>author:         Jeff Garzik &lt;jeff@garzik.org&gt;<br>description:    rados block device<br>author:         Yehuda Sadeh &lt;yehuda@hq.newdream.net&gt;<br>author:         Sage Weil &lt;sage@newdream.net&gt;<br>srcversion:     B03197D54ABE3BD7A32A276<br>depends:        libceph<br>intree:         Y<br>vermagic:       3.10.0+2 SMP mod_unload modversions<br></code></pre></td></tr></table></figure><p>查看系统上软件包的信息，可以看到xenserver6.5虽然用的是centos7同等的内核，实际上环境还是基于centos5的软件版本进行的定制，这个地方本来准备把ceph的软件包安装上去，版本上的依赖相隔太大，就没装了，也没太多的必要，能实现需求即可</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@xenserver]<span class="hljs-comment"># rpm -qa|grep ssh</span><br>openssh-4.3p2-82.el5<br></code></pre></td></tr></table></figure><p>这个地方在xenserver的机器上使用这个方式使用rbd，需要做下面几个事情：</p><ul><li>一个是写rbdmap配置文件 </li><li>一个是rbdmap启动的脚本 </li><li>一个是ceph.conf的配置文件</li></ul><h2 id="修改-etc-ceph-rbdmap配置文件"><a href="#修改-etc-ceph-rbdmap配置文件" class="headerlink" title="修改&#x2F;etc&#x2F;ceph&#x2F;rbdmap配置文件"></a>修改&#x2F;etc&#x2F;ceph&#x2F;rbdmap配置文件</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">/etc/ceph/rbdmap 里面的配置文件书写方式<br><span class="hljs-comment"># RbdDevice             Parameters</span><br><span class="hljs-comment">#poolname/imagename     id=client,keyring=/etc/ceph/ceph.client.keyring</span><br>rbd/testrbd             <span class="hljs-built_in">id</span>=admin<br></code></pre></td></tr></table></figure><p>&#x2F;etc&#x2F;ceph&#x2F;rbdmap根据需要去写，我不喜欢用keyring就没写keyring,但是id是必须写，否则会报错</p><h2 id="修改rbdmap启动的脚本"><a href="#修改rbdmap启动的脚本" class="headerlink" title="修改rbdmap启动的脚本"></a>修改rbdmap启动的脚本</h2><p>&#x2F;etc&#x2F;init.d&#x2F;rbdmap这个脚本要修改，默认的脚本里面是要去使用rbd命令的，rbd命令是在ceph-common里面的这个里面，可以修改一个不需要安装ceph-common的版本的启动脚本</p><p>改版的如下：<br>&#x2F;etc&#x2F;init.d&#x2F;rbdmap文件内容替换如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-meta">#!/bin/bash</span><br><span class="hljs-comment">#</span><br><span class="hljs-comment"># rbdmap Ceph RBD Mapping</span><br><span class="hljs-comment">#</span><br><span class="hljs-comment"># chkconfig: 2345 20 80</span><br><span class="hljs-comment"># description: Ceph RBD Mapping</span><br><br><span class="hljs-comment">### BEGIN INIT INFO</span><br><span class="hljs-comment"># Provides:          rbdmap</span><br><span class="hljs-comment"># Required-Start:    $network</span><br><span class="hljs-comment"># Required-Stop:     $network</span><br><span class="hljs-comment"># Default-Start:     2 3 4 5</span><br><span class="hljs-comment"># Default-Stop:      0 1 6</span><br><span class="hljs-comment"># Short-Description: Ceph RBD Mapping</span><br><span class="hljs-comment"># Description:       Ceph RBD Mapping</span><br><span class="hljs-comment">### END INIT INFO</span><br><br>DESC=<span class="hljs-string">&quot;RBD Mapping&quot;</span><br>RBDMAPFILE=<span class="hljs-string">&quot;/etc/ceph/rbdmap&quot;</span><br><br>. /lib/lsb/init-functions<br><br>modprobe rbd || <span class="hljs-built_in">exit</span> 1<br><br><span class="hljs-function"><span class="hljs-title">do_map</span></span>() &#123;<br><span class="hljs-built_in">touch</span>  /var/lock/subsys/rbdmap<br><span class="hljs-keyword">if</span> [ ! -f <span class="hljs-string">&quot;<span class="hljs-variable">$RBDMAPFILE</span>&quot;</span> ]; <span class="hljs-keyword">then</span><br><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;<span class="hljs-variable">$DESC</span> : No <span class="hljs-variable">$RBDMAPFILE</span> found.&quot;</span><br><span class="hljs-built_in">exit</span> 0<br><span class="hljs-keyword">fi</span><br><br><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;Starting <span class="hljs-variable">$DESC</span>&quot;</span><br><span class="hljs-comment"># Read /etc/rbdtab to create non-existant mapping</span><br>newrbd=<br>RET=0<br><span class="hljs-keyword">while</span> <span class="hljs-built_in">read</span> DEV PARAMS; <span class="hljs-keyword">do</span><br><span class="hljs-keyword">case</span> <span class="hljs-string">&quot;<span class="hljs-variable">$DEV</span>&quot;</span> <span class="hljs-keyword">in</span><br>  <span class="hljs-string">&quot;&quot;</span>|\<span class="hljs-comment">#*)</span><br><span class="hljs-built_in">continue</span><br>;;<br>  */*)<br>;;<br>  *)<br>DEV=rbd/<span class="hljs-variable">$DEV</span><br>;;<br><span class="hljs-keyword">esac</span><br><span class="hljs-keyword">if</span> [ ! -b /dev/rbd/<span class="hljs-variable">$DEV</span> ]; <span class="hljs-keyword">then</span><br><span class="hljs-built_in">echo</span> <span class="hljs-variable">$DEV</span><br><span class="hljs-comment">#rbd map $DEV $CMDPARAMS</span><br>mons=`egrep <span class="hljs-string">&#x27;mon[ _]host&#x27;</span> /etc/ceph/ceph.conf | <span class="hljs-built_in">cut</span> -f2 -d<span class="hljs-string">&#x27;=&#x27;</span> | sed <span class="hljs-string">&#x27;s/ //g&#x27;</span>`<br>args=`<span class="hljs-built_in">echo</span> <span class="hljs-variable">$PARAMS</span> | sed <span class="hljs-string">&#x27;s/id/name/g&#x27;</span>`<br>rbddev=`<span class="hljs-built_in">echo</span> <span class="hljs-variable">$DEV</span> | <span class="hljs-built_in">tr</span> <span class="hljs-string">&#x27;/&#x27;</span> <span class="hljs-string">&#x27; &#x27;</span>`<br><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;<span class="hljs-variable">$mons</span> <span class="hljs-variable">$args</span> <span class="hljs-variable">$rbddev</span>&quot;</span> &gt; /sys/bus/rbd/add<br>[ $? -ne <span class="hljs-string">&quot;0&quot;</span> ] &amp;&amp; RET=1<br>newrbd=<span class="hljs-string">&quot;yes&quot;</span><br><span class="hljs-keyword">fi</span><br><span class="hljs-keyword">done</span> &lt; <span class="hljs-variable">$RBDMAPFILE</span><br><span class="hljs-built_in">echo</span> <span class="hljs-variable">$RET</span><br><br><span class="hljs-comment"># Mount new rbd</span><br><span class="hljs-keyword">if</span> [ <span class="hljs-string">&quot;<span class="hljs-variable">$newrbd</span>&quot;</span> ]; <span class="hljs-keyword">then</span><br>                <span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;Mounting all filesystems&quot;</span><br>mount -a<br><span class="hljs-built_in">echo</span> $?<br><span class="hljs-keyword">fi</span><br>&#125;<br><br><span class="hljs-function"><span class="hljs-title">do_unmap</span></span>() &#123;<br><span class="hljs-built_in">rm</span> -rf /var/lock/subsys/rbdmap<br><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;Stopping <span class="hljs-variable">$DESC</span>&quot;</span><br>RET=0<br><span class="hljs-comment"># Recursive umount that depends /dev/rbd*</span><br><span class="hljs-comment">#原始版本可能没这个命令</span><br><span class="hljs-comment">#MNTDEP=$(findmnt --mtab | awk &#x27;$2 ~ /^\/dev\/rbd[0-9]*$/ &#123;print $1&#125;&#x27; | sort -r)</span><br><span class="hljs-comment">#修改如下：</span><br>MNTDEP=$(mount| awk <span class="hljs-string">&#x27;$1 ~ /^\/dev\/rbd[0-9]*$/ &#123;print $3&#125;&#x27;</span> | <span class="hljs-built_in">sort</span> -r)<br><br><span class="hljs-keyword">for</span> MNT <span class="hljs-keyword">in</span> <span class="hljs-variable">$MNTDEP</span>; <span class="hljs-keyword">do</span><br>umount <span class="hljs-variable">$MNT</span><br><span class="hljs-keyword">done</span> <br><span class="hljs-comment"># Unmap all rbd device</span><br><span class="hljs-built_in">cd</span> /sys/bus/rbd/devices/<br><span class="hljs-keyword">if</span> <span class="hljs-built_in">ls</span> * &gt;/dev/null 2&gt;&amp;1; <span class="hljs-keyword">then</span><br><span class="hljs-keyword">for</span> DEV <span class="hljs-keyword">in</span> *; <span class="hljs-keyword">do</span><br><span class="hljs-built_in">echo</span> <span class="hljs-variable">$DEV</span><br><span class="hljs-built_in">echo</span> <span class="hljs-variable">$DEV</span> &gt; /sys/bus/rbd/remove<br>[ $? -ne <span class="hljs-string">&quot;0&quot;</span> ] &amp;&amp; RET=1<br><span class="hljs-keyword">done</span><br><span class="hljs-keyword">fi</span><br><span class="hljs-built_in">echo</span> <span class="hljs-variable">$RET</span><br>&#125;<br><br><br><span class="hljs-keyword">case</span> <span class="hljs-string">&quot;<span class="hljs-variable">$1</span>&quot;</span> <span class="hljs-keyword">in</span><br>  start)<br>do_map<br>;;<br><br>  stop)<br>do_unmap<br>;;<br><br>  reload)<br>do_map<br>;;<br><br>  status)<br><span class="hljs-built_in">ls</span> /sys/bus/rbd/devices/<br>;;<br><br>  *)<br><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;Usage: rbdmap &#123;start|stop|reload|status&#125;&quot;</span><br><span class="hljs-built_in">exit</span> 1<br>;;<br><span class="hljs-keyword">esac</span><br><br><span class="hljs-built_in">exit</span> 0<br></code></pre></td></tr></table></figure><h2 id="使用下面的命令进行rbd的挂载"><a href="#使用下面的命令进行rbd的挂载" class="headerlink" title="使用下面的命令进行rbd的挂载:"></a>使用下面的命令进行rbd的挂载:</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">/etc/init.d/rbdmap start<br></code></pre></td></tr></table></figure><h4 id="启动后可以查看本地rbd映射的磁盘"><a href="#启动后可以查看本地rbd映射的磁盘" class="headerlink" title="启动后可以查看本地rbd映射的磁盘"></a>启动后可以查看本地rbd映射的磁盘</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@xenserver15 ceph]<span class="hljs-comment"># ll /dev/rbd1 </span><br>brw-r----- 1 root disk 252, 0 Dec 16 11:35 /dev/rbd1<br></code></pre></td></tr></table></figure><h4 id="卸载rbd的命令为"><a href="#卸载rbd的命令为" class="headerlink" title="卸载rbd的命令为"></a>卸载rbd的命令为</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">/etc/init.d/rbdmap stop<br></code></pre></td></tr></table></figure><h4 id="加入到自启动"><a href="#加入到自启动" class="headerlink" title="加入到自启动"></a>加入到自启动</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@xenserver]<span class="hljs-comment"># chkconfig rbdmap on</span><br></code></pre></td></tr></table></figure><h4 id="检查rbdmap的自启动状态"><a href="#检查rbdmap的自启动状态" class="headerlink" title="检查rbdmap的自启动状态"></a>检查rbdmap的自启动状态</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@xenserver]<span class="hljs-comment"># chkconfig --list|grep rbdmap</span><br>rbdmap         0:off1:off2:on3:on4:on5:on6:off<br></code></pre></td></tr></table></figure><h4 id="将rbdmap从自启动删除"><a href="#将rbdmap从自启动删除" class="headerlink" title="将rbdmap从自启动删除"></a>将rbdmap从自启动删除</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@xenserver]<span class="hljs-comment"># chkconfig rbdmap off</span><br></code></pre></td></tr></table></figure><p>这个地方因为xenserver添加存储的时候选择类型为lvm,而系统默认是识别不了rbdmap到本地的那个文件系统的类型的，所以需要修改一点东西：<br>在&#x2F;etc&#x2F;lvm&#x2F;lvm.conf的98行修改如下</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment">#types= [ &quot;nvme&quot;, 64, &quot;mtip32xx&quot;, 64 ]</span><br> types= [ <span class="hljs-string">&quot;nvme&quot;</span>, 64, <span class="hljs-string">&quot;mtip32xx&quot;</span>, 64 , <span class="hljs-string">&quot;rbd&quot;</span>, 64 ]<br></code></pre></td></tr></table></figure><h4 id="查询新加磁盘的uuid（只有格式化了的磁盘才有uuid，格式化完了start-udev）"><a href="#查询新加磁盘的uuid（只有格式化了的磁盘才有uuid，格式化完了start-udev）" class="headerlink" title="查询新加磁盘的uuid（只有格式化了的磁盘才有uuid，格式化完了start_udev）"></a>查询新加磁盘的uuid（只有格式化了的磁盘才有uuid，格式化完了start_udev）</h4><p>用绝对路径的时候可能会出现编号错乱的问题，因为&#x2F;dev&#x2F;rbd*只是一个软链接</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@xenserver ceph]<span class="hljs-comment"># ll /dev/disk/by-uuid/</span><br>total 0<br>lrwxrwxrwx 1 root root 10 Dec 16 11:35 0edeba9a-8b58-463c-bdea-0d46e90dd929 -&gt; ../../rbd1<br>lrwxrwxrwx 1 root root 10 Dec  7 09:23 5170e462-18db-4fc6-a45c-dfc160cb86ee -&gt; ../../sda1<br></code></pre></td></tr></table></figure><h4 id="如果上面的方法做完没有找到uuid"><a href="#如果上面的方法做完没有找到uuid" class="headerlink" title="如果上面的方法做完没有找到uuid"></a>如果上面的方法做完没有找到uuid</h4><p>如果没有找到uuid，那么就用udev进行下控制，流程如下</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">vi /usr/bin/ceph-rbdnamer<br></code></pre></td></tr></table></figure><p>创建一个文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-meta">#!/bin/sh</span><br><br>DEV=<span class="hljs-variable">$1</span><br>NUM=`<span class="hljs-built_in">echo</span> <span class="hljs-variable">$DEV</span> | sed <span class="hljs-string">&#x27;s#p.*##g; s#[a-z]##g&#x27;</span>`<br>POOL=`<span class="hljs-built_in">cat</span> /sys/devices/rbd/<span class="hljs-variable">$NUM</span>/pool`<br>IMAGE=`<span class="hljs-built_in">cat</span> /sys/devices/rbd/<span class="hljs-variable">$NUM</span>/name`<br>SNAP=`<span class="hljs-built_in">cat</span> /sys/devices/rbd/<span class="hljs-variable">$NUM</span>/current_snap`<br><span class="hljs-keyword">if</span> [ <span class="hljs-string">&quot;<span class="hljs-variable">$SNAP</span>&quot;</span> = <span class="hljs-string">&quot;-&quot;</span> ]; <span class="hljs-keyword">then</span><br>    <span class="hljs-built_in">echo</span> -n <span class="hljs-string">&quot;<span class="hljs-variable">$POOL</span> <span class="hljs-variable">$IMAGE</span>&quot;</span><br><span class="hljs-keyword">else</span><br>    <span class="hljs-built_in">echo</span> -n <span class="hljs-string">&quot;<span class="hljs-variable">$POOL</span> <span class="hljs-variable">$IMAGE</span>@<span class="hljs-variable">$SNAP</span>&quot;</span><br><span class="hljs-keyword">fi</span><br></code></pre></td></tr></table></figure><p>给权限</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">chmod</span> 777 /usr/bin/ceph-rbdnamer<br></code></pre></td></tr></table></figure><p>添加规则文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">vi /etc/udev/rules.d/50-rbd.rules <br></code></pre></td></tr></table></figure><p>添加</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">KERNEL==<span class="hljs-string">&quot;rbd[0-9]*&quot;</span>, ENV&#123;DEVTYPE&#125;==<span class="hljs-string">&quot;disk&quot;</span>, PROGRAM=<span class="hljs-string">&quot;/usr/bin/ceph-rbdnamer %k&quot;</span>, SYMLINK+=<span class="hljs-string">&quot;rbd/%c&#123;1&#125;/%c&#123;2&#125;&quot;</span><br>KERNEL==<span class="hljs-string">&quot;rbd[0-9]*&quot;</span>, ENV&#123;DEVTYPE&#125;==<span class="hljs-string">&quot;partition&quot;</span>, PROGRAM=<span class="hljs-string">&quot;/usr/bin/ceph-rbdnamer %k&quot;</span>, SYMLINK+=<span class="hljs-string">&quot;rbd/%c&#123;1&#125;/%c&#123;2&#125;-part%n&quot;</span><br></code></pre></td></tr></table></figure><p>让udev生效</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@xenserver15 rules.d]<span class="hljs-comment">#  /sbin/udevcontrol reload_rules</span><br>[root@xenserver15 rules.d]<span class="hljs-comment"># start_udev</span><br></code></pre></td></tr></table></figure><p>检查</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@xenserver15 rules.d]<span class="hljs-comment"># ll /dev/rbd/rbd/zp</span><br>lrwxrwxrwx 1 root root 10 Dec 29 11:57 /dev/rbd/rbd/zp -&gt; ../../rbd1<br></code></pre></td></tr></table></figure><p>现在用设备的时候就用下面的路径，这个是唯一值</p><blockquote><p>&#x2F;dev&#x2F;rbd&#x2F;rbd&#x2F;zp</p></blockquote><h2 id="添加磁盘rbd到xenserver中"><a href="#添加磁盘rbd到xenserver中" class="headerlink" title="添加磁盘rbd到xenserver中"></a>添加磁盘rbd到xenserver中</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@xenserver ceph]<span class="hljs-comment">#  xe sr-create type=lvm content-type=user device-config:device=/dev/disk/by-uuid/0edeba9a-8b58-463c-bdea-0d46e90dd929 name-label=&quot;ceph storage (rbdtest)&quot;</span><br></code></pre></td></tr></table></figure><p>然后去图形管理界面就可以看到添加的存储了</p><p>到这里就创建好了，这里介绍下删除存储的的操作</p><h4 id="列出pdb模块，找到对应存储的UUID"><a href="#列出pdb模块，找到对应存储的UUID" class="headerlink" title="列出pdb模块，找到对应存储的UUID"></a>列出pdb模块，找到对应存储的UUID</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">xe pbd-list<br></code></pre></td></tr></table></figure><h4 id="找到对应设备的-uuid-卸载对应uuid的存储"><a href="#找到对应设备的-uuid-卸载对应uuid的存储" class="headerlink" title="找到对应设备的 uuid 卸载对应uuid的存储"></a>找到对应设备的 uuid 卸载对应uuid的存储</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@xenserver ceph]<span class="hljs-comment"># xe pbd-unplug uuid=&quot;09b97cda-24ad-0a36-8cf7-f7fb0b61cd55&quot;</span><br></code></pre></td></tr></table></figure><h4 id="列出存储的UUID，找到对应存储的UUID"><a href="#列出存储的UUID，找到对应存储的UUID" class="headerlink" title="列出存储的UUID，找到对应存储的UUID"></a>列出存储的UUID，找到对应存储的UUID</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@xenserver ceph]<span class="hljs-comment"># xe sr-list</span><br></code></pre></td></tr></table></figure><h4 id="删除本地存储连接"><a href="#删除本地存储连接" class="headerlink" title="删除本地存储连接"></a>删除本地存储连接</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@xenserver ceph]<span class="hljs-comment"># xe sr-forget uuid=bb24ee6f-e457-685b-f0b9-fe7c92387042</span><br></code></pre></td></tr></table></figure><h3 id="对于已经挂载的rbd磁盘的信息查询的问题"><a href="#对于已经挂载的rbd磁盘的信息查询的问题" class="headerlink" title="对于已经挂载的rbd磁盘的信息查询的问题"></a>对于已经挂载的rbd磁盘的信息查询的问题</h3><p>如果用的是&#x2F;dev&#x2F;rbd4</p><h4 id="查询存储池"><a href="#查询存储池" class="headerlink" title="查询存储池"></a>查询存储池</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@xenserver ceph]<span class="hljs-comment"># cat /sys/bus/rbd/devices/4/pool_id</span><br></code></pre></td></tr></table></figure><h4 id="查询镜像名称"><a href="#查询镜像名称" class="headerlink" title="查询镜像名称"></a>查询镜像名称</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@xenserver ceph]<span class="hljs-comment"># cat /sys/bus/rbd/devices/4/name</span><br></code></pre></td></tr></table></figure><h2 id="附加知识"><a href="#附加知识" class="headerlink" title="附加知识"></a>附加知识</h2><h3 id="删除重新加进来的过程"><a href="#删除重新加进来的过程" class="headerlink" title="删除重新加进来的过程"></a>删除重新加进来的过程</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@xenserver15 rules.d]<span class="hljs-comment"># xe pbd-list |grep rbd -A 2 -B 5</span><br><br><br>uuid ( RO)                  : 6242d000-567d-257f-d030-7f360855f87f<br>             host-uuid ( RO): 91171862-9954-46e1-9210-3cebb55df395<br>               sr-uuid ( RO): bbfd0739-b0a1-d2e3-986f-10827e4637ea<br>         device-config (MRO): device: /dev/rbd/rbd/zp<br>    currently-attached ( RO): <span class="hljs-literal">true</span><br></code></pre></td></tr></table></figure><p>删除掉pbd</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@xenserver15 rules.d]<span class="hljs-comment"># xe pbd-unplug uuid=6242d000-567d-257f-d030-7f360855f87f</span><br>[root@xenserver15 rules.d]<span class="hljs-comment"># xe pbd-destroy  uuid=6242d000-567d-257f-d030-7f360855f87f</span><br></code></pre></td></tr></table></figure><p>删掉sr(sr的uuid在vg的名称里面包含了)</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@xenserver15 rules.d]<span class="hljs-comment"># xe sr-list |grep rbd -A 4 -B 2</span><br><br>uuid ( RO)                : bbfd0739-b0a1-d2e3-986f-10827e4637ea<br>          name-label ( RW): ceph storage (rbdtest)<br>    name-description ( RW): <br>                host ( RO): &lt;not <span class="hljs-keyword">in</span> database&gt;<br>                <span class="hljs-built_in">type</span> ( RO): lvm<br>        content-type ( RO): user<br><br>[root@xenserver15 rules.d]<span class="hljs-comment"># xe sr-forget uuid=bbfd0739-b0a1-d2e3-986f-10827e4637ea</span><br></code></pre></td></tr></table></figure><p>重新加入sr</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@xenserver15 rules.d]<span class="hljs-comment"># xe sr-introduce uuid=bbfd0739-b0a1-d2e3-986f-10827e4637ea  type=lvm content-type=user device-config:device=/dev/rbd/rbd/zp  name-label=&quot;ceph storage (rbdtest)&quot;</span><br>bbfd0739-b0a1-d2e3-986f-10827e4637ea<br></code></pre></td></tr></table></figure><p>查询主机的uuid</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@xenserver15 rules.d]<span class="hljs-comment"># xe host-list </span><br>uuid ( RO)                : 91171862-9954-46e1-9210-3cebb55df395<br>          name-label ( RW): xenserver1.5<br>    name-description ( RW): Default install of XenServer<br><br></code></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@xenserver15 rules.d]<span class="hljs-comment"># xe pbd-create   sr-uuid=bbfd0739-b0a1-d2e3-986f-10827e4637ea device-config:device=/dev/rbd/rbd/zp host-uuid=91171862-9954-46e1-9210-3cebb55df395</span><br>51a40dd6-2b91-7590-8a81-04d129aed1ea<br></code></pre></td></tr></table></figure><p>加载存储</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">xe pbd-plug uuid=51a40dd6-2b91-7590-8a81-04d129aed1ea<br></code></pre></td></tr></table></figure><p>然后在前台repair下就可以了</p><h2 id="附加"><a href="#附加" class="headerlink" title="附加"></a>附加</h2><p>xenserver7升级了lvm</p><h3 id="增加lvm的识别"><a href="#增加lvm的识别" class="headerlink" title="增加lvm的识别"></a>增加lvm的识别</h3><p>需要识别rbd的话需要修改下面两个文件</p><blockquote><p>&#x2F;etc&#x2F;lvm&#x2F;lvm.conf和&#x2F;etc&#x2F;lvm&#x2F;master&#x2F;lvm.conf</p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># Configuration option devices/types.</span><br><span class="hljs-comment"># List of additional acceptable block device types.</span><br><span class="hljs-comment"># These are of device type names from /proc/devices, followed by the</span><br><span class="hljs-comment"># maximum number of partitions.</span><br><span class="hljs-comment"># </span><br><span class="hljs-comment"># Example</span><br><span class="hljs-comment"># types = [ &quot;fd&quot;, 16 ]</span><br><span class="hljs-comment"># </span><br><span class="hljs-comment"># This configuration option is advanced.</span><br><span class="hljs-comment"># This configuration option does not have a default value defined.</span><br></code></pre></td></tr></table></figure><p>修改为</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># Configuration option devices/types.</span><br><span class="hljs-comment"># List of additional acceptable block device types.</span><br><span class="hljs-comment"># These are of device type names from /proc/devices, followed by the</span><br><span class="hljs-comment"># maximum number of partitions.</span><br><span class="hljs-comment"># </span><br><span class="hljs-comment"># Example</span><br>types = [ <span class="hljs-string">&quot;rbd&quot;</span>, 64 ]<br><span class="hljs-comment"># </span><br><span class="hljs-comment"># This configuration option is advanced.</span><br><span class="hljs-comment"># This configuration option does not have a default value defined.</span><br></code></pre></td></tr></table></figure><p>执行命令</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">sed -i  <span class="hljs-string">&quot;s/metadata_read_only =.*/metadata_read_only = 0/g&quot;</span> /etc/lvm/master/lvm.conf<br>sed -i  <span class="hljs-string">&quot;s/metadata_read_only =.*/metadata_read_only = 0/g&quot;</span> /etc/lvm/lvm.conf<br></code></pre></td></tr></table></figure><h3 id="创建sr"><a href="#创建sr" class="headerlink" title="创建sr"></a>创建sr</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">xe sr-create <span class="hljs-built_in">type</span>=lvm content-type=user device-config:device=/dev/rbd1 name-label=<span class="hljs-string">&quot;ceph&quot;</span><br></code></pre></td></tr></table></figure><p>然后就可以用了</p><h3 id="相关文件路径"><a href="#相关文件路径" class="headerlink" title="相关文件路径"></a>相关文件路径</h3><p>&#x2F;var&#x2F;log&#x2F;SMlog 存储创建日志路径<br>&#x2F;opt&#x2F;xensource&#x2F;sm&#x2F;lvutil.py  创建lv的代码</p><h2 id="变更记录"><a href="#变更记录" class="headerlink" title="变更记录"></a>变更记录</h2><table><thead><tr><th align="left">Why</th><th align="center">Who</th><th align="center">When</th></tr></thead><tbody><tr><td align="left">创建</td><td align="center">武汉-运维-磨渣</td><td align="center">2015-12-16</td></tr><tr><td align="left">增加 touch  &#x2F;var&#x2F;lock&#x2F;subsys&#x2F;rbdmap</td><td align="center">武汉-运维-磨渣</td><td align="center">2016-05-10</td></tr><tr><td align="left">增加xenserver7lvm支持</td><td align="center">武汉-运维-磨渣</td><td align="center">2017-02-07</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>如何在所有的mon的损坏情况下将数据恢复如初</title>
    <link href="/2015/12/13/%E5%A6%82%E4%BD%95%E5%9C%A8%E6%89%80%E6%9C%89%E7%9A%84mon%E7%9A%84%E6%8D%9F%E5%9D%8F%E6%83%85%E5%86%B5%E4%B8%8B%E5%B0%86%E6%95%B0%E6%8D%AE%E6%81%A2%E5%A4%8D%E5%A6%82%E5%88%9D/"/>
    <url>/2015/12/13/%E5%A6%82%E4%BD%95%E5%9C%A8%E6%89%80%E6%9C%89%E7%9A%84mon%E7%9A%84%E6%8D%9F%E5%9D%8F%E6%83%85%E5%86%B5%E4%B8%8B%E5%B0%86%E6%95%B0%E6%8D%AE%E6%81%A2%E5%A4%8D%E5%A6%82%E5%88%9D/</url>
    
    <content type="html"><![CDATA[<h2 id="本篇主题"><a href="#本篇主题" class="headerlink" title="本篇主题"></a>本篇主题</h2><p>在mon无法启动，或者所有的mon的数据盘都损坏的情况下，如何把所有的数据恢复如初</p><h2 id="写本章的缘由"><a href="#写本章的缘由" class="headerlink" title="写本章的缘由"></a>写本章的缘由</h2><p>在ceph中国的群里有看到一个技术人员有提到，在一次意外机房掉电后，三台mon的系统盘同时损坏了，这个对于熟悉ceph的人都知道这意味着什么，所有的集群数据将无法访问，关于这个的解决办法目前没有在哪里有看到，这个对于大多数人是用不上的，但是一旦出现了，这个损失将是无法估量的，当然谁都不希望这个情况的发生</p><p>所以在研究了下ceph的一些操作后，自己尝试去找了一些关于集群的故障修复的，目前看到了一个是关于单个rbd镜像的恢复的文章，那个需要将数据映射本地的loop设备后重新读取，这个我曾经验证过一遍，确实可以实现，在周末的时候我尝试了另外一个办法，实现了在mon完全失效的情况下全集群的完整数据的恢复，并且保留了原来的数据结构和数据信息，当然这中间需要进行一定的操作去完成它，这个我准备写成一个标准的操作流程，并用视频的方式来记录这个恢复的流程</p>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>ceph集群的安装和配置教程</title>
    <link href="/2015/12/12/ceph%E9%9B%86%E7%BE%A4%E7%9A%84%E5%AE%89%E8%A3%85%E5%92%8C%E9%85%8D%E7%BD%AE%E6%95%99%E7%A8%8B/"/>
    <url>/2015/12/12/ceph%E9%9B%86%E7%BE%A4%E7%9A%84%E5%AE%89%E8%A3%85%E5%92%8C%E9%85%8D%E7%BD%AE%E6%95%99%E7%A8%8B/</url>
    
    <content type="html"><![CDATA[<h3 id="本篇主题："><a href="#本篇主题：" class="headerlink" title="本篇主题："></a>本篇主题：</h3><ul><li><p>1、怎样配置ssh免登陆访问</p></li><li><p>2、为什么搭建集群要关闭防火墙和selinux，如何关闭</p></li><li><p>3、从哪里获取ceph的安装包，怎样安装才是快速正确的</p></li><li><p>4、为什么要配置时间同步服务，怎样配置</p></li><li><p>5、怎样创建集群</p></li><li><p>6、怎样使用不同的方式增加osd（这里我总结了五种）</p><ul><li><pre><code class="hljs">默认方式</code></pre></li><li><pre><code class="hljs">磁盘journal</code></pre></li><li><pre><code class="hljs">目录配置方式</code></pre></li><li><pre><code class="hljs">btrfs文件系统</code></pre></li><li><pre><code class="hljs">disk+ssd方式</code></pre></li></ul></li><li><p>7、怎样配置文件系统</p></li><li><p>8、怎样配置块设备系统</p></li><li><p>9、怎样配置S3服务</p></li><li><p>10、如何干净的将集群清理到初始状态</p></li></ul><blockquote><p>（本篇基于centos7,其他系统除了安装方式其他通用）</p></blockquote><h3 id="本篇资源"><a href="#本篇资源" class="headerlink" title="本篇资源:"></a>本篇资源:</h3><p>包括了视频，操作文档，相关安装包资源，S3的windows客户端</p><p>付费资源有需要联系</p>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>在线调整ceph的参数</title>
    <link href="/2015/11/18/%E5%9C%A8%E7%BA%BF%E8%B0%83%E6%95%B4ceph%E7%9A%84%E5%8F%82%E6%95%B0/"/>
    <url>/2015/11/18/%E5%9C%A8%E7%BA%BF%E8%B0%83%E6%95%B4ceph%E7%9A%84%E5%8F%82%E6%95%B0/</url>
    
    <content type="html"><![CDATA[<p>能够动态的进行系统参数的调整是一个很重要并且有用的属性<br>ceph的集群提供两种方式的调整，使用tell的方式和daemon设置的方式</p><h3 id="一、tell方式设置"><a href="#一、tell方式设置" class="headerlink" title="一、tell方式设置"></a>一、tell方式设置</h3><p>调整配置使用命令：</p><h4 id="调整mon的参数"><a href="#调整mon的参数" class="headerlink" title="调整mon的参数"></a>调整mon的参数</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment">#ceph tell mon.* injectargs &#x27;--&#123;tunable value_to_be_set&#125;&#x27;</span><br></code></pre></td></tr></table></figure><h4 id="调整osd的参数"><a href="#调整osd的参数" class="headerlink" title="调整osd的参数"></a>调整osd的参数</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment">#ceph tell osd.* injectargs &#x27;--&#123;tunable value_to_be_set&#125;&#x27;</span><br></code></pre></td></tr></table></figure><h3 id="调整mds的参数"><a href="#调整mds的参数" class="headerlink" title="调整mds的参数"></a>调整mds的参数</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment">#ceph tell mds.* injectargs &#x27;--&#123;tunable value_to_be_set&#125;&#x27;</span><br></code></pre></td></tr></table></figure><p>例子：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab5134 ~]<span class="hljs-comment"># ceph tell mon.* injectargs &#x27;--mon_osd_report_timeout 400&#x27;</span><br>injectargs:mon_osd_report_timeout = <span class="hljs-string">&#x27;400&#x27;</span><br></code></pre></td></tr></table></figure><p>除了上面的tell的方式调整，还可以使用daemon的方式进行设置</p><h3 id="二、daemon方式设置"><a href="#二、daemon方式设置" class="headerlink" title="二、daemon方式设置"></a>二、daemon方式设置</h3><h4 id="1、获取当前的参数"><a href="#1、获取当前的参数" class="headerlink" title="1、获取当前的参数"></a>1、获取当前的参数</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab5134 ~]<span class="hljs-comment"># ceph daemon osd.1 config get mon_osd_full_ratio</span><br>&#123;<br>    <span class="hljs-string">&quot;mon_osd_full_ratio&quot;</span>: <span class="hljs-string">&quot;0.98&quot;</span><br>&#125;<br></code></pre></td></tr></table></figure><h4 id="2、修改配置"><a href="#2、修改配置" class="headerlink" title="2、修改配置"></a>2、修改配置</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab5134 ~]<span class="hljs-comment"># ceph daemon osd.1 config set mon_osd_full_ratio 0.97</span><br>&#123;<br>    <span class="hljs-string">&quot;success&quot;</span>: <span class="hljs-string">&quot;mon_osd_full_ratio = &#x27;0.97&#x27; &quot;</span><br>&#125;<br></code></pre></td></tr></table></figure><h4 id="3、检查配置"><a href="#3、检查配置" class="headerlink" title="3、检查配置"></a>3、检查配置</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab5134 ~]<span class="hljs-comment"># ceph daemon osd.1 config get mon_osd_full_ratio</span><br>&#123;<br>    <span class="hljs-string">&quot;mon_osd_full_ratio&quot;</span>: <span class="hljs-string">&quot;0.97&quot;</span><br>&#125;<br></code></pre></td></tr></table></figure><p>注意重启进程后配置会恢复到默认参数，在进行在线调整后，如果这个参数是后续是需要使用的，那么就需要将相关的参数写入到配置文件ceph.conf当中</p><h3 id="两种设置的使用场景"><a href="#两种设置的使用场景" class="headerlink" title="两种设置的使用场景"></a>两种设置的使用场景</h3><p>使用tell的方式适合对整个集群进行设置，使用*号进行匹配，就可以对整个集群的角色进行设置，而出现节点异常无法设置时候，只会在命令行当中进行报错，不太便于查找</p><p>使用daemon进行设置的方式就是一个个的去设置，这样可以比较好的反馈，这个设置是需要在设置的角色所在的主机上进行设置，daemon的方式还提供通过asok去获取到进行的其他的信息，可以使用 ceph daemon osd.1 help去查询相关的可以使用的命令</p>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>验证rbd的缓存是否开启</title>
    <link href="/2015/11/16/%E9%AA%8C%E8%AF%81rbd%E7%9A%84%E7%BC%93%E5%AD%98%E6%98%AF%E5%90%A6%E5%BC%80%E5%90%AF/"/>
    <url>/2015/11/16/%E9%AA%8C%E8%AF%81rbd%E7%9A%84%E7%BC%93%E5%AD%98%E6%98%AF%E5%90%A6%E5%BC%80%E5%90%AF/</url>
    
    <content type="html"><![CDATA[<p>简单快速的在客户端验证rbd的cache是否开启<br>首先修改配置文件</p><p>在ceph.conf中添加：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">[client]<br>rbd cache = <span class="hljs-literal">true</span><br>rbd cache writethrough <span class="hljs-keyword">until</span> flush = <span class="hljs-literal">true</span><br></code></pre></td></tr></table></figure><p>开启以后，在这台机器上进行测试</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@node8109 ~]<span class="hljs-comment"># rbd -p rbd bench-write zp  --io-size 4096 --io-threads 256  --io-total 102400000   --io-pattern seq</span><br>bench-write  io_size 4096 io_threads 256 bytes 102400000 pattern <span class="hljs-built_in">seq</span><br>  SEC       OPS   OPS/SEC   BYTES/SEC<br>elapsed:     0  ops:    25000  ops/sec: 26830.05  bytes/sec: 109895890.09<br></code></pre></td></tr></table></figure><p>可以看到io达到了26830每秒</p><p>下面进行关闭后再测试：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@node8109 ~]<span class="hljs-comment"># rbd -p rbd bench-write zp  --io-size 4096 --io-threads 256  --io-total 102400000   --io-pattern seq</span><br>bench-write  io_size 4096 io_threads 256 bytes 102400000 pattern <span class="hljs-built_in">seq</span><br>  SEC       OPS   OPS/SEC   BYTES/SEC<br>    1       893   1076.16  4407933.78<br>    2      1344    795.81  3259636.35<br>    3      1794    655.20  2683695.60<br>    4      2198    613.23  2511789.77<br></code></pre></td></tr></table></figure><p>可以看到大概只有600左右的IOPS</p><p>结论：</p><p>开启和关闭cache的差别还是很大的，可以通过上面简单的测试来验证rbd的cache是否开启</p>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>如何删除一台OSD主机</title>
    <link href="/2015/11/12/%E5%A6%82%E4%BD%95%E5%88%A0%E9%99%A4%E4%B8%80%E5%8F%B0OSD%E4%B8%BB%E6%9C%BA/"/>
    <url>/2015/11/12/%E5%A6%82%E4%BD%95%E5%88%A0%E9%99%A4%E4%B8%80%E5%8F%B0OSD%E4%B8%BB%E6%9C%BA/</url>
    
    <content type="html"><![CDATA[<p>在ceph的一台OSD主机出现故障的时候，数据可以通过副本的机制进行恢复，之后通过删除osd的操作也能够将故障osd从osd tree当中删除掉，但是故障的 osd 的主机仍然会留在集群当中，通过 ceph osd tree 或者打印 crush map 都可以看到这个损坏的节点主机名，所以这里讲下怎么删除掉这个无用的host</p><p>首先集群环境为两台主机 node8109 node8110 , node8110主机出现故障需要清理掉</p><h4 id="先看下当前的osd-tree状态"><a href="#先看下当前的osd-tree状态" class="headerlink" title="先看下当前的osd tree状态"></a>先看下当前的osd tree状态</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@node8109 webui]<span class="hljs-comment"># ceph osd tree</span><br>ID WEIGHT  TYPE NAME             UP/DOWN REWEIGHT PRIMARY-AFFINITY <br>-1 4.00000 root default                                            <br>-3 4.00000     rack localrack                                      <br>-2 2.00000         host node8109                                   <br> 0 1.00000             osd.0          up  1.00000          1.00000 <br> 1 1.00000             osd.1          up  1.00000          1.00000 <br>-4 2.00000         host node8110                                   <br> 2 1.00000             osd.2        down  1.00000          1.00000 <br> 3 1.00000             osd.3        down  1.00000          1.00000 <br></code></pre></td></tr></table></figure><h4 id="查看osd的状态"><a href="#查看osd的状态" class="headerlink" title="查看osd的状态"></a>查看osd的状态</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@node8109 webui]<span class="hljs-comment"># ceph osd stat</span><br>     osdmap e66: 4 osds: 2 up, 4 <span class="hljs-keyword">in</span>; 52 remapped pgs<br></code></pre></td></tr></table></figure><h4 id="首先out掉osd"><a href="#首先out掉osd" class="headerlink" title="首先out掉osd"></a>首先out掉osd</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@node8109 ~]<span class="hljs-comment"># ceph osd out 2</span><br>marked out osd.2. <br>[root@node8109 ~]<span class="hljs-comment"># ceph osd out 3</span><br>marked out osd.3. <br></code></pre></td></tr></table></figure><h4 id="从crush里面删除osd"><a href="#从crush里面删除osd" class="headerlink" title="从crush里面删除osd"></a>从crush里面删除osd</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@node8109 ~]<span class="hljs-comment"># ceph osd crush remove osd.2</span><br>removed item <span class="hljs-built_in">id</span> 2 name <span class="hljs-string">&#x27;osd.2&#x27;</span> from crush map<br>[root@node8109 ~]<span class="hljs-comment"># ceph osd crush remove osd.3</span><br>removed item <span class="hljs-built_in">id</span> 3 name <span class="hljs-string">&#x27;osd.3&#x27;</span> from crush map<br></code></pre></td></tr></table></figure><h4 id="从集群中删除OSD"><a href="#从集群中删除OSD" class="headerlink" title="从集群中删除OSD"></a>从集群中删除OSD</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@node8109 ~]<span class="hljs-comment"># ceph osd rm osd.3</span><br>removed osd.3<br>[root@node8109 ~]<span class="hljs-comment"># ceph osd rm osd.2</span><br>removed osd.2<br></code></pre></td></tr></table></figure><h4 id="从集群认证里面删除osd"><a href="#从集群认证里面删除osd" class="headerlink" title="从集群认证里面删除osd"></a>从集群认证里面删除osd</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@node8109 ~]<span class="hljs-comment"># ceph auth del osd.2</span><br>updated<br>[root@node8109 ~]<span class="hljs-comment"># ceph auth del osd.3</span><br>updated<br></code></pre></td></tr></table></figure><p>查看当前的crush map</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs bash">root@node8109 ~]<span class="hljs-comment"># ceph osd tree</span><br>ID WEIGHT  TYPE NAME             UP/DOWN REWEIGHT PRIMARY-AFFINITY <br>-1 2.00000 root default                                            <br>-3 2.00000     rack localrack                                      <br>-2 2.00000         host node8109                                   <br> 0 1.00000             osd.0          up  1.00000          1.00000 <br> 1 1.00000             osd.1          up  1.00000          1.00000 <br>-4       0         host node8110<br></code></pre></td></tr></table></figure><p>下面有两种方法从osd tree 删除掉node8110,为命令方式和修改crush map 方式</p><h3 id="方式一：命令方式"><a href="#方式一：命令方式" class="headerlink" title="方式一：命令方式"></a>方式一：命令方式</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@node8109 ~]<span class="hljs-comment"># ceph osd crush remove node8110</span><br>removed item <span class="hljs-built_in">id</span> -4 name <span class="hljs-string">&#x27;node8110&#x27;</span> from crush map<br>[root@node8109 ~]<span class="hljs-comment"># ceph osd tree</span><br>ID WEIGHT  TYPE NAME             UP/DOWN REWEIGHT PRIMARY-AFFINITY <br>-1 2.00000 root default                                            <br>-3 2.00000     rack localrack                                      <br>-2 2.00000         host node8109                                   <br> 0 1.00000             osd.0          up  1.00000          1.00000 <br> 1 1.00000             osd.1          up  1.00000          1.00000<br></code></pre></td></tr></table></figure><h3 id="方式二：通过修改-crush-map-的方式"><a href="#方式二：通过修改-crush-map-的方式" class="headerlink" title="方式二：通过修改 crush map 的方式"></a>方式二：通过修改 crush map 的方式</h3><h4 id="导出当前的crush-map"><a href="#导出当前的crush-map" class="headerlink" title="导出当前的crush map"></a>导出当前的crush map</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@node8109 ~]<span class="hljs-comment">#ceph osd getcrushmap -o crushmap.txt</span><br>[root@node8109 ~]<span class="hljs-comment">#crushtool -d crushmap.txt -o crushmap-decompile</span><br>[root@node8109 ~]<span class="hljs-comment"># vim crushmap-decompile </span><br></code></pre></td></tr></table></figure><h4 id="删除掉node8109相关的信息"><a href="#删除掉node8109相关的信息" class="headerlink" title="删除掉node8109相关的信息"></a>删除掉node8109相关的信息</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@node8109 ~]<span class="hljs-comment">#crushtool -c crushmap-decompile  -o crushmap-compile</span><br>[root@node8109 ~]<span class="hljs-comment"># ceph osd setcrushmap -i crushmap-compile </span><br><span class="hljs-built_in">set</span> crush map<br>[root@node8109 ~]<span class="hljs-comment"># ceph osd tree</span><br>ID WEIGHT  TYPE NAME             UP/DOWN REWEIGHT PRIMARY-AFFINITY <br>-1 2.00000 root default                                            <br>-3 2.00000     rack localrack                                      <br>-2 2.00000         host node8109                                   <br> 0 1.00000             osd.0          up  1.00000          1.00000 <br> 1 1.00000             osd.1          up  1.00000          1.00000<br></code></pre></td></tr></table></figure><h3 id="总结："><a href="#总结：" class="headerlink" title="总结："></a>总结：</h3><p>从上面的两种方式可以看出，使用命令的方式更为简单直接，而修改crush map的方式需要去做修改的操作，有一定的修改错误的风险，所以在做crush map的相关操作的时候，建议是能用命令方式做的就用命令方式去做操作</p>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Mon失效处理方法</title>
    <link href="/2015/11/01/Mon%E5%A4%B1%E6%95%88%E5%A4%84%E7%90%86%E6%96%B9%E6%B3%95/"/>
    <url>/2015/11/01/Mon%E5%A4%B1%E6%95%88%E5%A4%84%E7%90%86%E6%96%B9%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<p>假设环境为三个mon，主机名为mon1、mon2、mon3，现在mon3上面的系统盘损坏，mon的数据完全丢失，现在需要对mon3进行恢复处理</p><p>1、停止所有mon进程，可以不停其他进程，需要停止内核客户端以及对外的服务，防止卡死<br>在mon1机器上执行</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">/etc/init.d/ceph stop mon<br></code></pre></td></tr></table></figure><p>在mon2机器上执行</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">/etc/init.d/ceph stop mon<br></code></pre></td></tr></table></figure><p>2、分别在mon主机上获取当前的monmap<br>在mon1机器上执行</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">ceph-mon -i mon1 --extract-monmap /tmp/monmap<br></code></pre></td></tr></table></figure><p>备份原始monmap</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">cp</span> /tmp/monmap /tmp/monmapbk<br></code></pre></td></tr></table></figure><p>在mon2机器上执行</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">ceph-mon -i mon2 --extract-monmap /tmp/monmap<br></code></pre></td></tr></table></figure><p>备份原始monmap</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">cp</span> /tmp/monmap /tmp/monmapbk<br></code></pre></td></tr></table></figure><p>3、处理monmap，去掉损坏的mon3的map信息<br>在mon1机器上执行</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">monmaptool /tmp/monmap --<span class="hljs-built_in">rm</span> mon3<br></code></pre></td></tr></table></figure><p>在mon2机器上执行</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">monmaptool /tmp/monmap --<span class="hljs-built_in">rm</span> mon3<br></code></pre></td></tr></table></figure><p>4、导入修改后的monmap信息<br>在mon1机器上执行</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">ceph-mon -i mon1 --inject-monmap /tmp/monmap<br></code></pre></td></tr></table></figure><p>在mon2机器上执行</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">ceph-mon -i mon2 --inject-monmap /tmp/monmap<br></code></pre></td></tr></table></figure><p>5、启动mon进程<br>在mon1机器上执行</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">/etc/init.d/ceph start mon<br></code></pre></td></tr></table></figure><p>在mon2机器上执行</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">/etc/init.d/ceph start mon<br></code></pre></td></tr></table></figure><p>6、检查当前的mon信息，应该显示的是只有两个mon,再新加mon即可</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">ceph -s<br></code></pre></td></tr></table></figure><hr><p>写于：2015年11月1日</p>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>查询osd上的pg数</title>
    <link href="/2015/10/14/%E6%9F%A5%E8%AF%A2osd%E4%B8%8A%E7%9A%84pg%E6%95%B0/"/>
    <url>/2015/10/14/%E6%9F%A5%E8%AF%A2osd%E4%B8%8A%E7%9A%84pg%E6%95%B0/</url>
    
    <content type="html"><![CDATA[<p>本文中的命令的第一版来源于国外的一个博客，后面的版本为我自己修改的版本</p><!--break--><p>查询的命令如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs bash">ceph pg dump | awk <span class="hljs-string">&#x27;</span><br><span class="hljs-string"> /^pg_stat/ &#123; col=1; while($col!=&quot;up&quot;) &#123;col++&#125;; col++ &#125;</span><br><span class="hljs-string"> /^[0-9a-f]+\.[0-9a-f]+/ &#123; match($0,/^[0-9a-f]+/); pool=substr($0, RSTART, RLENGTH); poollist[pool]=0;</span><br><span class="hljs-string"> up=$col; i=0; RSTART=0; RLENGTH=0; delete osds; while(match(up,/[0-9]+/)&gt;0) &#123; osds[++i]=substr(up,RSTART,RLENGTH); up = substr(up, RSTART+RLENGTH) &#125;</span><br><span class="hljs-string"> for(i in osds) &#123;array[osds[i],pool]++; osdlist[osds[i]];&#125;</span><br><span class="hljs-string">&#125;</span><br><span class="hljs-string">END &#123;</span><br><span class="hljs-string"> printf(&quot;\n&quot;);</span><br><span class="hljs-string"> printf(&quot;pool :\t&quot;); for (i in poollist) printf(&quot;%s\t&quot;,i); printf(&quot;| SUM \n&quot;);</span><br><span class="hljs-string"> for (i in poollist) printf(&quot;--------&quot;); printf(&quot;----------------\n&quot;);</span><br><span class="hljs-string"> for (i in osdlist) &#123; printf(&quot;osd.%i\t&quot;, i); sum=0;</span><br><span class="hljs-string"> for (j in poollist) &#123; printf(&quot;%i\t&quot;, array[i,j]); sum+=array[i,j]; poollist[j]+=array[i,j] &#125;; printf(&quot;| %i\n&quot;,sum) &#125;</span><br><span class="hljs-string"> for (i in poollist) printf(&quot;--------&quot;); printf(&quot;----------------\n&quot;);</span><br><span class="hljs-string"> printf(&quot;SUM :\t&quot;); for (i in poollist) printf(&quot;%s\t&quot;,poollist[i]); printf(&quot;|\n&quot;);</span><br><span class="hljs-string">&#125;&#x27;</span><br></code></pre></td></tr></table></figure><p>默认的输出如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs bash"><br>pool :012| SUM <br>----------------------------------------<br>osd.45413379| 266<br>osd.55710488| 249<br>osd.66113286| 279<br>osd.75411485| 253<br>osd.86312385| 271<br>osd.06212087| 269<br>osd.15212681| 259<br>osd.25210388| 243<br>osd.35712589| 271<br>----------------------------------------<br>SUM :5121080768|<br><br></code></pre></td></tr></table></figure><p>这个有个问题就是osd是乱序的，并且对于一个存储池来说不清楚哪个osd的pg是最多的</p><h3 id="重构第一版："><a href="#重构第一版：" class="headerlink" title="重构第一版："></a>重构第一版：</h3><p>跟上面的相比按顺序来排列</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs bash">ceph pg dump | awk <span class="hljs-string">&#x27;</span><br><span class="hljs-string"> /^pg_stat/ &#123; col=1; while($col!=&quot;up&quot;) &#123;col++&#125;; col++ &#125;</span><br><span class="hljs-string"> /^[0-9a-f]+\.[0-9a-f]+/ &#123; match($0,/^[0-9a-f]+/); pool=substr($0, RSTART, RLENGTH); poollist[pool]=0;</span><br><span class="hljs-string"> up=$col; i=0; RSTART=0; RLENGTH=0; delete osds; while(match(up,/[0-9]+/)&gt;0) &#123; osds[++i]=substr(up,RSTART,RLENGTH); up = substr(up, RSTART+RLENGTH) &#125;</span><br><span class="hljs-string"> for(i in osds) &#123;array[osds[i],pool]++; osdlist[osds[i]];&#125;</span><br><span class="hljs-string">&#125;</span><br><span class="hljs-string">END &#123;</span><br><span class="hljs-string"> printf(&quot;\n&quot;);</span><br><span class="hljs-string"> slen=asorti(poollist,newpoollist);</span><br><span class="hljs-string"> printf(&quot;pool :\t&quot;);for (i=1;i&lt;=slen;i++) &#123;printf(&quot;%s\t&quot;, newpoollist[i])&#125;; printf(&quot;| SUM \n&quot;);</span><br><span class="hljs-string"> for (i in poollist) printf(&quot;--------&quot;); printf(&quot;----------------\n&quot;);</span><br><span class="hljs-string"> slen1=asorti(osdlist,newosdlist)</span><br><span class="hljs-string"> delete poollist;</span><br><span class="hljs-string"> for (i=1;i&lt;=slen1;i++) &#123; printf(&quot;osd.%i\t&quot;, newosdlist[i]); sum=0; </span><br><span class="hljs-string"> for (j=1;j&lt;=slen;j++)  &#123; printf(&quot;%i\t&quot;, array[newosdlist[i],newpoollist[j]]); sum+=array[newosdlist[i],newpoollist[j]]; poollist[j]+=array[newosdlist[i],newpoollist[j]] &#125;; printf(&quot;| %i\n&quot;,sum)</span><br><span class="hljs-string">&#125; </span><br><span class="hljs-string">for (i in poollist) printf(&quot;--------&quot;); printf(&quot;----------------\n&quot;);</span><br><span class="hljs-string"> printf(&quot;SUM :\t&quot;); for (i=1;i&lt;=slen;i++) printf(&quot;%s\t&quot;,poollist[i]); printf(&quot;|\n&quot;);</span><br><span class="hljs-string">&#125;&#x27;</span><br></code></pre></td></tr></table></figure><p>输出结果为下面的，可以看到现在是按顺序来的，存储池是顺序的，osd编号也是顺序的</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs bash"><br>pool :012| SUM <br>----------------------------------------<br>osd.06212087| 269<br>osd.15212681| 259<br>osd.25210388| 243<br>osd.35712589| 271<br>osd.45413379| 266<br>osd.55710488| 249<br>osd.66113286| 279<br>osd.75411485| 253<br>osd.86312385| 271<br>----------------------------------------<br>SUM :5121080768|<br></code></pre></td></tr></table></figure><h3 id="重构第二版："><a href="#重构第二版：" class="headerlink" title="重构第二版："></a>重构第二版：</h3><p>包含osd pool的排序，包含osd的排序，输出平均pg数目，输出最大的osd编号，输出超过平均值的百分比</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs bash">ceph pg dump | awk <span class="hljs-string">&#x27;</span><br><span class="hljs-string"> /^pg_stat/ &#123; col=1; while($col!=&quot;up&quot;) &#123;col++&#125;; col++ &#125;</span><br><span class="hljs-string"> /^[0-9a-f]+\.[0-9a-f]+/ &#123; match($0,/^[0-9a-f]+/); pool=substr($0, RSTART, RLENGTH); poollist[pool]=0;</span><br><span class="hljs-string"> up=$col; i=0; RSTART=0; RLENGTH=0; delete osds; while(match(up,/[0-9]+/)&gt;0) &#123; osds[++i]=substr(up,RSTART,RLENGTH); up = substr(up, RSTART+RLENGTH) &#125;</span><br><span class="hljs-string"> for(i in osds) &#123;array[osds[i],pool]++; osdlist[osds[i]];&#125;</span><br><span class="hljs-string">&#125;</span><br><span class="hljs-string">END &#123;</span><br><span class="hljs-string"> printf(&quot;\n&quot;);</span><br><span class="hljs-string"> slen=asorti(poollist,newpoollist);</span><br><span class="hljs-string"> printf(&quot;pool :\t&quot;);for (i=1;i&lt;=slen;i++) &#123;printf(&quot;%s\t&quot;, newpoollist[i])&#125;; printf(&quot;| SUM \n&quot;);</span><br><span class="hljs-string"> for (i in poollist) printf(&quot;--------&quot;); printf(&quot;----------------\n&quot;);</span><br><span class="hljs-string"> slen1=asorti(osdlist,newosdlist)</span><br><span class="hljs-string"> delete poollist;</span><br><span class="hljs-string"> for (i=1;i&lt;=slen1;i++) &#123; printf(&quot;osd.%i\t&quot;, newosdlist[i]); sum=0; </span><br><span class="hljs-string"> for (j=1;j&lt;=slen;j++)  &#123; printf(&quot;%i\t&quot;, array[newosdlist[i],newpoollist[j]]); sum+=array[newosdlist[i],newpoollist[j]]; poollist[j]+=array[newosdlist[i],newpoollist[j]];if(array[newosdlist[i],newpoollist[j]] != 0)&#123;poolhasid[j]+=1 &#125;;if(array[newosdlist[i],newpoollist[j]]&gt;maxpoolosd[j])&#123;maxpoolosd[j]=array[newosdlist[i],newpoollist[j]];maxosdid[j]=newosdlist[i]&#125;&#125;; printf(&quot;| %i\n&quot;,sum)&#125; </span><br><span class="hljs-string">for (i in poollist) printf(&quot;--------&quot;); printf(&quot;----------------\n&quot;);</span><br><span class="hljs-string"> printf(&quot;SUM :\t&quot;); for (i=1;i&lt;=slen;i++) printf(&quot;%s\t&quot;,poollist[i]); printf(&quot;|\n&quot;);</span><br><span class="hljs-string"> printf(&quot;AVE :\t&quot;); for (i=1;i&lt;=slen;i++) printf(&quot;%d\t&quot;,poollist[i]/poolhasid[i]); printf(&quot;|\n&quot;);</span><br><span class="hljs-string"> printf(&quot;max :\t&quot;); for (i=1;i&lt;=slen;i++) printf(&quot;%s\t&quot;,maxpoolosd[i]); printf(&quot;|\n&quot;);</span><br><span class="hljs-string"> printf(&quot;osdid :\t&quot;); for (i=1;i&lt;=slen;i++) printf(&quot;osd.%s\t&quot;,maxosdid[i]); printf(&quot;|\n&quot;);</span><br><span class="hljs-string"> printf(&quot;per:\t&quot;); for (i=1;i&lt;=slen;i++) printf(&quot;%.1f%\t&quot;,100*(maxpoolosd[i]-poollist[i]/poolhasid[i])/(poollist[i]/poolhasid[i])); printf(&quot;|\n&quot;);</span><br><span class="hljs-string">&#125;&#x27;</span><br></code></pre></td></tr></table></figure><p>输出如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs bash">pool :012| SUM <br>----------------------------------------<br>osd.06212087| 269<br>osd.15212681| 259<br>osd.25210388| 243<br>osd.35712589| 271<br>osd.45413379| 266<br>osd.55710488| 249<br>osd.66113286| 279<br>osd.75411485| 253<br>osd.86312385| 271<br>----------------------------------------<br>SUM :5121080768|<br>AVE :5612085|<br>max :6313389|<br>osdid :osd.8osd.4osd.3|<br>per:10.7%10.8%4.3%|<br></code></pre></td></tr></table></figure><h3 id="重构第三版："><a href="#重构第三版：" class="headerlink" title="重构第三版："></a>重构第三版：</h3><p>包含osd pool的排序，包含osd的排序，输出平均pg数目，输出最大的osd编号，输出最大超过平均值的百分比，输出最少pg的osd编号，输出最小低于平均值的百分比</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs bash">ceph pg dump | awk <span class="hljs-string">&#x27;</span><br><span class="hljs-string"> /^pg_stat/ &#123; col=1; while($col!=&quot;up&quot;) &#123;col++&#125;; col++ &#125;</span><br><span class="hljs-string"> /^[0-9a-f]+\.[0-9a-f]+/ &#123; match($0,/^[0-9a-f]+/); pool=substr($0, RSTART, RLENGTH); poollist[pool]=0;</span><br><span class="hljs-string"> up=$col; i=0; RSTART=0; RLENGTH=0; delete osds; while(match(up,/[0-9]+/)&gt;0) &#123; osds[++i]=substr(up,RSTART,RLENGTH); up = substr(up, RSTART+RLENGTH) &#125;</span><br><span class="hljs-string"> for(i in osds) &#123;array[osds[i],pool]++; osdlist[osds[i]];&#125;</span><br><span class="hljs-string">&#125;</span><br><span class="hljs-string">END &#123;</span><br><span class="hljs-string"> printf(&quot;\n&quot;);</span><br><span class="hljs-string"> slen=asorti(poollist,newpoollist);</span><br><span class="hljs-string"> printf(&quot;pool :\t&quot;);for (i=1;i&lt;=slen;i++) &#123;printf(&quot;%s\t&quot;, newpoollist[i])&#125;; printf(&quot;| SUM \n&quot;);</span><br><span class="hljs-string"> for (i in poollist) printf(&quot;--------&quot;); printf(&quot;----------------\n&quot;);</span><br><span class="hljs-string"> slen1=asorti(osdlist,newosdlist)</span><br><span class="hljs-string"> delete poollist;</span><br><span class="hljs-string"> for (j=1;j&lt;=slen;j++) &#123;maxpoolosd[j]=0&#125;;</span><br><span class="hljs-string"> for (j=1;j&lt;=slen;j++) &#123;for (i=1;i&lt;=slen1;i++)&#123;if (array[newosdlist[i],newpoollist[j]] &gt;0  )&#123;minpoolosd[j]=array[newosdlist[i],newpoollist[j]] ;break &#125; &#125;&#125;;</span><br><span class="hljs-string"> for (i=1;i&lt;=slen1;i++) &#123; printf(&quot;osd.%i\t&quot;, newosdlist[i]); sum=0; </span><br><span class="hljs-string"> for (j=1;j&lt;=slen;j++)  &#123; printf(&quot;%i\t&quot;, array[newosdlist[i],newpoollist[j]]); sum+=array[newosdlist[i],newpoollist[j]]; poollist[j]+=array[newosdlist[i],newpoollist[j]];if(array[newosdlist[i],newpoollist[j]] != 0)&#123;poolhasid[j]+=1 &#125;;if(array[newosdlist[i],newpoollist[j]]&gt;maxpoolosd[j])&#123;maxpoolosd[j]=array[newosdlist[i],newpoollist[j]];maxosdid[j]=newosdlist[i]&#125;;if(array[newosdlist[i],newpoollist[j]] != 0)&#123;if(array[newosdlist[i],newpoollist[j]]&lt;=minpoolosd[j])&#123;minpoolosd[j]=array[newosdlist[i],newpoollist[j]];minosdid[j]=newosdlist[i]&#125;&#125;&#125;; printf(&quot;| %i\n&quot;,sum)&#125; for (i in poollist) printf(&quot;--------&quot;); printf(&quot;----------------\n&quot;);</span><br><span class="hljs-string"> slen2=asorti(poollist,newpoollist);</span><br><span class="hljs-string"> printf(&quot;SUM :\t&quot;); for (i=1;i&lt;=slen;i++) printf(&quot;%s\t&quot;,poollist[i]); printf(&quot;|\n&quot;);</span><br><span class="hljs-string"> printf(&quot;Osd :\t&quot;); for (i=1;i&lt;=slen;i++) printf(&quot;%s\t&quot;,poolhasid[i]); printf(&quot;|\n&quot;);</span><br><span class="hljs-string"> printf(&quot;AVE :\t&quot;); for (i=1;i&lt;=slen;i++) printf(&quot;%.2f\t&quot;,poollist[i]/poolhasid[i]); printf(&quot;|\n&quot;);</span><br><span class="hljs-string"> printf(&quot;Max :\t&quot;); for (i=1;i&lt;=slen;i++) printf(&quot;%s\t&quot;,maxpoolosd[i]); printf(&quot;|\n&quot;);</span><br><span class="hljs-string"> printf(&quot;Osdid :\t&quot;); for (i=1;i&lt;=slen;i++) printf(&quot;osd.%s\t&quot;,maxosdid[i]); printf(&quot;|\n&quot;);</span><br><span class="hljs-string"> printf(&quot;per:\t&quot;); for (i=1;i&lt;=slen;i++) printf(&quot;%.1f%\t&quot;,100*(maxpoolosd[i]-poollist[i]/poolhasid[i])/(poollist[i]/poolhasid[i])); printf(&quot;|\n&quot;);</span><br><span class="hljs-string"> for (i=1;i&lt;=slen2;i++) printf(&quot;--------&quot;);printf(&quot;----------------\n&quot;);</span><br><span class="hljs-string"> printf(&quot;min :\t&quot;); for (i=1;i&lt;=slen;i++) printf(&quot;%s\t&quot;,minpoolosd[i]); printf(&quot;|\n&quot;);</span><br><span class="hljs-string"> printf(&quot;osdid :\t&quot;); for (i=1;i&lt;=slen;i++) printf(&quot;osd.%s\t&quot;,minosdid[i]); printf(&quot;|\n&quot;);</span><br><span class="hljs-string"> printf(&quot;per:\t&quot;); for (i=1;i&lt;=slen;i++) printf(&quot;%.1f%\t&quot;,100*(minpoolosd[i]-poollist[i]/poolhasid[i])/(poollist[i]/poolhasid[i])); printf(&quot;|\n&quot;);</span><br><span class="hljs-string">&#125;&#x27;</span><br></code></pre></td></tr></table></figure><p>输出如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs bash">dumped all <span class="hljs-keyword">in</span> format plain<br><br>pool :012| SUM <br>----------------------------------------<br>osd.020620653| 465<br>osd.122195| 46<br>osd.220219649| 447<br>osd.319256| 50<br>osd.429359| 73<br>osd.534316| 71<br>----------------------------------------<br>SUM :512512128|<br>AVE :858521|<br>max :20620653|<br>osdid :osd.0osd.0osd.0|<br>per:141.4%141.4%148.4%|<br>---------------------------------------<br>min :19195|<br>osdid :osd.3osd.1osd.1|<br>per:-77.7%-77.7%-76.6%|<br></code></pre></td></tr></table></figure><h3 id="Luminous以及之后的版本"><a href="#Luminous以及之后的版本" class="headerlink" title="Luminous以及之后的版本"></a>Luminous以及之后的版本</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs bash">ceph pg dump | awk <span class="hljs-string">&#x27;</span><br><span class="hljs-string"> /^PG_STAT/ &#123; col=1; while($col!=&quot;UP&quot;) &#123;col++&#125;; col++ &#125;</span><br><span class="hljs-string"> /^[0-9a-f]+\.[0-9a-f]+/ &#123; match($0,/^[0-9a-f]+/); pool=substr($0, RSTART, RLENGTH); poollist[pool]=0;</span><br><span class="hljs-string"> up=$col; i=0; RSTART=0; RLENGTH=0; delete osds; while(match(up,/[0-9]+/)&gt;0) &#123; osds[++i]=substr(up,RSTART,RLENGTH); up = substr(up, RSTART+RLENGTH) &#125;</span><br><span class="hljs-string"> for(i in osds) &#123;array[osds[i],pool]++; osdlist[osds[i]];&#125;</span><br><span class="hljs-string">&#125;</span><br><span class="hljs-string">END &#123;</span><br><span class="hljs-string"> printf(&quot;\n&quot;);</span><br><span class="hljs-string"> slen=asorti(poollist,newpoollist);</span><br><span class="hljs-string"> printf(&quot;pool :\t&quot;);for (i=1;i&lt;=slen;i++) &#123;printf(&quot;%s\t&quot;, newpoollist[i])&#125;; printf(&quot;| SUM \n&quot;);</span><br><span class="hljs-string"> for (i in poollist) printf(&quot;--------&quot;); printf(&quot;----------------\n&quot;);</span><br><span class="hljs-string"> slen1=asorti(osdlist,newosdlist)</span><br><span class="hljs-string"> delete poollist;</span><br><span class="hljs-string"> for (j=1;j&lt;=slen;j++) &#123;maxpoolosd[j]=0&#125;;</span><br><span class="hljs-string"> for (j=1;j&lt;=slen;j++) &#123;for (i=1;i&lt;=slen1;i++)&#123;if (array[newosdlist[i],newpoollist[j]] &gt;0  )&#123;minpoolosd[j]=array[newosdlist[i],newpoollist[j]] ;break &#125; &#125;&#125;; </span><br><span class="hljs-string"> for (i=1;i&lt;=slen1;i++) &#123; printf(&quot;osd.%i\t&quot;, newosdlist[i]); sum=0; </span><br><span class="hljs-string"> for (j=1;j&lt;=slen;j++)  &#123; printf(&quot;%i\t&quot;, array[newosdlist[i],newpoollist[j]]); sum+=array[newosdlist[i],newpoollist[j]]; poollist[j]+=array[newosdlist[i],newpoollist[j]];if(array[newosdlist[i],newpoollist[j]] != 0)&#123;poolhasid[j]+=1 &#125;;if(array[newosdlist[i],newpoollist[j]]&gt;maxpoolosd[j])&#123;maxpoolosd[j]=array[newosdlist[i],newpoollist[j]];maxosdid[j]=newosdlist[i]&#125;;if(array[newosdlist[i],newpoollist[j]] != 0)&#123;if(array[newosdlist[i],newpoollist[j]]&lt;=minpoolosd[j])&#123;minpoolosd[j]=array[newosdlist[i],newpoollist[j]];minosdid[j]=newosdlist[i]&#125;&#125;&#125;; printf(&quot;| %i\n&quot;,sum)&#125; for (i in poollist) printf(&quot;--------&quot;); printf(&quot;----------------\n&quot;);</span><br><span class="hljs-string"> slen2=asorti(poollist,newpoollist);</span><br><span class="hljs-string"> printf(&quot;SUM :\t&quot;); for (i=1;i&lt;=slen;i++) printf(&quot;%s\t&quot;,poollist[i]); printf(&quot;|\n&quot;);</span><br><span class="hljs-string"> printf(&quot;Osd :\t&quot;); for (i=1;i&lt;=slen;i++) printf(&quot;%s\t&quot;,poolhasid[i]); printf(&quot;|\n&quot;);</span><br><span class="hljs-string"> printf(&quot;AVE :\t&quot;); for (i=1;i&lt;=slen;i++) printf(&quot;%.2f\t&quot;,poollist[i]/poolhasid[i]); printf(&quot;|\n&quot;);</span><br><span class="hljs-string"> printf(&quot;Max :\t&quot;); for (i=1;i&lt;=slen;i++) printf(&quot;%s\t&quot;,maxpoolosd[i]); printf(&quot;|\n&quot;);</span><br><span class="hljs-string"> printf(&quot;Osdid :\t&quot;); for (i=1;i&lt;=slen;i++) printf(&quot;osd.%s\t&quot;,maxosdid[i]); printf(&quot;|\n&quot;);</span><br><span class="hljs-string"> printf(&quot;per:\t&quot;); for (i=1;i&lt;=slen;i++) printf(&quot;%.1f%\t&quot;,100*(maxpoolosd[i]-poollist[i]/poolhasid[i])/(poollist[i]/poolhasid[i])); printf(&quot;|\n&quot;);</span><br><span class="hljs-string"> for (i=1;i&lt;=slen2;i++) printf(&quot;--------&quot;);printf(&quot;----------------\n&quot;);</span><br><span class="hljs-string"> printf(&quot;min :\t&quot;); for (i=1;i&lt;=slen;i++) printf(&quot;%s\t&quot;,minpoolosd[i]); printf(&quot;|\n&quot;);</span><br><span class="hljs-string"> printf(&quot;osdid :\t&quot;); for (i=1;i&lt;=slen;i++) printf(&quot;osd.%s\t&quot;,minosdid[i]); printf(&quot;|\n&quot;);</span><br><span class="hljs-string"> printf(&quot;per:\t&quot;); for (i=1;i&lt;=slen;i++) printf(&quot;%.1f%\t&quot;,100*(minpoolosd[i]-poollist[i]/poolhasid[i])/(poollist[i]/poolhasid[i])); printf(&quot;|\n&quot;);</span><br><span class="hljs-string">&#125;&#x27;</span><br></code></pre></td></tr></table></figure><p>luminous之后的版本json的输出小写改成大写了，需要替换几个字段，上面的已经增加了</p><p>上面的处理使用的是awk处理，开始的时候看不懂什么意思，然后就去看了这本书《The AWK Programming Language》</p><h3 id="语法的解释"><a href="#语法的解释" class="headerlink" title="语法的解释"></a>语法的解释</h3><blockquote><p>&#x2F;^pg_stat&#x2F; { col&#x3D;1; while($col!&#x3D;”up”) {col++}; col++ }</p></blockquote><p>这个是匹配pg dump 的输出结果里面pg_stat那个字段，开始计数为1，不是up值就将col的值加1，这个匹配到的就是我们经常看到的[1,10]这个值最后的col++是将col值+1,因为字段里面有up,up_primary,我们需要的是up_primary</p><blockquote><p>&#x2F;^[0-9a-f]+.[0-9a-f]+&#x2F; { match($0,&#x2F;^[0-9a-f]+&#x2F;); pool&#x3D;substr($0, RSTART, RLENGTH); poollist[pool]&#x3D;0;</p></blockquote><p>这个是匹配前面的 1.17a pg号 ，使用自带的match函数 做字符串的过滤统计匹配.号前面的存储池ID， 并得到 RSTART, RLENGTH 值，这个是取到前面的存储池ID，使用substr 函数，就可以得到pool的值了，poollist[pool]&#x3D;0，是将数组的值置为0</p><blockquote><p>up&#x3D;$col; i&#x3D;0; RSTART&#x3D;0; RLENGTH&#x3D;0; delete osds; while(match(up,&#x2F;[0-9]+&#x2F;)&gt;0) { osds[++i]&#x3D;substr(up,RSTART,RLENGTH); up &#x3D; substr(up, RSTART+RLENGTH) }</p></blockquote><p>先将变量置0，然后将osd编号一个个输入到osds[i]的数组当中去</p><blockquote><p>for(i in osds) {array[osds[i],pool]++; osdlist[osds[i]];}</p></blockquote><p>将osds数组中的值输入到数组当中去，并且记录成osdlist，和数组array[osd[i],pool]</p><blockquote><p>printf(“\n”);<br> printf(“pool :\t”); for (i in poollist) printf(“%s\t”,i); printf(“| SUM \n”);</p></blockquote><p>打印osd pool的编号</p><blockquote><p>for (i in poollist) printf(“——–”); printf(“—————-\n”);</p></blockquote><p>根据osd pool的长度打印—-</p><blockquote><p>for (i in osdlist) { printf(“osd.%i\t”, i); sum&#x3D;0;</p></blockquote><p>打印osd的编号</p><blockquote><p>for (j in poollist) { printf(“%i\t”, array[i,j]); sum+&#x3D;array[i,j]; poollist[j]+&#x3D;array[i,j] }; printf(“| %i\n”,sum) }<br>打印对应的osd的pg数目，并做求和的统计</p></blockquote><blockquote><p>for (i in poollist) printf(“——–”); printf(“—————-\n”);<br> printf(“SUM :\t”); for (i in poollist) printf(“%s\t”,poollist[i]); printf(“|\n”);</p></blockquote><p>打印新的poollist里面的求和的值</p><p>修改版本里面用到的函数</p><blockquote><p>slen1&#x3D;asorti(osdlist,newosdlist)</p></blockquote><p>这个是将数组里面的下标进行排序，这里是对osd和poollist的编号进行排序 slen1是拿到数组的长度，使用for进行遍历输出</p><h3 id="脚本的逻辑"><a href="#脚本的逻辑" class="headerlink" title="脚本的逻辑"></a>脚本的逻辑</h3><ul><li>匹配到pg的id和pg对应的osd，</li><li>使用数组的方式，将统计到的osd id存储起来，</li><li>然后打印数组</li></ul><p>其他资源：<br>pg设置的计算器：<br><a href="http://ceph.com/pgcalc/">http://ceph.com/pgcalc/</a><br>pg的查询和设置：<br><a href="http://ceph.com/docs/master/rados/operations/placement-groups/">http://ceph.com/docs/master/rados/operations/placement-groups/</a></p><h2 id="变更记录"><a href="#变更记录" class="headerlink" title="变更记录"></a>变更记录</h2><table><thead><tr><th align="center">Why</th><th align="center">Who</th><th align="center">When</th></tr></thead><tbody><tr><td align="center">创建</td><td align="center">武汉-运维-磨渣</td><td align="center">2015-10-04</td></tr><tr><td align="center">修改</td><td align="center">武汉-运维-磨渣</td><td align="center">2016-08-24</td></tr><tr><td align="center">修改有0的统计BUG</td><td align="center">武汉-运维-磨渣</td><td align="center">2016-09-08</td></tr><tr><td align="center">增加luminous版本脚本</td><td align="center">武汉-运维-磨渣</td><td align="center">2020-06-17</td></tr></tbody></table><p>引用博客地址如下：</p><p><a href="http://cephnotes.ksperis.com/blog/2015/02/23/get-the-number-of-placement-groups-per-osd/">http://cephnotes.ksperis.com/blog/2015/02/23/get-the-number-of-placement-groups-per-osd/</a></p>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>rbd的数据在哪里</title>
    <link href="/2015/09/28/rbd%E7%9A%84%E6%95%B0%E6%8D%AE%E5%9C%A8%E5%93%AA%E9%87%8C/"/>
    <url>/2015/09/28/rbd%E7%9A%84%E6%95%B0%E6%8D%AE%E5%9C%A8%E5%93%AA%E9%87%8C/</url>
    
    <content type="html"><![CDATA[<h3 id="创建一个rbd"><a href="#创建一个rbd" class="headerlink" title="创建一个rbd"></a>创建一个rbd</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@mytest ~]<span class="hljs-comment"># rbd create test1 --size 4000</span><br></code></pre></td></tr></table></figure><h3 id="查看rbd信息"><a href="#查看rbd信息" class="headerlink" title="查看rbd信息"></a>查看rbd信息</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@mytest ~]<span class="hljs-comment"># rbd info test1</span><br>rbd image <span class="hljs-string">&#x27;test1&#x27;</span>:<br>size 4000 MB <span class="hljs-keyword">in</span> 1000 objects<br>order 22 (4096 kB objects)<br>block_name_prefix: rb.0.fa6c.6b8b4567<br>format: 1<br></code></pre></td></tr></table></figure><p>可以看出是没写入真实数据的</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@mytest ~]<span class="hljs-comment"># ceph df|grep rbd</span><br>    rbd          0       133         0        30627M           2 <br></code></pre></td></tr></table></figure><h3 id="查询rbd池里面的对象信息"><a href="#查询rbd池里面的对象信息" class="headerlink" title="查询rbd池里面的对象信息"></a>查询rbd池里面的对象信息</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@mytest ~]<span class="hljs-comment"># rados ls -p rbd</span><br>test1.rbd<br>rbd_directory<br></code></pre></td></tr></table></figure><p>查看下这两个对象的内容</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@mytest ~]<span class="hljs-comment">#rados -p rbd get test1.rbd test1rbd.txt</span><br>[root@mytest ~]<span class="hljs-comment"># echo -e `cat test1rbd.txt`</span><br>&lt;&lt;&lt; <span class="hljs-string">Rados Block Device Image &gt;&gt;&gt; rb.0.fa6c.6b8b4567RBD001.005 </span><br></code></pre></td></tr></table></figure><p>这个是记录的rbd镜像的信息</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@mytest ~]# rados -p rbd get rbd_directory rbddirectory.txt<br>[root@mytest ~]# echo -e `cat rbddirectory.txt`<br>test1<br></code></pre></td></tr></table></figure><p>这个是记录的rbd的目录信息</p><h3 id="rbd映射到本地"><a href="#rbd映射到本地" class="headerlink" title="rbd映射到本地"></a>rbd映射到本地</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@mytest ~]<span class="hljs-comment"># rbd map test1</span><br>/dev/rbd0<br></code></pre></td></tr></table></figure><h3 id="格式化rbd设备"><a href="#格式化rbd设备" class="headerlink" title="格式化rbd设备"></a>格式化rbd设备</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@mytest ~]<span class="hljs-comment"># mkfs.xfs /dev/rbd/rbd/test1 </span><br><span class="hljs-built_in">log</span> stripe unit (4194304 bytes) is too large (maximum is 256KiB)<br><span class="hljs-built_in">log</span> stripe unit adjusted to 32KiB<br>meta-data=/dev/rbd/rbd/test1     isize=256    agcount=9, agsize=126976 blks<br>         =                       sectsz=512   attr=2, projid32bit=1<br>         =                       crc=0        finobt=0<br>data     =                       bsize=4096   blocks=1024000, imaxpct=25<br>         =                       sunit=1024   swidth=1024 blks<br>naming   =version 2              bsize=4096   ascii-ci=0 ftype=0<br><span class="hljs-built_in">log</span>      =internal <span class="hljs-built_in">log</span>           bsize=4096   blocks=2560, version=2<br>         =                       sectsz=512   sunit=8 blks, lazy-count=1<br>realtime =none                   extsz=4096   blocks=0, rtextents=0<br></code></pre></td></tr></table></figure><h3 id="查看当前的rbd池里面的对象信息"><a href="#查看当前的rbd池里面的对象信息" class="headerlink" title="查看当前的rbd池里面的对象信息"></a>查看当前的rbd池里面的对象信息</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@mytest mnt]<span class="hljs-comment"># rados -p rbd ls</span><br>rb.0.fa6c.6b8b4567.0000000001f0<br>rb.0.fa6c.6b8b4567.0000000001f1<br>rb.0.fa6c.6b8b4567.000000000174<br>test1.rbd<br>rb.0.fa6c.6b8b4567.0000000002e8<br>rb.0.fa6c.6b8b4567.0000000001f2<br>rbd_directory<br>rb.0.fa6c.6b8b4567.0000000000f8<br>rb.0.fa6c.6b8b4567.0000000003e0<br>rb.0.fa6c.6b8b4567.000000000000<br>rb.0.fa6c.6b8b4567.00000000007c<br>rb.0.fa6c.6b8b4567.0000000003e7<br>rb.0.fa6c.6b8b4567.00000000026c<br>rb.0.fa6c.6b8b4567.000000000001<br>rb.0.fa6c.6b8b4567.000000000364<br></code></pre></td></tr></table></figure><p>可以看到格式化过程中写入了一些对象信息，这些应该是存储文件系统信息的，写入的对象，数据的写入的前缀是rb.0.fa6c.6b8b4567</p><h3 id="查看对象数据在哪里"><a href="#查看对象数据在哪里" class="headerlink" title="查看对象数据在哪里"></a>查看对象数据在哪里</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@mytest 0.1_head]<span class="hljs-comment"># ceph osd map rbd rb.0.fa6c.6b8b4567.0000000001f0</span><br>osdmap e78 pool <span class="hljs-string">&#x27;rbd&#x27;</span> (0) object <span class="hljs-string">&#x27;rb.0.fa6c.6b8b4567.0000000001f0&#x27;</span> -&gt; pg 0.1cdfe181 (0.1) -&gt; up ([1], p1) acting ([1], p1)<br></code></pre></td></tr></table></figure><p>可以查看到数据是在节点1的pg 0.1 里面</p><h3 id="去节点一上查看"><a href="#去节点一上查看" class="headerlink" title="去节点一上查看"></a>去节点一上查看</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@mytest 0.1_head]<span class="hljs-comment"># ll  /var/lib/ceph/osd/ceph-1/current/0.1_head/</span><br>total 4100<br>-rw-r--r-- 1 root root       0 Aug 10 14:02 __head_00000001__0<br>-rw-r--r-- 1 root root 4194304 Aug 23 12:36 rb.0.fa6c.6b8b4567.0000000001f0__head_1CDFE181__0<br></code></pre></td></tr></table></figure><p>可以看到这个对象</p><p>上面的步骤实现的是: </p><ul><li>查看一个rbd image</li><li>查看这个image 里面的包含的对象    </li><li>查看这个 rbd image的对象在哪个具体的磁盘上</li></ul><p>无法实现的是查询文件系统之上的某个文件在哪里，这个在cephfs 文件系统接口中是有的</p>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>删除ceph集群mds</title>
    <link href="/2015/09/24/%E5%88%A0%E9%99%A4ceph%E9%9B%86%E7%BE%A4mds/"/>
    <url>/2015/09/24/%E5%88%A0%E9%99%A4ceph%E9%9B%86%E7%BE%A4mds/</url>
    
    <content type="html"><![CDATA[<br>ceph集群新搭建以后是只有一个默认的存储池rbd的池## 创建文件接口集群<h3 id="1-创建一个元数据池"><a href="#1-创建一个元数据池" class="headerlink" title="1.创建一个元数据池"></a>1.创建一个元数据池</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@mytest ~]<span class="hljs-comment"># ceph osd pool create metadata  20 20</span><br>pool <span class="hljs-string">&#x27;metadata&#x27;</span> created<br></code></pre></td></tr></table></figure><h3 id="2-创建一个数据池"><a href="#2-创建一个数据池" class="headerlink" title="2.创建一个数据池"></a>2.创建一个数据池</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@mytest ~]<span class="hljs-comment"># ceph osd pool create data   20 20</span><br>pool <span class="hljs-string">&#x27;data&#x27;</span> created<br></code></pre></td></tr></table></figure><h3 id="3-创建一个文件系统"><a href="#3-创建一个文件系统" class="headerlink" title="3.创建一个文件系统"></a>3.创建一个文件系统</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@mytest ~]<span class="hljs-comment"># ceph fs new ceph  metadata data</span><br>new fs with metadata pool 4 and data pool 5<br></code></pre></td></tr></table></figure><h3 id="4-创建一个mds"><a href="#4-创建一个mds" class="headerlink" title="4.创建一个mds"></a>4.创建一个mds</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@mytest ~]<span class="hljs-comment"># ceph-deploy mds create mytest</span><br></code></pre></td></tr></table></figure><h3 id="5-部署完检查状态"><a href="#5-部署完检查状态" class="headerlink" title="5.部署完检查状态"></a>5.部署完检查状态</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@mytest ceph]<span class="hljs-comment"># ceph -s</span><br>    cluster 7e5469ac-ae1f-494f-9913-901f60c0a76b<br>     health HEALTH_OK<br>     monmap e1: 1 mons at &#123;mytest=192.168.0.76:6789/0&#125;<br>            election epoch 1, quorum 0 mytest<br>     mdsmap e60: 1/1/1 up &#123;0=mytest=up:active&#125;<br>     osdmap e70: 2 osds: 2 up, 2 <span class="hljs-keyword">in</span><br>      pgmap v252: 104 pgs, 3 pools, 1962 bytes data, 20 objects<br>            75144 kB used, 30624 MB / 30697 MB avail<br>                 104 active+clean<br>  client io 108 B/s wr, 0 op/s<br></code></pre></td></tr></table></figure><h2 id="删除文件接口集群（删除mds）"><a href="#删除文件接口集群（删除mds）" class="headerlink" title="删除文件接口集群（删除mds）"></a>删除文件接口集群（删除mds）</h2><h3 id="1-停止mds进程"><a href="#1-停止mds进程" class="headerlink" title="1.停止mds进程"></a>1.停止mds进程</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@mytest ceph]<span class="hljs-comment"># /etc/init.d/ceph stop mds</span><br>=== mds.mytest === <br>Stopping Ceph mds.mytest on mytest...kill 9638...<span class="hljs-keyword">done</span><br></code></pre></td></tr></table></figure><h3 id="2-将mds状态标记为失效"><a href="#2-将mds状态标记为失效" class="headerlink" title="2.将mds状态标记为失效"></a>2.将mds状态标记为失效</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@mytest ceph]<span class="hljs-comment"># ceph mds fail 0</span><br>failed mds.0<br></code></pre></td></tr></table></figure><h3 id="3-删除ceph文件系统"><a href="#3-删除ceph文件系统" class="headerlink" title="3.删除ceph文件系统"></a>3.删除ceph文件系统</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@mytest ceph]<span class="hljs-comment"># ceph fs rm ceph --yes-i-really-mean-it</span><br></code></pre></td></tr></table></figure><h3 id="4-删除完了检查状态"><a href="#4-删除完了检查状态" class="headerlink" title="4.删除完了检查状态"></a>4.删除完了检查状态</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@mytest ceph]<span class="hljs-comment"># ceph -s</span><br>    cluster 7e5469ac-ae1f-494f-9913-901f60c0a76b<br>     health HEALTH_OK<br>     monmap e1: 1 mons at &#123;mytest=192.168.0.76:6789/0&#125;<br>            election epoch 1, quorum 0 mytest<br>     osdmap e71: 2 osds: 2 up, 2 <span class="hljs-keyword">in</span><br>      pgmap v253: 104 pgs, 3 pools, 1962 bytes data, 20 objects<br>            75144 kB used, 30624 MB / 30697 MB avail<br>                 104 active+clean<br></code></pre></td></tr></table></figure><p>可以看到已经没有了mds的那一条了</p>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>ceph客户端服务端属性匹配关系</title>
    <link href="/2015/09/18/ceph%E5%AE%A2%E6%88%B7%E7%AB%AF%E6%9C%8D%E5%8A%A1%E7%AB%AF%E5%B1%9E%E6%80%A7%E5%8C%B9%E9%85%8D%E5%85%B3%E7%B3%BB/"/>
    <url>/2015/09/18/ceph%E5%AE%A2%E6%88%B7%E7%AB%AF%E6%9C%8D%E5%8A%A1%E7%AB%AF%E5%B1%9E%E6%80%A7%E5%8C%B9%E9%85%8D%E5%85%B3%E7%B3%BB/</url>
    
    <content type="html"><![CDATA[<p>ceph的server是定期会发布版本，而它的客户端是放到linux 内核当中的，一些属性的支持是依赖于内核版本的，这样就存在一些问题，一些功能后端支持，而客户端旧了；还有可能是客户端用的很新的内核，而后端比较旧不支持，所以查看了下内核中的 <code>features</code> 文件，可以看到这个对应关系，总结了下就是下面的列表</p><p>注意表中：</p><ul><li><strong>S</strong>&#x3D;SUPPORTED_DEFAULT     代表客户端支持这个属性</li><li><strong>R</strong>&#x3D;REQUIRED_DEFAULT      代表需要服务端支持这个属性</li></ul><p><img src="/images/blog/o_200901030738%E5%B1%9E%E6%80%A7%E5%8C%B9%E9%85%8D%E7%9A%84%E9%97%AE%E9%A2%98.png"></p><p> missing 2040000  意思是  CEPH_FEATURE_CRUSH_TUNABLES (40000)   和   CEPH_FEATURE_CRUSH_TUNABLES2 (2000000)   不被当前客户端支持，一般要么关闭新的server端的这个属性，或者升级到支持的版本的内核.</p><p>内核代码查看地址：<a href="https://elixir.bootlin.com/linux/v5.7/source/include/linux/ceph/ceph_features.h">https://elixir.bootlin.com/linux/v5.7/source/include/linux/ceph/ceph_features.h</a></p><p>Some examples of errors that can be encountered :</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">mon0 192.168.0.1:6789 feature <span class="hljs-built_in">set</span> mismatch, my XXXXXX &lt; server<span class="hljs-string">&#x27;s XXXXXX, missing 2040000</span><br></code></pre></td></tr></table></figure><p>—&gt; Upgrade kernel client up to 3.9 or set tunables to legacy : ceph osd crush tunables legacy</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">mon0 192.168.0.1:6789 feature <span class="hljs-built_in">set</span> mismatch, my XXXXXX &lt; server<span class="hljs-string">&#x27;s XXXXXX, missing 40000000</span><br></code></pre></td></tr></table></figure><p>—&gt; Upgrade kernel client up to 3.9 or unset hashpspool : ceph osd pool set rbd hashpspool false</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">mon0 192.168.0.1:6789 feature <span class="hljs-built_in">set</span> mismatch, my XXXXXX &lt; server<span class="hljs-string">&#x27;s XXXXXX, missing 800000000</span><br></code></pre></td></tr></table></figure><p>—&gt; Remove cache pool and reload monitors or upgrade kernel client up to 3.14</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">mon0 192.168.0.1:6789 feature <span class="hljs-built_in">set</span> mismatch, my XXXXXX &lt; server<span class="hljs-string">&#x27;s XXXXXX, missing 1000000000</span><br></code></pre></td></tr></table></figure><p>—&gt; Upgrade kernel client up to 3.14</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">mon0 192.168.0.1:6789 feature <span class="hljs-built_in">set</span> mismatch, my 4a042a42 &lt; server<span class="hljs-string">&#x27;s 2004a042a42, missing 20000000000</span><br></code></pre></td></tr></table></figure><p>—&gt; Upgrade kernel client up to 3.15 or disable tunable 3 features</p><h2 id="变更记录"><a href="#变更记录" class="headerlink" title="变更记录"></a>变更记录</h2><table><thead><tr><th align="center">Why</th><th align="center">Who</th><th align="center">When</th></tr></thead><tbody><tr><td align="center">创建</td><td align="center">武汉-运维-磨渣</td><td align="center">2015-09-18</td></tr><tr><td align="center">增加解决方案</td><td align="center">武汉-运维-磨渣</td><td align="center">2017-01-12</td></tr></tbody></table><blockquote><p>参考文档：<a href="http://cephnotes.ksperis.com/blog/2014/01/21/feature-set-mismatch-error-on-ceph-kernel-client/">http://cephnotes.ksperis.com/blog/2014/01/21/feature-set-mismatch-error-on-ceph-kernel-client/</a></p></blockquote>]]></content>
    
    
    <categories>
      
      <category>存储</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>nginx配置代理缓存</title>
    <link href="/2015/07/14/nginx%E9%85%8D%E7%BD%AE%E4%BB%A3%E7%90%86%E7%BC%93%E5%AD%98/"/>
    <url>/2015/07/14/nginx%E9%85%8D%E7%BD%AE%E4%BB%A3%E7%90%86%E7%BC%93%E5%AD%98/</url>
    
    <content type="html"><![CDATA[<p>nginx可以实现反向代理的配置，并且可以使用缓存来加速，本文是简单的实现功能的配置，暂时没有做其他的优化的部分的配置，从网上的资料来看，很多配置都是没有讲哪些是必须配置的，我自己在配置过程中就发现没有生成缓存文件，下面来记录下配置的内容</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs bash">upstream <span class="hljs-built_in">test</span>&#123;<br>            server 127.0.0.1:8080;<br>        &#125;<br><br>proxy_cache_path /var/cache/nginx/proxy_cache keys_zone=cache_zone:2000m max_size=1000m;<br><br>server &#123;<br>    listen       80;<br>    server_name  localhost;<br>    location / &#123;<br>        proxy_pass http://127.0.0.1:8080;<br>        proxy_cache cache_zone;<br>        proxy_cache_valid  200 304 302 24h;<br>    &#125;<br><br>    error_page   500 502 503 504  /50x.html;<br><br>    location = /50x.html &#123;<br>        root   /usr/share/nginx/html;<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure><p>以上为我的配置文件，说明：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">upstream <span class="hljs-built_in">test</span>&#123;<br>        server 127.0.0.1:8080;<br>    &#125;<br></code></pre></td></tr></table></figure><p>这个字段是配置代理的部分，这个配置没有做多机的配置，只是将本机的nginx的80端口配置到了apache的8080端口，</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">proxy_cache_path /var/cache/nginx/proxy_cache keys_zone=cache_zone:2000m max_size=1000m;<br></code></pre></td></tr></table></figure><p>这个是配置缓存的放置的路径的，这个路径最好是跟proxy_temp在一个分区上，后面是写得名称，共享内存大小，缓存的最大值</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">location / &#123;<br>    proxy_pass http://127.0.0.1:8080;<br>    proxy_cache cache_zone;<br>    proxy_cache_valid  200 304 302 24h;<br>&#125;<br></code></pre></td></tr></table></figure><p>这个是缓存的关键配置，proxy_pass是将请求转发到你要代理的机器上，proxy_cache是指明使用的规则，上面的keys_zone，下面的proxy_cache_valid是配置允许缓存的请求，这个地方不配置就无法生成缓存的文件，</p><p>到这里基本的代理缓存就配置成功了，可以通过访问来检查是否生成了缓存文件，下载检查下是否web直接将请求返回了前端</p>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>linux服务器远程网络开机（wake on lan）</title>
    <link href="/2015/05/27/linux%E6%9C%8D%E5%8A%A1%E5%99%A8%E8%BF%9C%E7%A8%8B%E7%BD%91%E7%BB%9C%E5%BC%80%E6%9C%BA%EF%BC%88wake%20on%20lan%EF%BC%89/"/>
    <url>/2015/05/27/linux%E6%9C%8D%E5%8A%A1%E5%99%A8%E8%BF%9C%E7%A8%8B%E7%BD%91%E7%BB%9C%E5%BC%80%E6%9C%BA%EF%BC%88wake%20on%20lan%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<p>通过网络可以远程开关机，某些时候比较方便管理机器</p><h3 id="检查服务器是否支持远程网络开机"><a href="#检查服务器是否支持远程网络开机" class="headerlink" title="检查服务器是否支持远程网络开机"></a>检查服务器是否支持远程网络开机</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab5101 ~]<span class="hljs-comment"># ethtool eth0</span><br>Settings <span class="hljs-keyword">for</span> eth0:<br>    Supported ports: [ TP ]<br>    Supported <span class="hljs-built_in">link</span> modes: 10baseT/Half 10baseT/Full <br>                            100baseT/Half 100baseT/Full <br>                            1000baseT/Full <br>    Supported pause frame use: No<br>    Supports auto-negotiation: Yes<br>    Advertised <span class="hljs-built_in">link</span> modes: <br>                            10baseT/Half 10baseT/Full <br>                            100baseT/Half 100baseT/Full <br>                            1000baseT/Full <br>    Advertised pause frame use: No<br>    Advertised auto-negotiation: Yes<br>    Speed: 1000Mb/s<br>    Duplex: Full<br>    Port: Twisted Pair<br>    PHYAD: 2<br>    Transceiver: internal<br>    Auto-negotiation: onMDI-X: on<br>    Supports Wake-on: pumbg<br>    Wake-on: g<br>    Current message level: 0x00000007 (7) <br>                            drv probe linkLink detected: <span class="hljs-built_in">yes</span><br></code></pre></td></tr></table></figure><p>注意这两项：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">Supports Wake-on: pumbg<br>Wake-on: g<br></code></pre></td></tr></table></figure><p>可以通过命令设置，也可以去bios中设置<br>d表示禁用，g表示开启</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab5101 ~]<span class="hljs-comment"># ethtool -s eth0 wol g</span><br></code></pre></td></tr></table></figure><p>拿到网卡的mac地址</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab5101 ~]<span class="hljs-comment"># ifconfig </span><br>eth0  Link encap:Ethernet HWaddr 00:30:67:6D:3D:7C <br>     inet addr:192.168.5.101 Bcast:192.168.255.255 Mask:255.255.0.0 <br>     inet6 addr: fe80::215:17ff:fed0:be85/64 Scope:Link<br>     UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 <br>     RX packets:17752 errors:0 dropped:4 overruns:0 frame:0 <br>     TX packets:1101 errors:0 dropped:0 overruns:0 carrier:0 <br>     collisions:0 txqueuelen:1000 <br>     RX bytes:1288183 (1.2 MiB) TX bytes:262525 (256.3 KiB) <br>     Interrupt:16 Memory:b1100000-b1120000 <br>lo   Link encap:Local Loopback inet addr:127.0.0.1 Mask:255.0.0.0 <br>     inet6 addr: ::1/128 Scope:Host <br>     UP LOOPBACK RUNNING MTU:65536 Metric:1 <br>     RX packets:62566 errors:0 dropped:0 overruns:0 frame:0 <br>     TX packets:62566 errors:0 dropped:0 overruns:0 carrier:0 <br>     collisions:0 txqueuelen:0 <br>     RX bytes:91540393 (87.2 MiB) TX bytes:91540393 (87.2 MiB)<br></code></pre></td></tr></table></figure><p>得到：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">HWaddr 00:15:17:D0:BE:85 <br></code></pre></td></tr></table></figure><p>在另外一台服务器上执行：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab901 ~]<span class="hljs-comment"># ether-wake -i eth0 00:30:67:6d:3d:7c</span><br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>dm-crypt加密磁盘</title>
    <link href="/2015/05/25/dm-crypt%E5%8A%A0%E5%AF%86%E7%A3%81%E7%9B%98/"/>
    <url>/2015/05/25/dm-crypt%E5%8A%A0%E5%AF%86%E7%A3%81%E7%9B%98/</url>
    
    <content type="html"><![CDATA[<h3 id="dm-cry加密方式密码与文件"><a href="#dm-cry加密方式密码与文件" class="headerlink" title="dm-cry加密方式密码与文件"></a>dm-cry加密方式密码与文件</h3><p>与其它创建加密文件系统的方法相比，dm-crypt系统有着无可比拟的优越性：它的速度更快，易用性更强。除此之外，它的适用面也很广，能够运行在各种块设备上，即使这些设备使用了RAID和 LVM也毫无障碍。</p><!--break--><p>如果看到类似下面的输出，说明AES模块已经加载了。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">cat</span> /proc/crypto<br><br>name         : aes<br>driver       : aes-generic<br>module       : kernel<br>priority     : 100<br>refcnt       : 3<br>selftest     : passed<br><span class="hljs-built_in">type</span>         : cipher<br>blocksize    : 16<br>min keysize  : 16<br>max keysize  : 32<br></code></pre></td></tr></table></figure><p>否则可以用modprobe命令来手工加载AES模块。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">modprobe aes<br></code></pre></td></tr></table></figure><p>检查dmsetup软件包是否已经建立了设备映像程序，用如下命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">ls</span> -l /dev/mapper/control<br></code></pre></td></tr></table></figure><p>检查dm-crypt内核模块是否加载</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@node1 mnt]<span class="hljs-comment"># dmsetup targets</span><br>crypt            v1.13.0<br>mirror           v1.13.2<br>striped          v1.5.1<br>linear           v1.2.1<br>error            v1.2.0<br></code></pre></td></tr></table></figure><p>这说明系统已经为加密设备做好了准备。如果没有输出，可以用如下命令来加载dm-crypt模块。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">modprobe dm-crypt<br></code></pre></td></tr></table></figure><p>建立加密设备</p><p>这里用 fdisk命令来创建需要加密的磁盘，怎么创建磁盘这里不再冗述。假设这里创建好的磁盘分区是&#x2F;dev&#x2F;sdb1</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">cryptsetup -y create sdb_cry /dev/sdb1<br></code></pre></td></tr></table></figure><p>sdb_cry是逻辑卷的名称。输入上面命令后，还要输入2次密码，这个密码就是磁盘加密的密码。请牢记！<br>创建好后，用下面命令检查所建立的逻辑卷：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@node1 ~]<span class="hljs-comment"># dmsetup ls</span><br>sdb_cry(252:0)<br></code></pre></td></tr></table></figure><p>device-mapper会把它的虚拟设备装载到&#x2F;dev&#x2F;mapper下面，所以，你的虚拟块设备应该是&#x2F;dev&#x2F;mapper&#x2F;sdb_cry，尽管用起来和其他块设备没什么不同，实际上它却是经过透明加密的。</p><p>创建文件系统：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@node1 ~]<span class="hljs-comment"># mkfs.xfs /dev/mapper/sdb_cry -f</span><br></code></pre></td></tr></table></figure><p>装载加密磁盘：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@node1 ~]<span class="hljs-comment"># mount /dev/mapper/sdb_cry /mnt</span><br><br>[root@node1 ~]<span class="hljs-comment"># cp /etc/networks /mnt/</span><br>[root@node1 ~]<span class="hljs-comment"># cat /mnt/networks </span><br>default 0.0.0.0<br>loopback 127.0.0.0<br>link-local 169.254.0.0<br></code></pre></td></tr></table></figure><p>卸载加密设备</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment">#cryptsetup remove sdb_cry </span><br></code></pre></td></tr></table></figure><p>重新加载加密设备</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@node1 ~]<span class="hljs-comment"># cryptsetup create sdb_cry /dev/sdb1</span><br></code></pre></td></tr></table></figure><p>这里重新挂载会要求输入密码，密码输入不正确，设备会映射过去，但是找不到文件系统，是无法看到数据的</p><p>重新挂载</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">mount /dev/mapper/sdb_cry /mnt<br></code></pre></td></tr></table></figure><p>注意 cryptsetup -y create sdb_cry &#x2F;dev&#x2F;sdb1 会重新为磁盘设置新的密码，只要不对磁盘进行格式化，即使设置了新的密码，同样是看不到数据的，设置了新的密码后，只要不格式化，还是能够使用旧的密码来对磁盘进行挂载的</p><p>以上为采用密码加密磁盘的方式</p><p>使用luks方式的加密</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">cryptsetup -y -v luksFormat /dev/sdb1<br></code></pre></td></tr></table></figure><p>输入大写的YES，小写不行</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">cryptsetup isLuks /dev/sdb1 <br></code></pre></td></tr></table></figure><p>判断设备</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">cryptsetup luksOpen /dev/sdb1 sdb1_luks<br></code></pre></td></tr></table></figure><p>映射设备到&#x2F;dev&#x2F;mapper&#x2F;下面</p><p>卸载</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">cryptsetup remove sdb1_luks<br></code></pre></td></tr></table></figure><p>再次加载</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">cryptsetup isLuks /dev/sdb1 <br></code></pre></td></tr></table></figure><p>采用文件方式的加密</p><p>创建加密设备，并指定的文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">cryptsetup --key-file /etc/ceph/key/UUID --key-size 256 create sdb1_cry /dev/sdb1<br></code></pre></td></tr></table></figure><p>使用上面的命令后会在&#x2F;dev&#x2F;mapper&#x2F;下生成对应设备</p><p>然后格式化使用设备即可</p><p>卸载设备</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">cryptsetup remove sdb1_cry  <br></code></pre></td></tr></table></figure><p>需要再次使用就</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">cryptsetup --key-file /etc/ceph/key/UUID --key-size 256 create sdb1_cry /dev/sdb1<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>linux系统克隆系统盘</title>
    <link href="/2015/05/12/linux%E7%B3%BB%E7%BB%9F%E5%85%8B%E9%9A%86%E7%B3%BB%E7%BB%9F%E7%9B%98/"/>
    <url>/2015/05/12/linux%E7%B3%BB%E7%BB%9F%E5%85%8B%E9%9A%86%E7%B3%BB%E7%BB%9F%E7%9B%98/</url>
    
    <content type="html"><![CDATA[<p>本文将介绍两种方式的系统盘的完整的备份，两种方式各有优缺点，需要根据实际情况来进行选择</p><ul><li>使用dd的完整镜像克隆的方式</li><li>使用tar去备份数据，安装grub的方式</li></ul><h4 id="dd方式"><a href="#dd方式" class="headerlink" title="dd方式"></a>dd方式</h4><p>优点：<br>简单，一条命令 dd if&#x3D;&#x2F;dev&#x2F;sda of&#x3D;&#x2F;dev&#x2F;sdb 就可以进行完整的系统备份了</p><p>缺点：<br>时间非常长，备份一个系统盘，无论数据多少，系统盘全部会读取一遍，时间比较久<br>备份目的盘需要大于原盘<br>分区无法调整，根原分区一模一样</p><h4 id="tar方式"><a href="#tar方式" class="headerlink" title="tar方式"></a>tar方式</h4><p>优点：<br>时间短，只需要备份实际磁盘上的数据即可<br>自定义，可以修改分区的大小，可以控制备份的类容，可以定期备份<br>对目的盘大小无限制，比较灵活</p><p>缺点：<br>人为修改东西比较多，需要手动去修改一些信息<br>当然在掌握了一定的linux基础的情况下建议是使用tar方式的，，下面将介绍tar方式的处理方式：</p><h3 id="给准备用来备份的磁盘进行分区和格式化"><a href="#给准备用来备份的磁盘进行分区和格式化" class="headerlink" title="给准备用来备份的磁盘进行分区和格式化"></a>给准备用来备份的磁盘进行分区和格式化</h3><p>首先使用 df -h 来检查当前的挂载的系统分区：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">Filesystem      Size  Used Avail Use% Mounted on<br>/dev/sdb5       285G  4.1G  266G   2% /<br>tmpfs           2.0G  4.0K  2.0G   1% /dev/shm<br>/dev/sdb1       239M   81M  141M  37% /boot<br>/dev/sdb2       6.7G   17M  6.3G   1% /var/log<br></code></pre></td></tr></table></figure><p>查看磁盘的大小：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@zhongbo ~]<span class="hljs-comment"># fdisk -l</span><br>Disk /dev/sdb: 320.1 GB, 320072933376 bytes<br></code></pre></td></tr></table></figure><p>本例子中备用磁盘为sda,大小为240G：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@zhongbo ~]<span class="hljs-comment"># fdisk -l /dev/sda </span><br>Disk /dev/sda: 240.1 GB, 240057409536 bytes<br></code></pre></td></tr></table></figure><p>查看系统盘详细的分区信息</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@zhongbo ~]<span class="hljs-comment"># parted -l</span><br>Model: ATA WDC WD3200AAJS-2 (scsi)<br>Disk /dev/sdb: 320GB<br>Sector size (logical/physical): 512B/512B<br>Partition Table: msdos<br><br>Number  Start   End     Size    Type      File system     Flags<br>    1      1049kB  263MB   262MB   primary   ext4            boot<br>    2      263MB   7603MB  7340MB  primary   ext4<br>    3      7603MB  9751MB  2147MB  primary   linux-swap(v1)<br>    4      9751MB  320GB   310GB   extended<br>    5      9752MB  320GB   310GB   logical   ext4<br></code></pre></td></tr></table></figure><p>可以看到分区表模式为msdos，分区信息为含有扩展分区，逻辑分区了，下面为新准备的盘进行分区，分区信息跟原来尽量一样，这个地方也可以根据自己的需要进行调整</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@zhongbo ~]<span class="hljs-comment"># parted /dev/sda</span><br>(parted) mklabel msdos <br>(parted) mkpart primary 1049kB 263<br>(parted) mkpart primary 263 7603<br>(parted) mkpart primary 7603 9751 <br>(parted) mkpart extended 9751 100%<br>(parted) mkpart logical 9752 100%<br>(parted) p                                                                <br>Model: ATA INTEL SSDSC2BF24 (scsi)<br>Disk /dev/sda: 240GB<br>Sector size (logical/physical): 512B/512B<br>Partition Table: msdos<br><br>Number  Start   End     Size    Type      File system     Flags<br>    1      1049kB  263MB   262MB   primary   ext4<br>    2      263MB   7603MB  7340MB  primary   ext4<br>    3      7603MB  9751MB  2147MB  primary   linux-swap(v1)<br>    4      9751MB  240GB   230GB   extended                  lba<br>    5      9752MB  240GB   230GB   logical   ext4  <br></code></pre></td></tr></table></figure><p>格式化硬盘</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@zhongbo ~]<span class="hljs-comment"># mkfs.ext4 /dev/sda1  </span><br>[root@zhongbo ~]<span class="hljs-comment"># mkfs.ext4 /dev/sda2</span><br>[root@zhongbo ~]<span class="hljs-comment"># mkswap /dev/sda3 </span><br>[root@zhongbo ~]<span class="hljs-comment"># mkfs.ext4 /dev/sda5</span><br></code></pre></td></tr></table></figure><h3 id="备份原始磁盘上的数据到备份目的盘"><a href="#备份原始磁盘上的数据到备份目的盘" class="headerlink" title="备份原始磁盘上的数据到备份目的盘"></a>备份原始磁盘上的数据到备份目的盘</h3><p>在本地系统盘上创建一个备份的目录，将数据打包放到这个目录下面</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@zhongbo ~]<span class="hljs-comment"># mkdir /backup</span><br></code></pre></td></tr></table></figure><p>按这个系统来说需要备份三个分区&#x2F; , &#x2F;var&#x2F;log&#x2F;, &#x2F;boot </p><p>开始备份&#x2F;<br>所有的操作都进入到&#x2F;目录下操作</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@zhongbo ~]<span class="hljs-comment"># cd /</span><br>[root@zhongbo /]<span class="hljs-comment"># tar zcvpf /backup/gen.tar.gz -C /  --exclude=boot/* --exclude=var/log/* --exclude=backup/*  --exclude=lost+found/* --exclude=mnt/*  --exclude=dev/* --exclude=proc/* --exclude=tmp/* --exclude=sys/* .</span><br></code></pre></td></tr></table></figure><p>注意这里排除了很多不需要备份的目录</p><p>备份boot分区</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@zhongbo /]<span class="hljs-comment"># tar zcvpf /backup/boot.tar.gz -C /boot/ .</span><br></code></pre></td></tr></table></figure><p>备份&#x2F;var&#x2F;log</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@zhongbo /]<span class="hljs-comment"># tar zcvpf /backup/log.tar.gz -C /var/log/ .</span><br></code></pre></td></tr></table></figure><p>恢复&#x2F;的数据</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@zhongbo /]<span class="hljs-comment"># mkdir /mnt/gen</span><br>[root@zhongbo /]<span class="hljs-comment"># mount /dev/sda5 /mnt/gen/</span><br>[root@zhongbo /]<span class="hljs-comment"># tar zxvpf /backup/gen.tar.gz -C /mnt/gen/</span><br></code></pre></td></tr></table></figure><p>恢复&#x2F;boot的数据</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@zhongbo /]<span class="hljs-comment"># mkdir /mnt/boot</span><br>[root@zhongbo /]<span class="hljs-comment"># mount /dev/sda1 /mnt/boot/</span><br>[root@zhongbo /]<span class="hljs-comment"># tar zxvpf /backup/boot.tar.gz -C /mnt/boot/</span><br></code></pre></td></tr></table></figure><p>恢复&#x2F;var&#x2F;log</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@zhongbo /]<span class="hljs-comment"># mkdir /mnt/log</span><br>[root@zhongbo /]<span class="hljs-comment"># mount /dev/sda2 /mnt/log/</span><br>[root@zhongbo /]<span class="hljs-comment"># tar zxvpf /backup/log.tar.gz -C /mnt/log/</span><br></code></pre></td></tr></table></figure><h3 id="修改启动文件"><a href="#修改启动文件" class="headerlink" title="修改启动文件"></a>修改启动文件</h3><p>拿到新的分区的blkid</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@zhongbo /]<span class="hljs-comment"># blkid /dev/sda1</span><br>/dev/sda1: UUID=<span class="hljs-string">&quot;a6ca4369-109c-47ed-a522-7e1752a6681b&quot;</span> TYPE=<span class="hljs-string">&quot;ext4&quot;</span> <br>对应boot<br>[root@zhongbo /]<span class="hljs-comment"># blkid /dev/sda2</span><br>/dev/sda2: UUID=<span class="hljs-string">&quot;f775f248-57d8-49a7-9334-60bc75a53685&quot;</span> TYPE=<span class="hljs-string">&quot;ext4&quot;</span> <br>对应<span class="hljs-built_in">log</span><br><br>[root@zhongbo /]<span class="hljs-comment"># blkid /dev/sda5</span><br>/dev/sda5: UUID=<span class="hljs-string">&quot;6942c7d6-486c-4d51-bb4d-a126ee0c05b1&quot;</span> TYPE=<span class="hljs-string">&quot;ext4&quot;</span><br>对应/<br><br>[root@zhongbo /]<span class="hljs-comment"># blkid /dev/sda3</span><br>/dev/sda3: UUID=<span class="hljs-string">&quot;154784d8-dbe3-45c8-8fcb-cb4f5a14ae44&quot;</span> TYPE=<span class="hljs-string">&quot;swap&quot;</span> <br>对应swap<br></code></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@zhongbo /]<span class="hljs-comment"># vim /mnt/gen/etc/fstab </span><br>修改对应分区的uuid信息<br><br>修改menu.list <br>[root@zhongbo ~]<span class="hljs-comment"># vim /mnt/boot/grub/menu.lst</span><br>修改root= 根分区的uuid，第一次做的时候，就是在这个地方忘了修改，进入系统后能够看到启动画面，但是没法启动内核，修改好了后，就可以正常的启动了<br></code></pre></td></tr></table></figure><h3 id="安装grub"><a href="#安装grub" class="headerlink" title="安装grub"></a>安装grub</h3><p>查找grub分区</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">grub&gt; find /grub/grub.conf<br>find /grub/grub.conf<br> (hd0,0)<br> (hd1,0)<br>grub&gt; <br></code></pre></td></tr></table></figure><p>修改其中的新盘上的grub然后分辨新的grub在哪台机器上，我的新盘是(hd0,0)</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs bash">grub&gt; root (hd0,0) <br>grub&gt; setup (hd0)    <br>setup (hd0)<br> Checking <span class="hljs-keyword">if</span> <span class="hljs-string">&quot;/boot/grub/stage1&quot;</span> exists... no<br> Checking <span class="hljs-keyword">if</span> <span class="hljs-string">&quot;/grub/stage1&quot;</span> exists... <span class="hljs-built_in">yes</span><br> Checking <span class="hljs-keyword">if</span> <span class="hljs-string">&quot;/grub/stage2&quot;</span> exists... <span class="hljs-built_in">yes</span><br> Checking <span class="hljs-keyword">if</span> <span class="hljs-string">&quot;/grub/e2fs_stage1_5&quot;</span> exists... <span class="hljs-built_in">yes</span><br> Running <span class="hljs-string">&quot;embed /grub/e2fs_stage1_5 (hd0)&quot;</span>...  27 sectors are embedded.<br>succeeded<br> Running <span class="hljs-string">&quot;install /grub/stage1 (hd0) (hd0)1+27 p (hd0,0)/grub/stage2 /grub/grub.conf&quot;</span>... succeeded<br>Done.<br>grub&gt; quit<br></code></pre></td></tr></table></figure><p>完成后，系统盘就完全备份了一份了，重启就可以启动一个一模一样的系统了</p>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>linux下制作软件包安装服务器</title>
    <link href="/2015/04/28/linux%E4%B8%8B%E5%88%B6%E4%BD%9C%E8%BD%AF%E4%BB%B6%E5%8C%85%E5%AE%89%E8%A3%85%E6%9C%8D%E5%8A%A1%E5%99%A8/"/>
    <url>/2015/04/28/linux%E4%B8%8B%E5%88%B6%E4%BD%9C%E8%BD%AF%E4%BB%B6%E5%8C%85%E5%AE%89%E8%A3%85%E6%9C%8D%E5%8A%A1%E5%99%A8/</url>
    
    <content type="html"><![CDATA[<p>linux下的软件包在有网络的情况下比较好安装，在ubuntu下，更新sourcelist，然后使用apt-get就可以很方便的安装包，在centos下面，更新yum列表，然后使用yum也可以进行方便的软件安装，但是在没有网络的情况下就比较难安装，可以用一个个包的安装的方式去安装，这个在少量的包的情况下比较好处理，在多的情况下就比较麻烦了，本篇文档，就是介绍了在无网的情况下，根据自己的需要制作内网的包的安装服务器</p><h2 id="centos系列"><a href="#centos系列" class="headerlink" title="centos系列"></a>centos系列</h2><h3 id="使用光驱作为安装源"><a href="#使用光驱作为安装源" class="headerlink" title="使用光驱作为安装源"></a>使用光驱作为安装源</h3><p>1、将光驱挂载到服务器的本地目录</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@node1 ~]<span class="hljs-comment"># mount /dev/cdrom /mnt</span><br>mount: block device /dev/sr0 is write-protected, mounting read-only<br></code></pre></td></tr></table></figure><p>2、修改本地的yum源文件，将源指向光驱挂载的目录</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">vim /etc/yum.repos.d/myiso.repo<br></code></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs bash">添加<br>[myiso]<br>name=myiso<br>baseurl=file:///mnt<br>gpgcheck=0<br>enabled=1<br>gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-6<br></code></pre></td></tr></table></figure><p>3、更新本地的源缓存</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@node1 yum.repos.d]<span class="hljs-comment"># yum makecache</span><br>Loaded plugins: security<br>myiso                                 | 3.6 kB     00:00 ... <br>myiso/group_gz                        |  17 kB     00:00 ... <br>myiso/filelists_db                    | 3.4 MB     00:00 ... <br>myiso/primary_db                      | 6.8 MB     00:00 ... <br>myiso/other_db                        | 2.3 MB     00:00 ... <br>Metadata Cache Created<br></code></pre></td></tr></table></figure><p>做完上面的就可以使用本地的光驱的中的包使用yum安装了</p><h3 id="使用iso文件作为安装源"><a href="#使用iso文件作为安装源" class="headerlink" title="使用iso文件作为安装源"></a>使用iso文件作为安装源</h3><p>1、将iso文件拷贝到服务器，然后挂载到服务器本地</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@node1 ~]<span class="hljs-comment"># mount -t iso9660 OracleLinux-R6-U5-Server-x86_64-dvd.\[V41362-01\].iso /mnt -o loop</span><br>mount: /root/OracleLinux-R6-U5-Server-x86_64-dvd.[V41362-01].iso is write-protected, mounting read-only<br></code></pre></td></tr></table></figure><p>2、修改本地的yum源文件，将源指向光驱挂载的目录</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs bash">vim /etc/yum.repos.d/myiso.repo<br>添加<br>[myiso]<br>name=myiso<br>baseurl=file:///mnt<br>gpgcheck=0<br>enabled=1<br>gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-6<br></code></pre></td></tr></table></figure><p>3、更新本地的源缓存</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@node1 yum.repos.d]<span class="hljs-comment"># yum makecache</span><br>Loaded plugins: security<br>myiso                                 | 3.6 kB     00:00 ... <br>myiso/group_gz                        |  17 kB     00:00 ... <br>myiso/filelists_db                    | 3.4 MB     00:00 ... <br>myiso/primary_db                      | 6.8 MB     00:00 ... <br>myiso/other_db                        | 2.3 MB     00:00 ... <br>Metadata Cache Created<br></code></pre></td></tr></table></figure><p>做完上面的就可以使用本地的iso中的包使用yum安装了</p><h3 id="使用安装包做一个ftp的yum安装服务器"><a href="#使用安装包做一个ftp的yum安装服务器" class="headerlink" title="使用安装包做一个ftp的yum安装服务器"></a>使用安装包做一个ftp的yum安装服务器</h3><p>1、安装vsftpd服务器</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">rpm -ivh vsftpd-2.2.2-11.el6_4.1.x86_64.rpm <br></code></pre></td></tr></table></figure><p>这个包在操作iso中的包路径下面有</p><p>2、配置ftp服务器<br>默认的ftp目录为&#x2F;var&#x2F;ftp&#x2F;pub&#x2F;</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">创建源目录<br><span class="hljs-built_in">mkdir</span> /var/ftp/pub/centos<br>将iso挂载到源目录，也可以直接将文件拷贝到这个目录当中去<br>mount -t iso9660 OracleLinux-R6-U5-Server-x86_64-dvd.\[V41362-01\].iso /var/ftp/pub/centos/  -o loop<br></code></pre></td></tr></table></figure><p>配置完成后，镜像的下载地址就为:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">ftp://youripaddress/pub/centos/<br></code></pre></td></tr></table></figure><p>我的为:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">ftp://172.16.81.129/pub/centos/<br></code></pre></td></tr></table></figure><p>3、使用的机器的配置<br>修改yum源</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs bash">vim /etc/yum.repos.d/ftpcentos.repo<br>添加<br>[ftpcentos]<br>name=ftpcentos<br>baseurl=ftp://172.16.81.129/pub/centos/<br>enabled=1<br></code></pre></td></tr></table></figure><p>4、更新yum的缓存</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@node2 yum.repos.d]<span class="hljs-comment"># yum makecache</span><br>Loaded plugins: security<br>ftpcentos                          | 3.7 kB     00:00     <br>ftpcentos/filelists_db             | 3.3 MB     00:00     <br>ftpcentos/primary_db               | 3.0 MB     00:00     <br>ftpcentos/other_db                 | 1.3 MB     00:00     <br>ftpcentos/group_gz                 | 203 kB     00:00     <br>Metadata Cache Created<br></code></pre></td></tr></table></figure><h3 id="制作自己的定制源"><a href="#制作自己的定制源" class="headerlink" title="制作自己的定制源"></a>制作自己的定制源</h3><p>1、安装制作源的工具</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">rpm -ivh deltarpm-3.5-0.5.20090913git.el6.x86_64.rpm<br>rpm -ivh python-deltarpm-3.5-0.5.20090913git.el6.x86_64.rpm <br>rpm -ivh createrepo-0.9.9-18.0.1.el6.noarch.rpm <br></code></pre></td></tr></table></figure><p>这三个包在默认的iso当中有</p><p>2、将安装包拷贝到指定的目录</p><p>本例子使用leveldb，snappy举例，snappy是leveldb的依赖包。<br>将</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">leveldb-1.7.0-2.el6.x86_64.rpm<br>snappy-1.0.5-1.el6.x86_64.rpm <br></code></pre></td></tr></table></figure><p>拷贝到<br>&#x2F;usr&#x2F;src&#x2F;myepel&#x2F;目录当中去</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@node1 src]<span class="hljs-comment"># createrepo myepel/</span><br>Spawning worker 0 with 2 pkgs<br>Workers Finished<br>Gathering worker results<br><br>Saving Primary metadata<br>Saving file lists metadata<br>Saving other metadata<br>Generating sqlite DBs<br>Sqlite DBs complete<br></code></pre></td></tr></table></figure><p>检查目录下面会生成repodata，存储一些元数据信息</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@node1 src]<span class="hljs-comment"># ll myepel/</span><br>total 200<br>-rw-r--r-- 1 root root 162052 Apr 16 08:45 leveldb-1.7.0-2.el6.x86_64.rpm<br>drwxr-xr-x 2 root root   4096 Apr 16 08:48 repodata<br>-rw-r--r-- 1 root root  34372 Apr 16 08:45 snappy-1.0.5-1.el6.x86_64.rpm<br></code></pre></td></tr></table></figure><p>3、修改源列表</p><p>同样的将yum源的目录指向这个myepel&#x2F;</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">[myiso]<br>name=myiso<br>baseurl=file:///usr/src/myepel/<br>gpgcheck=0<br>enabled=1<br></code></pre></td></tr></table></figure><p>4、更新yum缓存</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@node1 yum.repos.d]<span class="hljs-comment"># yum makecache</span><br>[root@node1 yum.repos.d]<span class="hljs-comment"># yum install leveldb</span><br></code></pre></td></tr></table></figure><p>即可安装了</p>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>windows下命令行设置静态IP</title>
    <link href="/2015/04/03/windows%E4%B8%8B%E5%91%BD%E4%BB%A4%E8%A1%8C%E8%AE%BE%E7%BD%AE%E9%9D%99%E6%80%81IP/"/>
    <url>/2015/04/03/windows%E4%B8%8B%E5%91%BD%E4%BB%A4%E8%A1%8C%E8%AE%BE%E7%BD%AE%E9%9D%99%E6%80%81IP/</url>
    
    <content type="html"><![CDATA[<p>windows 10 预览版出现无法设置静态IP的bug，只能通过命令行进行设置，开启powershell，然后执行下列的命令即可<br>下面的“以太网 3” 为你设置的网卡的网卡名称,注意不要忘了空格</p><!--break--><p>设置静态IP:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">netsh interface ipv4 <span class="hljs-built_in">set</span> address <span class="hljs-string">&quot;以太网 3&quot;</span>  static 192.168.0.71 255.255.0.0 192.168.26.1<br></code></pre></td></tr></table></figure><p>增加静态IP:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">netsh interface ipv4 <span class="hljs-built_in">set</span> dns <span class="hljs-string">&quot;以太网 3&quot;</span>  static 223.5.5.5<br></code></pre></td></tr></table></figure><p>增加静态IP:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">netsh interface ipv4 add address <span class="hljs-string">&quot;以太网 3&quot;</span> 11.12.0.0 255.255.0.0<br></code></pre></td></tr></table></figure><p>重置为dhcp:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">netsh interface ipv4 <span class="hljs-built_in">set</span> address <span class="hljs-string">&quot;以太网 3&quot;</span>  dhcp<br></code></pre></td></tr></table></figure><p>基本设置应该够用了</p>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>shell脚本的自动交互</title>
    <link href="/2015/04/02/shell%E8%84%9A%E6%9C%AC%E7%9A%84%E8%87%AA%E5%8A%A8%E4%BA%A4%E4%BA%92/"/>
    <url>/2015/04/02/shell%E8%84%9A%E6%9C%AC%E7%9A%84%E8%87%AA%E5%8A%A8%E4%BA%A4%E4%BA%92/</url>
    
    <content type="html"><![CDATA[<p>使用expect来自动应答shell的交互</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-meta">#!/usr/bin/expect</span><br>spawn openssl req -new -key server.key -out server1.csr<br>expect <span class="hljs-string">&quot;Country Name&quot;</span><br>send <span class="hljs-string">&quot;\n&quot;</span><br>expect <span class="hljs-string">&quot;State or Province Name&quot;</span><br>send <span class="hljs-string">&quot;\n&quot;</span><br>interact <br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>history命令的优化</title>
    <link href="/2015/03/31/history%E5%91%BD%E4%BB%A4%E7%9A%84%E4%BC%98%E5%8C%96/"/>
    <url>/2015/03/31/history%E5%91%BD%E4%BB%A4%E7%9A%84%E4%BC%98%E5%8C%96/</url>
    
    <content type="html"><![CDATA[<p>现在在项目中遇到这个情况比较多，在执行了一系列的命令后，想去翻历史记录的时候，翻不到历史记录，不同终端的命令，没有汇总，也不清楚那条命令是什么时候执行的，所以需要对默认的命令进行下面两个优化：</p><!--break--><ul><li>让历史记录里面带有时间</li><li>让所有终端命令都记录到history当中去</li></ul><h3 id="添加日期"><a href="#添加日期" class="headerlink" title="添加日期"></a>添加日期</h3><p>在 &#x2F;etc&#x2F;bashrc 末尾添加：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">HISTTIMEFORMAT=<span class="hljs-string">&quot;%F %T &quot;</span><br><span class="hljs-built_in">export</span> HISTTIMEFORMAT<br></code></pre></td></tr></table></figure><p>然后执行:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">source</span> /etc/bashrc<br></code></pre></td></tr></table></figure><p>新开终端，history就已经带有日期</p><p>###汇总命令，并且不同的终端可以通过上翻查询到命令</p><p>在 &#x2F;etc&#x2F;bashrc 末尾添加：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># Avoid duplicates</span><br><span class="hljs-built_in">export</span> HISTCONTROL=ignoredups:erasedups<br><span class="hljs-comment"># When the shell exits, append to the history file instead of overwriting it</span><br><span class="hljs-built_in">shopt</span> -s histappend<br><br><span class="hljs-comment"># After each command, append to the history file and reread it</span><br><span class="hljs-built_in">export</span> PROMPT_COMMAND=<span class="hljs-string">&quot;<span class="hljs-variable">$&#123;PROMPT_COMMAND:+$PROMPT_COMMAND$&#x27;\n&#x27;&#125;</span>history -a; history -c; history -r&quot;</span><br></code></pre></td></tr></table></figure><p>然后执行:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">source</span> /etc/bashrc<br></code></pre></td></tr></table></figure><p>###调整大小，增加到10000条<br>在 &#x2F;etc&#x2F;bashrc 末尾添加：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">export</span> HISTSIZE=10000<br></code></pre></td></tr></table></figure><p>然后执行:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">source</span> /etc/bashrc<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>压缩css与js</title>
    <link href="/2015/03/25/%E5%8E%8B%E7%BC%A9css%E4%B8%8Ejs/"/>
    <url>/2015/03/25/%E5%8E%8B%E7%BC%A9css%E4%B8%8Ejs/</url>
    
    <content type="html"><![CDATA[<p>使用yuicompressor 进行css和js的压缩</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-meta">#! /bin/sh</span><br>yasuocss=<span class="hljs-string">&quot;java -jar /root/yuicompressor-2.4.8.jar --type css  --charset utf-8&quot;</span><br>yasuojs=<span class="hljs-string">&quot;java -jar /root/yuicompressor-2.4.8.jar --type js  --charset utf-8&quot;</span><br><span class="hljs-comment">######################css</span><br><span class="hljs-keyword">for</span> file <span class="hljs-keyword">in</span>  ./SGMag/sites/media/css/sgmag/*.css ./SGMag/sites/media/css/*.css<br><span class="hljs-keyword">do</span><br><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;<span class="hljs-variable">$file</span>&quot;</span><br><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;<span class="hljs-variable">$file</span>&quot;</span>bk<br><span class="hljs-built_in">mv</span> <span class="hljs-variable">$file</span>  <span class="hljs-string">&quot;<span class="hljs-variable">$file</span>&quot;</span>bk<br><span class="hljs-variable">$yasuocss</span>  <span class="hljs-variable">$file</span><span class="hljs-string">&quot;bk&quot;</span> &gt;  <span class="hljs-variable">$file</span><br><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;<span class="hljs-variable">$file</span>&quot;</span><br><span class="hljs-keyword">done</span><br><span class="hljs-comment">#######################js</span><br><span class="hljs-keyword">for</span> file <span class="hljs-keyword">in</span>  ./SGMag/sites/media/component/*.js ./SGMag/sites/media/*.js  ./SGMag/sites/media/pagejs/*.js  <br><span class="hljs-keyword">do</span><br><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;<span class="hljs-variable">$file</span>&quot;</span><br><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;<span class="hljs-variable">$file</span>&quot;</span>bk<br><span class="hljs-built_in">mv</span> <span class="hljs-variable">$file</span>  <span class="hljs-string">&quot;<span class="hljs-variable">$file</span>&quot;</span>bk<br><span class="hljs-variable">$yasuojs</span>  <span class="hljs-variable">$file</span><span class="hljs-string">&quot;bk&quot;</span> &gt;  <span class="hljs-variable">$file</span><br><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;<span class="hljs-variable">$file</span>&quot;</span><br><span class="hljs-keyword">done</span><br><span class="hljs-comment">########################报错处理</span><br><span class="hljs-built_in">cp</span> -rf  ./SGMag/sites/media/pagejs/cluster/hosttools.jsbk  ./SGMag/sites/media/pagejs/cluster/hosttools.js<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>cgroup实践-资源控制</title>
    <link href="/2015/03/24/cgroup%E5%AE%9E%E8%B7%B5-%E8%B5%84%E6%BA%90%E6%8E%A7%E5%88%B6/"/>
    <url>/2015/03/24/cgroup%E5%AE%9E%E8%B7%B5-%E8%B5%84%E6%BA%90%E6%8E%A7%E5%88%B6/</url>
    
    <content type="html"><![CDATA[<p>1、Cgroup安装<br>安装Cgroups需要libcap-devel和libcgroup两个相关的包</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">yum install gcc libcap-devel <br></code></pre></td></tr></table></figure><p>2、Cgroup挂载配置</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs bash">Cgroup对应服务名称为cgconfig，cgconfig默认采用“多挂载点”挂载。经过实际测试，发现在CentOS环境中应采用“单挂载点”进行挂载，因此应当卸载原有cgroup文件系统，并禁用cgconfig。<br>cgclear或者sudo service cgconfig stop <span class="hljs-comment"># 停止cgconfig，卸载cgroup目录</span><br>sudo chkconfig cgconfig off          <span class="hljs-comment"># 禁用cgconfig服务，避免其开机启动</span><br>然后采用“单挂载点”方式重新挂载cgroup。<br>可以直接手动挂载，这样仅当次挂载成功。<br>mount -t cgroup none /cgroup<br>然后编辑/etc/fstab/，输入下列内容。这样每次开机后都会自动挂载。<br>none   /cgroup  cgroup  defaults   0 0<br></code></pre></td></tr></table></figure><p>3、常用的Cgroup相关命令和配置文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs bash">service cgconfig status|start|stop|restart    <span class="hljs-comment">#查看已存在子系统</span><br>lssubsys –am    <span class="hljs-comment">#查看已存在子系统</span><br>cgclear   <span class="hljs-comment"># 清除所有挂载点内部文件，相当于service  cgconfig stop</span><br>cgconfigparser -l /etc/cgconfig.conf    <span class="hljs-comment">#重新挂载</span><br><br>Cgroup默认挂载点（CentOS）：/cgroup<br>cgconfig配置文件：/etc/cgconfig.conf<br></code></pre></td></tr></table></figure><p>4、libcgroup Man Page简介</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs bash">man 1 cgclassify -- cgclassify命令是用来将运行的任务移动到一个或者多个cgroup。<br>man 1 cgclear -- cgclear 命令是用来删除层级中的所有cgroup。<br>man 5 cgconfig.conf -- 在cgconfig.conf文件中定义cgroup。<br>man 8 cgconfigparser -- cgconfigparser命令解析cgconfig.conf文件和并挂载层级。<br><br>man 1 cgcreate -- cgcreate在层级中创建新cgroup。<br>man 1 cgdelete -- cgdelete命令删除指定的cgroup。<br>man 1 cgexec -- cgexec命令在指定的cgroup中运行任务。<br>man 1 cgget -- cgget命令显示cgroup参数。<br>man 5 cgred.conf -- cgred.conf是cgred服务的配置文件。<br>man 5 cgrules.conf -- cgrules.conf 包含用来决定何时任务术语某些  cgroup的规则。<br><br>man 8 cgrulesengd -- cgrulesengd 在  cgroup 中发布任务。<br>man 1 cgset -- cgset 命令为  cgroup 设定参数。<br>man 1 lscgroup -- lscgroup 命令列出层级中的  cgroup。<br>man 1 lssubsys -- lssubsys 命令列出包含指定子系统的层级。<br></code></pre></td></tr></table></figure><h3 id="测试一：限制cpu的资源"><a href="#测试一：限制cpu的资源" class="headerlink" title="测试一：限制cpu的资源"></a>测试一：限制cpu的资源</h3><p>测试后验证了可以做到：</p><ul><li><pre><code class="hljs">限制进程的cpu占用百分比</code></pre></li><li><pre><code class="hljs">限制多个进程组的之间的cpu使用权重</code></pre></li><li><pre><code class="hljs">指定进程的使用的cpu和内存组（绑定cpu）</code></pre></li></ul><p>跑一个耗cpu的脚本</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">x=0<br><span class="hljs-keyword">while</span> [ True ];<span class="hljs-keyword">do</span><br>    x=<span class="hljs-variable">$x</span>+1<br><span class="hljs-keyword">done</span>;<br></code></pre></td></tr></table></figure><p>top可以看到这个脚本基本占了100%的cpu资源</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs bash">top - 15:30:01 up  1:03,  5 <span class="hljs-built_in">users</span>,  load average: 0.30, 0.50, 0.39<br>Tasks: 210 total,   2 running, 208 sleeping,   0 stopped,   0 zombie<br>Cpu(s):  6.3%us,  0.1%sy,  0.0%ni, 93.5%<span class="hljs-built_in">id</span>,  0.2%wa,  0.0%hi,  0.0%si,  0.0%st<br>Mem:  49461228k total, 13412644k used, 36048584k free,    75384k buffers<br>Swap:  2097148k total,        0k used,  2097148k free, 12498636k cached<br><br>    PID USER      PR  NI  VIRT  RES  SHR S %CPU %MEM    TIME+  COMMAND<br>11605 root      20   0  104m 1528 1016 R 99.7  0.0   2:30.48 sh<br>105 root      20   0     0    0    0 S  0.3  0.0   0:00.11 kworker/8:1  <br></code></pre></td></tr></table></figure><p>创建一个控制组控制这个进程的cpu资源</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">mkdir</span> -p /cgroup/cpu/foo     <span class="hljs-comment">#新建一个控制组foo</span><br><span class="hljs-built_in">echo</span> 50000 &gt; /cgroup/cpu/foo/cpu.cfs_quota_us  <span class="hljs-comment">#将cpu.cfs_quota_us设为50000，相对于cpu.cfs_period_us的100000是50%</span><br><span class="hljs-built_in">echo</span> 11605 &gt; /cgroup/cpu/foo/tasks<br></code></pre></td></tr></table></figure><p>然后top的实时统计数据如下，cpu占用率将近50%，看来cgroups关于cpu的控制起了效果</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs bash">top - 15:32:48 up  1:06,  5 <span class="hljs-built_in">users</span>,  load average: 0.80, 0.68, 0.48<br>Tasks: 210 total,   2 running, 208 sleeping,   0 stopped,   0 zombie<br>Cpu(s):  3.2%us,  0.0%sy,  0.0%ni, 96.8%<span class="hljs-built_in">id</span>,  0.0%wa,  0.0%hi,  0.0%si,  0.0%st<br>Mem:  49461228k total, 13412276k used, 36048952k free,    75400k buffers<br>Swap:  2097148k total,        0k used,  2097148k free, 12498652k cached<br><br>PID USER      PR  NI  VIRT  RES  SHR S %CPU %MEM    TIME+  COMMAND<br>11605 root      20   0  104m 1724 1016 R 50.2  0.0   5:09.97 sh<br>11639 root      20   0 15200 1200  820 R  0.3  0.0   0:00.03 top    <br></code></pre></td></tr></table></figure><p>可以看到，进程的 cpu 占用已经被成功地限制到了 50% 。这里，测试的虚拟机只有一个核心。在多核情况下，看到的值会不一样。另外，cfs_quota_us 也是可以大于 cfs_period_us 的，这主要是对于多核情况。有 n 个核时，一个控制组中的进程自然最多就能用到 n 倍的 cpu 时间。</p><p>这两个值在 cgroups 层次中是有限制的，下层的资源不能超过上层。具体的说，就是下层的 cpu.cfs_period_us 值不能小于上层的值，cpu.cfs_quota_us 值不能大于上层的值。</p><p>另外的一组 cpu.rt_period_us、cpu.rt_runtime_us 对应的是实时进程的限制，平时可能不会有机会用到。</p><p>在 cpu 子系统中，cpu.stat 就是用前面那种方法做的资源限制的统计了。nr_periods、nr_throttled 就是总共经过的周期，和其中受限制的周期。throttled_time 就是总共被控制组掐掉的 cpu 使用时间。</p><p>还有个 cpu.shares， 它也是用来限制 cpu 使用的。但是与 cpu.cfs_quota_us、cpu.cfs_period_us 有挺大区别。cpu.shares 不是限制进程能使用的绝对的 cpu 时间，而是控制各个组之间的配额。比如</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">/cpu/cpu.shares : 1024<br>/cpu/foo/cpu.shares : 2048<br></code></pre></td></tr></table></figure><p>那么当两个组中的进程都满负荷运行时，&#x2F;foo 中的进程所能占用的 cpu 就是 &#x2F; 中的进程的两倍。如果再建一个 &#x2F;foo&#x2F;bar 的 cpu.shares 也是 1024，且也有满负荷运行的进程，那 &#x2F;、&#x2F;foo、&#x2F;foo&#x2F;bar 的 cpu 占用比就是 1:2:1 。前面说的是各自都跑满的情况。如果其他控制组中的进程闲着，那某一个组的进程完全可以用满全部 cpu。可见通常情况下，这种方式在保证公平的情况下能更充分利用资源。</p><p>此外，还可以限定进程可以使用哪些 cpu 核心。cpuset 子系统就是处理进程可以使用的 cpu 核心和内存节点，以及其他一些相关配置。这部分的很多配置都和 NUMA 有关。其中 cpuset.cpus、cpuset.mems 就是用来限制进程可以使用的 cpu 核心和内存节点的。这两个参数中 cpu 核心、内存节点都用 id 表示，之间用 “,” 分隔。比如 0,1,2 。也可以用 “-” 表示范围，如 0-3 。两者可以结合起来用。如“0-2,6,7”。在添加进程前，cpuset.cpus、cpuset.mems 必须同时设置，而且必须是兼容的，否则会出错。例如</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># echo 0 &gt;/sys/fs/cgroup/cpuset/foo/cpuset.cpus</span><br><span class="hljs-comment"># echo 0 &gt;/sys/fs/cgroup/cpuset/foo/cpuset.mems</span><br>这样， /foo 中的进程只能使用 cpu0 和内存节点0。用<br><br><span class="hljs-comment"># cat /proc/&lt;pid&gt;/status|grep &#x27;_allowed_list&#x27;</span><br></code></pre></td></tr></table></figure><p>cgroups 除了用来限制资源使用外，还有资源统计的功能。做云计算的计费就可以用到它。有一个 cpuacct 子系统专门用来做 cpu 资源统计。cpuacct.stat 统计了该控制组中进程用户态和内核态的 cpu 使用量，单位是 USER_HZ，也就是 jiffies、cpu 滴答数。每秒的滴答数可以用 getconf CLK_TCK 来获取，通常是 100。将看到的值除以这个值就可以换算成秒。</p><h3 id="测试二：限制进程的内存资源"><a href="#测试二：限制进程的内存资源" class="headerlink" title="测试二：限制进程的内存资源"></a>测试二：限制进程的内存资源</h3><p>测试后验证了：</p><ul><li>限制了资源的占用，达到内存以后，进程直接杀掉</li></ul><p>测试方法：</p><p>跑一个耗内存的脚本，内存不断增长</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">x=<span class="hljs-string">&quot;a&quot;</span><br><span class="hljs-keyword">while</span> [ True ];<span class="hljs-keyword">do</span><br>    x=$x<span class="hljs-variable">$x</span><br><span class="hljs-keyword">done</span>;<br></code></pre></td></tr></table></figure><p>top看内存占用稳步上升</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">    PID USER      PR  NI  VIRT  RES  SHR S %CPU %MEM    TIME+  COMMAND<br>    30215 root      20   0  871m 501m 1036 R 99.8 26.7   0:38.69 sh  <br>30215 root      20   0 1639m 721m 1036 R 98.7 38.4   1:03.99 sh <br>30215 root      20   0 1639m 929m 1036 R 98.6 49.5   1:13.73 sh<br></code></pre></td></tr></table></figure><p>下面用cgroups控制这个进程的内存资源</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">mkdir</span> -p /cgroup/memory/foo<br><span class="hljs-built_in">echo</span> 1048576 &gt;  /cgroup/memory/foo/memory.limit_in_bytes   <span class="hljs-comment">#分配1MB的内存给这个控制组</span><br><span class="hljs-built_in">echo</span> 30215 &gt; /cgroup/memory/foo/tasks <br></code></pre></td></tr></table></figure><p>发现之前的脚本被kill掉</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@localhost ~]<span class="hljs-comment"># sh /home/test.sh </span><br>已杀死<br></code></pre></td></tr></table></figure><p>因为这是强硬的限制内存，当进程试图占用的内存超过了cgroups的限制，会触发out of memory，导致进程被kill掉。</p><p>实际情况中对进程的内存使用会有一个预估，然后会给这个进程的限制超配50%比如，除非发生内存泄露等异常情况，才会因为cgroups的限制被kill掉。</p><p>也可以通过配置关掉cgroups oom kill进程，通过memory.oom_control来实现（oom_kill_disable 1），但是尽管进程不会被直接杀死，但进程也进入了休眠状态，无法继续执行，仍然无法服务。</p><p>关于内存的控制，还有以下配置文件，关于虚拟内存的控制，以及权值比重式的内存控制等</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@localhost /]<span class="hljs-comment"># ls /cgroup/memory/foo/</span><br>cgroup.event_control  memory.force_empty         memory.memsw.failcnt             <br>memory.memsw.usage_in_bytes      memory.soft_limit_in_bytes  memory.usage_in_bytes  tasks<br>cgroup.procs          memory.limit_in_bytes      memory.memsw.limit_in_bytes      <br>memory.move_charge_at_immigrate  memory.stat                 memory.use_hierarchy<br>memory.failcnt        memory.max_usage_in_bytes  memory.memsw.max_usage_in_bytes  <br>memory.oom_control               memory.swappiness           notify_on_release<br></code></pre></td></tr></table></figure><h3 id="测试三：限制进程的IO资源"><a href="#测试三：限制进程的IO资源" class="headerlink" title="测试三：限制进程的IO资源"></a>测试三：限制进程的IO资源</h3><p>测试验证了：</p><ul><li>能够控制io设备的读写速度</li></ul><p>跑一个耗io的脚本</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs bash">    <span class="hljs-built_in">dd</span> <span class="hljs-keyword">if</span>=/dev/sda of=/dev/null <br><br>通过iotop看io占用情况，磁盘速度到了284M/s<br><br><br>30252 be/4 root      284.71 M/s    0.00 B/s  0.00 %  0.00 % <span class="hljs-built_in">dd</span> <span class="hljs-keyword">if</span>=/dev/sda of=/dev/null<br></code></pre></td></tr></table></figure><p>下面用cgroups控制这个进程的io资源</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">mkdir</span> -p /cgroup/blkio/foo<br><br><span class="hljs-built_in">echo</span> <span class="hljs-string">&#x27;8:0   1048576&#x27;</span> &gt;  /cgroup/blkio/foo/blkio.throttle.read_bps_device<br><span class="hljs-comment">#8:0对应主设备号和副设备号，可以通过ls -l /dev/sda查看</span><br><span class="hljs-built_in">echo</span> 30252 &gt; /cgroup/blkio/foo/tasks<br></code></pre></td></tr></table></figure><p>再通过iotop看，确实将读速度降到了1M&#x2F;s</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">30252 be/4 root      993.36 K/s    0.00 B/s  0.00 %  0.00 % <span class="hljs-built_in">dd</span> <span class="hljs-keyword">if</span>=/dev/sda of=/dev/null  <br></code></pre></td></tr></table></figure><p>对于io还有很多其他可以控制层面和方式，如下</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@localhost ~]<span class="hljs-comment"># ls /cgroup/blkio/foo/</span><br>blkio.io_merged         blkio.io_serviced      blkio.reset_stats                <br>blkio.throttle.io_serviced       blkio.throttle.write_bps_device   blkio.weight          cgroup.procs<br>blkio.io_queued         blkio.io_service_time  blkio.sectors                    <br>blkio.throttle.read_bps_device   blkio.throttle.write_iops_device  blkio.weight_device   notify_on_release<br>blkio.io_service_bytes  blkio.io_wait_time     blkio.throttle.io_service_bytes  <br>blkio.throttle.read_iops_device  blkio.time                        cgroup.event_control  tasks<br></code></pre></td></tr></table></figure><p>blkio 子系统里东西很多。不过大部分都是只读的状态报告，可写的参数就只有下面这几个：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs bash">blkio.throttle.read_bps_device<br>blkio.throttle.read_iops_device<br>blkio.throttle.write_bps_device<br>blkio.throttle.write_iops_device<br>blkio.weight<br>blkio.weight_device<br><br>这些都是用来控制进程的磁盘 io 的。很明显地分成两类，其中带“throttle”的，顾名思义就是节流阀，将流量限制在某个值下。而“weight”就是分配 io 的权重。<br>再看看 blkio.weight 。blkio 的 throttle 和 weight 方式和 cpu 子系统的 quota 和 shares 有点像，都是一种是绝对限制，另一种是相对限制，并且在不繁忙的时候可以充分利用资源，权重值的范围在 10 – 1000 之间。<br></code></pre></td></tr></table></figure><p>测试权重方式要麻烦一点。因为不是绝对限制，所以会受到文件系统缓存的影响。如在虚拟机中测试，要关闭虚机如我用的 VirtualBox 在宿主机上的缓存。如要测试读 io 的效果，先生成两个几个 G 的大文件 &#x2F;tmp&#x2F;file_1，&#x2F;tmp&#x2F;file_2 ，可以用 dd 搞。然后设置两个权重</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># echo 500 &gt;/sys/fs/cgroup/blkio/foo/blkio.weight</span><br><span class="hljs-comment"># echo 100 &gt;/sys/fs/cgroup/blkio/bar/blkio.weight</span><br></code></pre></td></tr></table></figure><p>测试前清空文件系统缓存，以免干扰测试结果</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">sync</span><br><span class="hljs-built_in">echo</span> 3 &gt;/proc/sys/vm/drop_caches<br></code></pre></td></tr></table></figure><p>在这两个控制组中用 dd 产生 io 测试效果。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># cgexec -g &quot;blkio:foo&quot; dd if=/tmp/file_1 of=/dev/null &amp;</span><br>[1] 1838<br><span class="hljs-comment"># cgexec -g &quot;blkio:bar&quot; dd if=/tmp/file_2 of=/dev/null &amp;</span><br>[2] 1839<br></code></pre></td></tr></table></figure><p>还是用 iotop 看看效果</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">TID  PRIO  USER     DISK READ  DISK WRITE  SWAPIN     IO&gt;    COMMAND<br>1839 be/4 root       48.14 M/s    0.00 B/s  0.00 % 99.21 % <span class="hljs-built_in">dd</span> <span class="hljs-keyword">if</span>=/tmp/file_2 of=/dev/null<br>1838 be/4 root      223.59 M/s    0.00 B/s  0.00 % 16.44 % <span class="hljs-built_in">dd</span> <span class="hljs-keyword">if</span>=/tmp/file_1 of=/dev/null<br></code></pre></td></tr></table></figure><p>两个进程每秒读的字节数虽然会不断变动，但是大致趋势还是维持在 1:5 左右，和设定的 weight 比例一致。blkio.weight_device 是分设备的。写入时，前面再加上设备号即可。</p><h2 id="实践记录"><a href="#实践记录" class="headerlink" title="实践记录"></a>实践记录</h2><p>1、假如已经配置好一个资源组，现在想让一个服务按这个组的资源分配来运行服务，而不需要去找到进程号再写入到tasks中</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8105 ~]<span class="hljs-comment"># cgexec -g cpu:daemons/ftp top</span><br></code></pre></td></tr></table></figure><p>这个运行以后有会自动将top进程号写入到tasks当中去</p><p>2、查询一个组里面设置的资源的限制</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs bash">设置的值会显示出来，没有设置的就会提示没有找到<br><br>[root@lab8105 ~]<span class="hljs-comment"># cgget  daemons/ftp</span><br>daemons/ftp:<br>cgget: cannot find controller <span class="hljs-string">&#x27;cpuset&#x27;</span> <span class="hljs-keyword">in</span> group <span class="hljs-string">&#x27;daemons/ftp&#x27;</span><br>cpu.rt_period_us: 1000000<br>cpu.rt_runtime_us: 0<br>cpu.stat: nr_periods 0<br>    nr_throttled 0<br>    throttled_time 0<br>cpu.cfs_period_us: 5000<br>cpu.cfs_quota_us: -1<br>cpu.shares: 1000<br>cgget: cannot find controller <span class="hljs-string">&#x27;cpuacct&#x27;</span> <span class="hljs-keyword">in</span> group <span class="hljs-string">&#x27;daemons/ftp&#x27;</span><br>cgget: cannot find controller <span class="hljs-string">&#x27;memory&#x27;</span> <span class="hljs-keyword">in</span> group <span class="hljs-string">&#x27;daemons/ftp&#x27;</span><br>cgget: cannot find controller <span class="hljs-string">&#x27;devices&#x27;</span> <span class="hljs-keyword">in</span> group <span class="hljs-string">&#x27;daemons/ftp&#x27;</span><br>cgget: cannot find controller <span class="hljs-string">&#x27;freezer&#x27;</span> <span class="hljs-keyword">in</span> group <span class="hljs-string">&#x27;daemons/ftp&#x27;</span><br>cgget: cannot find controller <span class="hljs-string">&#x27;net_cls&#x27;</span> <span class="hljs-keyword">in</span> group <span class="hljs-string">&#x27;daemons/ftp&#x27;</span><br>cgget: cannot find controller <span class="hljs-string">&#x27;blkio&#x27;</span> <span class="hljs-keyword">in</span> group <span class="hljs-string">&#x27;daemons/ftp&#x27;</span><br></code></pre></td></tr></table></figure><p>3、需要用两个限制条件对进程进行限制</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8105 ~]<span class="hljs-comment"># cgexec -g cpu:daemons/ftp -g memory:daemons/ftp top</span><br></code></pre></td></tr></table></figure><p>4、默认情况下是一个大根，然后分了几个资源系统，还支持做一个子系统组，即单独组建一个资源组，然后对这个资源组里面进行配置，具体方法如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs bash">mount &#123;<br>        cpu     = /cgroup/cpu_and_mem;<br>        memory  = /cgroup/cpu_and_mem;<br>&#125;<br>    <br>group daemons/ftp &#123;<br>                        cpu &#123;<br>                                cpu.shares = <span class="hljs-string">&quot;1000&quot;</span>;<br>                                cpu.cfs_period_us = <span class="hljs-string">&quot;5000&quot;</span>;<br>                        &#125;<br>                        memory &#123;<br>                                memory.swappiness = <span class="hljs-string">&quot;20&quot;</span>;<br>                        &#125;<br>                &#125;<br></code></pre></td></tr></table></figure><p>5、需要创建控制组群，如上的daemons&#x2F;ftp，想通过命令行的方式创建</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8105 ~]<span class="hljs-comment"># cgcreate -g cpu:/zp -g memory:/zp</span><br>如上命令使用后会在/cgroup/cpu/中多了zp目录，并且里面是继承的上级的cpu里面的参数，这样就创建了一个zp的组群<br><br>删除组群的方式如下（删除cgroup时，其所有任务都移动到了父组群当中）：<br>[root@lab8105 ~]<span class="hljs-comment"># cgdelete cpu:/zp memory:/zp</span><br></code></pre></td></tr></table></figure><p>6、设置里面的配置参数</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs bash">需要设置<br>    /cgroup/cpu/daemons/ftp/cpu.shares<br>执行<br>[root@lab8105 ~]<span class="hljs-comment"># cgset -r cpu.shares=500 daemons/ftp</span><br>daemons/ftp路径是相对于根的，如果想设置根的这个参数那么就执行<br>[root@lab8105 ~]<span class="hljs-comment"># gset -r cpuacct.usage=0 /</span><br>这里需要注意，只有某些参数是可以修改的，某些参数是不能修改的<br>也可以直接<span class="hljs-built_in">echo</span>的方式进行参数的设置<br></code></pre></td></tr></table></figure><p>7，移动某个进程到控制组群当中（动态的进行资源的调配）</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs bash">移动指定进程到指定的控制组当中，创建两个资源组，使用上面的cpu的脚本，然后运行后，使用top进行监控<br>group half &#123;<br>                        cpu &#123;<br>                                    cpu.cfs_period_us=<span class="hljs-string">&quot;100000&quot;</span>;<br>                                    cpu.cfs_quota_us=<span class="hljs-string">&quot;50000&quot;</span>;<br>                        &#125;<br>                        memory &#123;<br>                                memory.swappiness = <span class="hljs-string">&quot;50&quot;</span>;<br>                        &#125;<br>                &#125;<br><br>group eighty &#123;<br>                        cpu &#123;<br>                                cpu.cfs_period_us = <span class="hljs-string">&quot;100000&quot;</span>;<br>                                cpu.cfs_quota_us=<span class="hljs-string">&quot;50000&quot;</span>;<br>                        &#125;<br>                        memory &#123;<br>                                memory.swappiness = <span class="hljs-string">&quot;80&quot;</span>;<br>                        &#125;<br>                &#125;<br>[root@lab8105 ~]<span class="hljs-comment"># cgclassify -g cpu:half 14245</span><br>top监控看到cpu的占用为50%<br>[root@lab8105 ~]<span class="hljs-comment"># cgclassify -g cpu:eighty 14245</span><br>top监控看到cpu的占用为80%<br>注意支持多进程，多资源组同时移动<br>[root@lab8105 ~]<span class="hljs-comment"># cgclassify -g cpu,memory:eighty 14245 14565</span><br>备用方法就是直接<span class="hljs-built_in">echo</span> <br></code></pre></td></tr></table></figure><p>8、通过规则对指定的进程进行控制</p><p>我们还可以通过设置规则来让 cgred（cgroup 规则引擎后台程序）自动将进程分配给特定组。cgred 后台程序根据 &#x2F;etc&#x2F;cgrules.conf 文件中的设置将任务移到 cgroup 中</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8105 ~]<span class="hljs-comment"># vim /etc/cgrules.conf </span><br>[root@lab8105 ~]<span class="hljs-comment"># man cgrules.conf       </span><br><br><span class="hljs-comment"># /etc/cgrules.conf</span><br><span class="hljs-comment">#The format of this file is described in cgrules.conf(5)</span><br><span class="hljs-comment">#manual page.</span><br><span class="hljs-comment">#</span><br><span class="hljs-comment"># Example:</span><br><span class="hljs-comment">#&lt;user&gt;         &lt;controllers&gt;   &lt;destination&gt;</span><br><span class="hljs-comment">#@student       cpu,memory      usergroup/student/</span><br><span class="hljs-comment">#peter          cpu             test1/</span><br><span class="hljs-comment">#%              memory          test2/</span><br><span class="hljs-comment"># End of file</span><br><span class="hljs-comment">#</span><br>root:cpu.sh           cpu             half/<br>root                  cpu             half/<br><br>启动监控进程服务<br>[root@lab8105 ~]<span class="hljs-comment"># /etc/init.d/cgred </span><br></code></pre></td></tr></table></figure><p>效果如下，运行相同的命令，所占用的cpu的资源按指定的比例进行占用</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8105 ~]<span class="hljs-comment"># top</span><br>top - 16:00:40 up 1 day,  1:34,  5 <span class="hljs-built_in">users</span>,  load average: 1.57, 1.13, 0.90<br>Tasks: 216 total,   3 running, 213 sleeping,   0 stopped,   0 zombie<br>Cpu(s):  6.9%us,  0.0%sy,  0.0%ni, 93.1%<span class="hljs-built_in">id</span>,  0.0%wa,  0.0%hi,  0.0%si,  0.0%st<br>Mem:  49461228k total, 49346312k used,   114916k free, 47374260k buffers<br>Swap:  2097148k total,        0k used,  2097148k free,    33116k cached<br><br>    PID USER      PR  NI  VIRT  RES  SHR S %CPU %MEM    TIME+  COMMAND                                               <br>14648 root      20   0  104m 1716 1012 R 99.7  0.0   4:58.38 cpu1.sh                                                <br>14565 root      20   0  104m 1704 1012 R 10.0  0.0   4:30.53 cpu.sh <br></code></pre></td></tr></table></figure><p>如上所述，指定用户，可以指定进程进行控制，也可以指定用户的所有进程进行控制，后台的做的操作就是把进行的号移动到了指定的资源组的task当中去了</p>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>xenserver中linux虚拟机修改启动顺序</title>
    <link href="/2015/03/24/xenserver%E4%B8%ADlinux%E8%99%9A%E6%8B%9F%E6%9C%BA%E4%BF%AE%E6%94%B9%E5%90%AF%E5%8A%A8%E9%A1%BA%E5%BA%8F/"/>
    <url>/2015/03/24/xenserver%E4%B8%ADlinux%E8%99%9A%E6%8B%9F%E6%9C%BA%E4%BF%AE%E6%94%B9%E5%90%AF%E5%8A%A8%E9%A1%BA%E5%BA%8F/</url>
    
    <content type="html"><![CDATA[<p>xenserver是思杰的一款类似于vmware ESXI的虚拟化平台,或者说虚拟化操作系统,上面可以安装许多虚拟机,但是当你装完linux虚拟机，你会发现一个问题,不能像windows vm那样直接通过xencenter的虚拟机属性去修改。</p><p>windows的启动属性,很容易修改,</p><p>而linux的启动属性,没有这些</p><p>下面我们介绍一些啊，怎么可以把linux的启动属性,通过命令行解决掉。</p><p>选择xenserver主机的console端,找到linux虚拟机的uuid。</p><p>执行:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">xe vm-list name-lable=xx(虚拟机名称) 就会显示虚拟机的uuid<br></code></pre></td></tr></table></figure><p>然后执行: </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">xe vm-param-set uuid=xx(上面获取到的) HVM-boot-policy=BIOS\ order<br>xe vm-param-set uuid=xx(上面获取到的) HVM-boot-params:order=<span class="hljs-string">&quot;dc&quot;</span><br></code></pre></td></tr></table></figure><p>执行完,你就可以通过XenCenter打开Linux虚拟机的属性查看,和windows一样,可以修改启动顺序了</p><p>写于: 2014年06月19日<br>更新于: 2015年03月24日</p>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>ubuntu使用iso作为本地源</title>
    <link href="/2015/03/24/ubuntu%E4%BD%BF%E7%94%A8iso%E4%BD%9C%E4%B8%BA%E6%9C%AC%E5%9C%B0%E6%BA%90/"/>
    <url>/2015/03/24/ubuntu%E4%BD%BF%E7%94%A8iso%E4%BD%9C%E4%B8%BA%E6%9C%AC%E5%9C%B0%E6%BA%90/</url>
    
    <content type="html"><![CDATA[<p>方式一(路径不要改):<br>挂载光驱到到本地的指定目录</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">mount /dev/cdrom /media/cdrom<br></code></pre></td></tr></table></figure><p>然后执行:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">apt-cdrom -m -d /media/cdrom add <br></code></pre></td></tr></table></figure><p>会写配置文件<br>然后执行:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">apt-get update <br></code></pre></td></tr></table></figure><p>就可以使用</p><p>方式二:直接添加方式</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">mount /dev/cdrom /mnt<br>root@grandocean:/etc/apt<span class="hljs-comment"># vim /etc/apt/sources.list</span><br></code></pre></td></tr></table></figure><p>添加</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">deb file:///mnt raring main<br>deb file:///mnt raring restricted<br></code></pre></td></tr></table></figure><p>方式三：通过目录本地创建的方式</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">apt install dpkg-dev<br></code></pre></td></tr></table></figure><p>进入到放包的目录的根目录，比如&#x2F;iso&#x2F;main</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">cd</span> /iso/main<br>dpkg-scanpackages -m . | gzip -c &gt; Packages.gz<br></code></pre></td></tr></table></figure><p>修改源列表</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">deb [trusted=<span class="hljs-built_in">yes</span>] file:///iso/main ./<br></code></pre></td></tr></table></figure><p>写于: 2014年08月28日<br>更新于: 2015年03月24日</p>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>ubuntu配置简单的DNS服务器</title>
    <link href="/2015/03/24/ubuntu%E9%85%8D%E7%BD%AE%E7%AE%80%E5%8D%95%E7%9A%84DNS%E6%9C%8D%E5%8A%A1%E5%99%A8/"/>
    <url>/2015/03/24/ubuntu%E9%85%8D%E7%BD%AE%E7%AE%80%E5%8D%95%E7%9A%84DNS%E6%9C%8D%E5%8A%A1%E5%99%A8/</url>
    
    <content type="html"><![CDATA[<p>之所以说是简单的服务器，实现的功能很简单，通过这个dns server 查询制定域名的时候，能够根据设置的值来返回IP，当前的需求是需要轮询的返回IP</p><p>DNS 轮询机制会受到多方面的影响，如：A记录的TTL时间长短的影响；别的 DNS 服务器 Cache 的影响；windows 客户端也有一个DNS Cache。这些都会影响 DNS 轮询的效果。</p><p>下面的配置就是实现解析test.zp.com到不同的IP地址</p><h3 id="安装dns-server软件包"><a href="#安装dns-server软件包" class="headerlink" title="安装dns server软件包"></a>安装dns server软件包</h3><p>ubuntu下是通过安装bind9软件包来配置dns-server的</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab5106 ~]<span class="hljs-comment"># apt-get install bind9</span><br></code></pre></td></tr></table></figure><h3 id="配置dns"><a href="#配置dns" class="headerlink" title="配置dns"></a>配置dns</h3><p>配置文件的路径在&#x2F;etc&#x2F;bind路径下面</p><h4 id="添加一个zone"><a href="#添加一个zone" class="headerlink" title="添加一个zone"></a>添加一个zone</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">root@ubuntu14:/etc/bind<span class="hljs-comment"># vim /etc/bind/named.conf.local </span><br>添加下面，语法可以参照/etc/bind/zones.rfc1918中的语法添加，如下：<br><br>zone <span class="hljs-string">&quot;zp.com&quot;</span>  &#123; <span class="hljs-built_in">type</span> master; file <span class="hljs-string">&quot;/etc/bind/db.zp.com&quot;</span>; &#125;;<br></code></pre></td></tr></table></figure><p>修改db的配置文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs bash">root@ubuntu14:/etc/bind<span class="hljs-comment"># cp db.local db.zp.com</span><br>root@ubuntu14:/etc/bind<span class="hljs-comment"># vim db.zp.com</span><br>;<br>; BIND data file <span class="hljs-keyword">for</span> <span class="hljs-built_in">local</span> loopback interface<br>;<br><span class="hljs-variable">$TTL</span>    604800<br>@       IN      SOA     zp.com. root.localhost. (<br>                                2         ; Serial<br>                            604800         ; Refresh<br>                            86400         ; Retry<br>                        2419200         ; Expire<br>                            604800 )       ; Negative Cache TTL<br>;<br>@       IN      NS      localhost.<br>@       IN      A       127.0.0.1<br>@       IN      AAAA    ::1<br><span class="hljs-built_in">test</span>       IN      A       192.168.0.11<br><span class="hljs-built_in">test</span>       IN      A       192.168.0.12<br><span class="hljs-built_in">test</span>       IN      A       192.168.0.13<br><span class="hljs-built_in">test</span>       IN      A       192.168.0.14<br><span class="hljs-built_in">test</span>       IN      A       192.168.0.15<br><span class="hljs-built_in">test</span>       IN      A       192.168.0.16<br></code></pre></td></tr></table></figure><p>修改&#x2F;etc&#x2F;bind&#x2F;named.conf.option  配置文件，在  named.conf.option 中可以设置 bind 的 round-robin 的给出结果的顺序：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs bash">rrset-order &#123; order cyclic; &#125;;<br><br>rrset-order 支持三个参数：fixed, random, cyclic 。<br>fixed 会将多个A记录按配置文件的顺序固定给出<br>random 会随机给出<br>cyclic 会循环给出<br></code></pre></td></tr></table></figure><h3 id="重启服务"><a href="#重启服务" class="headerlink" title="重启服务"></a>重启服务</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">root@ubuntu14:/etc/bind<span class="hljs-comment"># /etc/init.d/bind9 restart</span><br></code></pre></td></tr></table></figure><p>###检查配置效果</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs bash">修改域名解析配置文件<br>root@ubuntu14:/etc/bind<span class="hljs-comment"># vim /etc/resolv.conf </span><br>nameserver 192.168.0.122<br>添加你的域名服务器的IP地址<br><br>通过多次ping域名检查返回的结果<br>root@ubuntu14:/etc/bind<span class="hljs-comment"># ping test.zp.com</span><br>PING test.zp.com (192.168.0.13) 56(84) bytes of data.<br>root@ubuntu14:/etc/bind<span class="hljs-comment"># ping test.zp.com</span><br>PING test.zp.com (192.168.0.14) 56(84) bytes of data.<br></code></pre></td></tr></table></figure><h3 id="后话"><a href="#后话" class="headerlink" title="后话"></a>后话</h3><p>window的dns缓存的处理办法：<br>清空dns缓存</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">ipconfig /flushdns<br></code></pre></td></tr></table></figure><p>显示缓存的dns信息</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">ipconfig/displaydns<br></code></pre></td></tr></table></figure><p>临时禁用dns缓存</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">net stop dnscache<br></code></pre></td></tr></table></figure><p>启动dns缓存</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">net start dnscache<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>ubuntu13.04修改默认启动内核</title>
    <link href="/2015/03/24/ubuntu13.04%E4%BF%AE%E6%94%B9%E9%BB%98%E8%AE%A4%E5%90%AF%E5%8A%A8%E5%86%85%E6%A0%B8/"/>
    <url>/2015/03/24/ubuntu13.04%E4%BF%AE%E6%94%B9%E9%BB%98%E8%AE%A4%E5%90%AF%E5%8A%A8%E5%86%85%E6%A0%B8/</url>
    
    <content type="html"><![CDATA[<p>ubuntu下面的启动内核选项跟其他操作系统不一样,有个子菜单,比如我在默认的ubuntu13.04上安装了一个新的内核3.14.5,那么默认的第一项是3.14.5内核,第二项是一个子菜单,第二项里面的第一项是3.14.5,第二项是3.14.5 recovery 模式,第三项是3.8.0,第四项是3.8.0(recover)</p><p>那么应该修改</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">/boot/grub/grub.cfg<br></code></pre></td></tr></table></figure><p>中的</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">GRUB_DEFAULT=0为GRUB_DEFAULT=<span class="hljs-string">&quot;1&gt;2&quot;</span><br></code></pre></td></tr></table></figure><p>然后:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">update-grub<br></code></pre></td></tr></table></figure><p>重启即可</p><p>写于: 2014年07月11日<br>更新于: 2015年03月24日</p>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>ubuntu服务器启动过程中重启卡死的问题解决</title>
    <link href="/2015/03/24/ubuntu%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%90%AF%E5%8A%A8%E8%BF%87%E7%A8%8B%E4%B8%AD%E9%87%8D%E5%90%AF%E5%8D%A1%E6%AD%BB%E7%9A%84%E9%97%AE%E9%A2%98%E8%A7%A3%E5%86%B3/"/>
    <url>/2015/03/24/ubuntu%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%90%AF%E5%8A%A8%E8%BF%87%E7%A8%8B%E4%B8%AD%E9%87%8D%E5%90%AF%E5%8D%A1%E6%AD%BB%E7%9A%84%E9%97%AE%E9%A2%98%E8%A7%A3%E5%86%B3/</url>
    
    <content type="html"><![CDATA[<p>在grub默认参数当中添加</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">GRUB_RECORDFAIL_TIMEOUT=0<br></code></pre></td></tr></table></figure><p>写于: 2014年07月23日<br>更新于: 2015年03月24日</p>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>nginx开启目录浏览</title>
    <link href="/2015/03/24/nginx%E5%BC%80%E5%90%AF%E7%9B%AE%E5%BD%95%E6%B5%8F%E8%A7%88/"/>
    <url>/2015/03/24/nginx%E5%BC%80%E5%90%AF%E7%9B%AE%E5%BD%95%E6%B5%8F%E8%A7%88/</url>
    
    <content type="html"><![CDATA[<p>使用nginx作为下载站点,开启目录浏览的功能<br>在&#x2F;etc&#x2F;nginx&#x2F;sites-enabled&#x2F;default中添加:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">autoindex on ;<br>autoindex_exact_size off;<br>autoindex_localtime on;<br></code></pre></td></tr></table></figure><p>说明:</p><blockquote><p>第一个为目录浏览功能开始<br><br>第二个为不精确计算文件大小<br><br>第三个为取时间为服务器本地的时间</p></blockquote><p>删除location相关的内容<br>修改完成重启nginx服务</p><p>写于: 2014年07月16日<br>更新于: 2015年03月24日</p>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>mdtest测试工具</title>
    <link href="/2015/03/24/mdtest%E6%B5%8B%E8%AF%95%E5%B7%A5%E5%85%B7/"/>
    <url>/2015/03/24/mdtest%E6%B5%8B%E8%AF%95%E5%B7%A5%E5%85%B7/</url>
    
    <content type="html"><![CDATA[<h2 id="软件介绍"><a href="#软件介绍" class="headerlink" title="软件介绍"></a>软件介绍</h2><p>mdstest是软件的元数据操作基准测试工具，用来模拟对文件或者目录的open、stat、close操作，然后报告性能</p><!--break--><p>下载软件压缩包：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs bash">yum install openmpi openmpi-devel -y<br>在/root/.bashrc中添加（注意也要添加mdtest的路径）<br><span class="hljs-built_in">export</span> PATH=<span class="hljs-variable">$PATH</span>:/usr/lib64/openmpi/bin/<br><br><span class="hljs-built_in">source</span> /root/.bashrc<br>下载<br>[root@lab8105 ~]<span class="hljs-comment"># wget http://sourceforge.net/projects/mdtest/files/latest/download</span><br>解压<br>[root@lab8105 ~]<span class="hljs-comment"># tar -xvf mdtest-1.9.3.tgz</span><br>修改makefile：<br>mdtest: mdtest.c<br>    mpicc -Wall -D $(OS) $(LARGE_FILE) $(MDTEST_FLAGS) -g -o mdtest mdtest.c -lm<br></code></pre></td></tr></table></figure><span id="more"></span><p>参数如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br></pre></td><td class="code"><pre><code class="hljs bash">Usage:  mdtest [-b <span class="hljs-comment">#] [-B] [-c] [-C] [-d testdir] [-D] [-e] [-E] [-f first] [-F]</span><br>                [-h] [-i iterations] [-I <span class="hljs-comment">#] [-l last] [-L] [-n #] [-N #] [-p seconds]</span><br>                [-r] [-R[<span class="hljs-comment">#]] [-s #] [-S] [-t] [-T] [-u] [-v] [-V #] [-w #] [-y]</span><br>                [-z <span class="hljs-comment">#]</span><br><br>    -b: branching <span class="hljs-built_in">factor</span> of hierarchical directory structure<br>        目录树的分支参数<br>    -B: no barriers between phases (create/stat/remove)<br>            不同的阶段没有隔离 (create/stat/remove)；<br>    -c: collective creates: task 0 does all creates and deletes<br>        共同创建: task 0 完成所有的创建和删除工作；<br>    -C: only create files/dirs<br>        只创建文件或目录，不作删除；<br>    -d: the directory <span class="hljs-keyword">in</span> <span class="hljs-built_in">which</span> the tests will run<br>        指出测试运行的目录（若不指定，则默认当前目录）；<br>    -D: perform <span class="hljs-built_in">test</span> on directories only (no files)<br>        只对目录操作进行测试（不包括文件）；<br>    -e: number of bytes to <span class="hljs-built_in">read</span> from each file<br>        从每个文件读出的文件大小<br>    -E: only <span class="hljs-built_in">read</span> files<br>        只读取文件<br>    -f: first number of tasks on <span class="hljs-built_in">which</span> the <span class="hljs-built_in">test</span> will run<br>        首先运行的任务号；<br>    -F: perform <span class="hljs-built_in">test</span> on files only (no directories)<br>        只创建文件，没有目录；<br>    -h: prints <span class="hljs-built_in">help</span> message<br>        输出帮助信息<br>    -i: number of iterations the <span class="hljs-built_in">test</span> will run<br>        测试迭代循环次数；<br>    -I: number of items per tree node<br>        每个树节点包含的项目<br>    -l: last number of tasks on <span class="hljs-built_in">which</span> the <span class="hljs-built_in">test</span> will run<br>        最后运行的任务号<br>    -L: files/dirs created only at leaf level<br>        只在目录树的“叶子”层创建文件/目录；<br>    -n: every task will create/stat/remove <span class="hljs-comment"># files/dirs per tree</span><br>        每个任务需要再每棵树中create/stat/remove的文件/目录数；<br>    -N: stride <span class="hljs-comment"># between neighbor tasks for file/dir stat (local=0)</span><br>        遍历时指定和相邻任务的跨度<br>    -p: pre-iteration delay (<span class="hljs-keyword">in</span> seconds)<br>        每次迭代之间延时（以秒计算）<br>    -r: only remove files/dirs<br>            删除文件/目录<br>    -R: randomly <span class="hljs-built_in">stat</span> files/dirs (optional seed can be provided)<br>        随机遍历文件/目录 ；<br>    -s: stride between the number of tasks <span class="hljs-keyword">for</span> each <span class="hljs-built_in">test</span><br>        每次测试的任务数的跨度<br>    -S: shared file access (file only, no directories)<br>        共享文件访问（只针对文件操作）；<br>    -t: time unique working directory overhead<br>        记录特定目录的时间开销<br>    -T: only <span class="hljs-built_in">stat</span> files/dirs<br>        <br>    -u: unique working directory <span class="hljs-keyword">for</span> each task<br>        为每个任务指定工作目录；<br>    -v: verbosity (each instance of option increments by one)<br>        <br>    -V: verbosity value<br>    -w: number of bytes to write to each file<br>            写到每个文件的字节数<br>    -y: <span class="hljs-built_in">sync</span> file after write completion<br>            再写执行完后同步文件到磁盘（同步写）<br>    -z: depth of hierarchical directory structure<br>        目录树的深度；<br><br>NOTES:<br>    * -N allows a <span class="hljs-string">&quot;read-your-neighbor&quot;</span> approach by setting stride to<br>    tasks-per-node. Do not use it with -B, as it creates race conditions.<br>    允许<span class="hljs-string">&quot;read-your-neighbor&quot;</span> 方法<br>    * -d allows multiple paths <span class="hljs-keyword">for</span> the form <span class="hljs-string">&#x27;-d fullpath1@fullpath2@fullpath3&#x27;</span><br>        可以指定多个测试路径，<span class="hljs-string">&#x27;-d fullpath1@fullpath2@fullpath3&#x27;</span><br>    * -B allows each task to time itself. The aggregate results reflect this<br>    change.<br>    允许每个任务对自己进行计时；<br>    * -n and -I cannot be used together.  -I specifies the number of files/dirs<br>    created per tree node, whereas the -n specifies the total number of<br>    files/dirs created over an entire tree.  When using -n, <span class="hljs-built_in">integer</span> division is<br>    used to determine the number of files/dirs per tree node.  (E.g. <span class="hljs-keyword">if</span> -n is<br>    10 and there are 4 tree nodes (z=1 and b=3), there will be 2 files/dirs per<br>    tree node.)<br>    <br>    * -R and -T can be used separately.  -R merely indicates that <span class="hljs-keyword">if</span> files/dirs<br>    are going to be <span class="hljs-built_in">stat</span><span class="hljs-string">&#x27;ed, then they will be stat&#x27;</span>ed randomly.<br>    不能同时使用，因为指定了每个树节点的文件/目录数量，而-I指定的是整棵树的文件/目录数量。<br><br>Illustration of terminology:<br><br>                        Hierarchical directory structure (tree)<br><br>                                    =======<br>                                    |       |  (tree node)<br>                                    =======<br>                                    /   |   \<br>                            ------    |    ------<br>                            /          |          \<br>                        =======     =======     =======<br>                        |       |   |       |   |       |    (leaf level)<br>                        =======     =======     =======<br><br>    In this example, the tree has a depth of one (z=1) and branching <span class="hljs-built_in">factor</span> of<br>    three (b=3).  The node at the top of the tree is the root node.  The level<br>    of nodes furthest from the root is the leaf level.  All trees created by<br>    mdtest are balanced.<br>    这个例子中，目录树深度为1（z=1）,每个节点分支为3（b=3）.最上面的节点为根节点，<br></code></pre></td></tr></table></figure><p>关于openmpi的软件的使用</p><p>openmpi是并行的运行程序，配置的时候需要注意下</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs bash">增加并行的运行节点<br>[root@lab8105 ~]<span class="hljs-comment"># vim /etc/openmpi-x86_64/openmpi-default-hostfile</span><br>添加<br>lab8105 slots=1<br>lab8106 slots=1<br>特别注意加slots 这个是配置权重的，如果不配置，第一条有默认权重，就无法在数目小时进行均衡操作<br>检查是否配置成功，np为操作线程数<br>[root@lab8105 ~]<span class="hljs-comment"># mpirun  --allow-run-as-root -np 2 hostname</span><br>lab8105<br>lab8106<br>如果想单机执行多进程，可以用-host指定主机<br>[root@lab8105 ~]<span class="hljs-comment"># mpirun -host lab8105  --allow-run-as-root -np 2 hostname</span><br></code></pre></td></tr></table></figure><h3 id="单机下多进程测试"><a href="#单机下多进程测试" class="headerlink" title="单机下多进程测试"></a>单机下多进程测试</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs bash">2 task on 1 node<br>[root@lab8105 ~]<span class="hljs-comment"># mpirun -host lab8105  --allow-run-as-root -np 2 mdtest -I 10  -z 5 -b 2 -d /mnt/zptest/ -t -c 2</span><br>-- started at 01/27/2015 21:06:23 --<br><br>mdtest-1.9.3 was launched with 2 total task(s) on 1 node(s)<br>Command line used: mdtest -I 10 -z 5 -b 2 -d /mnt/zptest/ -t -c 2<br>Path: /mnt/zptest<br>FS: 6.5 TiB   Used FS: 59.2%   Inodes: 0.6 Mi   Used Inodes: 100.0%<br><br>2 tasks, 1260 files/directories<br><br>SUMMARY: (of 1 iterations)<br>    Operation                      Max            Min           Mean        Std Dev<br>    ---------                      ---            ---           ----        -------<br>    Directory creation:        312.088        312.088        312.088          0.000<br>    Directory <span class="hljs-built_in">stat</span>    :      73447.245      73447.245      73447.245          0.000<br>    Directory removal :        255.755        255.755        255.755          0.000<br>    File creation     :        638.824        638.824        638.824          0.000<br>    File <span class="hljs-built_in">stat</span>         :      86747.366      86747.366      86747.366          0.000<br>    File <span class="hljs-built_in">read</span>         :      84434.232      84434.232      84434.232          0.000<br>    File removal      :        207.545        207.545        207.545          0.000<br>    Tree creation     :         39.062         39.062         39.062          0.000<br>    Tree removal      :         46.971         46.971         46.971          0.000<br><br>-- finished at 01/27/2015 21:06:43 --<br></code></pre></td></tr></table></figure><h3 id="多机并发测试"><a href="#多机并发测试" class="headerlink" title="多机并发测试"></a>多机并发测试</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs bash">如下显示的2 task on 2 node<br><br>[root@lab8105 ~]<span class="hljs-comment">#  mpirun  --allow-run-as-root -np 2 mdtest -I 10  -z 5 -b 2 -d /mnt/zptest/ -t -c 2</span><br>-- started at 01/27/2015 21:04:35 --<br><br>mdtest-1.9.3 was launched with 2 total task(s) on 2 node(s)<br>Command line used: mdtest -I 10 -z 5 -b 2 -d /mnt/zptest/ -t -c 2<br>Path: /mnt/zptest<br>FS: 6.5 TiB   Used FS: 59.2%   Inodes: 0.6 Mi   Used Inodes: 100.0%<br><br>2 tasks, 1260 files/directories<br><br>SUMMARY: (of 1 iterations)<br>    Operation                      Max            Min           Mean        Std Dev<br>    ---------                      ---            ---           ----        -------<br>    Directory creation:        312.355        312.355        312.355          0.000<br>    Directory <span class="hljs-built_in">stat</span>    :       1611.000       1611.000       1611.000          0.000<br>    Directory removal :        127.333        127.333        127.333          0.000<br>    File creation     :        309.295        309.295        309.295          0.000<br>    File <span class="hljs-built_in">stat</span>         :     113257.534     113257.534     113257.534          0.000<br>    File <span class="hljs-built_in">read</span>         :     203458.057     203458.057     203458.057          0.000<br>    File removal      :         98.523         98.523         98.523          0.000<br>    Tree creation     :         36.566         36.566         36.566          0.000<br>    Tree removal      :         20.191         20.191         20.191          0.000<br><br>-- finished at 01/27/2015 21:05:12 --<br></code></pre></td></tr></table></figure><p>关于目录生成的问题：<br>这个是 -z 2 -b 3 </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 mnt]<span class="hljs-comment"># ls mdtest2/#test-dir.0/mdtest_tree.0/</span><br>mdtest_tree.1  mdtest_tree.2  mdtest_tree.3<br>[root@lab8106 mnt]<span class="hljs-comment"># ls mdtest2/#test-dir.0/mdtest_tree.0/mdtest_tree.1/</span><br>mdtest_tree.4  mdtest_tree.5  mdtest_tree.6<br></code></pre></td></tr></table></figure><p>这个是以这个目录开始的</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab8106 mnt]<span class="hljs-comment"># ls mdtest2/#test-dir.0/mdtest_tree.0/</span><br></code></pre></td></tr></table></figure><h2 id="注意"><a href="#注意" class="headerlink" title="注意"></a>注意</h2><p>最好找一台独立机器做控制端<br>所有机器的hosts要有相互的hosts信息，并且发起mpirun需要与测试机器免密</p><p>注意增加参数-u 这个参数是控制每个任务都有一个自己的工作目录的</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">time mpirun  --allow-run-as-root -np 15 --map-by node  mdtest -u  -n 1000  -z 1 -b 2 -d /test/2mdsnp15size0 -F -L<br></code></pre></td></tr></table></figure><p>-F是指定只测试文件<br>-L指定只在子树上面创建任务</p>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>linux域名解析引起登陆慢</title>
    <link href="/2015/03/24/linux%E5%9F%9F%E5%90%8D%E8%A7%A3%E6%9E%90%E5%BC%95%E8%B5%B7%E7%99%BB%E9%99%86%E6%85%A2/"/>
    <url>/2015/03/24/linux%E5%9F%9F%E5%90%8D%E8%A7%A3%E6%9E%90%E5%BC%95%E8%B5%B7%E7%99%BB%E9%99%86%E6%85%A2/</url>
    
    <content type="html"><![CDATA[<p>linux域名解析引起登陆慢的问题在于,ssh去登录这个台机器的时候,本机会去通过域名解析获取登录主机的主机名，所有一旦域名解析是无效的,需要等待较长时间</p><p>解决办法一:<br>将域名解析指到127.0.0.1</p><p>解决办法二:<br>修改配置文件 &#x2F;etc&#x2F;ssh&#x2F;sshd_config </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">UseDNS no<br></code></pre></td></tr></table></figure><p>然后重启ssh服务 </p><p>写于: 2014年07月10日<br>更新于: 2015年03月24日</p>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>linux绑定盘符</title>
    <link href="/2015/03/24/linux%E7%BB%91%E5%AE%9A%E7%9B%98%E7%AC%A6/"/>
    <url>/2015/03/24/linux%E7%BB%91%E5%AE%9A%E7%9B%98%E7%AC%A6/</url>
    
    <content type="html"><![CDATA[<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@centos6 ~]<span class="hljs-comment"># udevadm info -q path -n /dev/sdb</span><br>[root@centos6 ~]<span class="hljs-comment"># udevadm info -q path -n /dev/sdc</span><br>/devices/pci0000:00/0000:00:10.0/host2/target2:0:1/2:0:1:0/block/sdc<br></code></pre></td></tr></table></figure><p>拿到编号</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@centos6 ~]<span class="hljs-comment"># vim /etc/udev/rules.d/80-mydisk.rules </span><br></code></pre></td></tr></table></figure><p>增加</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">DEVPATH==<span class="hljs-string">&quot;/devices/pci0000:00/0000:00:10.0/host2/target2:0:2/2:0:2:0/block/sd*&quot;</span>, NAME=<span class="hljs-string">&quot;sdc&quot;</span>, MODE=<span class="hljs-string">&quot;0660&quot;</span><br></code></pre></td></tr></table></figure><p>这样对应的插槽的第二个就会一直对应盘符sdc 而不会出现跳盘符的问题了</p><p>写于: 2014年11月07日<br>更新于: 2015年03月24日</p>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>重置ubuntu13.04 密码</title>
    <link href="/2015/03/24/%E9%87%8D%E7%BD%AEubuntu13.04%20%E5%AF%86%E7%A0%81/"/>
    <url>/2015/03/24/%E9%87%8D%E7%BD%AEubuntu13.04%20%E5%AF%86%E7%A0%81/</url>
    
    <content type="html"><![CDATA[<p>方法如下:</p><ol><li><p>Restart Machine </p></li><li><p>HOLD Shift Button ( You will get message “GRUB Loading“)</p><!--break--></li><li><p>Select the Kernel ( Don’t select Recovery Mode)</p></li><li><p>Press the e key to edit the entry</p></li><li><p>Select the line starting with the word “linux”</p></li><li><p>Append the init&#x3D;&#x2F;bin&#x2F;bash to the end of the linux line.</p></li><li><p>Now Press F10 to boot with provided option.</p></li></ol><p>Now You need to mount File System ( &#x2F; ) into Read-Write Mode</p><ol start="8"><li>mount -o remount,rw &#x2F;</li></ol><p>Now execute passwd command to Reset root password.</p><ol start="9"><li>For root User passwd</li></ol><p>Other User passwd <user-name></p><p>写于: 2014年06月25日<br>更新于: 2015年03月24日</p>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>重置ubuntu13.04密码</title>
    <link href="/2015/03/24/%E9%87%8D%E7%BD%AEubuntu13.04%E5%AF%86%E7%A0%81/"/>
    <url>/2015/03/24/%E9%87%8D%E7%BD%AEubuntu13.04%E5%AF%86%E7%A0%81/</url>
    
    <content type="html"><![CDATA[<p>方法如下:</p><ol><li><p>Restart Machine </p></li><li><p>HOLD Shift Button ( You will get message “GRUB Loading“)</p><!--break--></li><li><p>Select the Kernel ( Don’t select Recovery Mode)</p></li><li><p>Press the e key to edit the entry</p></li><li><p>Select the line starting with the word “linux”</p></li><li><p>Append the init&#x3D;&#x2F;bin&#x2F;bash to the end of the linux line.</p></li><li><p>Now Press F10 to boot with provided option.</p></li></ol><p>Now You need to mount File System ( &#x2F; ) into Read-Write Mode</p><ol start="8"><li>mount -o remount,rw &#x2F;</li></ol><p>Now execute passwd command to Reset root password.</p><ol start="9"><li>For root User passwd</li></ol><p>Other User passwd <user-name></p><p>写于: 2014年06月25日<br>更新于: 2015年03月24日</p>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>使用iptables做端口转发</title>
    <link href="/2015/03/24/%E4%BD%BF%E7%94%A8iptables%E5%81%9A%E7%AB%AF%E5%8F%A3%E8%BD%AC%E5%8F%91/"/>
    <url>/2015/03/24/%E4%BD%BF%E7%94%A8iptables%E5%81%9A%E7%AB%AF%E5%8F%A3%E8%BD%AC%E5%8F%91/</url>
    
    <content type="html"><![CDATA[<p>通过iptables可以做转发</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><code class="hljs bash">IPT=<span class="hljs-string">&quot;/sbin/iptables&quot;</span><br>/bin/echo <span class="hljs-string">&quot;1&quot;</span> &gt; /proc/sys/net/ipv4/ip_forward<br>/sbin/modprobe ip_tables<br>/sbin/modprobe iptable_filter<br>/sbin/modprobe iptable_nat<br>/sbin/modprobe ip_conntrack<br>/sbin/modprobe ip_conntrack_ftp<br>/sbin/modprobe ip_nat_ftp<br><span class="hljs-variable">$IPT</span> -F<br><span class="hljs-variable">$IPT</span> -t nat -F<br><span class="hljs-variable">$IPT</span> -X<br><span class="hljs-variable">$IPT</span> -t nat -X<br><span class="hljs-variable">$IPT</span> -Z<br><span class="hljs-variable">$IPT</span> -t nat -Z<br><span class="hljs-comment">#DNAT 做端口转发</span><br><span class="hljs-variable">$IPT</span> -t nat -A PREROUTING -d 192.168.19.102 -p tcp --dport 33 -j DNAT --to-destination 192.167.19.101:22<br><span class="hljs-comment">#SNAT 做端口转发</span><br><span class="hljs-variable">$IPT</span> -t nat -A POSTROUTING -p tcp -d 192.167.19.101 --dport 22 -j SNAT --to-source 192.167.19.102<br>    <br><span class="hljs-comment">#SNAT 做网关转发</span><br><span class="hljs-comment">#$IPT -t nat -A POSTROUTING  -s 192.168.0.0/16  -j SNAT --to-source 210.72.24.15</span><br>``` <br>说明：<br>DNAT 做端口转发<br>是将对外网ip 192.168.19.102的访问，映射到对内网ip 192.167.19.101的访问；<br><br>一内一外的双网卡转一内的单网卡的时候，上面的配置信息里面实际是有三个IP信息的<br>  <br>备注:<br><br>首先需要启用NAT需要在Linux上打开内核对IP包的转发支持，linux上编辑:/etc/sysctl.conf 文件:<br>```bash<br>net.ipv4.ip_forward = 1<br></code></pre></td></tr></table></figure><p>然后执行</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">sysctl -p<br></code></pre></td></tr></table></figure><h2 id="对于单网卡虚拟网卡的场景"><a href="#对于单网卡虚拟网卡的场景" class="headerlink" title="对于单网卡虚拟网卡的场景"></a>对于单网卡虚拟网卡的场景</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-meta">#!/bin/sh </span><br>IPT=<span class="hljs-string">&quot;/sbin/iptables&quot;</span><br>/bin/echo <span class="hljs-string">&quot;1&quot;</span> &gt; /proc/sys/net/ipv4/ip_forward<br>/sbin/modprobe ip_tables<br>/sbin/modprobe iptable_filter<br>/sbin/modprobe iptable_nat<br>/sbin/modprobe ip_conntrack<br>/sbin/modprobe ip_conntrack_ftp<br>/sbin/modprobe ip_nat_ftp<br><span class="hljs-variable">$IPT</span> -F<br><span class="hljs-variable">$IPT</span> -t nat -F<br><span class="hljs-variable">$IPT</span> -X<br><span class="hljs-variable">$IPT</span> -t nat -X<br><span class="hljs-variable">$IPT</span> -Z<br><span class="hljs-variable">$IPT</span> -t nat -Z<br><br>iptables -t nat -A PREROUTING  -p tcp --dport 8082   -i enp6s0f1 -d 20.20.20.247  -j DNAT --to-destination 192.168.0.1:80<br>iptables -t nat -A POSTROUTING  -j MASQUERADE<br><br></code></pre></td></tr></table></figure><p>上面的操作可以配置虚拟网卡iptable转发的访问</p><p>写于: 2014年08月04日<br>更新于: 2015年03月24日</p>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>配置xenserver本地存储</title>
    <link href="/2015/03/24/%E9%85%8D%E7%BD%AExenserver%E6%9C%AC%E5%9C%B0%E5%AD%98%E5%82%A8/"/>
    <url>/2015/03/24/%E9%85%8D%E7%BD%AExenserver%E6%9C%AC%E5%9C%B0%E5%AD%98%E5%82%A8/</url>
    
    <content type="html"><![CDATA[<p>查询磁盘对应关系:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@xenserver-eqtwbths ~]<span class="hljs-comment"># ll /dev/disk/by-id/</span><br>total 0<br>lrwxrwxrwx 1 root root  9 Jun  5 13:24 edd-int13_dev81 -&gt; ../../sdb<br>lrwxrwxrwx 1 root root 10 Jun  5 14:14 edd-int13_dev81-part1 -&gt; ../../sdb1<br>lrwxrwxrwx 1 root root  9 Jun  5 13:24 edd-int13_dev82 -&gt; ../../sdc<br>lrwxrwxrwx 1 root root  9 Jun  5 13:24 edd-int13_dev83 -&gt; ../../sdd<br>lrwxrwxrwx 1 root root 10 Jun  5 13:24 edd-int13_dev83-part1 -&gt; ../../sdd1<br>lrwxrwxrwx 1 root root  9 Jun  5 13:24 scsi-SATA_ST1000VX000-9YW_S1D3PNHD -&gt; ../../sdc<br>lrwxrwxrwx 1 root root  9 Jun  5 13:24 scsi-SATA_ST1000VX000-9YW_S1D3PPH8 -&gt; ../../sdd<br>lrwxrwxrwx 1 root root 10 Jun  5 13:24 scsi-SATA_ST1000VX000-9YW_S1D3PPH8-part1 -&gt; ../../sdd1<br>lrwxrwxrwx 1 root root  9 Jun  5 13:24 scsi-SATA_ST1000VX000-9YW_S1D4AGVJ -&gt; ../../sdb<br>lrwxrwxrwx 1 root root 10 Jun  5 14:14 scsi-SATA_ST1000VX000-9YW_S1D4AGVJ-part1 -&gt; ../../sdb1<br>lrwxrwxrwx 1 root root  9 Jun  5 13:24 scsi-SATA_ST1000VX000-9YW_S1D4AGX3 -&gt; ../../sda<br>lrwxrwxrwx 1 root root 10 Jun  5 13:24 scsi-SATA_ST1000VX000-9YW_S1D4AGX3-part1 -&gt; ../../sda1<br>lrwxrwxrwx 1 root root 10 Jun  5 13:24 scsi-SATA_ST1000VX000-9YW_S1D4AGX3-part2 -&gt; ../../sda2<br>lrwxrwxrwx 1 root root 10 Jun  5 13:24 scsi-SATA_ST1000VX000-9YW_S1D4AGX3-part3 -&gt; ../../sda3<br></code></pre></td></tr></table></figure><p>准备添加sdb1作为存储</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">root@xenserver-eqtwbths ~]<span class="hljs-comment"># xe sr-create type=lvm content-type=user device-config:device=/dev/disk/by-id/edd-int13_dev81-part1 name-label=&quot;Local storage 2(sdb)&quot; </span><br></code></pre></td></tr></table></figure><p>2.XS删除本地存储连接 </p><p>列出pdb模块，找到对应存储的UUID </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">xe pbd-list  <br></code></pre></td></tr></table></figure><p>卸载对应uuid的存储 </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">xe pbd-unplug uuid=<span class="hljs-string">&quot;uuid of PBD&quot;</span>  <br></code></pre></td></tr></table></figure><p>列出存储的UUID，找到对应存储的UUID </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">xe sr-list  <br></code></pre></td></tr></table></figure><p>删除本地存储连接 </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">xe sr-destroy uuid=<span class="hljs-string">&quot;uuid of SR&quot;</span> <br>xe sr-forget uuid=<span class="hljs-string">&quot;uuid of SR&quot;</span><br></code></pre></td></tr></table></figure><p>写于: 2014年06月05日<br>更新于: 2015年12月15日</p>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>配置cobbler步骤</title>
    <link href="/2015/03/24/%E9%85%8D%E7%BD%AEcobbler%E6%AD%A5%E9%AA%A4/"/>
    <url>/2015/03/24/%E9%85%8D%E7%BD%AEcobbler%E6%AD%A5%E9%AA%A4/</url>
    
    <content type="html"><![CDATA[<p>首先找到下载包的地址 (使用的是centos6)</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">http://download.opensuse.org/repositories/home:/libertas-ict:/cobbler26/CentOS_CentOS-6/noarch/<br></code></pre></td></tr></table></figure><!--break--><p>添加国内epel源:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">wget http://mirrors.aliyun.com/epel/6/x86_64/epel-release-6-8.noarch.rpm <br></code></pre></td></tr></table></figure><p>本次测试下载这三个包:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">cobbler-2.6.5-9.1.noarch.rpm <br>cobbler-web-2.6.5-9.1.noarch.rpm <br>koan-2.6.5-9.1.noarch.rpm<br></code></pre></td></tr></table></figure><p>下载rpm包:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">wget http://download.opensuse.org/repositories/home:/libertas-ict:/cobbler26/CentOS_CentOS-6/noarch/cobbler-2.6.5-9.1.noarch.rpm <br>wget http://download.opensuse.org/repositories/home:/libertas-ict:/cobbler26/CentOS_CentOS-6/noarch/cob_CentOS-6/noarch/cobbler-web-2.6.5-9.1.noarch.rpm <br>wget http://download.opensuse.org/repositories/home:/libertas-ict:/cobbler26/CentOS_CentOS-6/noarch/cob_CentOS-6/noarch/koan-2.6.5-9.1.noarch.rpm <br></code></pre></td></tr></table></figure><p>安装依赖包:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">yum install redhat-rpm-config createrepo python-netaddr python-simplejson rsync syslinux yum-utils genisoimage python-cheetah PyYAML httpd mod_wsgi dhcp fence-agents pykickstart <span class="hljs-built_in">bind</span> xinetd tftp-server <br></code></pre></td></tr></table></figure><p>安装依赖包:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">yum install Django <br></code></pre></td></tr></table></figure><p>安装依赖包:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">yum install virt-install <br></code></pre></td></tr></table></figure><p>安装上面下载的三个cobbler相关包</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">rpm -ivh *.rpm <br></code></pre></td></tr></table></figure><p>根据提示进行配置</p><p>修改默认密码 </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">openssl passwd -1 -salt<br></code></pre></td></tr></table></figure><p>修改&#x2F;etc&#x2F;cobbler&#x2F;settings 中的127.0.0.1为当前准备使用部署机器的IP</p><p>修改配置文件修改完成后会修改默认的dhcp的配置 </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">vim /etc/cobbler/dhcp.template<br></code></pre></td></tr></table></figure><p>dhcp配置OK 注意dhcp应该和网络一致</p><p>配置用户密码 </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">htdigest /etc/cobbler/users.digest <span class="hljs-string">&quot;Cobbler&quot;</span> cobbler<br></code></pre></td></tr></table></figure><p>配置tftp要重启xint 可以写kicstart</p><p>导入一个镜像</p><p>命令行下操作:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">cobbler import --name=grandocean --<span class="hljs-built_in">arch</span>=x86_64 --path=/mnt<br></code></pre></td></tr></table></figure><p>web对应操作: 从这个来看在后台导入镜像要简单一些 同时还会生成一个profile文件信息 后台对应的操作: </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">rsync -a /mnt/ /var/www/cobbler/ksmirror/grandocean-x8664 --progress<br></code></pre></td></tr></table></figure><p>增加一个系统,类似于定制的意思:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">cobbler system add --name=<span class="hljs-built_in">test</span> --profile=grandocean-x86_64 <br></code></pre></td></tr></table></figure><p>编辑kickstart文件相当于system</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">cobbler system edit --name=<span class="hljs-built_in">test</span> --interface=eth0 --mac=00:11:22:AA:BB:CC --ip-address=192.168.1.100 --netmask=255.255.255.0 --static=1 --dns-name=test.mydomain.com <br></code></pre></td></tr></table></figure><p>加入的标签</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># LINUX /memdisk # APPEND iso # INITRD /winpe_x86.iso </span><br></code></pre></td></tr></table></figure><p>另外一种 </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">LABEL winxppe MENU <br>LABEL winxppe kernel memdisk bash iso <br>initrd=/winxppe.iso MENU end<br><br>initrd winre.iso kernel memdisk iso bash boot<br></code></pre></td></tr></table></figure><p>System是针对指定的硬件的情况做的操作，目前来说不需要</p><p>SNIPPET里面的变量识别</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-variable">$yumrepostanza</span><br><br>repo --name=source-1 --baseurl=http://10.0.6.130/cobbler/ksmirror/centos6.4oriigin-x8664<br><span class="hljs-variable">$SNIPPET</span>(<span class="hljs-string">&#x27;network_config&#x27;</span>)<br><br>network --bootproto=dhcp --device=eth0 --onboot=on<br><span class="hljs-variable">$SNIPPET</span>(<span class="hljs-string">&#x27;logkspre&#x27;</span>)<br><br><span class="hljs-built_in">set</span> -x -v <span class="hljs-built_in">exec</span> 1&gt;/tmp/ks-pre.log 2&gt;&amp;1 <span class="hljs-comment"># Once root&#x27;s homedir is there, copy over the log. while : ; do sleep 10 if [ -d /mnt/sysimage/root ]; then cp /tmp/ks-pre.log /mnt/sysimage/root/ logger &quot;Copied %pre section log to system&quot; break fi done &amp; </span><br>SNIPPET(<span class="hljs-string">&#x27;kickstart_start&#x27;</span>)<br><br>wget <span class="hljs-string">&quot;http://10.0.6.130/cblr/svc/op/trig/mode/pre/profile/centos6.4oriigin-x86_64&quot;</span> -O /dev/null<br><span class="hljs-variable">$yumconfigstanza</span><br><br>wget <span class="hljs-string">&quot;http://10.0.6.130/cblr/svc/op/yum/profile/centos6.4oriigin-x86_64&quot;</span> --output-document=/etc/yum.repos.d/cobbler-config.repo<br></code></pre></td></tr></table></figure><p>写于: 2014年09月15日<br>更新于: 2015年03月24日</p>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>custom-ubuntu-server-iso</title>
    <link href="/2015/03/24/custom-ubuntu-server-iso/"/>
    <url>/2015/03/24/custom-ubuntu-server-iso/</url>
    
    <content type="html"><![CDATA[<p>Remastering the Ubuntu Desktop ISO is easy considering the existing graphical tools but did you ever wanted to build your custom Ubuntu Server Edition ISO ?</p><p>Preparing the Environment<br>You’ll need a clean copy of the Ubuntu Server ISO that you want to customize. Since 10.04 is the latest Ubuntu version I will write down my examples using it but everything should work pretty much unchanged for older or newer versions.</p><p>After you have you have the original iso image downloaded, make a copy it’s contents so we can later apply our patching there.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">cd</span> /home/rgavril/Work<br><br><span class="hljs-built_in">mkdir</span> original-iso custom-iso<br>mount -o loop ./ubuntu-10.04-server-i386.iso ./original-iso<br><br><span class="hljs-built_in">cp</span> -r ./original-iso/* ./custom-iso/<br><span class="hljs-built_in">cp</span> -r ./original-iso/.disk/ ./custom-iso/<br><br>umount ./original-iso/<br></code></pre></td></tr></table></figure><p>Adding a Boot Menu Option<br>For start let’s add a option to the cd boot menu. You’ll need to modify isolinux&#x2F;text.cfg and insert the next lines between default install and label install:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">label custom<br>  menu label ^Install Custom Ubuntu Server<br>  kernel /install/vmlinuz<br>  append  file=/cdrom/preseed/ubuntu-custom.seed initrd=/install/initrd.gz quiet ks=cdrom:/isolinux/ks-custom.cfg --<br></code></pre></td></tr></table></figure><p>This newly added block is very similar to the label install one. The two important additions being the file and ks boot options that I will later explain.</p><p>If you want to make your menu option the default one, you only need to change the default install to default custom or whatever you used as a label.</p><p>Kickstart-ing<br>KickStart is a unattended installation method developed by Red Hat and later adopted and adapted by Debian and Ubuntu. To keep it short, the Ubuntu installer can read an external file and figure out how to configure the system (create partitions, set timezones and keyboard layouts ..) without asking the user.</p><p>There’s no need to write everything using a text editor as we have a powerful tool to create such configuration files:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">apt-get install system-config-kickstart ksconfig<br></code></pre></td></tr></table></figure><p>After creating your ks.cfg file, you will need to move it on the custom-iso&#x2F; at the path specified in text.cfg. That’s the location where the ubuntu installer will look for it.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">cp</span> ks.cfg custom-iso/isolinux/ks-custom.cfg<br></code></pre></td></tr></table></figure><p>Preseeding Packages<br>The installer in not the only one that may present questions to the user. When installed, some package rely on the user to explicitly set different parameters.</p><p>Preseeding is the action of setting, in advance, this kind of package parameters. It’s a very powerful method that you can use even for replacing kickstart. In this article, I’ll use it as a addition to kickstart as is more harder to tune the ubuntu installer, mostly because it lacks a gui.</p><p>The params can be set trough a configuration file specified by the file in text.cfg. I recommend starting with the existing preseed file from the original cd:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">cp</span> custom-iso/preseed/ubuntu-server.seed custom-iso/preseed/ubuntu-custom.seed<br></code></pre></td></tr></table></figure><p>Using debconf-get-selections from the debconf-utils package you can look over a running ubuntu system to figure out what parameters you can tune. For example here are the possible configuration settings for the openssh-server together with their values :</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">root@xps1330:~/work<span class="hljs-comment"># debconf-get-selections | grep openssh</span><br>openssh-serverssh/vulnerable_host_keysnote<br>openssh-serverssh/use_old_init_scriptboolean<span class="hljs-literal">true</span><br>openssh-serverssh/encrypted_host_key_but_no_keygennote<br>openssh-serverssh/disable_cr_authboolean<span class="hljs-literal">false</span><br></code></pre></td></tr></table></figure><p>Adding Extra Packages<br>Ubuntu’s ksconfig utility does’n provide a way to select extra packages as the Package Selection is not working. In order to specify what extra packages you want to install you’ll need to modify the kickstart configuration file by hand.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">vim custom-iso/isolinux/ks-custom.cfg<br></code></pre></td></tr></table></figure><p>Depending on the extra packages that you want to install by default, add some similar lines to the end of the file :</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">%packages<br>openssh-server<br>asterisk<br>asterisk-mysql<br></code></pre></td></tr></table></figure><p>Offline Installation<br>When having additional packages installed by default, there’s a big chance are that those particular packages may not be on the default iso. If you have an active internet connection during the install, the packages will be automatically fetched from the online repositories. But if you’re internet connection is not working, the install process will fail. In order to prevent the problem and create a offline installable cd, you’ll need to perform some extra steps.</p><p>You’ll need to downloading and copying the extra debs and all their dependencies on your iso. Please note that in doing so you may increase the iso size a lot and be forced to use a dvd.</p><p>It’s very hard to figure out what debs are missing from your iso. In order to make a list of needed debs I use the following steps:</p><p>On a virtual machine, install a ubuntu server using the original cd image.<br>Boot the newly installed ubuntu server, make sure the cd is mountable and cdrom directive is available in sources.list<br>Do an apt-get install of the extra packages that I want to have installed by default<br>Make a backup of the debs located in &#x2F;var&#x2F;cache&#x2F;apt&#x2F;archives as this are the missing ones that need to be on the iso<br>Considering that you downloaded the extra packages, and that they are located in the extradebs&#x2F; directory within the current folder. You will need to copy them on the cd and create a repository by running this commands:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">mkdir</span> -p custom-iso/dists/stable/extras/binary-i386<br><span class="hljs-built_in">mkdir</span> -p custom-iso/pool/extras/<br><br><span class="hljs-built_in">cp</span> ./extradebs/*.deb custom-iso/pool/extras/<br><br><span class="hljs-built_in">pushd</span> custom-iso<br>apt-ftparchive packages ./pool/extras/ &gt; dists/stable/extras/binary-i386/Packages<br>gzip -c ./dists/stable/extras/binary-i386/Packages | <span class="hljs-built_in">tee</span> ./dists/stable/extras/binary-i386/Packages.gz &gt; /dev/null<br><span class="hljs-built_in">popd</span><br></code></pre></td></tr></table></figure><p>Generating the new ISO<br>All you need to do now is build you new iso and give it a test drive to see how it works:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">mkisofs -J -l -b isolinux/isolinux.bin -no-emul-boot -boot-load-size 4 -boot-info-table -z -iso-level 4 -c isolinux/isolinux.cat -o ./ubuntu-10.04-custom-i386.iso custom-iso/<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>定制ubuntu的时候修改proseed</title>
    <link href="/2015/03/24/%E5%AE%9A%E5%88%B6ubuntu%E7%9A%84%E6%97%B6%E5%80%99%E4%BF%AE%E6%94%B9proseed/"/>
    <url>/2015/03/24/%E5%AE%9A%E5%88%B6ubuntu%E7%9A%84%E6%97%B6%E5%80%99%E4%BF%AE%E6%94%B9proseed/</url>
    
    <content type="html"><![CDATA[<p>一个参数的修改</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">d-i clock-setup/utc-auto boolean <span class="hljs-literal">false</span> (不用utc)<br>d-i clock-setup/ntp boolean <span class="hljs-literal">false</span>   (不时间同步)<br>d-i netcfg/disable_autoconfig boolean <span class="hljs-literal">true</span>   (关闭自动网络配置)<br>d-i netcfg/use_autoconfig boolean <span class="hljs-literal">false</span>   (不使用自动配置文件)<br>d-i apt-setup/use_mirror boolean <span class="hljs-literal">false</span> (不使用apt 镜像)<br></code></pre></td></tr></table></figure><p>写于: 2014年08月07日<br>更新于: 2015年03月24日</p>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>centos使用U盘做启动盘</title>
    <link href="/2015/03/24/centos%E4%BD%BF%E7%94%A8U%E7%9B%98%E5%81%9A%E5%90%AF%E5%8A%A8%E7%9B%98/"/>
    <url>/2015/03/24/centos%E4%BD%BF%E7%94%A8U%E7%9B%98%E5%81%9A%E5%90%AF%E5%8A%A8%E7%9B%98/</url>
    
    <content type="html"><![CDATA[<p>软件下载地址:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">http://sourceforge.net/projects/iso2usb/files/latest/download?<span class="hljs-built_in">source</span>=dlp<br></code></pre></td></tr></table></figure><!--break--><p>写于: 2014年08月04日<br>更新于: 2015年03月24日</p>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>fio的配置使用</title>
    <link href="/2015/03/24/fio%E7%9A%84%E9%85%8D%E7%BD%AE%E4%BD%BF%E7%94%A8/"/>
    <url>/2015/03/24/fio%E7%9A%84%E9%85%8D%E7%BD%AE%E4%BD%BF%E7%94%A8/</url>
    
    <content type="html"><![CDATA[<p>将fio-2.1.10.tar.gz拷贝到linux服务器的&#x2F;usr&#x2F;src&#x2F;下</p><p>解压源码包:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">root@grandocean:/usr/src<span class="hljs-comment"># tar xvf fio-2.1.10.tar.gz </span><br>root@grandocean:/usr/src<span class="hljs-comment"># cd fio-2.1.10/ </span><br></code></pre></td></tr></table></figure><p>安装依赖包:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">root@grandocean:/usr/src/fio-2.1.10<span class="hljs-comment"># apt-get install pkg-config gtk+-2.0 libaio-dev </span><br></code></pre></td></tr></table></figure><p>开启gfio:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">root@grandocean:/usr/src/fio-2.1.10<span class="hljs-comment"># ./configure --enable-gfio </span><br></code></pre></td></tr></table></figure><p>编译fio:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">root@grandocean:/usr/src/fio-2.1.10<span class="hljs-comment"># make fio </span><br></code></pre></td></tr></table></figure><p>编译gfio:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">root@grandocean:/usr/src/fio-2.1.10<span class="hljs-comment"># make gfio </span><br></code></pre></td></tr></table></figure><p>启动server模式:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">root@grandocean:/usr/src/fio-2.1.10<span class="hljs-comment"># ./fio -S</span><br></code></pre></td></tr></table></figure><p>测试脚本:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-meta">#! /bin/sh</span><br><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;-------------------------------------------------------------------------------&quot;</span><br><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;iozone测试&quot;</span>  &gt; result.fio<br>iozone -i 0 -i 1 -i 2 -i 5 -r 4M -s 16G -f /mnt/test1/iotestthru.iso &gt;&gt;  result.fio<br><span class="hljs-built_in">sleep</span> 30<br><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;-------------------------------------------------------------------------------&quot;</span><br><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;顺序读&quot;</span> &gt;&gt; result.fio<br>fio -filename=/mnt/fio/cachethrusqread -direct=1 -iodepth 1 -thread  -rw=<span class="hljs-built_in">read</span> -ioengine=psync  -bs=16k  -size=50G -numjobs=30 -runtime=1200 -group_reporting -name=cachesqread  &gt;&gt; result.fio<br><span class="hljs-built_in">sleep</span> 30<br><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;-------------------------------------------------------------------------------&quot;</span><br><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;顺序写&quot;</span> &gt;&gt; result.fio<br>fio -filename=/mnt/fio/cachethrusqwrite -direct=1 -iodepth 1 -thread  -rw=write -ioengine=psync  -bs=16k  -size=50G -numjobs=30 -runtime=1200 -group_reporting -name=cachewrite<br><span class="hljs-built_in">sleep</span> 30<br><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;-------------------------------------------------------------------------------&quot;</span><br><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;随机写&quot;</span> &gt;&gt; result.fio<br>fio -filename=/mnt/fio/cachethrurandwrite -direct=1 -iodepth 1 -thread  -rw=randwrite -ioengine=psync  -bs=16k  -size=50G -numjobs=30 -runtime=1200 -group_reporting -name=cacherandwrite &gt;&gt; result.fio<br><span class="hljs-built_in">sleep</span> 30<br><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;-------------------------------------------------------------------------------&quot;</span><br><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;随机读&quot;</span>  &gt;&gt; result.fio<br>fio -filename=/mnt/fio/cachethrurandread -direct=1 -iodepth 1 -thread  -rw=randread -ioengine=psync  -bs=16k  -size=50G -numjobs=30 -runtime=1200 -group_reporting -name=cacherandread     &gt;&gt;  result.fio<br><span class="hljs-built_in">sleep</span> 30<br><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;-------------------------------------------------------------------------------&quot;</span><br><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;随机读写比例各50%&quot;</span> &gt;&gt; result.fio<br>fio -filename=/mnt/fio/cachethrurandreadwrite -direct=1 -iodepth 1 -thread  -rw=randrw -rwmixread=50 -ioengine=psync  -bs=16k  -size=50G -numjobs=30 -runtime=1200 -group_reporting -name=cacherandreadwrite &gt;&gt;  result.fio<br><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;-------------------------------------------------------------------------------&quot;</span><br></code></pre></td></tr></table></figure><h2 id="centos7-安装"><a href="#centos7-安装" class="headerlink" title="centos7 安装"></a>centos7 安装</h2><h3 id="下载"><a href="#下载" class="headerlink" title="下载"></a>下载</h3><p><a href="https://github.com/axboe/fio/archive/fio-3.26.tar.gz">https://github.com/axboe/fio/archive/fio-3.26.tar.gz</a></p><p>这个版本对gcc有要求，需要支持c11，需要升级到gcc 4.9版本或者以上</p><p>yum install centos-release-scl -y<br>yum install devtoolset-7-toolchain -y<br>scl enable devtoolset-7 bash<br>执行完gcc升级到7.3.1了</p><p>安装ui相关的库<br>yum install libgnomeui-devel</p><p>make fio<br>make gfio</p><h2 id="FIO的几个测试模型"><a href="#FIO的几个测试模型" class="headerlink" title="FIO的几个测试模型"></a>FIO的几个测试模型</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-meta">#! /bin/sh</span><br><span class="hljs-comment"># SEQ 1M Q8 T1</span><br>fio -direct=1 -iodepth=8 -rw=write -ioengine=libaio -bs=1M -size=1G -numjobs=1 -runtime=120 -group_reporting -filename=/nvmedisk/test.temp -name=Q8T1-SEQ1M1G-WRITE &gt;&gt; resoult.txt<br><span class="hljs-built_in">sleep</span> 5;<span class="hljs-built_in">sync</span>;<br>fio -direct=1 -iodepth=8 -rw=<span class="hljs-built_in">read</span> -ioengine=libaio -bs=1M -size=1G -numjobs=1 -runtime=120 -group_reporting -filename=/nvmedisk/test.temp -name=Q8T1-SEQ1M1G-READ &gt;&gt; resoult.txt<br><span class="hljs-built_in">sleep</span> 5;<span class="hljs-built_in">sync</span>;<br><span class="hljs-comment"># SEQ 1M Q1T1</span><br>fio -direct=1 -iodepth=1 -rw=write -ioengine=libaio -bs=1M -size=1G -numjobs=1 -runtime=120 -group_reporting -filename=/nvmedisk/test.temp -name=Q1T1-SEQ1M1G-WRITE &gt;&gt; resoult.txt<br><span class="hljs-built_in">sleep</span> 5;<span class="hljs-built_in">sync</span>;<br>fio -direct=1 -iodepth=1 -rw=<span class="hljs-built_in">read</span> -ioengine=libaio -bs=1M -size=1G -numjobs=1 -runtime=120 -group_reporting -filename=/nvmedisk/test.temp -name=Q1T1-SEQ1M1G-READ &gt;&gt; resoult.txt<br><span class="hljs-built_in">sleep</span> 5;<span class="hljs-built_in">sync</span>;<br><span class="hljs-comment"># Q32T1-RND4K</span><br>fio -direct=1 -iodepth=32 -rw=randwrite -ioengine=libaio -bs=4k -size=1G -numjobs=1 -runtime=120 -group_reporting -filename=/nvmedisk/test.temp -name=Q32T1-RND4K1G-WRITE &gt;&gt; resoult.txt<br><span class="hljs-built_in">sleep</span> 5;<span class="hljs-built_in">sync</span>;<br>fio -direct=1 -iodepth=32 -rw=randread -ioengine=libaio -bs=4k -size=1G -numjobs=1 -runtime=120 -group_reporting -filename=/nvmedisk/test.temp -name=Q32T1-RND4K1G-READ &gt;&gt; resoult.txt<br><span class="hljs-built_in">sleep</span> 5;<span class="hljs-built_in">sync</span>;<br><span class="hljs-comment"># Q1T1-RND4K Q1 T1</span><br>fio -direct=1 -iodepth=1 -rw=randwrite -ioengine=libaio -bs=4k -size=1G -numjobs=1 -runtime=120 -group_reporting -filename=/nvmedisk/test.temp -name=Q1T1-RND4K1G-WRITE &gt;&gt; resoult.txt<br><span class="hljs-built_in">sleep</span> 5;<span class="hljs-built_in">sync</span>;<br>fio -direct=1 -iodepth=1 -rw=randread -ioengine=libaio -bs=4k -size=1G -numjobs=1 -runtime=120 -group_reporting -filename=/nvmedisk/test.temp -name=Q1T1-RND4K1G-READ &gt;&gt; resoult.txt<br><span class="hljs-built_in">sleep</span> 5;<span class="hljs-built_in">sync</span>;<br></code></pre></td></tr></table></figure><p>这个对应到diskwindows测试工具的io模型，可以根据自己的需要进行修改</p>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>持续运行一个命令-并且将结果输出到文本</title>
    <link href="/2015/03/24/%E6%8C%81%E7%BB%AD%E8%BF%90%E8%A1%8C%E4%B8%80%E4%B8%AA%E5%91%BD%E4%BB%A4-%E5%B9%B6%E4%B8%94%E5%B0%86%E7%BB%93%E6%9E%9C%E8%BE%93%E5%87%BA%E5%88%B0%E6%96%87%E6%9C%AC/"/>
    <url>/2015/03/24/%E6%8C%81%E7%BB%AD%E8%BF%90%E8%A1%8C%E4%B8%80%E4%B8%AA%E5%91%BD%E4%BB%A4-%E5%B9%B6%E4%B8%94%E5%B0%86%E7%BB%93%E6%9E%9C%E8%BE%93%E5%87%BA%E5%88%B0%E6%96%87%E6%9C%AC/</url>
    
    <content type="html"><![CDATA[<p>持续运行一个命令，并且将结果输出到文本</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-meta">#! /bin/sh</span><br><span class="hljs-keyword">while</span> [ 2 &gt; 1 ]<br><span class="hljs-keyword">do</span><br>    <span class="hljs-built_in">echo</span> `<span class="hljs-built_in">date</span>` &gt;&gt; recode.txt<br>    <span class="hljs-built_in">df</span> -h &gt;&gt; recode.txt<br>    <span class="hljs-built_in">sleep</span>  5<br><span class="hljs-keyword">done</span>  <br></code></pre></td></tr></table></figure><p>写于: 2014年06月12日<br>更新于: 2015年03月24日</p>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>ubuntu无法关机</title>
    <link href="/2015/03/23/ubuntu%E6%97%A0%E6%B3%95%E5%85%B3%E6%9C%BA/"/>
    <url>/2015/03/23/ubuntu%E6%97%A0%E6%B3%95%E5%85%B3%E6%9C%BA/</url>
    
    <content type="html"><![CDATA[<p>在&#x2F;etc&#x2F;default&#x2F;halt 增加下面</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">INIT_HALT = POWEROFF<br></code></pre></td></tr></table></figure><p>另一种方法:</p><p>I have the same problem and found a solution which worked out for me.</p><p>Type in terminal:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs bash">sudo -i (to get a root shell, sudo gedit is not recommended)<br>gedit /etc/default/grub<br>Find the line: GRUB_CMDLINE_LINUX_DEFAULT=<span class="hljs-string">&quot;quiet splash&quot;</span><br>Change this to: GRUB_CMDLINE_LINUX_DEFAULT=<span class="hljs-string">&quot;quiet splash acpi=force&quot;</span><br>Save the file and close the file.<br>Finally, <span class="hljs-keyword">in</span> terminal: update-grub<br><span class="hljs-built_in">exit</span> (to end the root shell)<br></code></pre></td></tr></table></figure><p>写于: 2014年05月07日<br>更新于: 2015年03月23日</p>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>linux中KVM桥接网卡br0</title>
    <link href="/2015/03/23/linux%E4%B8%ADKVM%E6%A1%A5%E6%8E%A5%E7%BD%91%E5%8D%A1br0/"/>
    <url>/2015/03/23/linux%E4%B8%ADKVM%E6%A1%A5%E6%8E%A5%E7%BD%91%E5%8D%A1br0/</url>
    
    <content type="html"><![CDATA[<p>在centos虚拟化当中需要增加一个桥接网卡,然后将虚拟化当中的机器的网卡桥接到桥接网卡,下面将描述设置方法:</p><p>查看现有网卡</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@zb ~]<span class="hljs-comment"># vim /etc/sysconfig/network-scripts/ifcfg-eth0</span><br>DEVICE=eth0<br>HWADDR=70:71:BC:1F:E4:86<br>TYPE=Ethernet<br>UUID=0213211a-d451-4944-881c-e7475b8bee34<br>ONBOOT=<span class="hljs-built_in">yes</span><br>NM_CONTROLLED=<span class="hljs-built_in">yes</span><br>BOOTPROTO=static<br>IPADDR=192.168.7.107<br>NETMASK=255.255.0.0<br>GATEWAY=192.168.1.1<br></code></pre></td></tr></table></figure><p>将网卡信息copy一份到br0再进行修改</p><p>修改eth0的信息为:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs bash">DEVICE=eth0<br>HWADDR=70:71:BC:1F:E4:86<br>TYPE=Ethernet<br>ONBOOT=<span class="hljs-built_in">yes</span><br>NM_CONTROLLED=<span class="hljs-built_in">yes</span><br>BOOTPROTO=static<br>BRIDGE=br0     <span class="hljs-comment">#新增加了这一句,重要</span><br></code></pre></td></tr></table></figure><p>修改br0的信息为：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs bash">DEVICE=br0     <span class="hljs-comment">#更改eth0为br0</span><br>HWADDR=70:71:BC:1F:E4:86<br>TYPE=Bridge       <span class="hljs-comment">#修改类型为bridge</span><br>ONBOOT=<span class="hljs-built_in">yes</span><br>BOOTPROTO=static<br>IPADDR=192.168.7.107<br>NETMASK=255.255.0.0<br>GATEWAY=192.168.1.1<br></code></pre></td></tr></table></figure><p>修改完成后,eth0实际是没有ip的,br0为当前主机的ip了</p><p>配置文件如下</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@cachenode network-scripts]<span class="hljs-comment"># cat ifcfg-ens33 </span><br>TYPE=<span class="hljs-string">&quot;Ethernet&quot;</span><br>BOOTPROTO=<span class="hljs-string">&quot;none&quot;</span><br>DEFROUTE=<span class="hljs-string">&quot;yes&quot;</span><br>NAME=<span class="hljs-string">&quot;ens33&quot;</span><br>DEVICE=<span class="hljs-string">&quot;ens33&quot;</span><br>ONBOOT=<span class="hljs-string">&quot;yes&quot;</span><br>BRIDGE=br0<br>NM_CONTROLLED=<span class="hljs-string">&quot;no&quot;</span><br>[root@cachenode network-scripts]<span class="hljs-comment"># cat ifcfg-br0 </span><br>TYPE=Bridge<br>ONBOOT=<span class="hljs-built_in">yes</span><br>DEVICE=br0<br>BOOTPROTO=<span class="hljs-string">&quot;none&quot;</span><br>IPADDR=192.168.220.128<br>NETMASK=255.255.255.0<br>GATEWAY=192.168.220.2<br>NM_CONTROLLED=<span class="hljs-string">&quot;no&quot;</span><br></code></pre></td></tr></table></figure><p>ubuntu下的配置方式</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs bash">auto lo<br>iface lo inet loopback<br>auto br0<br>iface br0 inet static<br>bridge_ports eth0<br>bridge_stp off<br>bridge_maxwait 0<br>bridge_fd 0<br>address 192.168.5.108<br>netmask 255.255.0.0<br>gateway 192.168.1.1<br>dns-nameservers 192.168.1.1<br></code></pre></td></tr></table></figure><p>配置完成后:</p><p><img src="/images/blog/o_200901023620%E6%A1%A5%E6%8E%A5%E7%BD%91%E5%8D%A1br0.jpg"></p><p>写于: 2013年12月24日<br>更新于: 2015年03月23日</p>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>ubuntu掉电出现检查文件系统的问题</title>
    <link href="/2015/03/23/ubuntu%E6%8E%89%E7%94%B5%E5%87%BA%E7%8E%B0%E6%A3%80%E6%9F%A5%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E7%9A%84%E9%97%AE%E9%A2%98/"/>
    <url>/2015/03/23/ubuntu%E6%8E%89%E7%94%B5%E5%87%BA%E7%8E%B0%E6%A3%80%E6%9F%A5%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E7%9A%84%E9%97%AE%E9%A2%98/</url>
    
    <content type="html"><![CDATA[<p>修改:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">/etc/default/rcS <br>FSCKFIX=no 为 FSCKFIX=<span class="hljs-built_in">yes</span><br></code></pre></td></tr></table></figure><p>出现这个情况的原因是硬件时钟偏移了<br>显示上次挂载根目录在未来时间。</p><p>写于: 2013年11月28日<br>更新于: 2015年03月23日</p>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>ssh配好无密码登录（RSA公钥）后，还要密码登录的问题的解决办法</title>
    <link href="/2015/03/23/ssh%E9%85%8D%E5%A5%BD%E6%97%A0%E5%AF%86%E7%A0%81%E7%99%BB%E5%BD%95%EF%BC%88RSA%E5%85%AC%E9%92%A5%EF%BC%89%E5%90%8E%EF%BC%8C%E8%BF%98%E8%A6%81%E5%AF%86%E7%A0%81%E7%99%BB%E5%BD%95%E7%9A%84%E9%97%AE%E9%A2%98%E7%9A%84%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95/"/>
    <url>/2015/03/23/ssh%E9%85%8D%E5%A5%BD%E6%97%A0%E5%AF%86%E7%A0%81%E7%99%BB%E5%BD%95%EF%BC%88RSA%E5%85%AC%E9%92%A5%EF%BC%89%E5%90%8E%EF%BC%8C%E8%BF%98%E8%A6%81%E5%AF%86%E7%A0%81%E7%99%BB%E5%BD%95%E7%9A%84%E9%97%AE%E9%A2%98%E7%9A%84%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<p>首先删除 &#x2F;root&#x2F;.ssh目录<br>然后ssh-keygen 生成新的认证目录<br>然后检查能否免密码登陆</p><p>如果还不能可能是&#x2F;root&#x2F;目录的权限不对了 可能被异常改到777了<br>做操作 </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">chmod</span> 650 /root/<br></code></pre></td></tr></table></figure><p>然后再尝试看能不能免登陆访问</p><p>写于: 2013年11月28日<br>更新于: 2015年03日23日</p>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>ubuntu服务器dns重启失效问题</title>
    <link href="/2015/03/23/ubuntu%E6%9C%8D%E5%8A%A1%E5%99%A8dns%E9%87%8D%E5%90%AF%E5%A4%B1%E6%95%88%E9%97%AE%E9%A2%98/"/>
    <url>/2015/03/23/ubuntu%E6%9C%8D%E5%8A%A1%E5%99%A8dns%E9%87%8D%E5%90%AF%E5%A4%B1%E6%95%88%E9%97%AE%E9%A2%98/</url>
    
    <content type="html"><![CDATA[<h3 id="方法一-通过-etc-network-interfaces-在它的最后增加一句"><a href="#方法一-通过-etc-network-interfaces-在它的最后增加一句" class="headerlink" title="方法一 通过&#x2F;etc&#x2F;network&#x2F;interfaces,在它的最后增加一句:"></a>方法一 通过&#x2F;etc&#x2F;network&#x2F;interfaces,在它的最后增加一句:</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">dns-nameservers 8.8.8.8<br></code></pre></td></tr></table></figure><p>8.8.8.8是Google提供的DNS服务，这里只是举一个例子,你也可以改成电信运营商的DNS。重启后DNS就生效了，这时候再看&#x2F;etc&#x2F;resolv.conf,最下面就多了一行:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># Dynamic resolv.conf(5) file for glibc resolver(3) generated by resolvconf(8)</span><br><span class="hljs-comment">#     DO NOT EDIT THIS FILE BY HAND -- YOUR CHANGES WILL BE OVERWRITTEN</span><br>nameserver 8.8.8.8<br></code></pre></td></tr></table></figure><h3 id="方法二-通过修改"><a href="#方法二-通过修改" class="headerlink" title="方法二 通过修改:"></a>方法二 通过修改:</h3><p>&#x2F;etc&#x2F;resolvconf&#x2F;resolv.conf.d&#x2F;base,这个文件默认是空的:<br>在里面插入:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">nameserver 8.8.8.8<br>nameserver 8.8.4.4<br></code></pre></td></tr></table></figure><p>如果有多个DNS就一行一个<br>修改好保存,然后执行:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">resolvconf -u<br></code></pre></td></tr></table></figure><p>再看&#x2F;etc&#x2F;resolv.conf,最下面就多了2行:<br>cat &#x2F;etc&#x2F;resolv.conf</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># Dynamic resolv.conf(5) file for glibc resolver(3) generated by resolvconf(8)</span><br><span class="hljs-comment">#     DO NOT EDIT THIS FILE BY HAND -- YOUR CHANGES WILL BE OVERWRITTEN</span><br>nameserver 8.8.8.8<br>nameserver 8.8.4.4<br></code></pre></td></tr></table></figure><p>可以看到我们的设置已经加上了,然后再ping一个域名,当时就可以解析了,无需重启。<br>附:CentOS下修改DNS则容易的多了,直接修改&#x2F;etc&#x2F;resolv.conf,内容是:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">nameserver 8.8.8.8<br>nameserver 8.8.4.4<br></code></pre></td></tr></table></figure><p>保存就生效了,重启也没问题。<br>或者在网卡配置文件当中添加:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">DNS1=8.8.8.8<br></code></pre></td></tr></table></figure><p>写于: 2013年11月26日<br>更新于: 2015年03月23日</p>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>编译一个Centos6.4下可用的内核rpm升级包-3.8.13内核rpm包</title>
    <link href="/2015/03/23/%E7%BC%96%E8%AF%91%E4%B8%80%E4%B8%AACentos6.4%E4%B8%8B%E5%8F%AF%E7%94%A8%E7%9A%84%E5%86%85%E6%A0%B8rpm%E5%8D%87%E7%BA%A7%E5%8C%85-3.8.13%E5%86%85%E6%A0%B8rpm%E5%8C%85/"/>
    <url>/2015/03/23/%E7%BC%96%E8%AF%91%E4%B8%80%E4%B8%AACentos6.4%E4%B8%8B%E5%8F%AF%E7%94%A8%E7%9A%84%E5%86%85%E6%A0%B8rpm%E5%8D%87%E7%BA%A7%E5%8C%85-3.8.13%E5%86%85%E6%A0%B8rpm%E5%8C%85/</url>
    
    <content type="html"><![CDATA[<p>在Centos6.4下进行内核升级,采用内核源码的升级方式比较简单，但是需要升级的机器多的情况下进行内核升级就比较麻烦,并且编译内核的速度依赖于机器的性能,一般需要20分钟,而通过rpm内核包的方式进行安装,就比较快,一般在几分钟之内就可以完成内核的安装,本文档以3.8.13内核源码为例子进行的操作。<br>  根据网上的指导教程编译的rpm包，编译完成后只会生成如下两个rpm包:</p><ul><li>kernel-3.8.13-1.x86_64.rpm</li><li>kernel-headers-3.8.13-1.x86_64.rpm<!--break-->而实际进行内核升级的时候还需要kernel-firmwsare-3.8.13-1.x86_64.rpm这个包,这个包是包含内核固件相关文件的,而在进行rpm打包的时候,这个包会内嵌到kernel-3.8.13-1.x86_64.rpm这个包当中去,而安装的时候系统没有找到这个包，进行升级的时候就会报错。<br>所以需要对内核源码编译文件进行一定的修改，这个地方需要修改的文件为linux-3.8.13&#x2F;scripts&#x2F;package&#x2F;mkspec路径下的这个文件,具体修改如下(新版内核解决了这个问题,可以不需要修改,根据生成包的情况来看是否修改):</li></ul><h4 id="加入kernel-devel支持需要修改"><a href="#加入kernel-devel支持需要修改" class="headerlink" title="加入kernel-devel支持需要修改"></a>加入kernel-devel支持需要修改</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><code class="hljs bash">scripts/package/mkspec |   26 ++++++++++++++++++++++++++<br>    1 file changed, 26 insertions(+)<br>diff --git a/scripts/package/mkspec b/scripts/package/mkspec<br>index 514aeb2..65131df 100755<br>--- a/scripts/package/mkspec<br>+++ b/scripts/package/mkspec<br>@@ -59,6 +59,14 @@ <span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;header files define structures and constants that are needed for&quot;</span><br>    <span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;building most standard programs and are also needed for rebuilding the&quot;</span><br>    <span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;glibc package.&quot;</span><br>    <span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;&quot;</span><br>+<span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;%package devel&quot;</span><br>+<span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;Summary: Development package for building kernel modules to match the <span class="hljs-variable">$__KERNELRELEASE</span> kernel&quot;</span><br>+<span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;Group: System Environment/Kernel&quot;</span><br>+<span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;AutoReqProv: no&quot;</span><br>+<span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;%description -n kernel-devel&quot;</span><br>+<span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;This package provides kernel headers and makefiles sufficient to build modules&quot;</span><br>+<span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;against the <span class="hljs-variable">$__KERNELRELEASE</span> kernel package.&quot;</span><br>+<span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;&quot;</span><br>    <br>    <span class="hljs-keyword">if</span> ! <span class="hljs-variable">$PREBUILT</span>; <span class="hljs-keyword">then</span><br>    <span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;%prep&quot;</span><br>@@ -109,6 +117,11 @@ <span class="hljs-built_in">echo</span> <span class="hljs-string">&#x27;mv vmlinux.bz2 $RPM_BUILD_ROOT&#x27;</span><span class="hljs-string">&quot;/boot/vmlinux-<span class="hljs-variable">$KERNELRELEASE</span>.bz2&quot;</span><br>    <span class="hljs-built_in">echo</span> <span class="hljs-string">&#x27;mv vmlinux.orig vmlinux&#x27;</span><br>    <span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;%endif&quot;</span><br>    <br>+<span class="hljs-built_in">echo</span> <span class="hljs-string">&#x27;rm -rf $RPM_BUILD_ROOT&#x27;</span><span class="hljs-string">&quot;/lib/modules/<span class="hljs-variable">$KERNELRELEASE</span>/&#123;build,source&#125;&quot;</span><br>+<span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;mkdir -p &quot;</span><span class="hljs-string">&#x27;$RPM_BUILD_ROOT&#x27;</span><span class="hljs-string">&quot;/usr/src/kernels/<span class="hljs-variable">$KERNELRELEASE</span>&quot;</span><br>+<span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;EXCLUDES=\&quot;--exclude-vcs --exclude .tmp_versions --exclude=*vmlinux* --exclude=*.o --exclude=*.ko --exclude=*.cmd --exclude=Documentation --exclude=firmware --exclude .config.old --exclude .missing-syscalls.d\&quot;&quot;</span><br>+<span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;tar &quot;</span><span class="hljs-string">&#x27;$EXCLUDES&#x27;</span><span class="hljs-string">&quot; -cf- . | (cd &quot;</span><span class="hljs-string">&#x27;$RPM_BUILD_ROOT&#x27;</span><span class="hljs-string">&quot;/usr/src/kernels/<span class="hljs-variable">$KERNELRELEASE</span>;tar xvf -)&quot;</span><br>+<br>    <span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;&quot;</span><br>    <span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;%clean&quot;</span><br>    <span class="hljs-built_in">echo</span> <span class="hljs-string">&#x27;rm -rf $RPM_BUILD_ROOT&#x27;</span><br>@@ -122,6 +135,15 @@ <span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;/sbin/installkernel <span class="hljs-variable">$KERNELRELEASE</span> /boot/vmlinuz-<span class="hljs-variable">$KERNELRELEASE</span>-rpm /boot/</span><br><span class="hljs-string">    echo &quot;</span><span class="hljs-built_in">rm</span> -f /boot/vmlinuz-<span class="hljs-variable">$KERNELRELEASE</span>-rpm /boot/System.map-<span class="hljs-variable">$KERNELRELEASE</span>-rpm<span class="hljs-string">&quot;</span><br><span class="hljs-string">    echo &quot;</span><span class="hljs-keyword">fi</span><span class="hljs-string">&quot;</span><br><span class="hljs-string">    echo &quot;</span><span class="hljs-string">&quot;</span><br><span class="hljs-string">+echo &quot;</span>%post devel<span class="hljs-string">&quot;</span><br><span class="hljs-string">+echo &quot;</span><span class="hljs-built_in">cd</span> /lib/modules/<span class="hljs-variable">$KERNELRELEASE</span><span class="hljs-string">&quot;</span><br><span class="hljs-string">+echo &quot;</span><span class="hljs-built_in">ln</span> -sf /usr/src/kernels/<span class="hljs-variable">$KERNELRELEASE</span> build<span class="hljs-string">&quot;</span><br><span class="hljs-string">+echo &quot;</span><span class="hljs-built_in">ln</span> -sf /usr/src/kernels/<span class="hljs-variable">$KERNELRELEASE</span> <span class="hljs-built_in">source</span><span class="hljs-string">&quot;</span><br><span class="hljs-string">+echo &quot;</span><span class="hljs-string">&quot;</span><br><span class="hljs-string">+echo &quot;</span>%postun devel<span class="hljs-string">&quot;</span><br><span class="hljs-string">+echo &quot;</span><span class="hljs-built_in">cd</span> /lib/modules/<span class="hljs-variable">$KERNELRELEASE</span><span class="hljs-string">&quot;</span><br><span class="hljs-string">+echo &quot;</span><span class="hljs-built_in">rm</span> -f build <span class="hljs-built_in">source</span><span class="hljs-string">&quot;</span><br><span class="hljs-string">+echo &quot;</span><span class="hljs-string">&quot;</span><br><span class="hljs-string">    echo &quot;</span>%files<span class="hljs-string">&quot;</span><br><span class="hljs-string">    echo &#x27;%defattr (-, root, root)&#x27;</span><br><span class="hljs-string">    echo &quot;</span>%<span class="hljs-built_in">dir</span> /lib/modules<span class="hljs-string">&quot;</span><br><span class="hljs-string">@@ -133,3 +155,7 @@ echo &quot;</span>%files headers<span class="hljs-string">&quot;</span><br><span class="hljs-string">    echo &#x27;%defattr (-, root, root)&#x27;</span><br><span class="hljs-string">    echo &quot;</span>/usr/include<span class="hljs-string">&quot;</span><br><span class="hljs-string">    echo &quot;</span><span class="hljs-string">&quot;</span><br><span class="hljs-string">+echo &quot;</span>%files devel<span class="hljs-string">&quot;</span><br><span class="hljs-string">+echo &#x27;%defattr (-, root, root)&#x27;</span><br><span class="hljs-string">+echo &quot;</span>/usr/src/kernels/<span class="hljs-variable">$KERNELRELEASE</span><span class="hljs-string">&quot;</span><br><span class="hljs-string">+echo &quot;</span><span class="hljs-string">&quot;</span><br></code></pre></td></tr></table></figure><h4 id="加入kernel-firmware支持需要修改："><a href="#加入kernel-firmware支持需要修改：" class="headerlink" title="加入kernel-firmware支持需要修改："></a>加入kernel-firmware支持需要修改：</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><code class="hljs bash">1 files changed, 11 insertions(+), 1 deletions(-) <br><br>diff --git a/scripts/package/mkspec b/scripts/package/mkspec <br>index b20bdac..35e68d1 100755 <br>--- a/scripts/package/mkspec <br>+++ b/scripts/package/mkspec <br>@@ -68,6 +68,13 @@ <span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;between the Linux kernel and userspace libraries and programs. The&quot;</span> <br><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;header files define structures and constants that are needed for&quot;</span> <br><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;building most standard programs and are also needed for rebuilding the&quot;</span> <br><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;glibc package.&quot;</span> <br>+<span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;&quot;</span> <br>+<span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;%package firmware&quot;</span> <br>+<span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;Summary: Set of firmware images in the kernel tree&quot;</span> <br>+<span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;Group: Development/System&quot;</span> <br>+<span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;%description firmware&quot;</span> <br>+<span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;Firmware images in the kernel tree provided for backward compability&quot;</span> <br>+<span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;&quot;</span><br><br><span class="hljs-keyword">if</span> ! <span class="hljs-variable">$PREBUILT</span>; <span class="hljs-keyword">then</span> <br><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;%prep&quot;</span> <br>@@ -137,7 +144,6 @@ <span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;%files&quot;</span> <br><span class="hljs-built_in">echo</span> <span class="hljs-string">&#x27;%defattr (-, root, root)&#x27;</span> <br><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;%dir /lib/modules&quot;</span> <br><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;/lib/modules/<span class="hljs-variable">$KERNELRELEASE</span>&quot;</span> <br>-<span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;/lib/firmware&quot;</span> <br><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;/boot/*&quot;</span> <br><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;&quot;</span> <br><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;%files devel&quot;</span> <br>@@ -148,3 +154,7 @@ <span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;%files headers&quot;</span> <br><span class="hljs-built_in">echo</span> <span class="hljs-string">&#x27;%defattr (-, root, root)&#x27;</span> <br><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;/usr/include&quot;</span> <br><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;&quot;</span> <br>+<span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;%files firmware&quot;</span> <br>+<span class="hljs-built_in">echo</span> <span class="hljs-string">&#x27;%defattr (-, root, root)&#x27;</span> <br>+<span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;/lib/firmware&quot;</span> <br>+<span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;&quot;</span><br></code></pre></td></tr></table></figure><p>修改完上面的然后在根目录进行 make rpm 即可生成相应的内核rpm包<br>这个是修改完的内核mkspec文件，可以直接使用<br><a href="https://www.dropbox.com/s/nsvyf607t91k2mq/mkspec">https://www.dropbox.com/s/nsvyf607t91k2mq/mkspec</a><br>安装前需要安装依赖包</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">yum install -y gcc make  bison ncurses-devel rpm-build<br></code></pre></td></tr></table></figure><p>写于: 2013年09月18日<br>更新于: 2015年03月23日</p>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>快速做所有主机相互认证</title>
    <link href="/2015/03/23/%E5%BF%AB%E9%80%9F%E5%81%9A%E6%89%80%E6%9C%89%E4%B8%BB%E6%9C%BA%E7%9B%B8%E4%BA%92%E8%AE%A4%E8%AF%81/"/>
    <url>/2015/03/23/%E5%BF%AB%E9%80%9F%E5%81%9A%E6%89%80%E6%9C%89%E4%B8%BB%E6%9C%BA%E7%9B%B8%E4%BA%92%E8%AE%A4%E8%AF%81/</url>
    
    <content type="html"><![CDATA[<p>脚本:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-meta">#!/usr/bin/expect</span><br>spawn sensors-detect<br><span class="hljs-keyword">for</span> &#123;<span class="hljs-built_in">set</span> i 0&#125; &#123;<span class="hljs-variable">$i</span>&lt;=10&#125; &#123;incr i&#125; &#123;<br>expect <span class="hljs-string">&quot;:&quot;</span><br>send <span class="hljs-string">&quot;\n&quot;</span><br>&#125;<br>interact<br></code></pre></td></tr></table></figure><p>解释:<br>spawn是启动命令<br>for为循环的写法<br>interact为退出<br>这个脚本目的是在一次运行过程中不断模拟用户的enter操作</p><h2 id="设置免密码交互"><a href="#设置免密码交互" class="headerlink" title="设置免密码交互"></a>设置免密码交互</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-meta">#!/bin/bash</span><br><br><span class="hljs-keyword">if</span> [ ! -f <span class="hljs-string">&quot;/root/.ssh/id_rsa&quot;</span> ]; <span class="hljs-keyword">then</span><br><br>/usr/bin/expect &lt;&lt;-<span class="hljs-string">EOF</span><br><span class="hljs-string">spawn ssh-keygen</span><br><span class="hljs-string">expect &#123;  </span><br><span class="hljs-string">    &quot;*save the key (/root/.ssh/id_rsa&quot; &#123; send &quot;\n&quot;; exp_continue &#125;  </span><br><span class="hljs-string">    &quot;*(empty for no passphrase)&quot; &#123; send &quot;\n&quot;; exp_continue &#125;  </span><br><span class="hljs-string">    &quot;*same passphrase again&quot; &#123; send &quot;\n&quot;;&#125;  </span><br><span class="hljs-string">&#125;  </span><br><span class="hljs-string">interact  </span><br><span class="hljs-string">expect eof</span><br><span class="hljs-string">EOF</span><br><span class="hljs-keyword">fi</span><br><br><span class="hljs-keyword">for</span> host <span class="hljs-keyword">in</span> `<span class="hljs-built_in">cat</span> /etc/hosts|awk <span class="hljs-string">&#x27;&#123;print $1&#125;&#x27;</span>`<br><span class="hljs-keyword">do</span><br><br>passwd=<span class="hljs-string">&quot;ubuntu&quot;</span><br>/usr/bin/expect &lt;&lt;-<span class="hljs-string">EOF</span><br><span class="hljs-string">spawn ssh-copy-id root@$host</span><br><span class="hljs-string"></span><br><span class="hljs-string">expect &#123;  </span><br><span class="hljs-string">    &quot;*yes/no&quot; &#123; send &quot;yes\r&quot;; exp_continue &#125;  </span><br><span class="hljs-string">    &quot;*password:&quot; &#123; send &quot;$passwd\n&quot; &#125;</span><br><span class="hljs-string">    &quot; All keys were skipped because they already exist on the remote system.&quot; &#123;send &quot;\n&quot;&#125;</span><br><span class="hljs-string">&#125;  </span><br><span class="hljs-string">interact  </span><br><span class="hljs-string">expect eof</span><br><span class="hljs-string">EOF</span><br><br><span class="hljs-keyword">done</span><br><br></code></pre></td></tr></table></figure><p>把host文件传到所有机器<br>先从第一台主机做做配置跟其他机器的交互<br>然后seq把脚本传递到其他所有机器<br>然后用seq 循环ssh 远程执行脚本</p><h2 id="变更记录"><a href="#变更记录" class="headerlink" title="变更记录"></a>变更记录</h2><table><thead><tr><th align="center">Why</th><th align="center">Who</th><th align="center">When</th></tr></thead><tbody><tr><td align="center">创建</td><td align="center">武汉-运维-磨渣</td><td align="center">2013-8-12</td></tr><tr><td align="center">更新</td><td align="center">武汉-运维-磨渣</td><td align="center">2015-3-23</td></tr><tr><td align="center">更新所有主机交互</td><td align="center">武汉-运维-磨渣</td><td align="center">2019-3-29</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>ntpd配置时间同步服务器</title>
    <link href="/2015/03/22/ntpd%E9%85%8D%E7%BD%AE%E6%97%B6%E9%97%B4%E5%90%8C%E6%AD%A5%E6%9C%8D%E5%8A%A1%E5%99%A8/"/>
    <url>/2015/03/22/ntpd%E9%85%8D%E7%BD%AE%E6%97%B6%E9%97%B4%E5%90%8C%E6%AD%A5%E6%9C%8D%E5%8A%A1%E5%99%A8/</url>
    
    <content type="html"><![CDATA[<p>修改同步服务器的配置文件&#x2F;etc&#x2F;ntp.conf ,删除所有的内容,添加</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">restrict default nomodify<br>server  127.127.1.0     <span class="hljs-comment"># local clock</span><br>fudge   127.127.1.0 stratum 8<br></code></pre></td></tr></table></figure><p>重启ntpd</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">/etc/init.d/ntpd restart<br></code></pre></td></tr></table></figure><p>等待ntp服务器自身同步完成</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">watch ntpq -p<br></code></pre></td></tr></table></figure><p><img src="/images/blog/o_200901022756centosntp1.jpg"></p><p>观察这个reach需要超过17<br>否则其他机器ntpdate同步的时候会报错<br><img src="/images/blog/o_200901022801centosntp2.jpg"></p><h2 id="配置文件方式"><a href="#配置文件方式" class="headerlink" title="配置文件方式"></a>配置文件方式</h2><h3 id="服务器端配置"><a href="#服务器端配置" class="headerlink" title="服务器端配置"></a>服务器端配置</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">restrict default nomodify<br>server  127.127.1.0<br>fudge   127.127.1.0 stratum 8<br></code></pre></td></tr></table></figure><h3 id="客户端配置"><a href="#客户端配置" class="headerlink" title="客户端配置"></a>客户端配置</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">driftfile /var/lib/ntp/drift<br>server 192.168.8.102<br>includefile /etc/ntp/crypto/pw<br>keys /etc/ntp/keys<br></code></pre></td></tr></table></figure><p>&#x2F;etc&#x2F;sysconfig&#x2F;ntpd</p><p>添加</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">SYNC_HWCLOCK=<span class="hljs-built_in">yes</span><br></code></pre></td></tr></table></figure><h2 id="多台ntp的情况"><a href="#多台ntp的情况" class="headerlink" title="多台ntp的情况"></a>多台ntp的情况</h2><h3 id="客户端配置-1"><a href="#客户端配置-1" class="headerlink" title="客户端配置"></a>客户端配置</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">driftfile /var/lib/ntp/drift<br>server 192.168.8.102 prefer<br>server 192.168.8.101<br>includefile /etc/ntp/crypto/pw<br>keys /etc/ntp/keys<br></code></pre></td></tr></table></figure><h3 id="主ntp配置（可以改成上级ntp）"><a href="#主ntp配置（可以改成上级ntp）" class="headerlink" title="主ntp配置（可以改成上级ntp）"></a>主ntp配置（可以改成上级ntp）</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">restrict default nomodify<br>server  127.127.1.0<br>fudge   127.127.1.0<br>stratum 5<br></code></pre></td></tr></table></figure><h3 id="备份ntp配置"><a href="#备份ntp配置" class="headerlink" title="备份ntp配置"></a>备份ntp配置</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">driftfile /var/lib/ntp/drift<br>server 192.168.8.102<br>includefile /etc/ntp/crypto/pw<br>keys /etc/ntp/keys<br>stratum 6<br></code></pre></td></tr></table></figure><h2 id="更新历史"><a href="#更新历史" class="headerlink" title="更新历史"></a>更新历史</h2><table><thead><tr><th>why</th><th>when</th></tr></thead><tbody><tr><td>创建</td><td>2013年08月06日</td></tr><tr><td>更新</td><td>2019年12月9日</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>ubuntu12.10安装sun java jdk</title>
    <link href="/2015/03/22/ubuntu12.10%E5%AE%89%E8%A3%85sun%20java%20jdk/"/>
    <url>/2015/03/22/ubuntu12.10%E5%AE%89%E8%A3%85sun%20java%20jdk/</url>
    
    <content type="html"><![CDATA[<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">add-apt-repository ppa:webupd8team/java  <br>apt-get update  <br>apt-get install oracle-java6-installer   <br></code></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">root@kangear:/etc/apt<span class="hljs-comment"># java -version</span><br>java version <span class="hljs-string">&quot;1.6.0_41&quot;</span><br>Java(TM) SE Runtime Environment (build 1.6.0_41-b02)<br>Java HotSpot(TM) Server VM (build 20.14-b01, mixed mode)<br></code></pre></td></tr></table></figure><p>说明:sun-java6-jre,sun-java6-plugin,sun-java6-fonts都同样的由的软件包 oracle-java6-installer 提供的虚拟软件包所以安装 oracle-java6-installer就可以了</p><h2 id="更新历史"><a href="#更新历史" class="headerlink" title="更新历史"></a>更新历史</h2><table><thead><tr><th>why</th><th>when</th></tr></thead><tbody><tr><td>创建</td><td>2013年08月03日</td></tr><tr><td>更新</td><td>2019年12月9日</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Bad magic number ImportError in python</title>
    <link href="/2015/03/22/Bad%20magic%20number%20ImportError%20in%20python/"/>
    <url>/2015/03/22/Bad%20magic%20number%20ImportError%20in%20python/</url>
    
    <content type="html"><![CDATA[<p>是源码编译里面版本不对，删除掉源码pyc然后重新编译就可以了</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">find .-name <span class="hljs-string">&#x27;*.pyc&#x27;</span>-delete<br>python -m compileall .<br></code></pre></td></tr></table></figure><h2 id="更新历史"><a href="#更新历史" class="headerlink" title="更新历史"></a>更新历史</h2><table><thead><tr><th>why</th><th>when</th></tr></thead><tbody><tr><td>创建</td><td>2013年08月01日</td></tr><tr><td>更新</td><td>2019年12月9日</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>centos下多网卡做bond脚本</title>
    <link href="/2015/03/22/centos%E4%B8%8B%E5%A4%9A%E7%BD%91%E5%8D%A1%E5%81%9Abond%E8%84%9A%E6%9C%AC/"/>
    <url>/2015/03/22/centos%E4%B8%8B%E5%A4%9A%E7%BD%91%E5%8D%A1%E5%81%9Abond%E8%84%9A%E6%9C%AC/</url>
    
    <content type="html"><![CDATA[<p>多网卡或者单网卡形式下的网卡bonding</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-meta">#! /bin/sh</span><br><span class="hljs-comment">#获取当前网卡数</span><br>ethnum=`lspci | grep Ethernet | <span class="hljs-built_in">wc</span> -l`<br><span class="hljs-built_in">echo</span> <span class="hljs-variable">$ethnum</span><br><span class="hljs-comment">#如果网卡数小于等于1则什么都不做</span><br><span class="hljs-keyword">if</span> [ <span class="hljs-variable">$ethnum</span> -le 1 ]<br>        <span class="hljs-keyword">then</span><br>        <span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;do nothing!&quot;</span><br><span class="hljs-keyword">fi</span><br><span class="hljs-comment">#如果网卡数大于等于2则</span><br><span class="hljs-keyword">if</span> [ <span class="hljs-variable">$ethnum</span> -ge 2 ]<br>        <span class="hljs-keyword">then</span><br><br>    <br><span class="hljs-comment">#-------  添加一个bond0的网卡</span><br>    <span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;DEVICE=bond0&quot;</span> &gt; /etc/sysconfig/network-scripts/ifcfg-bond0<br>    <span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;ONBOOT=yes&quot;</span> &gt;&gt; /etc/sysconfig/network-scripts/ifcfg-bond0<br>    <span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;IPADDR=192.168.3.104&quot;</span> &gt;&gt; /etc/sysconfig/network-scripts/ifcfg-bond0<br>    <span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;NETMASK=255.255.0.0&quot;</span> &gt;&gt; /etc/sysconfig/network-scripts/ifcfg-bond0<br>    <span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;GATEWAY=192.168.1.1&quot;</span> &gt;&gt; /etc/sysconfig/network-scripts/ifcfg-bond0<br>    <span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;BOOTPROTO=static&quot;</span> &gt;&gt; /etc/sysconfig/network-scripts/ifcfg-bond0<br>    <span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;USERCTL=no&quot;</span> &gt;&gt; /etc/sysconfig/network-scripts/ifcfg-bond0<br><span class="hljs-comment">#--------</span><br>    <span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;ifenslave bond0&quot;</span> &gt;&gt; /etc/rc.local<br><span class="hljs-comment">#--------写其他网卡的配置文件</span><br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> $(<span class="hljs-built_in">seq</span> <span class="hljs-variable">$ethnum</span>);<br>    <span class="hljs-keyword">do</span><br>num=`<span class="hljs-built_in">expr</span> <span class="hljs-variable">$i</span> - 1`<br>        <span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;BOOTPROTO=none&quot;</span> &gt;  /etc/sysconfig/network-scripts/ifcfg-eth<span class="hljs-variable">$num</span><br>        <span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;DEVICE=eth<span class="hljs-variable">$num</span>&quot;</span> &gt;&gt;  /etc/sysconfig/network-scripts/ifcfg-eth<span class="hljs-variable">$num</span><br>        <span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;ONBOOT=yes&quot;</span> &gt;&gt;  /etc/sysconfig/network-scripts/ifcfg-eth<span class="hljs-variable">$num</span><br>        <span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;MASTER=bond0&quot;</span> &gt;&gt;  /etc/sysconfig/network-scripts/ifcfg-eth<span class="hljs-variable">$num</span><br>        <span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;USERCTL=no&quot;</span> &gt;&gt;  /etc/sysconfig/network-scripts/ifcfg-eth<span class="hljs-variable">$num</span><br>        <span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;SLAVE=yes&quot;</span> &gt;&gt;  /etc/sysconfig/network-scripts/ifcfg-eth<span class="hljs-variable">$num</span><br>        sed -i <span class="hljs-string">&#x27;s/ifenslave.*/&amp; eth&#x27;</span><span class="hljs-string">&quot;<span class="hljs-variable">$num</span>&quot;</span><span class="hljs-string">&#x27;/g&#x27;</span> /etc/rc.local<br>    <span class="hljs-keyword">done</span>;<br><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;alias bond0 bonding&quot;</span> &gt; /etc/modprobe.d/modprobe.conf<br><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;options bond0 miimon=100 mode=balance-rr&quot;</span> &gt;&gt; /etc/modprobe.d/modprobe.conf<br>        modprobe bonding<br>    /etc/init.d/network   restart<br><span class="hljs-keyword">fi</span><br></code></pre></td></tr></table></figure><p>直接配置方法</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@lab101 ~]<span class="hljs-comment"># cat /etc/sysconfig/network-scripts/ifcfg-bond0</span><br>DEVICE=bond0<br>NAME=bond0<br>TYPE=Bond<br>BONDING_MASTER=<span class="hljs-built_in">yes</span><br>IPADDR=192.168.0.101<br>PREFIX=24<br>GATEWAY=192.168.0.1<br>ONBOOT=<span class="hljs-built_in">yes</span><br>BOOTPROTO=none<br>BONDING_OPTS=<span class="hljs-string">&quot;mode=1 miimon=100&quot;</span><br>NM_CONTROLLED=<span class="hljs-string">&quot;no&quot;</span><br><br><br>[root@lab101 ~]<span class="hljs-comment"># cat /etc/sysconfig/network-scripts/ifcfg-ens33</span><br>DEVICE=ens33<br>NAME=bond0-slave<br>TYPE=Ethernet<br>BOOTPROTO=none<br>ONBOOT=<span class="hljs-built_in">yes</span><br>MASTER=bond0<br>SLAVE=<span class="hljs-built_in">yes</span><br>NM_CONTROLLED=<span class="hljs-string">&quot;no&quot;</span><br><br><br>[root@lab101 ~]<span class="hljs-comment"># cat /etc/sysconfig/network-scripts/ifcfg-ens37</span><br>DEVICE=ens37<br>NAME=bond0-slave<br>TYPE=Ethernet<br>BOOTPROTO=none<br>ONBOOT=<span class="hljs-built_in">yes</span><br>MASTER=bond0<br>SLAVE=<span class="hljs-built_in">yes</span><br>NM_CONTROLLED=<span class="hljs-string">&quot;no&quot;</span><br></code></pre></td></tr></table></figure><p>还原</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs bash">DEVICE=ens33<br>NAME=ens33<br>TYPE=Ethernet<br>BOOTPROTO=static<br>ONBOOT=<span class="hljs-built_in">yes</span><br>NM_CONTROLLED=<span class="hljs-string">&quot;no&quot;</span><br>IPADDR=192.168.0.101<br>PREFIX=24<br>GATEWAY=192.168.0.1<br>ONBOOT=<span class="hljs-built_in">yes</span><br></code></pre></td></tr></table></figure><h2 id="更新历史"><a href="#更新历史" class="headerlink" title="更新历史"></a>更新历史</h2><table><thead><tr><th>why</th><th>when</th></tr></thead><tbody><tr><td>创建</td><td>2013年08月06日</td></tr><tr><td>更新</td><td>2019年12月9日</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>修改centos6启动动画（plymouth方式）</title>
    <link href="/2015/03/22/%E4%BF%AE%E6%94%B9centos6%E5%90%AF%E5%8A%A8%E5%8A%A8%E7%94%BB%EF%BC%88plymouth%E6%96%B9%E5%BC%8F%EF%BC%89/"/>
    <url>/2015/03/22/%E4%BF%AE%E6%94%B9centos6%E5%90%AF%E5%8A%A8%E5%8A%A8%E7%94%BB%EF%BC%88plymouth%E6%96%B9%E5%BC%8F%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<p>centos6默认的启动动画是一个白蓝色的进度条,背景全黑色,现在需要对centos的启动动画进行定制<br>在查询了一些资料以后,发现有一个软件是可以对启动动画进行定制的，名字叫plymouth<br>这个在centos6操作系统安装完成以后默认就带了的,主要涉及的是如下的几个操作</p><p>列出当前所有主题:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">plymouth-set-default-theme --list  <br></code></pre></td></tr></table></figure><p>主题存放目录:<br>&#x2F;usr&#x2F;share&#x2F;plymouth&#x2F;themes&#x2F;</p><p>不重启查看主题运行效果 Ctrl + Alt + F2(一定不要在F1操作)</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">plymouthd<br>plymouth --show-splash<br>plymouth --quit<br></code></pre></td></tr></table></figure><p>如果想调试:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">plymouthd --debug --debug-file=/tmp/plydebug<br>plymouth --show-splash<br>plymouth --quit<br></code></pre></td></tr></table></figure><p>如果在虚拟机下进行测试<br>需要修改grub里面的vga参数为785</p><p><img src="/images/blog/o_200901022332centos%E5%8A%A8%E7%94%BB.jpg"></p><p>其他环境无需设置<br>测试通过以后,进行设置:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">plymouth-set-default-theme details        (后面的detail为设置的主题)<br>plymouth-set-default-theme -R details     (本操作会将启动画面加入到内核当中,时间比较久,可以ps -ef 查看后台操作)<br></code></pre></td></tr></table></figure><p>重启电脑就可以看到效果了<br>下面是在github上找到的一个源码包,可以使用</p><p>链接:<a href="http://pan.baidu.com/s/1kTMMo55">http://pan.baidu.com/s/1kTMMo55</a> 密码:7doc</p><p>centos下使用自编译主题script形式需要安装依赖包</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">plymouth-graphics-libs-0.8.3-27.el6.centos.x86_64.rpm<br>plymouth-plugin-script-0.8.3-27.el6.centos.x86_64.rpm<br></code></pre></td></tr></table></figure><p>安装完成后,将wheat解压后目录放&#x2F;usr&#x2F;share&#x2F;plymouth&#x2F;themes&#x2F;下到就可以设置这个主题了</p><p>写于: 2013年07月30日<br>更新于: 2015年03月22日</p>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>samba配置用户访问方法</title>
    <link href="/2015/03/22/samba%E9%85%8D%E7%BD%AE%E7%94%A8%E6%88%B7%E8%AE%BF%E9%97%AE%E6%96%B9%E6%B3%95/"/>
    <url>/2015/03/22/samba%E9%85%8D%E7%BD%AE%E7%94%A8%E6%88%B7%E8%AE%BF%E9%97%AE%E6%96%B9%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<h3 id="配置目的"><a href="#配置目的" class="headerlink" title="配置目的:"></a>配置目的:</h3><p>为了给指定用户一个独立访问目录</p><p>首先在samba服务器安装samba软件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ apt-get install samba<br></code></pre></td></tr></table></figure><p>然后配置独立访问用户<br>配置samba用户前提需要是linux的用户,所以要创建linux用户</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ useradd    zp<br>$ passwd     zp  <br></code></pre></td></tr></table></figure><p>用户创建完了以后<br>需要创建samba用户</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ smbpasswd -a  zp<br></code></pre></td></tr></table></figure><p>然后输入密码,记住密码,这个是客户端访问samba的密码</p><p>在服务器的&#x2F;etc&#x2F;samba&#x2F;smb.conf文件的</p><p>[global]字段下面添加:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">client lanman auth = <span class="hljs-built_in">yes</span><br>security = user<br></code></pre></td></tr></table></figure><p>在文件末尾添加:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">[hj]<br>    path=/hj<br>    comment=zhangpengdemulu<br>    writeable=<span class="hljs-built_in">yes</span><br>    valid <span class="hljs-built_in">users</span> = zp<br></code></pre></td></tr></table></figure><p>修改配置文件以后,重启samba服务<br>注意 &#x2F;hj 目录需要有读写执行权限,在samba服务器执行:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ <span class="hljs-built_in">chmod</span> 777 /hj<br></code></pre></td></tr></table></figure><h4 id="客户端访问方法"><a href="#客户端访问方法" class="headerlink" title="客户端访问方法"></a>客户端访问方法</h4><p>linux下两种方式</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ smbmount 192.168.0.194:/hj /zp/test -o username=zp<br>$ mount -t cifs //192.168.0.194/hj /zp/test -o username=zp<br></code></pre></td></tr></table></figure><p>windows直接挂载</p><p>###附加命令:</p><p>查看samba的用户</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">sudo pdbedit -L<br></code></pre></td></tr></table></figure><p>增加一个用户到samba</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">sudo pdbedit -a username<br></code></pre></td></tr></table></figure><p>从samba账户中删除一个用户</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">sudo pdbedit -x username<br></code></pre></td></tr></table></figure><p>显示samba账户信息</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">sudo pdbedit -r username<br></code></pre></td></tr></table></figure><p>测试samba账户是否正常</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">smbclient -L 192.168.1.1 -U username -d 3<br></code></pre></td></tr></table></figure><blockquote><p>valid users：<br><br> @zp 为zp组<br><br>zp 为zp用户</p></blockquote><h2 id="配置匿名访问"><a href="#配置匿名访问" class="headerlink" title="配置匿名访问"></a>配置匿名访问</h2><p>访问&#x2F;root目录的共享需要配置force user &#x3D; root，否则权限不对</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs bash">[global]<br>        workgroup = SAMBA<br>        security = user<br><br>        passdb backend = tdbsam<br>    <br>        security = user<br>        map to guest = Bad User<br>    <br>        printing = cups<br>        printcap name = cups<br>        load printers = <span class="hljs-built_in">yes</span><br>        cups options = raw<br><br>[backup]<br>        comment = testshare<br>        path = /backup<br>        browseable = Yes<br>        guest ok=<span class="hljs-built_in">yes</span><br>        writable=<span class="hljs-built_in">yes</span><br>        <span class="hljs-built_in">read</span> only = No<br>        force user = root<br></code></pre></td></tr></table></figure><h2 id="更新历史"><a href="#更新历史" class="headerlink" title="更新历史"></a>更新历史</h2><table><thead><tr><th>why</th><th>when</th></tr></thead><tbody><tr><td>创建</td><td>2013年04月17日</td></tr><tr><td>更新</td><td>2019年12月9日</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Ubuntu12.10 设置默认命令行启动</title>
    <link href="/2015/03/22/Ubuntu12.10%20%E8%AE%BE%E7%BD%AE%E9%BB%98%E8%AE%A4%E5%91%BD%E4%BB%A4%E8%A1%8C%E5%90%AF%E5%8A%A8/"/>
    <url>/2015/03/22/Ubuntu12.10%20%E8%AE%BE%E7%BD%AE%E9%BB%98%E8%AE%A4%E5%91%BD%E4%BB%A4%E8%A1%8C%E5%90%AF%E5%8A%A8/</url>
    
    <content type="html"><![CDATA[<p>在虚拟机当中安装ubuntu12.10的时候默认把图形界面给装上了,由于不需要使用桌面,所以为了省去每次进入到图形界面然后再用ctrl+F1的方式切换到命令行的步骤,希望能够默认进入的是命令行模式,那么设置的方法如下:</p><p>终端执行:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">vim /etc/default/grub<br></code></pre></td></tr></table></figure><p>将 GRUB_CMDLINE_LINUX_DEFAULT&#x3D;”quiet splash” 改为:<br>GRUB_CMDLINE_LINUX_DEFAULT&#x3D;”quiet splash text”</p><p>然后执行: </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">update-grub<br></code></pre></td></tr></table></figure><p>重启ubuntu,这时默认就从命令行模式启动了。</p><p><img src="/images/blog/o_200901022058ubuntu-shell.jpg"></p><h2 id="更新历史"><a href="#更新历史" class="headerlink" title="更新历史"></a>更新历史</h2><table><thead><tr><th>why</th><th>when</th></tr></thead><tbody><tr><td>创建</td><td>2013年03月09日</td></tr><tr><td>更新</td><td>2019年12月9日</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Ubuntu 12.10设置root用户登录图形界面</title>
    <link href="/2015/03/22/Ubuntu%2012.10%E8%AE%BE%E7%BD%AEroot%E7%94%A8%E6%88%B7%E7%99%BB%E5%BD%95%E5%9B%BE%E5%BD%A2%E7%95%8C%E9%9D%A2/"/>
    <url>/2015/03/22/Ubuntu%2012.10%E8%AE%BE%E7%BD%AEroot%E7%94%A8%E6%88%B7%E7%99%BB%E5%BD%95%E5%9B%BE%E5%BD%A2%E7%95%8C%E9%9D%A2/</url>
    
    <content type="html"><![CDATA[<p>Ubuntu 12.04默认是不允许root登录的,在登录窗口只能看到普通用户和访客登录。以普通身份登陆Ubuntu后我们需要做一些修改,普通用户登录后,修改系统配置文件需要切换到超级用户模式,在终端窗口里面输入: </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">sudo  -s<br></code></pre></td></tr></table></figure><p>然后输入普通用户登陆的密码,回车即可进入 root用户权限模式。<br>启用root用户的前提是需要先给root设置一个密码:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">sudo passwd root <br></code></pre></td></tr></table></figure><p>输入root 密码即可</p><p>然后执行: </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">vi /etc/lightdm/lightdm.conf.<br></code></pre></td></tr></table></figure><p>增加 </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">greeter-show-manual-login=<span class="hljs-literal">true</span>  <br>allow-guest=<span class="hljs-literal">false</span>  <br></code></pre></td></tr></table></figure><p>修改完的整个配置文件是</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">[SeatDefaults]<br>greeter-session=unity-greeter<br>user-session=ubuntu<br>greeter-show-manual-login=<span class="hljs-literal">true</span> <span class="hljs-comment">#手工输入登陆系统的用户名和密码</span><br>allow-guest=<span class="hljs-literal">false</span>   <span class="hljs-comment">#不允许guest登录</span><br></code></pre></td></tr></table></figure><p>然后我们启动root帐号:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">sudo passwd root<br></code></pre></td></tr></table></figure><p>根据提示输入root帐号密码。</p><p>重启ubuntu,登录窗口会有“登录”选项,这时候我们就可以通过root登录了。</p><p>注意:如果root登陆后还没声音,又查了查,如下方法:<br>Ubuntu root登录没有声音这个问题的根本原因是使用root登录后pulseaudio没有启动。<br>将root加到pulse-access组:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">sudo usermod -a -G<br></code></pre></td></tr></table></figure><p>后修改配置文件&#x2F;etc&#x2F;default&#x2F;pulseaudio,将PULSEAUDIO_SYSTEM_START设为1</p><h2 id="Ubuntu-14-04-图形界面-root登陆"><a href="#Ubuntu-14-04-图形界面-root登陆" class="headerlink" title="Ubuntu 14.04 图形界面 root登陆"></a>Ubuntu 14.04 图形界面 root登陆</h2><p>Ubuntu 14.04 LTS root GUI login | Disable guest | Hide user from login screen </p><p>Go to &#x2F;usr&#x2F;share&#x2F;lightdm&#x2F;50-ubuntu.conf </p><p>and Write the following lines: </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">allow-guest=<span class="hljs-literal">false</span> -----------------------------------------<span class="hljs-keyword">for</span> disabling GUEST SESSION <br>greeter-show-manual-login=<span class="hljs-literal">true</span> --------------------------root GUI and other login <br>greeter-hide-users=<span class="hljs-literal">true</span> ---------------------------hiding all the <span class="hljs-built_in">users</span> from login screen<br></code></pre></td></tr></table></figure><h2 id="更新历史"><a href="#更新历史" class="headerlink" title="更新历史"></a>更新历史</h2><table><thead><tr><th>why</th><th>when</th></tr></thead><tbody><tr><td>创建</td><td>2012年12月14日</td></tr><tr><td>更新</td><td>2019年12月9日</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Vmware Tools is currently being installed on your system</title>
    <link href="/2015/03/22/Vmware%20Tools%20is%20currently%20being%20installed%20on%20your%20system/"/>
    <url>/2015/03/22/Vmware%20Tools%20is%20currently%20being%20installed%20on%20your%20system/</url>
    
    <content type="html"><![CDATA[<p>问题描述:<br>使用虚拟机安装Ubuntu过程中一直停留在“PLEASE WAIT! Vmware Tools is currently<br>being installed on your system”如下图:</p><p><img src="/images/blog/o_200901021748vmware1.jpg"></p><p>软件环境<br>VMWare 8<br>Ubuntu 12.10</p><p>解决方案如下</p><p>做下面三步操作后重启系统 :</p><p>Restore the &#x2F;etc&#x2F;issue file:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">sudo <span class="hljs-built_in">mv</span> /etc/issue.backup /etc/issue<br></code></pre></td></tr></table></figure><p>Restore the &#x2F;etc&#x2F;rc.local file:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">sudo <span class="hljs-built_in">mv</span> /etc/rc.local.backup /etc/rc.local<br></code></pre></td></tr></table></figure><p>Restore the &#x2F;etc&#x2F;init&#x2F;lightdm.conf file:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">sudo <span class="hljs-built_in">mv</span> /opt/vmware-tools-installer/lightdm.conf /etc/init<br></code></pre></td></tr></table></figure><p>Then reboot.</p><h2 id="更新历史"><a href="#更新历史" class="headerlink" title="更新历史"></a>更新历史</h2><table><thead><tr><th>why</th><th>when</th></tr></thead><tbody><tr><td>创建</td><td>2012年12月14日</td></tr><tr><td>更新</td><td>2019年12月9日</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>检查linux下服务器的带宽</title>
    <link href="/2015/03/22/%E6%A3%80%E6%9F%A5linux%E4%B8%8B%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%9A%84%E5%B8%A6%E5%AE%BD/"/>
    <url>/2015/03/22/%E6%A3%80%E6%9F%A5linux%E4%B8%8B%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%9A%84%E5%B8%A6%E5%AE%BD/</url>
    
    <content type="html"><![CDATA[<p>设想:公司 A 有一个名为 bsdocfs 的存储服务器,并通过名为 beckham 的客户端节点装载 NFS。公司 A 确定他们需要从 bsdocfs得到更多的带宽,因为有大量的节点需要访问 bsdocfs 的共享文件系统。<br>实现此操作的最常用和最便宜的方式是将两个千兆网卡组合在一起。这是最便宜的,因为您通常会有一个额外的可用 NIC 和一个额外的端口。<br>所以采取此这个方法。不过现在的问题是?到底需要多少带宽?</p><p>千兆网卡以太网理论上的限制是 128MBit&#x2F;s。这个数字从何而来,看看这些计算:<br>1Gb &#x3D; 1024Mb;1024Mb&#x2F;8 &#x3D; 128MB;”b” &#x3D; “bits,”、”B” &#x3D; “bytes”</p><p>但实际看到的是什么呢,有什么好的测量方法呢,我推荐一个工具 iperf。可以按照以下方法获得 iperf</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ wget http://dast.nlanr.net/Projects/Iperf2.0/iperf-2.0.2.tar.gz<br></code></pre></td></tr></table></figure><p>或者:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ apt-get install iperf<br></code></pre></td></tr></table></figure><p>需要在 bsdocfs 和 beckham 均可见的共享文件系统上安装此工具,或者在两个节点上编译并安装。<br>在 bsdocfs 上,运行:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ iperf -s -f M<br></code></pre></td></tr></table></figure><p>这台机器将用作服务器并以 MBit&#x2F;s 为单位输出执行速度。<br>在 beckham 节点上,运行:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ iperf -c bsdocfs -P 4 -f M -w 256k -t 60<br></code></pre></td></tr></table></figure><p>两个屏幕上的结果都指示了速度是多少。在使用千兆网卡的普通服务器上,可能会看到速度约为 112MBit&#x2F;s。这是 TCP 堆栈和物理电缆中的常用带宽。通过以端到端的方式连接两台服务器,每台服务器使用两个联结的以太网卡,我获得了约 220MBit&#x2F;s 的带宽。<br>事实上,在联结的网络上看到的 NFS 约为 150-160MBit&#x2F;s。这仍然表示带宽可以达到预期效果。如果看到更小的值,则应该检查是否有问题。</p><p>我最近碰到一种情况,即通过连接驱动程序连接两个使用了不同驱动程序的 NIC。这导致性能非常低,带宽约为 20MBit&#x2F;s,比不连接以太网卡时的带宽还小,做bond聚合一定要使用同型号网卡。</p><p>写于: 2013年08月07日<br>更新于: 2015年03月22日</p>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>linux利用screen进行shell下的屏幕协作</title>
    <link href="/2015/03/22/linux%E5%88%A9%E7%94%A8screen%E8%BF%9B%E8%A1%8Cshell%E4%B8%8B%E7%9A%84%E5%B1%8F%E5%B9%95%E5%8D%8F%E4%BD%9C/"/>
    <url>/2015/03/22/linux%E5%88%A9%E7%94%A8screen%E8%BF%9B%E8%A1%8Cshell%E4%B8%8B%E7%9A%84%E5%B1%8F%E5%B9%95%E5%8D%8F%E4%BD%9C/</url>
    
    <content type="html"><![CDATA[<p>我们都知道linux是支持多终端并行处理的<br>但是某些时候我们可能有比较特殊的需求需要两个人同时处理一个终端,screen 正好能满足这个要求</p><p>首先需要安装screen软件:</p><p>debian和ubuntu下面都是通过:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">apt-get install screen<br></code></pre></td></tr></table></figure><p>进行安装的</p><p>假如现在有a和b需要进行屏幕协作,那么操作步骤如下:<br>a在终端运行:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">screen -S   zp        <br></code></pre></td></tr></table></figure><p>b然后在终端运行:   </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">screen -x  zp  <br></code></pre></td></tr></table></figure><p>这个时候两边的用户的操作是实时同步的。</p><p>##执行命令但不进入</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">screen -dmS top top<br></code></pre></td></tr></table></figure><p>执行了top的命令，终端名称命名为top，但不进入</p><h2 id="更新历史"><a href="#更新历史" class="headerlink" title="更新历史"></a>更新历史</h2><table><thead><tr><th>why</th><th>when</th></tr></thead><tbody><tr><td>创建</td><td>2012年08月07日</td></tr><tr><td>更新</td><td>2019年12月9日</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>使用Ganglia监控系统监控集群(debian)</title>
    <link href="/2015/03/22/%E4%BD%BF%E7%94%A8Ganglia%E7%9B%91%E6%8E%A7%E7%B3%BB%E7%BB%9F%E7%9B%91%E6%8E%A7%E9%9B%86%E7%BE%A4(debian)/"/>
    <url>/2015/03/22/%E4%BD%BF%E7%94%A8Ganglia%E7%9B%91%E6%8E%A7%E7%B3%BB%E7%BB%9F%E7%9B%91%E6%8E%A7%E9%9B%86%E7%BE%A4(debian)/</url>
    
    <content type="html"><![CDATA[<p>ganglia是一个集群监控软件,底层使用RRDTool获得数据。<br>Ganglia分为ganglia-monitor和gmetad两部分,前者运行在集群每个节点上(被监控机器)收集RRDTool产生的数据,后者运行在监控服务器上,收集每个ganglia-monitor的数据,通过Web UI可以看到直观的各种图表。</p><p>在debian上安装Ganglia非常简单,首先安装下面三个包。因为要使用Web服务器才能看到图表,所以如果没有安装apache的话,会自动安装apache服务器。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs bash">apt-get installganglia-monitor  <span class="hljs-comment">#这个包安装在被监控机器即可</span><br>apt-get install ganglia-webfront gmetad  <span class="hljs-comment">#这两个包安装在监控服务器，前者提供web页面，后者收集其他机器的监控数据</span><br>```   <br>     <br>被监控安装完成之后，gmond服务会运行起来，使用下面的命令可以启动这个服务。<br>```raw<br>/etc/init.d/ganglia-monitor restart     <br></code></pre></td></tr></table></figure><p>(配置文件的路径在&#x2F;etc&#x2F;ganglia&#x2F;gmond.conf)基本不用配置就可以启用</p><p>配置监控服务器(展示页面的服务器)</p><p>因为ganglia-webfront这个包默认将Web相关的代码安装在”&#x2F;usr&#x2F;share&#x2F;ganglia-webfrontend&#x2F;”路径下,这样apache访问不到。可以使用软链接或者直接将目录移到”&#x2F;var&#x2F;www&#x2F;”目录下。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">ln -s /usr/share/ganglia-webfrontend//var/www/ganglia<br></code></pre></td></tr></table></figure><p>接着在浏览器输入”<a href="http://localhost/ganglia%E2%80%9D%E5%B0%B1%E5%8F%AF%E4%BB%A5%E7%9C%8B%E5%88%B0Web">http://localhost/ganglia”就可以看到Web</a> UI了</p><p>需要监控更多机器的时候，只需要在机器上安装ganglia-monitor 启动服务即可</p><p>补充说明:<br>一、主页的显示的时间为格林威治时间,所以看起来很奇怪,想要显示为本地时间,需要修改提供网页机器的php.ini文件<br>路径为:&#x2F;etc&#x2F;php5&#x2F;apache2&#x2F;php.ini </p><p>将[Date]下的设置为date.timezone &#x3D;Asia&#x2F;Shanghai</p><p>重启apache2 </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">/etc/init.d/apache2 restart<br></code></pre></td></tr></table></figure><p>再看主页,时间变成了当前机器时间</p><p>二、python监控插件的扩展<br>为了确保Ganglia的安装有了Python支持的设置,检查一下以下设置<br>gmond.conf 有一行  include (“&#x2F;etc&#x2F;ganglia&#x2F;conf.d&#x2F;*.conf”) ,这是你应该放置.conf格式的python模块配置文件所在的目录。<br>modpython.conf 这个文件应该存在于&#x2F;etc&#x2F;ganglia&#x2F;conf.d 中,它包含了pyconf文件的所在位置<br>modpython.so应该在&#x2F;usr&#x2F;lib&#x2F;ganglia中<br>&#x2F;usr&#x2F;lib&#x2F;ganglia&#x2F;python_modules这个目录应该存在,这是你放置以.py结尾的python模块文件所在的目录</p><p>写于: 2012年08月02日<br>更新于: 2015年03月22日</p>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Linux下如何创建loop device</title>
    <link href="/2015/03/22/Linux%E4%B8%8B%E5%A6%82%E4%BD%95%E5%88%9B%E5%BB%BAloop%20device/"/>
    <url>/2015/03/22/Linux%E4%B8%8B%E5%A6%82%E4%BD%95%E5%88%9B%E5%BB%BAloop%20device/</url>
    
    <content type="html"><![CDATA[<p>在Linux中,有一种特殊的块设备叫loop device,这种loop device设备是通过映射操作系统上的正常的文件而形成的虚拟块设备</p><p>因为这种设备的存在,就为我们提供了一种创建一个存在于其他文件中的虚拟文件系统的机制.下面是一个示例:<br>第一步:用dd创建一个大文件:</p><!--break--><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">dd</span> <span class="hljs-keyword">if</span>=/dev/zero of=node1 bs=4M count=500<br></code></pre></td></tr></table></figure><p>这样就在当前目录下创建了一个2G的文件”node1”</p><p>第二步:使用losetup命令创建一个loop device</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">losetup /dev/loop0 node1<br></code></pre></td></tr></table></figure><span id="more"></span><p>第三步:创建一个文件系统</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">mkfs -t ext3 /dev/loop0<br></code></pre></td></tr></table></figure><p>第四步:挂载这个文件系统</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">mount /dev/loop0 /mnt/<br></code></pre></td></tr></table></figure><p>最后:如果要删除刚才创建的这些对象:依次执行如下步骤:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ umount /dev/loop0<br>$ losetup -d /dev/loop0<br>$ <span class="hljs-built_in">rm</span> node1<br></code></pre></td></tr></table></figure><p>说明:<br>详细的losetup命令如下:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@vm11g ~]<span class="hljs-comment"># losetup</span><br>usage:<br>losetup loop_device <span class="hljs-comment"># give info</span><br>losetup -d loop_device <span class="hljs-comment"># delete</span><br>losetup [ -e encryption ] [ -o offset ] loop_device file <span class="hljs-comment"># setup</span><br></code></pre></td></tr></table></figure><p>其中加密选项有如下几种方式:<br>NONE use no encryption (default).<br>XOR use a simple XOR encryption.<br>DES use DES encryption. </p><p>DES encryption is only available if the optional DES package has been added to the kernel.<br>DES encryption uses an additional start value that is used to protect passwords against dictionary attacks.</p><p>默认情况下系统支持的loop device是8个</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@vm11g dev]<span class="hljs-comment"># ls -ltr /dev/loop*</span><br>brw-r----- 1 root disk 7, 0 Jul 19 2009 /dev/loop0<br>brw-r----- 1 root disk 7, 1 Jul 19 2009 /dev/loop1<br>brw-r----- 1 root disk 7, 2 Jul 19 2009 /dev/loop2<br>brw-r----- 1 root disk 7, 3 Jul 19 2009 /dev/loop3<br>brw-r----- 1 root disk 7, 4 Jul 19 2009 /dev/loop4<br>brw-r----- 1 root disk 7, 5 Jul 19 2009 /dev/loop5<br>brw-r----- 1 root disk 7, 6 Jul 19 2009 /dev/loop6<br>brw-r----- 1 root disk 7, 7 Jul 19 2009 /dev/loop7<br></code></pre></td></tr></table></figure><p>　<br>如果需要超过8个loop device,那么使用losetup命令的时候可能会遇到类似的错误 ‘no such device’,这是因为超过了可用</p><p>loop device设备的最大限制,依据你的Linux系统,可以通过修改&#x2F;etc&#x2F;modprobe.conf 配置文件,增加如下参数的方式进行扩展:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">options loop max_loop=20 --比如我增加到20个<br></code></pre></td></tr></table></figure><p>保存退出,如果要了马上生效的话,可以通过</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">modprobe -v loop<br></code></pre></td></tr></table></figure><p>命令立即加载该模块。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@vm11g dev]<span class="hljs-comment"># cat /etc/modprobe.conf|grep loop</span><br>options loop max_loop=20<br>[root@vm11g dev]<span class="hljs-comment"># modprobe -v loop</span><br>insmod /lib/modules/2.6.9-42.0.0.0.1.ELsmp/kernel/drivers/block/loop.ko max_loop=20<br>[root@vm11g dev]<span class="hljs-comment"># ls -ltr /dev/loop*</span><br>brw-rw---- 1 root disk 7, 8 Jul 19 07:44 /dev/loop8<br>brw-rw---- 1 root disk 7, 9 Jul 19 07:44 /dev/loop9<br>brw-rw---- 1 root disk 7, 10 Jul 19 07:44 /dev/loop10<br>brw-rw---- 1 root disk 7, 11 Jul 19 07:44 /dev/loop11<br>brw-rw---- 1 root disk 7, 12 Jul 19 07:44 /dev/loop12<br>brw-rw---- 1 root disk 7, 13 Jul 19 07:44 /dev/loop13<br>brw-rw---- 1 root disk 7, 14 Jul 19 07:44 /dev/loop14<br>brw-rw---- 1 root disk 7, 15 Jul 19 07:44 /dev/loop15<br>brw-rw---- 1 root disk 7, 16 Jul 19 07:44 /dev/loop16<br>brw-rw---- 1 root disk 7, 17 Jul 19 07:44 /dev/loop17<br>brw-rw---- 1 root disk 7, 18 Jul 19 07:44 /dev/loop18<br>brw-rw---- 1 root disk 7, 19 Jul 19 07:44 /dev/loop19<br>brw-rw---- 1 root disk 7, 0 Jul 19 2009 /dev/loop0<br>brw-rw---- 1 root disk 7, 1 Jul 19 2009 /dev/loop1<br>brw-rw---- 1 root disk 7, 2 Jul 19 2009 /dev/loop2<br>brw-rw---- 1 root disk 7, 3 Jul 19 2009 /dev/loop3<br>brw-rw---- 1 root disk 7, 4 Jul 19 2009 /dev/loop4<br>brw-rw---- 1 root disk 7, 5 Jul 19 2009 /dev/loop5<br>brw-rw---- 1 root disk 7, 6 Jul 19 2009 /dev/loop6<br>brw-rw---- 1 root disk 7, 7 Jul 19 2009 /dev/loop7<br></code></pre></td></tr></table></figure><p>有了这个东西,在Linux下就可以用文件来生成块设备进行测试了</p><h2 id="更新历史"><a href="#更新历史" class="headerlink" title="更新历史"></a>更新历史</h2><table><thead><tr><th>why</th><th>when</th></tr></thead><tbody><tr><td>创建</td><td>2012年08月02日</td></tr><tr><td>更新</td><td>2019年12月9日</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>用rsync备份一台linux服务器上的数据</title>
    <link href="/2015/03/22/%E7%94%A8rsync%E5%A4%87%E4%BB%BD%E4%B8%80%E5%8F%B0linux%E6%9C%8D%E5%8A%A1%E5%99%A8%E4%B8%8A%E7%9A%84%E6%95%B0%E6%8D%AE/"/>
    <url>/2015/03/22/%E7%94%A8rsync%E5%A4%87%E4%BB%BD%E4%B8%80%E5%8F%B0linux%E6%9C%8D%E5%8A%A1%E5%99%A8%E4%B8%8A%E7%9A%84%E6%95%B0%E6%8D%AE/</url>
    
    <content type="html"><![CDATA[<p>rsync是安装完linux后都会自带的,在机器上运行rsync命令看是否有安装即可</p><h2 id="备份到远程服务器"><a href="#备份到远程服务器" class="headerlink" title="备份到远程服务器"></a>备份到远程服务器</h2><p>这里介绍的rsync的用途是备份一台linux服务器上的数据到另外一台机器</p><h3 id="环境"><a href="#环境" class="headerlink" title="环境"></a>环境</h3><p>将需要备份机器叫做服务器端  (192.168.0.195)<br>将备份后数据存放机器叫做客户端 (192.168.0.196)</p><h3 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h3><h4 id="1-服务器端的配置"><a href="#1-服务器端的配置" class="headerlink" title="1.服务器端的配置:"></a>1.服务器端的配置:</h4><p>这里是准备备份&#x2F;chinfotech&#x2F;kernel&#x2F;zp&#x2F;这个目录,修改配置文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">vim /etc/rsyncd.conf (没有就创建)<br></code></pre></td></tr></table></figure><p>配置文件如下:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs bash">uid=root<br>gid=root<br>max connections=10     <br>use <span class="hljs-built_in">chroot</span>=no<br><span class="hljs-built_in">log</span> file=/var/log/rsyncd.log<br>pid file=/var/run/rsyncd.pid<br>lock file=/var/run/rsyncd.lock<br><br>[zp]<br>path=/chinfotech/kernel/zp/ <br>comment=beifen<br>ignore errors<br><span class="hljs-built_in">read</span> only = no<br>hosts allow=*<br></code></pre></td></tr></table></figure><p>启动进程:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">rsync --daemon --config=/etc/rsyncd.conf<br></code></pre></td></tr></table></figure><p>服务器端就配置好了.</p><h4 id="2-客户端机器的配置"><a href="#2-客户端机器的配置" class="headerlink" title="2.客户端机器的配置"></a>2.客户端机器的配置</h4><p>在客户端不需要太多配置只需要执行:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">rsync -avz --bwlimit=30000 root@192.168.0.195::zp /back/zp/<br></code></pre></td></tr></table></figure><p>参数解释:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">-a, --archive 归档模式，表示以递归方式传输文件,并保持所有文件属性<br>-v, --verbose 详细模式输出 <br>-z, --compress 对备份的文件在传输时进行压缩处理<br>--bwlimit=KBPS 限制I/O带宽，KBytes per second (最好限速,不然长时间数据传输硬盘受不了)<br>::zp这个是上面服务器配置当中的[zp]<br></code></pre></td></tr></table></figure><p>这样备份就开始了</p><h2 id="配置rsync的本地同步"><a href="#配置rsync的本地同步" class="headerlink" title="配置rsync的本地同步"></a>配置rsync的本地同步</h2><p>使用命令进行同步,这个是从lsyncd这个软件里面提取的命令，也可以用那个做实时同步,这个是定期同步</p><p>每天的三点过5分执行同步,加超时:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">5 3 * * * root <span class="hljs-built_in">timeout</span> 7200  rsync -qvzrtopg    /zbkc /sdl<br>5 3 * * * root <span class="hljs-built_in">timeout</span> 7200  rsync -qvzrtopg --delete   /zbkc /sdl<br></code></pre></td></tr></table></figure><p>让配置文件生效</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">crontab /etc/crontab <br></code></pre></td></tr></table></figure><p>重启服务</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">/etc/init.d/cron restart<br></code></pre></td></tr></table></figure><h2 id="配置远程的同步"><a href="#配置远程的同步" class="headerlink" title="配置远程的同步"></a>配置远程的同步</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">rsync -avP /a1/ 192.168.8.107:/a1/<br></code></pre></td></tr></table></figure><p>加个S可以传输稀疏文件</p><p>写于: 2012年07月20日,2014年06月24日<br>更新于: 2015年03月22日</p>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>修改ssh的默认22端口，并使用scp的方法</title>
    <link href="/2015/03/22/%E4%BF%AE%E6%94%B9ssh%E7%9A%84%E9%BB%98%E8%AE%A422%E7%AB%AF%E5%8F%A3%EF%BC%8C%E5%B9%B6%E4%BD%BF%E7%94%A8scp%E7%9A%84%E6%96%B9%E6%B3%95/"/>
    <url>/2015/03/22/%E4%BF%AE%E6%94%B9ssh%E7%9A%84%E9%BB%98%E8%AE%A422%E7%AB%AF%E5%8F%A3%EF%BC%8C%E5%B9%B6%E4%BD%BF%E7%94%A8scp%E7%9A%84%E6%96%B9%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<p>修改默认的22的ssh端口只需要修改 &#x2F;etc&#x2F;ssh&#x2F;sshd_config 中的 port 字段为你想要的端口就可以了<br>以后用其他机器ssh登录这台机器只需要:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">ssh -p (port)  (ip)  <br></code></pre></td></tr></table></figure><p>使用scp 就是:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">scp -P (port) /usr/myfile   IP:/tmp<br></code></pre></td></tr></table></figure><p>写于: 2012年05月05日<br>更新于: 2015年03月02日</p>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>debian修改crontab默认编辑器为vim</title>
    <link href="/2015/03/22/debian%E4%BF%AE%E6%94%B9crontab%E9%BB%98%E8%AE%A4%E7%BC%96%E8%BE%91%E5%99%A8%E4%B8%BAvim/"/>
    <url>/2015/03/22/debian%E4%BF%AE%E6%94%B9crontab%E9%BB%98%E8%AE%A4%E7%BC%96%E8%BE%91%E5%99%A8%E4%B8%BAvim/</url>
    
    <content type="html"><![CDATA[<p>debian终端下默认编辑器为nano,比如crontab -e就会打开nano,这个编辑器用起来很不习惯,想修改为vim,当然,你的debian系统必须先安装vim.如果已经安装vim,请输入如下命令:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">update-alternatives --config editor<br></code></pre></td></tr></table></figure><p>然后选择&#x2F;usr&#x2F;vim&#x2F;vim.basic就OK了</p><p>vim.tiny与vim.basic的区别是vim.basic为完全的vim,而vim.tiny为简化版,功能上有所区别,如不支持高亮等等.</p><h2 id="更新历史"><a href="#更新历史" class="headerlink" title="更新历史"></a>更新历史</h2><table><thead><tr><th>why</th><th>when</th></tr></thead><tbody><tr><td>创建</td><td>2012年5月04日</td></tr><tr><td>更新</td><td>2019年12月9日</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>正确用DD测试磁盘读写速度</title>
    <link href="/2015/03/22/%E6%AD%A3%E7%A1%AE%E7%94%A8DD%E6%B5%8B%E8%AF%95%E7%A3%81%E7%9B%98%E8%AF%BB%E5%86%99%E9%80%9F%E5%BA%A6/"/>
    <url>/2015/03/22/%E6%AD%A3%E7%A1%AE%E7%94%A8DD%E6%B5%8B%E8%AF%95%E7%A3%81%E7%9B%98%E8%AF%BB%E5%86%99%E9%80%9F%E5%BA%A6/</url>
    
    <content type="html"><![CDATA[<p>问: 以下几种方式测试磁盘读写速度有什么区别?</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">dd</span> <span class="hljs-keyword">if</span>=/dev/zero of=<span class="hljs-built_in">test</span> bs=1M count=128 <br><span class="hljs-built_in">dd</span> <span class="hljs-keyword">if</span>=/dev/zero of=<span class="hljs-built_in">test</span> bs=1M count=128;<span class="hljs-built_in">sync</span><br><span class="hljs-built_in">dd</span> <span class="hljs-keyword">if</span>=/dev/zero of=<span class="hljs-built_in">test</span> bs=1M count=128 conv=fdatasync<br><span class="hljs-built_in">dd</span> <span class="hljs-keyword">if</span>=/dev/zero of=<span class="hljs-built_in">test</span> bs=1M count=128 oflag=dsync<br></code></pre></td></tr></table></figure><p>答:区别在于内存中写缓存的处理方式。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">dd</span> <span class="hljs-keyword">if</span>=/dev/zero of=<span class="hljs-built_in">test</span> bs=1M count=128 <br></code></pre></td></tr></table></figure><p>没有加任何参数,dd默认的方式不包括“同步(sync)”命令。也就是说,dd命令完成前并没有让系统真正把文件写到磁盘上。所以以上命令只是单纯地把这128MB的数据读到内存缓冲当中,写缓存[write cache]。所以你得到的将是一个超级快的速度。因为其实dd给你的只是读取到缓存的速度,直到dd完成后系统才开始真正往磁盘上写数据,但这个速度你是看不到了。所以如果这个速度很快,先不要偷着乐。呵呵</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">dd</span> <span class="hljs-keyword">if</span>=/dev/zero of=<span class="hljs-built_in">test</span> bs=1M count=128 ; <span class="hljs-built_in">sync</span><br></code></pre></td></tr></table></figure><p>和前面1中的完全一样。分号隔开的只是先后两个独立的命令。当sync命令准备开始往磁盘上真正写入数据的时候,前面dd命令已经把错误的“写入速度”值显示在屏幕上了。所以你还是得不到真正的写入速度。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">dd</span> <span class="hljs-keyword">if</span>=/dev/zero of=<span class="hljs-built_in">test</span> bs=1M count=128 conv=fdatasync<br></code></pre></td></tr></table></figure><p>加入这个参数后,dd命令执行到最后会真正执行一次“同步(sync)”操作,所以这时候你得到的是读取这128M数据到内存并写入到磁盘上所需的时间,这样算出来的时间才是比较符合实际的。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">dd</span> <span class="hljs-keyword">if</span>=/dev/zero of=<span class="hljs-built_in">test</span> bs=1M count=128 oflag=dsync<br></code></pre></td></tr></table></figure><p>加入这个参数后,dd在执行时每次都会进行同步写入操作。也就是说,这条命令每次读取1M后就要先把这1M写入磁盘,然后再读取下面这1M,一共重复128次。这可能是最慢的一种方式了,因为基本上没有用到写缓存(write cache)。</p><p>问:那应该用哪一种呢?<br>答:建议使用</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">dd</span> bs=1M count=128 <span class="hljs-keyword">if</span>=/dev/zero of=<span class="hljs-built_in">test</span> conv=fdatasync<br></code></pre></td></tr></table></figure><p>因为这种方式最接近计算机实际操作,所以测出来的数据最有参考价值</p><p>写于: 2012年05月02日<br>更新于: 2015年03月22日</p>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>西数WD2T硬盘分区对齐的方法</title>
    <link href="/2015/03/22/%E8%A5%BF%E6%95%B0WD2T%E7%A1%AC%E7%9B%98%E5%88%86%E5%8C%BA%E5%AF%B9%E9%BD%90%E7%9A%84%E6%96%B9%E6%B3%95/"/>
    <url>/2015/03/22/%E8%A5%BF%E6%95%B0WD2T%E7%A1%AC%E7%9B%98%E5%88%86%E5%8C%BA%E5%AF%B9%E9%BD%90%E7%9A%84%E6%96%B9%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<p>新购一个西数2T硬盘,也就是绿盘的那种,淘宝500左右,支持高级格式化。 </p><p>到手以后,分区格式化,前几天格式化完成以后,fdisk -l 发现如下文字 </p><p>引用<br>Partition 1 does not start on physical sector boundary.</p><p>网上找了下,说是软件的问题,后来折腾了下,应该是分区没有对齐的原因,至于为什么要对齐,优点很多,好像这个是跟神马高级分区格式化有关,具体的原理神马的,我就不讨论了,反正有一点,对齐后,性能会有提升。 </p><p>对齐后,用fdisk -lu 查看的结果如下</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs bash">Disk /dev/sdc: 2000.4 GB, 2000398934016 bytes<br>255 heads, 63 sectors/track, 243201 cylinders, total 3907029168 sectors<br>Units = sectors of 1 * 512 = 512 bytes<br>Sector size (logical/physical): 512 bytes / 512 bytes<br>I/O size (minimum/optimal): 512 bytes / 512 bytes<br>Disk identifier: 0x2fa8ebd1<br><br>    Device Boot      Start         End      Blocks   Id  System<br>/dev/sdc1            2048  3907028991  1953513472   83  Linux<br></code></pre></td></tr></table></figure><p>怎么样个对齐的方法呢,就是分区的时候,要注意,不要用 fdisk 来分区,不是说fdisk不行,经过我的测试,用 fdisk 分区,我明明分的是2T,结果却是1T大小，用 parted 分区就没有这个问题 </p><p>debian 下面,如果没有 parted 这个工具,apt-get 安装,安装完成以后,就可以使用了 </p><p>引用</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs bash">root@localhost ~]<span class="hljs-comment"># parted /dev/sdb # 使用parted来对GPT磁盘操作,进入交互式模式</span><br>GNU Parted 1.8.1 Using /dev/sdb Welcome to GNU Parted! Type ‘<span class="hljs-built_in">help</span>’ to view a list of commands.<br>(parted) mklabel msdos           <span class="hljs-comment"># 将MBR磁盘格式化为msdos 2T的硬盘 msdos 没有问题</span><br>(parted) p                      <span class="hljs-comment">#打印当前分区</span><br>(parted) mkpart primary 1 2TB                <span class="hljs-comment"># 分一个2T的主分区,这里输入1,否则会提示性能问题</span><br>(parted) p                         <span class="hljs-comment">#打印当前分区</span><br>(parted) q 退出<br>Information: Don’t forget to update /etc/fstab, <span class="hljs-keyword">if</span> necessary.<br></code></pre></td></tr></table></figure><p>退出后,用 mkfs.ext4 格式化就好了 </p><p>然后再用 fdisk -lu 查看,已经没有那个提示了 </p><p>这里说明下,对齐,就是说分区的时候,开始的扇区要以 8 的倍数来开始,而 fisk 默认是 63 ,对老硬盘没有什么问题,但是对西数的新硬盘, 4k 扇区的,就会有对齐的问题,会导致性能下降</p><p>写于: 2012年05月02日<br>更新于:2015年03月22日</p>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>checkhost实时监控网站或者服务器是否可以访问</title>
    <link href="/2015/03/22/checkhost%E5%AE%9E%E6%97%B6%E7%9B%91%E6%8E%A7%E7%BD%91%E7%AB%99%E6%88%96%E8%80%85%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%98%AF%E5%90%A6%E5%8F%AF%E4%BB%A5%E8%AE%BF%E9%97%AE/"/>
    <url>/2015/03/22/checkhost%E5%AE%9E%E6%97%B6%E7%9B%91%E6%8E%A7%E7%BD%91%E7%AB%99%E6%88%96%E8%80%85%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%98%AF%E5%90%A6%E5%8F%AF%E4%BB%A5%E8%AE%BF%E9%97%AE/</url>
    
    <content type="html"><![CDATA[<p>如果你拥有一个网站，那么最重要的事情就是要保证它24小时都能够访问。不过国内的虚拟主机服务非常糟糕，经常会出现各种状况,所以我们需要一个软件,可以让我们第一时间知道网站出现了无法访问的情况,从而通知售后人员解决。而Check Host就是这么一个windows下的免费网站时时监控工具。</p><p>服务器无法访问,那么对网站是非常不利的,而搜索引擎也会知道这个状况,因为蜘蛛不能爬行你的网页了。所以对访客以及搜索引擎排名都是不利的。不过这个事情肯定会发生,我们能做的就是提高反应速度,让这样的情况在最短的时间内解决。</p><p>如此一来,对于网站的实时监控就非常有必要的,但是我们又不能每隔一段时间就手动打开网站一次,这样太过于麻烦,甚至是愚蠢的,所以我们必须要使用一个软件,让其代劳，当发生状况的时候还会给你发出警告，而Check Host就可以满足你的愿望。</p><p>Check Host是一个windows下的小软件，只有2.2M,十分的小巧,它的原理是每隔一段时间就自动ping一下你设定的网址。如果无法ping通的话,那么就会通过几种方法告诉你。</p><p>这个软件对监控的网站和服务器没有数量方面的限制,所以你同时可以监控多个网站。</p><p><img src="/images/blog/o_200901015841checkhost1.jpg"></p><h2 id="使用方法"><a href="#使用方法" class="headerlink" title="使用方法"></a>使用方法</h2><p>当你安装运行后,点击左上角的“new”按钮添加一个任务,在出现界面的“hostname”下面输入网址。“check interval”是让你设置测试间隔,这个默认是10秒,建议间隔长一点,不然可能会谎报军情。“timeout”是超时的时间,默认1000毫秒,建议也是填较短的时间,不然可能会导致软件无法响应。如果发生这样的状况,只需等待就可以了。</p><p>而在新建任务界面右侧的是当ping不通是触发什么行为,如果又再次ping通又触发什么。</p><p>这个你有三种选择,桌面提醒、往指定邮箱发送邮件、启动某个程序或者批处理命令。并且这个是可以多选的。<br>默认情况下,桌面提醒是被激活的,一旦出现状况,你就会在桌面右侧看到小的弹窗,上面会显示不能访问的网站网址。而系统托盘的图标也会从“勾”变成“叉”。</p><p>而触发程序的话，你可以指定音乐播放器,那么这样就会在网站无法访问时使用声音提醒你。</p><p>“check interval”在软件默认设置中是10秒,不过希望你在使用的时候改成1分钟以上，这样就不会非常敏感了。</p><p><img src="/images/blog/o_200901015845checkhost2.jpg"></p><h2 id="更新历史"><a href="#更新历史" class="headerlink" title="更新历史"></a>更新历史</h2><table><thead><tr><th>why</th><th>when</th></tr></thead><tbody><tr><td>创建</td><td>2012年04月26日</td></tr><tr><td>更新</td><td>2019年12月9日</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>关于nginx upstream的几种配置方式</title>
    <link href="/2015/03/22/%E5%85%B3%E4%BA%8Enginx%20upstream%E7%9A%84%E5%87%A0%E7%A7%8D%E9%85%8D%E7%BD%AE%E6%96%B9%E5%BC%8F/"/>
    <url>/2015/03/22/%E5%85%B3%E4%BA%8Enginx%20upstream%E7%9A%84%E5%87%A0%E7%A7%8D%E9%85%8D%E7%BD%AE%E6%96%B9%E5%BC%8F/</url>
    
    <content type="html"><![CDATA[<p>平时一直依赖硬件来作load blance,最近研究Nginx来做负载设备,记录下upstream的几种配置方式。</p><h3 id="第一种-轮询"><a href="#第一种-轮询" class="headerlink" title="第一种:轮询"></a>第一种:轮询</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">upstream <span class="hljs-built_in">test</span>&#123;<br>    server 192.168.0.1:3000;<br>    server 192.168.0.1:3001;<br>&#125;<br></code></pre></td></tr></table></figure><h3 id="第二种-权重"><a href="#第二种-权重" class="headerlink" title="第二种:权重"></a>第二种:权重</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">upstream <span class="hljs-built_in">test</span>&#123;<br>    server 192.168.0.1 weight=2;<br>    server 192.168.0.2 weight=3;<br>&#125;<br></code></pre></td></tr></table></figure><p>这种模式可解决服务器性能不等的情况下轮询比率的调配</p><h3 id="第三种-ip-hash"><a href="#第三种-ip-hash" class="headerlink" title="第三种:ip_hash"></a>第三种:ip_hash</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">upstream <span class="hljs-built_in">test</span>&#123;<br>    ip_hash;<br>    server 192.168.0.1;<br>    server 192.168.0.2;<br>&#125;<br></code></pre></td></tr></table></figure><p>这种模式会根据来源IP和后端配置来做hash分配,确保固定IP只访问一个后端</p><h3 id="第四种-fair"><a href="#第四种-fair" class="headerlink" title="第四种:fair"></a>第四种:fair</h3><p>需要安装Upstream Fair Balancer Module</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">upstream <span class="hljs-built_in">test</span>&#123;<br>    server 192.168.0.1;<br>    server 192.168.0.2;<br>    fair;<br>&#125;<br></code></pre></td></tr></table></figure><p>这种模式会根据后端服务的响应时间来分配,响应时间短的后端优先分配</p><h3 id="第五种-自定义hash"><a href="#第五种-自定义hash" class="headerlink" title="第五种:自定义hash"></a>第五种:自定义hash</h3><p>需要安装Upstream Hash Module</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">upstream <span class="hljs-built_in">test</span>&#123;<br>    server 192.168.0.1;<br>    server 192.168.0.2;<br>    <span class="hljs-built_in">hash</span> <span class="hljs-variable">$request_uri</span>;<br>&#125;<br></code></pre></td></tr></table></figure><p>这种模式可以根据给定的字符串进行Hash分配</p><h2 id="具体应用"><a href="#具体应用" class="headerlink" title="具体应用:"></a>具体应用:</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs bash">server&#123;<br>    listen 80;<br>    server_name .test.com;<br>    charset utf-8;<br>    <br>    location / &#123;<br>        proxy_pass http://test/;<br>    &#125; <br>&#125;<br></code></pre></td></tr></table></figure><p>此外upstream每个后端的可设置参数为:</p><ul><li>1.down: 表示此台server暂时不参与负载</li><li>2.weight: 默认为1,weight越大,负载的权重就越大</li><li>3.max_fails: 允许请求失败的次数默认为1.当超过最大次数时,返回proxy_next_upstream模块定义的错误</li><li>4.fail_timeout: max_fails次失败后,暂停的时间</li><li>5.backup: 其它所有的非backup机器down或者忙的时候，请求backup机器,应急措施</li></ul><h2 id="更新历史"><a href="#更新历史" class="headerlink" title="更新历史"></a>更新历史</h2><table><thead><tr><th>why</th><th>when</th></tr></thead><tbody><tr><td>创建</td><td>2012年4月23日</td></tr><tr><td>更新</td><td>2019年12月11日</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>系统运行后修改linux系统时区</title>
    <link href="/2015/03/22/%E7%B3%BB%E7%BB%9F%E8%BF%90%E8%A1%8C%E5%90%8E%E4%BF%AE%E6%94%B9linux%E7%B3%BB%E7%BB%9F%E6%97%B6%E5%8C%BA/"/>
    <url>/2015/03/22/%E7%B3%BB%E7%BB%9F%E8%BF%90%E8%A1%8C%E5%90%8E%E4%BF%AE%E6%94%B9linux%E7%B3%BB%E7%BB%9F%E6%97%B6%E5%8C%BA/</url>
    
    <content type="html"><![CDATA[<p>在网上看了很多改时间的帖子,都没能最终解决问题。最后还是下面的博客最终解决的时间的问题,感谢原作者<br>安装系统过程时没有选对当前的时区,即CST,Asia&#x2F;Shanghai,而是按默认的，EDT时区,这样跟我们的系统就都对不上,因此得改回CST,仔细琢磨一下,方法如下:</p><p>更改&#x2F;etc&#x2F;timezone里面的内容为:Asia&#x2F;Shanghai</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ <span class="hljs-built_in">mv</span> /etc/localtime /etc/localtime-bk<br>$ <span class="hljs-built_in">cp</span> /usr/share/zoneinfo/Asia/Shanghai /etc/localtime<br></code></pre></td></tr></table></figure><p>再用命令date看一下,就是我们要的CST。</p><p>然后:使用下面命令设置时间即可</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ <span class="hljs-built_in">date</span> -s <br></code></pre></td></tr></table></figure><p>还有一个方法,使用命令:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ tzselect<br></code></pre></td></tr></table></figure><p>写于: 2012年4月19日<br>更新于: 2015年03月22日</p>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>卸载python安装的软件</title>
    <link href="/2015/03/22/%E5%8D%B8%E8%BD%BDpython%E5%AE%89%E8%A3%85%E7%9A%84%E8%BD%AF%E4%BB%B6/"/>
    <url>/2015/03/22/%E5%8D%B8%E8%BD%BDpython%E5%AE%89%E8%A3%85%E7%9A%84%E8%BD%AF%E4%BB%B6/</url>
    
    <content type="html"><![CDATA[<p>python源码安装的软件是无法通过命令卸载的,这个可以通过记录安装过程的形式来卸载安装的软件</p><p>以 python2.7.2 为例,在这个目录中有一个 setup.py 的文件,很显然这是安装程序,还是python写的,用 python 写 python 到安装程序,大开眼界了。在这里,需要写一个 shell 脚本,把在重新安装中提到的安装位置全部删除,就实现了卸载到目的了</p><h3 id="一、创建del-sh脚本内容-在源码目录"><a href="#一、创建del-sh脚本内容-在源码目录" class="headerlink" title="一、创建del.sh脚本内容(在源码目录):"></a>一、创建del.sh脚本内容(在源码目录):</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-meta">#!/bin/bash  </span><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> $(less files.txt)  <br><span class="hljs-keyword">do</span>  <span class="hljs-built_in">rm</span> -rf <span class="hljs-variable">$i</span>  <br><span class="hljs-keyword">done</span>  <br></code></pre></td></tr></table></figure><p>脚本说明: files.txt 是需要事先在目录下创建的文件,目的是记录安装过程中提到到路径。脚本很简单,详细的就不说了,学过 shell 编程到肯定没问题,没学过到有其他语言编程基础的,仔细看看也可以看懂。</p><h3 id="二、重新安装软件"><a href="#二、重新安装软件" class="headerlink" title="二、重新安装软件"></a>二、重新安装软件</h3><p>在此过程中用 files.txt 记录安装的路径,比如安装 python2.7.2 ,进入安装目录,执行命令:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">python setup.py  install --record files.txt<br></code></pre></td></tr></table></figure><p>在此过程中会向 files.txt 中写入很多路径</p><h3 id="三、执行脚本-del-sh"><a href="#三、执行脚本-del-sh" class="headerlink" title="三、执行脚本.&#x2F;del.sh"></a>三、执行脚本.&#x2F;del.sh</h3><p>写于: 2012年04月09日<br>更新于: 2015年03月22日</p>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>常用linux源列表</title>
    <link href="/2015/03/22/%E5%B8%B8%E7%94%A8linux%E6%BA%90%E5%88%97%E8%A1%A8/"/>
    <url>/2015/03/22/%E5%B8%B8%E7%94%A8linux%E6%BA%90%E5%88%97%E8%A1%A8/</url>
    
    <content type="html"><![CDATA[<p>本篇记录一些常用的源文件，后面需要用到的时候，直接进行复制粘贴即可</p><h2 id="centos-相关"><a href="#centos-相关" class="headerlink" title="centos 相关"></a>centos 相关</h2><h3 id="base源"><a href="#base源" class="headerlink" title="base源"></a>base源</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><code class="hljs bash">[base]<br>name=CentOS-<span class="hljs-variable">$releasever</span> - Base - mirrors.aliyun.com<br>failovermethod=priority<br>baseurl=http://mirrors.aliyun.com/centos/<span class="hljs-variable">$releasever</span>/os/<span class="hljs-variable">$basearch</span>/<br>gpgcheck=0<br><br><span class="hljs-comment">#released updates </span><br>[updates]<br>name=CentOS-<span class="hljs-variable">$releasever</span> - Updates - mirrors.aliyun.com<br>failovermethod=priority<br>baseurl=http://mirrors.aliyun.com/centos/<span class="hljs-variable">$releasever</span>/updates/<span class="hljs-variable">$basearch</span>/<br>gpgcheck=0<br><br><span class="hljs-comment">#additional packages that may be useful</span><br>[extras]<br>name=CentOS-<span class="hljs-variable">$releasever</span> - Extras - mirrors.aliyun.com<br>failovermethod=priority<br>baseurl=http://mirrors.aliyun.com/centos/<span class="hljs-variable">$releasever</span>/extras/<span class="hljs-variable">$basearch</span>/<br>gpgcheck=0<br><br><span class="hljs-comment">#additional packages that extend functionality of existing packages</span><br>[centosplus]<br>name=CentOS-<span class="hljs-variable">$releasever</span> - Plus - mirrors.aliyun.com<br>failovermethod=priority<br>baseurl=http://mirrors.aliyun.com/centos/<span class="hljs-variable">$releasever</span>/centosplus/<span class="hljs-variable">$basearch</span>/<br>gpgcheck=0<br>enabled=0<br><br><span class="hljs-comment">#contrib - packages by Centos Users</span><br>[contrib]<br>name=CentOS-<span class="hljs-variable">$releasever</span> - Contrib - mirrors.aliyun.com<br>failovermethod=priority<br>baseurl=http://mirrors.aliyun.com/centos/<span class="hljs-variable">$releasever</span>/contrib/<span class="hljs-variable">$basearch</span>/<br>gpgcheck=0<br>enabled=0root<br></code></pre></td></tr></table></figure><h3 id="epel源"><a href="#epel源" class="headerlink" title="epel源"></a>epel源</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs bash">[epel]<br>name=Extra Packages <span class="hljs-keyword">for</span> Enterprise Linux 7 - <span class="hljs-variable">$basearch</span><br>baseurl=http://mirrors.aliyun.com/epel/7/<span class="hljs-variable">$basearch</span><br>failovermethod=priority<br>enabled=1<br>gpgcheck=0<br><br>[epel-debuginfo]<br>name=Extra Packages <span class="hljs-keyword">for</span> Enterprise Linux 7 - <span class="hljs-variable">$basearch</span> - Debug<br>baseurl=http://mirrors.aliyun.com/epel/7/<span class="hljs-variable">$basearch</span>/debug<br>failovermethod=priority<br>enabled=0<br>gpgcheck=0<br><br>[epel-source]<br>name=Extra Packages <span class="hljs-keyword">for</span> Enterprise Linux 7 - <span class="hljs-variable">$basearch</span> - Source<br>baseurl=http://mirrors.aliyun.com/epel/7/SRPMS<br>failovermethod=priority<br>enabled=0<br>gpgcheck=0<br></code></pre></td></tr></table></figure><h3 id="ceph-源"><a href="#ceph-源" class="headerlink" title="ceph 源"></a>ceph 源</h3><h3 id="ceph-luminous源"><a href="#ceph-luminous源" class="headerlink" title="ceph luminous源"></a>ceph luminous源</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs bash">[ceph]<br>name=ceph<br>baseurl=http://mirrors.aliyun.com/ceph/rpm-luminous/el7/x86_64/<br>gpgcheck=0<br>[ceph-noarch]<br>name=cephnoarch<br>baseurl=http://mirrors.aliyun.com/ceph/rpm-luminous/el7/noarch/<br>gpgcheck=0<br></code></pre></td></tr></table></figure><h2 id="ubuntu-相关"><a href="#ubuntu-相关" class="headerlink" title="ubuntu 相关"></a>ubuntu 相关</h2><h3 id="arm-ubuntu16-04源"><a href="#arm-ubuntu16-04源" class="headerlink" title="arm-ubuntu16.04源"></a>arm-ubuntu16.04源</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs bash">deb http://mirrors.tuna.tsinghua.edu.cn/ubuntu-ports/ xenial main restricted<br>deb http://mirrors.tuna.tsinghua.edu.cn/ubuntu-ports/ xenial-updates main restricted<br>deb http://mirrors.tuna.tsinghua.edu.cn/ubuntu-ports/ xenial universe<br>deb http://mirrors.tuna.tsinghua.edu.cn/ubuntu-ports/ xenial-updates universe<br>deb http://mirrors.tuna.tsinghua.edu.cn/ubuntu-ports/ xenial multiverse<br>deb http://mirrors.tuna.tsinghua.edu.cn/ubuntu-ports/ xenial-updates multiverse<br>deb http://mirrors.tuna.tsinghua.edu.cn/ubuntu-ports/ xenial-backports main restricted universe multiverse<br>deb http://mirrors.tuna.tsinghua.edu.cn/ubuntu-ports/ xenial-security main restricted<br>deb http://mirrors.tuna.tsinghua.edu.cn/ubuntu-ports/ xenial-security universe<br>deb http://mirrors.tuna.tsinghua.edu.cn/ubuntu-ports/ xenial-security multiverse<br></code></pre></td></tr></table></figure><h3 id="arm-ubuntu-linux-源"><a href="#arm-ubuntu-linux-源" class="headerlink" title="arm ubuntu linux 源:"></a>arm ubuntu linux 源:</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs bash"><br>deb http://ports.ubuntu.com/ubuntu-ports/ trusty main restricted<br>deb-src http://ports.ubuntu.com/ubuntu-ports/ trusty main restricted<br><br><br>deb http://ports.ubuntu.com/ubuntu-ports/ trusty-updates main restricted<br>deb-src http://ports.ubuntu.com/ubuntu-ports/ trusty-updates main restricted<br><br><br>deb http://ports.ubuntu.com/ubuntu-ports/ trusty universe<br>deb-src http://ports.ubuntu.com/ubuntu-ports/ trusty universe<br>deb http://ports.ubuntu.com/ubuntu-ports/ trusty-updates universe<br>deb-src http://ports.ubuntu.com/ubuntu-ports/ trusty-updates universe<br><br><br>deb http://ports.ubuntu.com/ubuntu-ports/ trusty multiverse<br>deb-src http://ports.ubuntu.com/ubuntu-ports/ trusty multiverse<br>deb http://ports.ubuntu.com/ubuntu-ports/ trusty-updates multiverse<br>deb-src http://ports.ubuntu.com/ubuntu-ports/ trusty-updates multiverse<br><br><span class="hljs-comment"># deb http://ports.ubuntu.com/ubuntu-ports/ trusty-backports multiverse universe restricted main</span><br><span class="hljs-comment"># deb-src http://ports.ubuntu.com/ubuntu-ports/ trusty-backports multiverse universe restricted main</span><br><br>deb http://ports.ubuntu.com/ubuntu-ports/ trusty-security main restricted<br>deb-src http://ports.ubuntu.com/ubuntu-ports/ trusty-security main restricted<br>deb http://ports.ubuntu.com/ubuntu-ports/ trusty-security universe<br>deb-src http://ports.ubuntu.com/ubuntu-ports/ trusty-security universe<br>deb http://ports.ubuntu.com/ubuntu-ports/ trusty-security multiverse<br>deb-src http://ports.ubuntu.com/ubuntu-ports/ trusty-security multiverse<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>监控磁盘读写状况</title>
    <link href="/2015/03/22/%E7%9B%91%E6%8E%A7%E7%A3%81%E7%9B%98%E8%AF%BB%E5%86%99%E7%8A%B6%E5%86%B5/"/>
    <url>/2015/03/22/%E7%9B%91%E6%8E%A7%E7%A3%81%E7%9B%98%E8%AF%BB%E5%86%99%E7%8A%B6%E5%86%B5/</url>
    
    <content type="html"><![CDATA[<p>你有没有碰到过:没有运行任何程序，磁盘却不断执行读写动作,io指示灯常亮,各种操作迟缓甚至卡顿。碰到这种状况往往会感到束手无策,因为并不是 cpu 居高不下,可以立即结束相关进程。而突然结束进程中断磁盘 io 操作甚至可能导致正在写入的数据丢失。</p><h3 id="什么进程在读写磁盘"><a href="#什么进程在读写磁盘" class="headerlink" title="什么进程在读写磁盘?"></a>什么进程在读写磁盘?</h3><p>可能是 firefox,可能是 updatedb,也可能是正在运行的 pacman -Syu,一切皆有可能 ……<br>怎么查看是什么进程在不断的读写磁盘呢?<br>请使用 iotop 命令查看:<br>通过输出结果我们可以清楚地知晓是什么程序在读写磁盘，速度以及命令行, pid 等信息。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># iotop    </span><br>Total DISK READ: 0.00 B/s | Total DISK WRITE: 0.00 B/s<br>    TID  PRIO  USER     DISK READ  DISK WRITE  SWAPIN     IO&gt;    COMMAND              <br>    1 be/4 root        0.00 B/s    0.00 B/s  0.00 %  0.00 % init<br>    2 be/4 root        0.00 B/s    0.00 B/s  0.00 %  0.00 % [kthreadd]<br>    3 be/4 root        0.00 B/s    0.00 B/s  0.00 %  0.00 % [ksoftirqd/0]<br>    5 be/0 root        0.00 B/s    0.00 B/s  0.00 %  0.00 % [kworker/0:0H]<br>    7 be/4 root        0.00 B/s    0.00 B/s  0.00 %  0.00 % [rcu_sched]<br>    8 be/4 root        0.00 B/s    0.00 B/s  0.00 %  0.00 % [rcu_bh]<br>    9 rt/4 root        0.00 B/s    0.00 B/s  0.00 %  0.00 % [migration/0]<br>    10 rt/4 root        0.00 B/s    0.00 B/s  0.00 %  0.00 % [migration/1]<br>    11 be/4 root        0.00 B/s    0.00 B/s  0.00 %  0.00 % [ksoftirqd/1]<br>    13 be/0 root        0.00 B/s    0.00 B/s  0.00 %  0.00 % [kworker/1:0H]<br>    14 rt/4 root        0.00 B/s    0.00 B/s  0.00 %  0.00 % [migration/2]<br>    15 be/4 root        0.00 B/s    0.00 B/s  0.00 %  0.00 % [ksoftirqd/2]<br>    17 be/0 root        0.00 B/s    0.00 B/s  0.00 %  0.00 % [kworker/2:0H]<br>    18 rt/4 root        0.00 B/s    0.00 B/s  0.00 %  0.00 % [migration/3]<br>    19 be/4 root        0.00 B/s    0.00 B/s  0.00 %  0.00 % [ksoftirqd/3]<br><br></code></pre></td></tr></table></figure><p>使用 arrow 键移动表头焦点,使列表排序,通过 iotop 我们可以轻松辨识频繁读写磁盘的程序。<br>可以用左右箭头操作,按 r 是相反方向, 按 o 是动态切换</p><p>用法 iotop -参数</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs bash">–version 查看版本信息的<br>-h, –<span class="hljs-built_in">help</span> 查看帮助信息的<br>-o, –only 只显示在划硬盘的程序<br>-b, –batch 批量处理 用来记录日志的<br>-n NUM  设定循环几次<br>-d SEC, –delay=SEC  设定显示时间间隔<br></code></pre></td></tr></table></figure><h3 id="进一步思考-该程序在读写什么文件"><a href="#进一步思考-该程序在读写什么文件" class="headerlink" title="进一步思考:该程序在读写什么文件?"></a>进一步思考:该程序在读写什么文件?</h3><p>这个问题其实很简单,通过 lsof 命令我们就可以达到目的:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">lsof -c APPNAME      //后面接程序名称<br>lsof FILE            // 也可以根据文件进行查询 <br>lsof | grep PATH     // 也可以根据目录进行查询 <br></code></pre></td></tr></table></figure><p>其他命令</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs bash">top   // 亦可使用 iostat 命令查看,请安装 sysstat 以使用该命令<br><br>[root@lab8107 ~]<span class="hljs-comment"># top</span><br>top - 14:57:05 up 11 days,  4:37,  2 <span class="hljs-built_in">users</span>,  load average: 0.09, 0.06, 0.05<br>Tasks: 220 total,   2 running, 218 sleeping,   0 stopped,   0 zombie<br>Cpu(s):  0.2%us,  0.2%sy,  0.0%ni, 99.5%<span class="hljs-built_in">id</span>,  0.0%wa,  0.0%hi,  0.0%si,  0.0%st<br>Mem:  24711544k total,  2407312k used, 22304232k free,   219808k buffers<br>Swap:  2097148k total,        0k used,  2097148k free,  1319932k cached<br><br>    PID USER      PR  NI  VIRT  RES  SHR S %CPU %MEM    TIME+  COMMAND               <br>    2025 root      20   0  178m  10m 3488 S  3.0  0.0   0:21.43 iotop                  <br>    76 root      39  19     0    0    0 S  1.3  0.0 172:23.93 kipmi0                 <br>    5347 root      20   0  393m  59m 1836 S  1.0  0.2 164:34.60 glusterfs              <br>    2189 root      20   0 15228 1216  852 R  0.3  0.0   0:00.01 top                    <br>    9065 zabbix    20   0  505m 7732 5976 S  0.3  0.0   0:13.75 zabbix_server          <br>    20306 zabbix    20   0 77568 1588 1164 S  0.3  0.0   1:33.32 zabbix_agentd          <br>    1 root      20   0 19412 1428 1128 S  0.0  0.0   0:01.45 init                   <br>    2 root      20   0     0    0    0 S  0.0  0.0   0:00.10 kthreadd   <br></code></pre></td></tr></table></figure><p>在 cpu(s) 一行,我们可以看到 wa 项,它就是 io waiting,如果该值过大且持续很久,就证明遇到了 io 瓶颈。需要对软件进行优化,或对硬件进行升级。</p><h3 id="如何进行-io-瓶颈测试"><a href="#如何进行-io-瓶颈测试" class="headerlink" title="如何进行 io 瓶颈测试?"></a>如何进行 io 瓶颈测试?</h3><p>大文件 io 测试命令:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ time <span class="hljs-built_in">dd</span> <span class="hljs-keyword">if</span>=/dev/zero of=test.file bs=1G count=5 // 生成 5g 大小的文件并输出时间,执行速度等信息<br></code></pre></td></tr></table></figure><p>小文件io 测试脚本:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-meta">#!/bin/bash</span><br>    var1=1<br>    <span class="hljs-keyword">while</span> <span class="hljs-built_in">test</span> <span class="hljs-variable">$var1</span> -le <span class="hljs-variable">$1</span><br>    <span class="hljs-keyword">do</span><br>    <span class="hljs-built_in">touch</span> <span class="hljs-variable">$var1</span><br>    var1=`<span class="hljs-built_in">expr</span>  <span class="hljs-variable">$var1</span> + 1`<br>    <span class="hljs-keyword">done</span><br></code></pre></td></tr></table></figure><p>执行该 shell 脚本前,请先运行 iotop 等程序监控 io 状况。运行脚本:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">sh ./test.sh NUM    //    NUM 为生成的文件数<br></code></pre></td></tr></table></figure><h3 id="为什么会产生-io-瓶颈"><a href="#为什么会产生-io-瓶颈" class="headerlink" title="为什么会产生 io 瓶颈?"></a>为什么会产生 io 瓶颈?</h3><p>原因是多种多样的，可能是坏道,也可能是程序bug,甚至是电压不稳<br>曾经碰到 io 100%,读写速率却只有 2m&#x2F;s 的移动硬盘,经过检测,大概有 80% 以上区域是坏道部分,还有可能是因为 pv 的直线上升.服务器无法承受如此大的荷载而导致 io 增高,或者 gnome 的 tracker 正在制作索引,也许您忘记了后台正在 making 的程序<br>由于原因是多种多样的,在此不能一一列举。读者发现 io 瓶颈后,可以对症下药,先软后硬排除问题,使系统恢复到最佳状态。</p><h3 id="查看磁盘读速度"><a href="#查看磁盘读速度" class="headerlink" title="查看磁盘读速度:"></a>查看磁盘读速度:</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@localhost ~]<span class="hljs-comment"># /sbin/hdparm -t /dev/sda</span><br>/dev/sda:<br>    Timing buffered disk reads:   84 MB <span class="hljs-keyword">in</span>  4.21 seconds =  19.95 MB/sec<br></code></pre></td></tr></table></figure><h3 id="磁盘坏道检测"><a href="#磁盘坏道检测" class="headerlink" title="磁盘坏道检测"></a>磁盘坏道检测</h3><p>建议使用livecd或者liveusb对本地磁盘进行检测。如果是对移动存储设备进行检测,请umount后再进行检测,以免数据受损。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">umount /dev/sd*<br></code></pre></td></tr></table></figure><p>对磁盘进行read-only检测:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">sudo badblocks -s  -v  /dev/sd*<br></code></pre></td></tr></table></figure><p>因为需要对磁盘进行检测,所以速度非常缓慢,在检测过程中注意不要断电,不要对硬盘进行任何操作,不要移除硬盘,不要物理损伤,不要震动等。<br>检测过程可以中途终止,也可以指定区块重新开始。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">sudo badblock -s -v  /dev/sd*   last  start<br></code></pre></td></tr></table></figure><p>如果您检测完成后看到 </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">Pass completed, 0 bad blocks found.<br>那么恭喜,此磁盘通过测试,没有坏道,坏块。您可以放心使用。<br></code></pre></td></tr></table></figure><h3 id="坏道的修复-屏蔽"><a href="#坏道的修复-屏蔽" class="headerlink" title="坏道的修复&#x2F;屏蔽"></a>坏道的修复&#x2F;屏蔽</h3><p>常见坏道分为以下几种类型:</p><ul><li>逻辑坏道</li><li>0磁道损坏</li><li>物理坏道</li></ul><p>坏道一般以单独或者组合形式出现。</p><h3 id="逻辑坏道修复"><a href="#逻辑坏道修复" class="headerlink" title="逻辑坏道修复"></a>逻辑坏道修复</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">fsck -a /dev/sd*<br></code></pre></td></tr></table></figure><p>就这么简单<br>更多fsck用法您可以查看这里或者查看man手册。</p><h3 id="0磁道损坏修复"><a href="#0磁道损坏修复" class="headerlink" title="0磁道损坏修复:"></a>0磁道损坏修复:</h3><p>使用1磁道代替零磁道,操作危险需谨慎:<br>大致流程就是全盘格式化,然后重新分区,编辑分区表使用1磁道,从而复活硬盘。</p><h3 id="物理坏道"><a href="#物理坏道" class="headerlink" title="物理坏道"></a>物理坏道</h3><p>物理坏道没有修复可能性,只能进行屏蔽。<br>如果您已经进行了坏道检测,那么您肯定已经知道坏道,坏块,大致位置以及坏块大小,您需要,备份硬盘数据,删除所有硬盘分区,根据坏块位置以及大小,估算出所占空间,例如共100个区块,磁盘大小为100g,20-30损坏,则坏块在20-30g这个区间<br>进行分区,接上,分区应为 0-15|15-35|35-100,中间的15-35g为有坏道的分区。要对有坏道的分区进行扩容处理,数值不要过小,以免坏道被分到其他分区。隔离15-35g这个分区,即不挂载,不读写,不操作）,您的磁盘可用空间减少,但是剩余空间均可用,坏道已经屏蔽<br>由于物理坏道具有扩散性,所以建议尽早让磁盘“退休”才是……</p><h3 id="分区表修复工具"><a href="#分区表修复工具" class="headerlink" title="分区表修复工具"></a>分区表修复工具</h3><p>如果您的分区表已经被损坏,建议使用testdisk进行修复。他可以快速回复分区表,真的非常好用,修复我的硬盘n次利器</p><h2 id="更新历史"><a href="#更新历史" class="headerlink" title="更新历史"></a>更新历史</h2><table><thead><tr><th>why</th><th>when</th></tr></thead><tbody><tr><td>创建</td><td>2012年03月23日</td></tr><tr><td>更新</td><td>2019年12月11日</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>du查看的目录大小与df查看的大小不同的时候用lsof查找</title>
    <link href="/2015/03/22/du%E6%9F%A5%E7%9C%8B%E7%9A%84%E7%9B%AE%E5%BD%95%E5%A4%A7%E5%B0%8F%E4%B8%8Edf%E6%9F%A5%E7%9C%8B%E7%9A%84%E5%A4%A7%E5%B0%8F%E4%B8%8D%E5%90%8C%E7%9A%84%E6%97%B6%E5%80%99%E7%94%A8lsof%E6%9F%A5%E6%89%BE/"/>
    <url>/2015/03/22/du%E6%9F%A5%E7%9C%8B%E7%9A%84%E7%9B%AE%E5%BD%95%E5%A4%A7%E5%B0%8F%E4%B8%8Edf%E6%9F%A5%E7%9C%8B%E7%9A%84%E5%A4%A7%E5%B0%8F%E4%B8%8D%E5%90%8C%E7%9A%84%E6%97%B6%E5%80%99%E7%94%A8lsof%E6%9F%A5%E6%89%BE/</url>
    
    <content type="html"><![CDATA[<p>首先MAN一下两个命令,看一下解释的区别:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">du</span> - estimate file space usage<br><span class="hljs-built_in">df</span> - report file system disk space usage<br></code></pre></td></tr></table></figure><p>du估计文件空间占用情况,df报告文件系统磁盘空间使用情况。两个命令所要获取的对象时不同的。在linux下,几乎所有东西都可以称为文件,常规文件,二进制文件,socket流。所以,通常情况下,使用du得到到的根路径文件空间占用情况应当与df得到的文件系统磁盘占用的情况是一致的或者说相差无几。但是在某些情况下却会出现du得到的空间占用情况要远小于df得到的。</p><p>后通过查询,发现问题出现在deleted文件上。</p><p>root下 lsof | grep deleted会发现多多少少有些文件状态出于deleted状态。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@eb152 ~]<span class="hljs-comment"># lsof |grep deleted</span><br>进程名   PID       USER    FD   文件类型        大小(字节)  索引节点  文件名  [(状态)]<br>java       3311    AuSP    8w      REG              253,0         778   10879268 ***.<span class="hljs-built_in">log</span> (deleted)<br>java       3311    AuSP    9w      REG              253,0        3275   10879269 ***.<span class="hljs-built_in">log</span> (deleted)<br>java       3311    AuSP   10w      REG              253,0           0   10879815 ***.<span class="hljs-built_in">log</span> (deleted)<br>java       3311    AuSP   11w      REG              253,0           0   10879816 ***.<span class="hljs-built_in">log</span> (deleted)<br>oracle     4195  oracle    7u      REG              253,0           0   14254321 *** (deleted)<br></code></pre></td></tr></table></figure><p>状态为deleted为标记被删除,其实该文件并没有从磁盘中删除,类似windows下的回收站状态。</p><p>据称当有其他进程打开某文件时文件被删除,就会将该文件标记为deleted,并删除其目录节点。使用du查看时,因为没有该删除状态文件的节点信息,所以就不做统计,从而导致与df的结果不一致。</p><p>若要将deleted状态文件删除,则根据pid直接kill调相应进程即可。</p><p>找回被删除文件;</p><p>根据以上分析,若删除的文件仍有进程进行操作是有找回数据的可能的。</p><p>若删除的文件为系统日志中的secure文件,则可以根据以下方法找回</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># lsof |grep /var/log/secure </span><br>syslogd 2574 root 2w REG 253,0 1099 525125 /var/log/secure (deleted)<br></code></pre></td></tr></table></figure><p>可以看到进程号为2574的syslogd进程仍然打开了 &#x2F;var&#x2F;log&#x2F;secure这个文件,这个文件所显示状态是deleted,已经被删除。现在FD文件描述符是2w。它的意思是文件描述符是2,状态是w写。</p><p>我们可以根据这个在&#x2F;proc&#x2F;2574&#x2F;fd&#x2F;2找到被删除的&#x2F;var&#x2F;log&#x2F;secure的内容。</p><p>当文件被误删时,切忌reboot操作。</p><h2 id="更新历史"><a href="#更新历史" class="headerlink" title="更新历史"></a>更新历史</h2><table><thead><tr><th>why</th><th>when</th></tr></thead><tbody><tr><td>创建</td><td>2013年03月23日</td></tr><tr><td>更新</td><td>2019年12月9日</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>linux修改网卡的mac地址</title>
    <link href="/2015/03/22/linux%E4%BF%AE%E6%94%B9%E7%BD%91%E5%8D%A1%E7%9A%84mac%E5%9C%B0%E5%9D%80/"/>
    <url>/2015/03/22/linux%E4%BF%AE%E6%94%B9%E7%BD%91%E5%8D%A1%E7%9A%84mac%E5%9C%B0%E5%9D%80/</url>
    
    <content type="html"><![CDATA[<p>linux在安装一些软件的时候可能会用到修改主机的mac地址的问题,在网卡配置文件 &#x2F;etc&#x2F;network&#x2F;interface 中添加mac地址的方式我在修改重启机器后没有生效,所以采用其他方式</p><p>在这里推荐使用脚本修改,将脚本写在开机启动配置文件中,这样开机后就能生成你需要的mac地址<br>在&#x2F;etc&#x2F;rc.local 中添加</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">ifconfig eth0 down<br>ifconfig eth0 hw ether  00:e0:34:5b:b4:i8<br>ifconfig eth0 up<br></code></pre></td></tr></table></figure><p>这样修改后重启机器的mac地址就是你设置的地址了<br>这个时候会出现连不上外网的问题,使用命令route查看你会发现default默认网关路由项不见了,所以这个时候需要添加默认路由</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">route add default gw 192.168.1.1<br></code></pre></td></tr></table></figure><p>将这句加在&#x2F;etc&#x2F;rc.local 中的 ifconfig eth0 up 后面让他开机启动,这样就完成了机器网卡mac地址的修改了</p><h2 id="更新历史"><a href="#更新历史" class="headerlink" title="更新历史"></a>更新历史</h2><table><thead><tr><th>why</th><th>when</th></tr></thead><tbody><tr><td>创建</td><td>2012年1月14日</td></tr><tr><td>更新</td><td>2019年12月9日</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Microsoft Visual C++ 2005 SP1无法安装</title>
    <link href="/2015/03/22/Microsoft%20Visual%20C++%202005%20SP1%E6%97%A0%E6%B3%95%E5%AE%89%E8%A3%85/"/>
    <url>/2015/03/22/Microsoft%20Visual%20C++%202005%20SP1%E6%97%A0%E6%B3%95%E5%AE%89%E8%A3%85/</url>
    
    <content type="html"><![CDATA[<p>安装时出现需要Microsoft Visual C++ 2005 Redistributble对话框,</p><p>里面说Command line option syntax error . Type Command&#x2F;?for Help</p><p>先解压 先脱一层,得到 VCREDI~3.EXE 这么个exe文件,执行安装。</p><p>使用WINRAR打开这个exe文件,得到vcredis1.cab 和 vcredist.msi 这两个安装程序,现在安装起来没有问题了</p><h2 id="更新历史"><a href="#更新历史" class="headerlink" title="更新历史"></a>更新历史</h2><table><thead><tr><th>why</th><th>when</th></tr></thead><tbody><tr><td>创建</td><td>2012年01月13日</td></tr><tr><td>更新</td><td>2019年12月9日</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>libstdc++.so.6错误的解决办法</title>
    <link href="/2015/03/22/libstdc++.so.6%E9%94%99%E8%AF%AF%E7%9A%84%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95/"/>
    <url>/2015/03/22/libstdc++.so.6%E9%94%99%E8%AF%AF%E7%9A%84%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<p>当出现</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ apt-get: symbol lookup error: /usr/lib/x86_64-linux-gnu/libstdc++.so.6: undefined symbol: _ZNSt8messagesIcE2idE, version GLIBCXX_3.4的时候<br></code></pre></td></tr></table></figure><p>需要重新配置下一个包</p><p>Fixed by manually downloading testing version of libc6 from here<br><a href="http://packages.debian.org/wheezy/libc6">http://packages.debian.org/wheezy/libc6</a> and installed it</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ dpkg -i --auto-deconfigure libc6_2.13-16_amd64.deb<br></code></pre></td></tr></table></figure><p>重新配置下libc6_2.13-16_amd64.deb就可以了</p><pre><code class="bash">strings /usr/lib/libstdc++.so.6| grep GLIBC</code></pre><p>查看版本,然后安装 libstdc++6 即可</p><h2 id="更新历史"><a href="#更新历史" class="headerlink" title="更新历史"></a>更新历史</h2><table><thead><tr><th>why</th><th>when</th></tr></thead><tbody><tr><td>创建</td><td>2011年12月31日</td></tr><tr><td>更新</td><td>2019年12月9日</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>error while loading shared libraries解決方法</title>
    <link href="/2015/03/22/error%20while%20loading%20shared%20libraries%E8%A7%A3%E6%B1%BA%E6%96%B9%E6%B3%95/"/>
    <url>/2015/03/22/error%20while%20loading%20shared%20libraries%E8%A7%A3%E6%B1%BA%E6%96%B9%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<p>在linux下运行程序时,发现了error while loading shared libraries这种错误,一时间不知道解决办法,在网上搜索,终于解决了.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">error <span class="hljs-keyword">while</span> loading shared libraries: xxx.so.0:cannot open shared object file: No such file or directory<br></code></pre></td></tr></table></figure><p>出现这类错误表示,系统不知道xxx.so放在哪个目录下,这时候就要在&#x2F;etc&#x2F;ld.so.conf中加入xxx.so所在的目录。<br>一般而言,有很多的so会存放在&#x2F;usr&#x2F;local&#x2F;lib这个目录底下,去这个目录底下找，果然发现自己所需要的.so文件。<br>所以在&#x2F;etc&#x2F;ld.so.conf中加入&#x2F;usr&#x2F;local&#x2F;lib这一行,保存之后,再运行,&#x2F;sbin&#x2F;ldconfig –v更新一下配置即可。<br>centos下安装qt时出现&#x2F;usr&#x2F;lib&#x2F;libstdc++.so.6: version &#96;GLIBCXX_3.4.9’ not found<br>在安装qt-creator的时候运行这个IDE就出现了这个问题,是由于libstdc++.so.6的版本过低,需要下载个新的重新建立软连接</p><p>linux 执行  strings &#x2F;usr&#x2F;lib&#x2F;libstdc++.so.6 | grep GLIBC<br>返回结果没有GLIBCXX_3.4.9</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs bash">GLIBCXX_3.4<br>GLIBCXX_3.4.1<br>GLIBCXX_3.4.2<br>GLIBCXX_3.4.3<br>GLIBCXX_3.4.4<br>GLIBCXX_3.4.5<br>GLIBCXX_3.4.6<br>GLIBCXX_3.4.7<br>GLIBCXX_3.4.8<br>GLIBC_2.0<br>GLIBC_2.3<br>GLIBC_2.4<br>GLIBC_2.3.4<br>GLIBC_2.1<br>GLIBC_2.1.3<br>GLIBC_2.2<br>GLIBCXX_FORCE_NEW<br>GLIBCXX_DEBUG_MESSAGE_LENGTH<br></code></pre></td></tr></table></figure><p>执行:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">ls</span> -l  /usr/lib/libstdc++.so.6<br></code></pre></td></tr></table></figure><p>发现&#x2F;usr&#x2F;lib&#x2F;libstdc++.so.6 -&gt; &#x2F;usr&#x2F;lib&#x2F;libstdc++.so.6.0.8<br>其实这里需要使用libstdc++.so.6.0.10<br>从网上下载这个文件,然后  rm -rf &#x2F;usr&#x2F;lib&#x2F;libstdc++.so.6 -&gt; &#x2F;usr&#x2F;lib&#x2F;libstdc++.so.6.0.8 软链接删除,重新做</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">ln</span> -s /usr/lib/libstdc++.so.6.10  /usr/lib/libstdc++.so.6<br></code></pre></td></tr></table></figure><h2 id="更新历史"><a href="#更新历史" class="headerlink" title="更新历史"></a>更新历史</h2><table><thead><tr><th>why</th><th>when</th></tr></thead><tbody><tr><td>创建</td><td>2011年12月31日</td></tr><tr><td>更新</td><td>2019年12月9日</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>linux下内存释放</title>
    <link href="/2015/03/22/linux%E4%B8%8B%E5%86%85%E5%AD%98%E9%87%8A%E6%94%BE/"/>
    <url>/2015/03/22/linux%E4%B8%8B%E5%86%85%E5%AD%98%E9%87%8A%E6%94%BE/</url>
    
    <content type="html"><![CDATA[<p>细心的朋友会注意到,当你在linux下频繁存取文件后,物理内存会很快被用光,当程序结束后,内存不会被正常释放,而是一直作为caching.这个问题,貌似有不少人在问,不过都没有看到有什么很好解决的办法.那么我来谈谈这个问题.<br>先来说说free命令 </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@server ~]<span class="hljs-comment"># free -m </span><br>             total       used       free     shared    buffers     cached <br>Mem:           249        163         86          0         10         94 <br>-/+ buffers/cache:         58        191 <br>Swap:          511          0        511 <br></code></pre></td></tr></table></figure><p>其中: </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs bash">total 内存总数 <br>used 已经使用的内存数 <br>free 空闲的内存数 <br>shared 多个进程共享的内存总额 <br>buffers Buffer Cache和cached Page Cache 磁盘缓存的大小 <br>-buffers/cache 的内存数:used - buffers - cached <br>+buffers/cache 的内存数:free + buffers + cached <br>可用的memory=free memory+buffers+cached <br>有了这个基础后,可以得知,我现在used为163MB,free为86,buffer和cached分别为10,94 <br>那么我们来看看,如果我执行复制文件,内存会发生什么变化. <br></code></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@server ~]<span class="hljs-comment"># cp -r /etc ~/test/ </span><br>[root@server ~]<span class="hljs-comment"># free -m </span><br>             total       used       free     shared    buffers     cached <br>Mem:           249        244          4          0          8        174 <br>-/+ buffers/cache:         62        187 <br>Swap:          511          0        511 <br></code></pre></td></tr></table></figure><p>在我命令执行结束后,used为244MB,free为4MB,buffers为8MB,cached为174MB,天呐都被cached吃掉了.别紧张,这是为了提高文件读取效率的做法.<br>为了提高磁盘存取效率, Linux做了一些精心的设计, 除了对dentry进行缓存(用于VFS,加速文件路径名到inode的转换), 还采取了两种主要Cache方式:Buffer Cache和Page Cache。前者针对磁盘块的读写,后者针对文件inode的读写。这些Cache有效缩短了 I&#x2F;O系统调用(比如read,write,getdents)的时间<br>那么有人说过段时间,linux会自动释放掉所用的内存,我们使用free再来试试,看看是否有释放 </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@server <span class="hljs-built_in">test</span>]<span class="hljs-comment"># free -m </span><br>             total       used       free     shared    buffers     cached <br>Mem:           249        244          5          0          8        174 <br>-/+ buffers/cache:         61        188 <br>Swap:          511          0        511 <br></code></pre></td></tr></table></figure><p>貌似没有任何变化,那么我能否手动释放掉这些内存呢?回答是可以的!<br>&#x2F;proc是一个虚拟文件系统,我们可以通过对它的读写操作做为与kernel实体间进行通信的一种手段.也就是说可以通过修改&#x2F;proc中的文件,来对当前kernel的行为做出调整.那么我们可以通过调整&#x2F;proc&#x2F;sys&#x2F;vm&#x2F;drop_caches来释放内存.操作如下: </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@server <span class="hljs-built_in">test</span>]<span class="hljs-comment"># cat /proc/sys/vm/drop_caches </span><br>0 <br></code></pre></td></tr></table></figure><p>首先,&#x2F;proc&#x2F;sys&#x2F;vm&#x2F;drop_caches的值,默认为0 </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@server <span class="hljs-built_in">test</span>]<span class="hljs-comment"># sync </span><br></code></pre></td></tr></table></figure><p>手动执行sync命令(描述:sync 命令运行 sync 子例程。如果必须停止系统，则运行 sync 命令以确保文件系统的完整性。sync 命令将所有未写的系统缓冲区写到磁盘中，包含已修改的 i-node、已延迟的块 I&#x2F;O 和读写映射文件) </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@server <span class="hljs-built_in">test</span>]<span class="hljs-comment"># echo 3 &gt; /proc/sys/vm/drop_caches </span><br>[root@server <span class="hljs-built_in">test</span>]<span class="hljs-comment"># cat /proc/sys/vm/drop_caches </span><br>3 <br></code></pre></td></tr></table></figure><p>将&#x2F;proc&#x2F;sys&#x2F;vm&#x2F;drop_caches值设为3 </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@server <span class="hljs-built_in">test</span>]<span class="hljs-comment"># free -m </span><br>             total       used       free     shared    buffers     cached <br>Mem:           249         66        182          0          0         11 <br>-/+ buffers/cache:         55        194 <br>Swap:          511          0        511 <br></code></pre></td></tr></table></figure><p>再来运行free命令,发现现在的used为66MB,free为182MB,buffers为0MB,cached为11MB.那么有效的释放了buffer和cache.<br>有关&#x2F;proc&#x2F;sys&#x2F;vm&#x2F;drop_caches的用法在下面进行了说明 </p><p>&#x2F;proc&#x2F;sys&#x2F;vm&#x2F;drop_caches (since Linux 2.6.16) </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs bash">Writing  to  this  file  causes the kernel to drop clean caches, <br>dentries and inodes from memory, causing that memory  to  become <br>free. <br><br>To  free  pagecache,  use  <span class="hljs-built_in">echo</span> 1 &gt; /proc/sys/vm/drop_caches; to <br>free dentries and inodes, use <span class="hljs-built_in">echo</span> 2 &gt; /proc/sys/vm/drop_caches; <br><br>to   free   pagecache,   dentries  and  inodes,  use  <span class="hljs-built_in">echo</span>  3  &gt; <br>/proc/sys/vm/drop_caches. <br>Because this is a non-destructive operation  and  dirty  objects <br>are not freeable, the user should run <span class="hljs-built_in">sync</span>(8) first. <br></code></pre></td></tr></table></figure><h2 id="更新历史"><a href="#更新历史" class="headerlink" title="更新历史"></a>更新历史</h2><table><thead><tr><th>why</th><th>when</th></tr></thead><tbody><tr><td>创建</td><td>2011年12月31日</td></tr><tr><td>更新</td><td>2019年12月9日</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>ubuntu配置bonding</title>
    <link href="/2015/03/21/ubuntu%E9%85%8D%E7%BD%AEbonding/"/>
    <url>/2015/03/21/ubuntu%E9%85%8D%E7%BD%AEbonding/</url>
    
    <content type="html"><![CDATA[<p>如果节点上有多个网络接口时可以通过bonding将多个网络接口虚拟为一个网络接口，bonding可以提供高可用及负载均衡功能，从而提高节点的网络接口性能及可用性。</p><h2 id="配置单bond"><a href="#配置单bond" class="headerlink" title="配置单bond"></a>配置单bond</h2><h3 id="一、使用如下命令安装"><a href="#一、使用如下命令安装" class="headerlink" title="一、使用如下命令安装"></a>一、使用如下命令安装</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">apt-get install ifenslave-2.6<br></code></pre></td></tr></table></figure><h3 id="二、配置bonding"><a href="#二、配置bonding" class="headerlink" title="二、配置bonding"></a>二、配置bonding</h3><p>修改网络接口配置文件&#x2F;etc&#x2F;network&#x2F;interfaces</p><p>配置一个bond</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs bash">auto lo<br>iface lo inet loopback<br>    <br>auto eth8<br>iface eth8 inet manual<br>bond-master bond0<br>    <br>auto eth7<br>iface eth7 inet manual<br>bond-master bond0<br>    <br>auto eth6<br>iface eth6 inet manual<br>bond-master bond0<br>    <br>auto eth5<br>iface eth5 inet manual<br>bond-master bond0<br>    <br>auto eth0<br>iface eth0 inet static<br>address 192.168.8.102<br>netmask 255.255.0.0<br>gateway 192.168.1.1<br>dns-nameservers 8.8.8.8<br>    <br>auto bond0<br>iface bond0 inet static<br>address 10.0.0.102<br>netmask 255.255.0.0<br>    <br>bond-slaves none<br>bond-miimon 100<br>bond-mode balance-alb<br></code></pre></td></tr></table></figure><h2 id="配置双bond"><a href="#配置双bond" class="headerlink" title="配置双bond"></a>配置双bond</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><code class="hljs bash">auto lo<br>iface lo inet loopback<br>    <br>auto eth2<br>iface eth2 inet manual<br>bond-master bond0<br>    <br>auto eth3<br>iface eth3 inet manual<br>bond-master bond0<br>    <br>auto eth4<br>iface eth4 inet manual<br>bond-master bond0<br>    <br>auto eth0<br>iface eth0 inet static<br>address 10.0.0.103<br>netmask 255.255.0.0<br>    <br>auto bond0<br>iface bond0 inet static<br>address 192.168.9.103<br>netmask 255.255.0.0<br>gateway 192.168.1.1<br>dns-nameservers 8.8.8.8<br>bond-slaves none<br>bond-miimon 100<br>bond-mode balance-alb<br>    <br>auto eth5<br>iface eth5 inet manual<br>bond-master bond1<br>    <br>auto eth6<br>iface eth6 inet manual<br>bond-master bond1<br>    <br>auto eth7<br>iface eth7 inet manual<br>bond-master bond1<br>    <br>auto bond1<br>iface bond1 inet static<br>address 10.1.0.103<br>netmask 255.255.0.0<br>bond-slaves none<br>bond-miimon 100<br>bond-mode balance-alb<br></code></pre></td></tr></table></figure><p>配置过程需要按照顺序配置,配置网卡,然后写bond的配置,两组错开就可以了</p><h2 id="更新历史"><a href="#更新历史" class="headerlink" title="更新历史"></a>更新历史</h2><table><thead><tr><th>why</th><th>when</th></tr></thead><tbody><tr><td>创建</td><td>2011年12月30日</td></tr><tr><td>更新</td><td>2019年12月9日</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>暂未分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>暂未分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>alias重启终端失效的问题</title>
    <link href="/2011/12/22/alias%E9%87%8D%E5%90%AF%E7%BB%88%E7%AB%AF%E5%A4%B1%E6%95%88%E7%9A%84%E9%97%AE%E9%A2%98/"/>
    <url>/2011/12/22/alias%E9%87%8D%E5%90%AF%E7%BB%88%E7%AB%AF%E5%A4%B1%E6%95%88%E7%9A%84%E9%97%AE%E9%A2%98/</url>
    
    <content type="html"><![CDATA[<p>如果使用命令:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">alias</span> xx=<span class="hljs-string">&#x27;xxxx&#x27;</span><br></code></pre></td></tr></table></figure><p>那么登出以后,别名就会失效。下次登入的时候就不能用了</p><p>为了保持别名可以把它写入 &#x2F;root&#x2F;.bashrc</p><p>在.bashrc的最后写入想要的别名,比如:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">alias</span> zp=<span class="hljs-string">&#x27;ll -al&#x27;</span><br></code></pre></td></tr></table></figure><p>然后退出,重新登入的时候就可以用了。<br>或者直接运行命令</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">source</span> ~/.bashrc<br></code></pre></td></tr></table></figure><p>这样不用重新登入就可以使用了</p><h2 id="更新历史"><a href="#更新历史" class="headerlink" title="更新历史"></a>更新历史</h2><table><thead><tr><th>why</th><th>when</th></tr></thead><tbody><tr><td>创建</td><td>2011年12月22日</td></tr><tr><td>更新</td><td>2019年12月8日</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>问题处理</category>
      
    </categories>
    
    
    <tags>
      
      <tag>系统服务</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Debian 64位内核升级步骤</title>
    <link href="/2011/12/16/Debian-64%E4%BD%8D%E5%86%85%E6%A0%B8%E5%8D%87%E7%BA%A7%E6%AD%A5%E9%AA%A4/"/>
    <url>/2011/12/16/Debian-64%E4%BD%8D%E5%86%85%E6%A0%B8%E5%8D%87%E7%BA%A7%E6%AD%A5%E9%AA%A4/</url>
    
    <content type="html"><![CDATA[<h2 id="安装相关依赖包"><a href="#安装相关依赖包" class="headerlink" title="安装相关依赖包"></a>安装相关依赖包</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">apt-get install bzip2 libncurses5-dev kernel-package zlib1g-dev gcc make kernel-package wget module-init-tools procps libncurses5-dev<br></code></pre></td></tr></table></figure><h2 id="下载源码"><a href="#下载源码" class="headerlink" title="下载源码"></a>下载源码</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">cd</span> /usr/src<br>wget http://www.kernel.org/pub/linux/kernel/v2.6/linux-2.6.35.9.tar.bz2<br>tar -xvjf linux-2.6.35.9.tar.bz2<br><span class="hljs-built_in">cp</span> /boot/config-`<span class="hljs-built_in">uname</span> -r` /usr/src/linux-2.6.35.9/.config<br>Then we must configure the new kernel:<br><span class="hljs-built_in">cd</span> linux-2.6.35.9<br>make menuconfig<br></code></pre></td></tr></table></figure><h2 id="裁剪不需要的东西"><a href="#裁剪不需要的东西" class="headerlink" title="裁剪不需要的东西"></a>裁剪不需要的东西</h2><p>我们在内核配置里面至少有几个地方要修改。如下所示:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs bash">&gt;File systems -&gt; Btrfs filesystem (EXPERIMENTAL) Unstable disk format<br>在<span class="hljs-string">&quot;Btrfs filesystem&quot;</span>之前标上&lt;*&gt;<br>Processor <span class="hljs-built_in">type</span> and features -&gt; Processor Family -&gt; <span class="hljs-string">&quot;our processor&quot;</span><br>删除Dell laptop support、Toshiba Laptop support。<br>裁减sound card support。<br>裁减Bluetooth subsystem support。<br>裁减Wireless。<br>裁减Kernel hacking 。<br>多次点击“ESC”退出,保存<br>make-kpkg linux-image linux-headers --initrd<br>编译成功后执行<br></code></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">cd</span> ..<br>dpkg -i linux-image-2.6.29_2.6.35.9-10.00.Custom_i386.deb<br>dpkg -i linux-headers-2.6.29_2.6.35.9-10.00.Custom_i386.deb<br></code></pre></td></tr></table></figure><p>重启,用 uname-a 查看内核版本</p><h2 id="错误处理"><a href="#错误处理" class="headerlink" title="错误处理"></a>错误处理</h2><p>编译完成后生成linux deb 包,先dpkg -i装image,再装headers</p><p>安装完成以后再update-grub 进入&#x2F;boot&#x2F;grub&#x2F;menu.lst 中选择启动的内核项 默认的第一项为0</p><p>在编译完成内核后开机可能出现 Kernel panic - not syncing: VFS: Unable to mount root fs on unknown-block(1,0)的错误 在&#x2F;boot&#x2F;grub&#x2F;menu.lst中可以发现新加的内核可能是没有 initrd   &#x2F;initrd.img-2.6.37.6 未生成的原因不明 可以手动生成<br>执行命令 </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">mkinitramfs -o /boot/initrd.img-2.6.34.10 2.6.34.10 <br></code></pre></td></tr></table></figure><blockquote><p>&#x2F;boot&#x2F;initrd.img-2.6.34.10是这个启动加载文件的生成路径<br>2.6.34.10是 &#x2F;lib&#x2F;modules&#x2F;2.6.34.10</p></blockquote><p>然后update-grub 再启动新内核就没有问题了<br>卸载一个已经安装的内核包<br>debian卸载旧内核要先看看有哪些旧的内核,用命令</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">dpkg --get-selections |grep linux<br></code></pre></td></tr></table></figure><p>如果你的内核是以kernel开头的就把上面的linux改成kernel,之后再用</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">dpkg --purge --force-remove-essential linux-image-XXX<br></code></pre></td></tr></table></figure><h2 id="更新历史"><a href="#更新历史" class="headerlink" title="更新历史"></a>更新历史</h2><table><thead><tr><th>why</th><th>when</th></tr></thead><tbody><tr><td>创建</td><td>2011年12月16日</td></tr><tr><td>更新</td><td>2019年12月7日</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>操作文档</category>
      
    </categories>
    
    
    <tags>
      
      <tag>内核</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>利用移动硬盘安装windows7系统</title>
    <link href="/2011/12/16/%E5%88%A9%E7%94%A8%E7%A7%BB%E5%8A%A8%E7%A1%AC%E7%9B%98%E5%AE%89%E8%A3%85windows7%E7%B3%BB%E7%BB%9F/"/>
    <url>/2011/12/16/%E5%88%A9%E7%94%A8%E7%A7%BB%E5%8A%A8%E7%A1%AC%E7%9B%98%E5%AE%89%E8%A3%85windows7%E7%B3%BB%E7%BB%9F/</url>
    
    <content type="html"><![CDATA[<p>首先把win7系统镜像的iso文件解压到移动硬盘中<br>将移动硬盘设置为活动分区<br>设置活动分区的方法<br>Diskpart程序实现U盘安装WIN7的方法: </p><p>将Win7安装盘中的所有文件拷贝到硬盘文件夹中,我们这里选择D盘win7back。<br>在开始菜单程序栏中输入cmd进入命令行模式，敲入diskpart进入磁盘分区管理程序,逐行输入</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-keyword">select</span> disk 1 (选择磁盘。此为disk 0硬盘,disk 1 U盘情况下,多硬盘时可输入list disk查询) <br>clean (清除该磁盘) <br>create partition primary (创建主分区) <br><span class="hljs-keyword">select</span> partition 1 (选择刚刚创建的1号分区) <br>active (激活该分区) <br>format fs=fat32 (格式化为FAT32，8GB闪盘的格式化需要较长时间) <br>assign (为该分区注册盘符) <br><span class="hljs-built_in">exit</span> (退出diskpart程序) <br></code></pre></td></tr></table></figure><p>注:以上操作必须在Vista&#x2F;Win7下完成，XP虽然也带有diskpart程序,但无法格式化U盘。</p><p>这里我们只使用上面的active选项 或者在win7的磁盘管理中将移动硬盘标记为活动分区<br>假设你的移动硬盘的活动分区的盘符为G盘<br>在win7运行cmd</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">G:<br><span class="hljs-built_in">cd</span> boot<br>bootsect /nt60 G <br></code></pre></td></tr></table></figure><p>命令将引导区写进你的移动硬盘G盘符<br>在开机启动项中选择移动设备启动就可以了</p><h2 id="更新历史"><a href="#更新历史" class="headerlink" title="更新历史"></a>更新历史</h2><table><thead><tr><th>why</th><th>when</th></tr></thead><tbody><tr><td>创建</td><td>2011年12月16日</td></tr><tr><td>更新</td><td>2019年12月9日</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>经验总结</category>
      
    </categories>
    
    
    <tags>
      
      <tag>操作系统</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>单独编译一个ext4内核模块</title>
    <link href="/2011/12/16/%E5%8D%95%E7%8B%AC%E7%BC%96%E8%AF%91%E4%B8%80%E4%B8%AAext4%E5%86%85%E6%A0%B8%E6%A8%A1%E5%9D%97/"/>
    <url>/2011/12/16/%E5%8D%95%E7%8B%AC%E7%BC%96%E8%AF%91%E4%B8%80%E4%B8%AAext4%E5%86%85%E6%A0%B8%E6%A8%A1%E5%9D%97/</url>
    
    <content type="html"><![CDATA[<p>当我们需要使用一个内核模块的时候,在当前使用版本内核编译的时候又没有加进去,在不改变内核版本的时候,再编译整个内核,可能会覆盖原来的内核,导致系统无法启动</p><p>现在我们能够单独选择需要的模块,然后加载进内核</p><h3 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h3><p>首先进入你当前使用的系统查看当前的使用内核版本,在这里是 2.3.39</p><p>下载对应内核源码包 linux-2.6.39.tar.bz2 将源码包放入 &#x2F;usr&#x2F;src&#x2F; 下</p><p>解压源码包,更新编译工具包</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">apt-get install bzip2 libncurses5-dev kernel-package zlib1g-dev gcc make kernel-package wget module-init-tools procps libncurses5-dev<br></code></pre></td></tr></table></figure><p>进入到你需要的模块的文件夹下面</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">cd</span> /usr/src/linux-2.6.39/fs/ext4/<br></code></pre></td></tr></table></figure><p>进入 Makefile 查看模块信息,编译模块  </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ T02-APP205:/usr/src/linux-2.6.39/fs/ext4<span class="hljs-comment"># make CONFIG_EXT4_FS=m -C /lib/modules/`uname -r`/build M=`pwd` modules</span><br></code></pre></td></tr></table></figure><p>可能会报错</p><p>找不到编译的文件路径,做一个软连接</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">ln</span> -s /usr/src/linux-headers-2.6.39/ /lib/modules/2.6.39/build<br></code></pre></td></tr></table></figure><p>再做一次</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ make CONFIG_EXT4_FS=m -C /lib/modules/`<span class="hljs-built_in">uname</span> -r`/build M=`<span class="hljs-built_in">pwd</span>` modules<br></code></pre></td></tr></table></figure><p>就生成了两个模块 ，将这两个模块拷到你的当前使用内核的模块的路径</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ <span class="hljs-built_in">mkdir</span> /lib/modules/2.6.39/kernel/fs/ext4<br>$ T02-APP205:/usr/src/linux-2.6.39/fs/ext4<span class="hljs-comment"># cp ext4.mod.o /lib/modules/2.6.39/kernel/fs/ext4</span><br>$ T02-APP205:/usr/src/linux-2.6.39/fs/ext4<span class="hljs-comment"># cp ext4.ko /lib/modules/2.6.39/kernel/fs/ext4</span><br>$ T02-APP205:/usr/src/linux-2.6.39/fs/ext4<span class="hljs-comment"># depmod</span><br>$ T02-APP205:/usr/src/linux-2.6.39/fs/ext4<span class="hljs-comment"># modprobe ext4</span><br></code></pre></td></tr></table></figure><p>就可以看到 ext4 模块了</p><h2 id="更新历史"><a href="#更新历史" class="headerlink" title="更新历史"></a>更新历史</h2><table><thead><tr><th>why</th><th>when</th></tr></thead><tbody><tr><td>创建</td><td>2011年12月16日</td></tr><tr><td>更新</td><td>2019年12月7日</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>操作文档</category>
      
    </categories>
    
    
    <tags>
      
      <tag>内核</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>CTDB与LVS搭建集群</title>
    <link href="/2011/12/10/CTDB%E4%B8%8ELVS%E6%90%AD%E5%BB%BA%E9%9B%86%E7%BE%A4/"/>
    <url>/2011/12/10/CTDB%E4%B8%8ELVS%E6%90%AD%E5%BB%BA%E9%9B%86%E7%BE%A4/</url>
    
    <content type="html"><![CDATA[<p>搭建一个采用lvs进行负载均衡的CTDB集群，整个集群的架构是采用如图所示</p><p align="center"><img src="/images/blog/ctdb.jpg" alt="ctdb"></p><p>在上图所示的架构图中，后端采用的集群是我们的存储,集群存储的三个samba服务器的node在作为CTDB的节点的同时,也是运行着我们的内核客户端,将三个内核客户端挂载到后端的集群中,同时这三个samba客户端对外采用三个虚拟的IP,然后采用LVS的网络负载均衡技术,虚拟成一个对外的IP,这样就实现了samba的带宽聚合以及负载均衡。</p><!--break--><h2 id="第一步-搭建一个CTDB集群"><a href="#第一步-搭建一个CTDB集群" class="headerlink" title="第一步,搭建一个CTDB集群"></a>第一步,搭建一个CTDB集群</h2><h3 id="一、配置环境"><a href="#一、配置环境" class="headerlink" title="一、配置环境"></a>一、配置环境</h3><ul><li>E5400采用2.6.39内核客户端挂在集群<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs raw">IP:192.168.0.210<br>GW:192.168.1.1<br>NETMASK:255.255.0.0<br>samba node1<br></code></pre></td></tr></table></figure></li><li>E5400采用2.6.39内核客户端挂在集群<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs raw">IP:192.168.0.211<br>GW:192.168.1.1<br>NETMASK：255.255.0.0<br>samba node2 <br></code></pre></td></tr></table></figure></li><li>E5400采用2.6.39内核客户端挂在集群<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs raw">IP:192.168.0.212<br>GW:192.168.1.1<br>NETMASK:255.255.0.0<br>samba node3<br></code></pre></td></tr></table></figure></li></ul><h3 id="二、配置samba-CTDB"><a href="#二、配置samba-CTDB" class="headerlink" title="二、配置samba CTDB"></a>二、配置samba CTDB</h3><p>1、在三个节点上使用如下命令安装samba，CTDB。<br>更新源以后执行</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs raw">T02-APP210:~#apt-get update<br>T02-APP210:~#apt-get install samba ctdb<br></code></pre></td></tr></table></figure><p>2、在三个节点的&#x2F;etc&#x2F;services文件中增加如下内容。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">ctdb 9999/tcp<br></code></pre></td></tr></table></figure><p>3、在三个节点中修改&#x2F;etc&#x2F;samba&#x2F;smb.conf文件为如下内容,其中&#x2F;smbcluster是后端集群存储系统的挂载点,&#x2F;smbcluster&#x2F;public为用户通过samba访问到的目录</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs raw">[global]<br>clustering = yes<br>idmap backend = tdb2<br>private dir=/smbcluster/ctdb<br>fileid:mapping = fsname<br>use mmap = no<br>nt acl support = yes<br>ea support = yes<br>[public]<br>comment = public share<br>path = /smbcluster/public<br>public = yes<br>writeable = yes<br></code></pre></td></tr></table></figure><p>4、在三个节点在修改&#x2F;etc&#x2F;default&#x2F;ctdb文件的如下内容。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs raw">CTDB_RECOVERY_LOCK=&quot;/smbcluster/ctdb/lock&quot;<br>CTDB_PUBLIC_INTERFACE=eth0<br>CTDB_PUBLIC_ADDRESSES=/etc/ctdb/public_addresses<br>CTDB_MANAGES_SAMBA=yes<br>ulimit -n 10000<br>CTDB_NODES=/etc/ctdb/nodes<br>CTDB_LOGFILE=/var/log/ctdb.log<br>CTDB_DEBUGLEVEL=2<br>CTDB_PUBLIC_NETWORK=&quot;192.168.0.0/24&quot;<br>CTDB_PUBLIC_GATEWAY=&quot;192.168.1.1&quot;<br></code></pre></td></tr></table></figure><p>5、在三个节点在修改&#x2F;etc&#x2F;ctdb&#x2F;public_addresses文件中增加如下内容，该文件记录Samba CTDB集群提供给客户访问的虚拟IP地址。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs raw">192.168.0.80/24<br>192.168.0.81/24<br>192.168.0.82/24<br></code></pre></td></tr></table></figure><p>6、在三个节点上修改&#x2F;etc&#x2F;ctdb&#x2F;nodes文件中增加如下内容，该文件记录Samba CTDB集群节点的IP地址。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs raw">192.168.0.210<br>192.168.0.211<br>192.168.0.212<br></code></pre></td></tr></table></figure><p>7、在三个节点上修改&#x2F;etc&#x2F;ctdb&#x2F;events.d&#x2F;11.route文件中增加如下内容。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs raw">#!/bin/sh<br>. /etc/ctdb/functions<br>loadconfig ctdb<br>cmd=&quot;$1&quot;<br>shift<br>case $cmd in<br>takeip)<br># we ignore errors from this, as the route might be up already when we&#x27;re grabbing<br># a 2nd IP on this interface<br>/sbin/ip route add $CTDB_PUBLIC_NETWORK via $CTDB_PUBLIC_GATEWAY dev $1 2&gt; /dev/null<br>;;<br>esac<br>exit 0<br></code></pre></td></tr></table></figure><h2 id="第二步-Lvs的配置及搭建"><a href="#第二步-Lvs的配置及搭建" class="headerlink" title="第二步:Lvs的配置及搭建"></a>第二步:Lvs的配置及搭建</h2><p>LVS是Linux Virtual Server的简写,意即Linux虚拟服务器是一个虚拟的服务器集群系统。本项目在1998年5月由章文嵩博士成立,是中国国内最早出现的自由软件项目之一。目前有三种IP负载均衡技术:VS&#x2F;NAT、VS&#x2F;TUN和VS&#x2F;DR<br>十种调度算法(rrr|wrr|lc|wlc|lblc|lblcr|dh|sh|sed|nq)。<br>官方网站:<a href="www.linuxvirtualserver.org">www.linuxvirtualserver.org</a><br>本次采用的是三个samba服务器节点上做lvs的调度,其中一台既是虚拟主机,又是真实服务器,下面将介绍搭建lvs服务器的步骤</p><p>1.基础环境配置<br>三台samba节点分别配置IP为:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs raw">192.168.0.80    #这三个IP为之前CTDB配置的虚拟IP,publicaddress<br>192.168.0.81<br>192.168.0.82<br></code></pre></td></tr></table></figure><p>2.下载和安装LVS</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">T02-APP210:~#apt-get install ipvsadm<br></code></pre></td></tr></table></figure><p>3.配置虚拟IP服务和负载均衡<br>以192.168.0.11作为LVS的主机,为其设置一个虚拟IP作为samba服务器的接口<br>192.168.0.85为LVS主机的虚拟IP<br>LVS SERVER 配置<br>在192.168.0.11LVS虚拟服务器上执行:<br>设置本级的虚拟IP,作为LVS的虚拟IP</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">ifconfig eth0:0 192.168.0.85 netmask 255.255.0.0 broadcast 192.168.0.85 up<br></code></pre></td></tr></table></figure><p>打开IP转发开关</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs raw">echo 1&gt;/proc/sys/net/ipv4/ip_forward<br></code></pre></td></tr></table></figure><p>添加虚拟服务器,虚拟IP为192.168.0.85,端口为137,调度算法为rr</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs raw">#! /bin/sh<br>ipvsadm -C<br>ipvsadm -A -u 192.168.0.85:137 -s rr<br>ipvsadm -A -u 192.168.0.85:138 -s rr<br>ipvsadm -A -t 192.168.0.85:139 -s rr<br>ipvsadm -A -t 192.168.0.85:445 -s rr<br><br>ipvsadm -a -u 192.168.0.85:137 -r 192.168.0.81:137 -g<br>ipvsadm -a -u 192.168.0.85:137 -r 192.168.0.80:137 -g<br>ipvsadm -a -u 192.168.0.85:137 -r 192.168.0.82:137 -g<br>    <br>ipvsadm -a -u 192.168.0.85:138 -r 192.168.0.82:138 -g<br>ipvsadm -a -u 192.168.0.85:138 -r 192.168.0.80:138 -g<br>ipvsadm -a -u 192.168.0.85:138 -r 192.168.0.81:138 -g<br>    <br>ipvsadm -a -t 192.168.0.85:139 -r 192.168.0.80:139 -g<br>ipvsadm -a -t 192.168.0.85:139 -r 192.168.0.81:139 -g<br>ipvsadm -a -t 192.168.0.85:139 -r 192.168.0.82:139 -g<br>    <br>ipvsadm -a -t 192.168.0.85:445 -r 192.168.0.82:445 -g<br>ipvsadm -a -t 192.168.0.85:445 -r 192.168.0.81:445 -g<br>ipvsadm -a -t 192.168.0.85:445 -r 192.168.0.80:445 -g <br></code></pre></td></tr></table></figure><p>启动lvs</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs raw">ipvsadm<br>IP Virtual Server version 1.2.1 (size=4096) <br>Prot LocalAddress:Port Scheduler Flags <br>  -&gt; RemoteAddress:Port           Forward Weight ActiveConn InActConn <br>TCP  192.168.91.9:http rr <br>  -&gt; 192.168.91.12:http           Route   1      0          0          <br>  -&gt; 192.168.91.11:http           Route   1      0          0<br></code></pre></td></tr></table></figure><p>在192.168.0.11虚拟服务器上运行查询命令 </p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs raw">[root@hadoop00 ~]# ipvsadm -l --stats <br>IP Virtual Server version 1.2.1 (size=4096) <br>Prot LocalAddress:Port               Conns   InPkts  OutPkts  InBytes OutBytes <br>  -&gt; RemoteAddress:Port <br>TCP  192.168.0.80:http                   0        0        0        0        0 <br>  -&gt; 192.168.0.81:http                  0        0        0        0        0 <br>  -&gt; 192.168.0.82:http                  0        0        0        0        0<br></code></pre></td></tr></table></figure><p>在真实服务器上执行下列脚本</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs raw">#! /bin/sh<br>/sbin/ifconfig lo:0 192.168.0.91 broadcast 192.168.0.91 netmask 255.255.255.255 up<br>echo 1 &gt; /proc/sys/net/ipv4/conf/lo/arp_ignore<br>echo 2 &gt; /proc/sys/net/ipv4/conf/lo/arp_announce<br>echo 1 &gt; /proc/sys/net/ipv4/conf/all/arp_ignore<br>echo 2 &gt; /proc/sys/net/ipv4/conf/all/arp_announce<br></code></pre></td></tr></table></figure><p>到这里整个集群就搭建成功，挂载samba服务器即可</p><p>参考文档:</p><p><a href="http://zh.linuxvirtualserver.org/node/5">http://zh.linuxvirtualserver.org/node/5</a><br><a href="http://zp820705.iteye.com/blog/1151921">http://zp820705.iteye.com/blog/1151921</a><br><a href="http://onlyzq.blog.51cto.com/1228/593940/">http://onlyzq.blog.51cto.com/1228/593940/</a></p><h2 id="更新历史"><a href="#更新历史" class="headerlink" title="更新历史"></a>更新历史</h2><table><thead><tr><th>why</th><th>when</th></tr></thead><tbody><tr><td>创建</td><td>2011年12月10日</td></tr><tr><td>更新</td><td>2019年12月9日</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>操作文档</category>
      
    </categories>
    
    
    <tags>
      
      <tag>高可用</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>The New Start</title>
    <link href="/2011/12/10/The-New-Start/"/>
    <url>/2011/12/10/The-New-Start/</url>
    
    <content type="html"><![CDATA[<p>banner_img: images&#x2F;default_back_pic.jpeg</p><h2>为什么写这个博客</h2><p>博客转移了很多地方,从最开始的人人小站,到后来的 CSDN ,再到简书,换来换去直到看到还有个 hexo,这个是搭建在 github 上的一个博客,只不过排版什么的可能需要自己来处理了,这个博客最大的优点就是简洁,可以很方便的找到自己需要了,打算用这个还有个原因是,看到了几个写的很好的技术文章正好来自这个平台,自己也尝试着把好东西都放到这里汇总</p><p>很多东西真的非常喜欢,比如 goagent,比如 google,比如 github,以及一些其他的开源的东西,也许很多人无法理解 share 是个什么感觉</p><p>这个博客是我技术文章开始的地方,也是我和嘎嘎开始减肥的日子,也该活的有生气一些,准备数据化我们的减肥数据,也许这样更能激励自己</p><p>谷歌的十大信条:</p><ul><li>1、以用户为中心,其他一切自然水到渠成</li><li>2、专心将一件事做到极致</li><li>3、越快越好</li><li>4、网络上也讲民主</li><li>5、信息随时随地可得</li><li>6、赚钱不必作恶</li><li>7、信息无极限</li><li>8、信息需求无国界</li><li>9、认真不在着装</li><li>10、追求无止境</li></ul><h2 id="现在"><a href="#现在" class="headerlink" title="现在"></a>现在</h2><p>很多东西已经没有，新的总会去替代旧的</p><p>博客就进行了一次重构，对整体进行一次梳理，博客再次整理</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs vim"><span class="hljs-keyword">vim</span> test<br><span class="hljs-keyword">vim</span> aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>经验总结</category>
      
    </categories>
    
    
    <tags>
      
      <tag>杂七杂八</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
